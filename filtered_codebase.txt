=== BACKEND FILES ===

===== backend\api_tools\advanced_metrics.py =====
"""
Advanced metrics module for NBA player analysis.
Provides both JSON and DataFrame outputs with CSV caching.
"""

import logging
import json
import time
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np
from nba_api.stats.endpoints import playerestimatedmetrics, leaguedashplayerstats, playerawards, playercareerstats
from nba_api.stats.static import players
from ..config import settings
from .utils import retry_on_timeout, format_response, get_player_id_from_name
import os

# Import our RAPTOR metrics implementation
try:
    from .raptor_metrics import get_player_raptor_metrics, generate_skill_grades
    RAPTOR_AVAILABLE = True
except ImportError:
    RAPTOR_AVAILABLE = False
    logging.warning("RAPTOR metrics module not available. Using fallback metrics.")

# Import path utilities
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

# Set up cache directories
ADVANCED_METRICS_CSV_DIR = get_cache_dir("advanced_metrics")
RAPTOR_METRICS_CSV_DIR = get_cache_dir("advanced_metrics/raptor")
SKILL_GRADES_CSV_DIR = get_cache_dir("advanced_metrics/skill_grades")
SIMILAR_PLAYERS_CSV_DIR = get_cache_dir("advanced_metrics/similar_players")

# Legacy cache directory for historical player data
CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_advanced_metrics(
    player_id: int,
    season: str
) -> str:
    """
    Generates a file path for saving player advanced metrics DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format

    Returns:
        Path to the CSV file
    """
    return get_cache_file_path(
        f"player_{player_id}_advanced_metrics_{season}.csv",
        "advanced_metrics"
    )

def _get_csv_path_for_raptor_metrics(
    player_id: int,
    season: str
) -> str:
    """
    Generates a file path for saving player RAPTOR metrics DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format

    Returns:
        Path to the CSV file
    """
    return get_cache_file_path(
        f"player_{player_id}_raptor_metrics_{season}.csv",
        "advanced_metrics/raptor"
    )

def _get_csv_path_for_skill_grades(
    player_id: int,
    season: str
) -> str:
    """
    Generates a file path for saving player skill grades DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format

    Returns:
        Path to the CSV file
    """
    return get_cache_file_path(
        f"player_{player_id}_skill_grades_{season}.csv",
        "advanced_metrics/skill_grades"
    )

def _get_csv_path_for_similar_players(
    player_id: int,
    season: str
) -> str:
    """
    Generates a file path for saving similar players DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format

    Returns:
        Path to the CSV file
    """
    return get_cache_file_path(
        f"player_{player_id}_similar_players_{season}.csv",
        "advanced_metrics/similar_players"
    )

# Award point values for achievements
AWARD_VALUES = {
    "MVP": 50,              # Most Valuable Player
    "Finals MVP": 40,       # Finals MVP
    "Defensive Player of the Year": 30,  # Defensive Player of the Year
    "All-NBA": 20,          # All-NBA Team
    "All-Defensive": 15,    # All-Defensive Team
    "All-Star": 10,         # All-Star
    "All-Rookie": 5,        # All-Rookie Team
    "Rookie of the Year": 15,  # Rookie of the Year
    "Scoring Champion": 20, # Scoring Champion
    "Blocks Champion": 15,  # Blocks Leader
    "Steals Champion": 15,  # Steals Leader
    "Assists Champion": 15, # Assists Leader
    "Rebounds Champion": 15,  # Rebounds Leader
}

logger = logging.getLogger(__name__)

def fetch_player_advanced_analysis_logic(
    player_name: str,
    season: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches advanced metrics, skill grades, and similar players for a specified player.

    Args:
        player_name: The full name of the player to analyze.
        season: The NBA season in format YYYY-YY (e.g., "2023-24").
            If None, uses the current season.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string containing advanced metrics, skill grades, and similar players.
                 Expected structure:
                 {
                     "player_name": str,
                     "player_id": int,
                     "advanced_metrics": {
                         "RAPTOR_OFFENSE": float,
                         "RAPTOR_DEFENSE": float,
                         "RAPTOR_TOTAL": float,
                         "WAR": float,
                         "ELO_RATING": float,
                         // ... other advanced metrics
                     },
                     "skill_grades": {
                         "perimeter_shooting": str, // A+, A, A-, B+, B, etc.
                         "interior_scoring": str,
                         // ... other skill grades
                     },
                     "similar_players": [
                         {"player_id": int, "player_name": str, "similarity_score": float},
                         // ... other similar players
                     ]
                 }
                 Or an {'error': 'Error message'} object if an issue occurs.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    # Store DataFrames if requested
    dataframes = {}

    try:
        # Get player ID
        player_id_result = get_player_id_from_name(player_name)
        if isinstance(player_id_result, dict) and 'error' in player_id_result:
            if return_dataframe:
                return json.dumps(player_id_result), dataframes
            return json.dumps(player_id_result)

        player_id = player_id_result

        # Find the player's official name
        all_players = players.get_players()
        for p in all_players:
            if p['id'] == player_id:
                player_name = p['full_name']  # Use the official name from the API
                break

        # Get current season if not provided
        if not season:
            # In a real implementation, you would determine the current season
            # For now, we'll use a hardcoded value
            season = settings.CURRENT_NBA_SEASON

        # Use RAPTOR metrics if available
        if RAPTOR_AVAILABLE:
            try:
                # Get RAPTOR metrics
                raptor_metrics = get_player_raptor_metrics(player_id, season)

                # Log the RAPTOR metrics for debugging
                logger.info(f"RAPTOR metrics for {player_name}: {raptor_metrics}")

                # Get basic stats for skill grades
                basic_stats = {}

                # Fetch basic stats
                def fetch_basic_stats():
                    return leaguedashplayerstats.LeagueDashPlayerStats(
                        season=season,
                        season_type_all_star='Regular Season',
                        measure_type_detailed_defense='Base',
                        per_mode_detailed='PerGame'
                    )

                basic_stats_data = retry_on_timeout(fetch_basic_stats)
                basic_stats_df = basic_stats_data.get_data_frames()[0]

                # Store basic stats DataFrame if requested
                if return_dataframe:
                    dataframes["basic_stats"] = basic_stats_df

                    # Save to CSV if not empty
                    if not basic_stats_df.empty:
                        csv_path = get_cache_file_path(
                            f"league_basic_stats_{season}.csv",
                            "advanced_metrics"
                        )
                        _save_dataframe_to_csv(basic_stats_df, csv_path)

                if not basic_stats_df.empty:
                    # Filter to find the player in the dataframe
                    player_stats = basic_stats_df[basic_stats_df['PLAYER_ID'] == player_id]
                    if not player_stats.empty:
                        basic_stats = player_stats.iloc[0].to_dict()
                        logger.info(f"Found basic stats for player ID {player_id}")

                        # Store player basic stats DataFrame if requested
                        if return_dataframe:
                            dataframes["player_basic_stats"] = player_stats

                            # Save to CSV
                            csv_path = get_cache_file_path(
                                f"player_{player_id}_basic_stats_{season}.csv",
                                "advanced_metrics"
                            )
                            _save_dataframe_to_csv(player_stats, csv_path)
                    else:
                        logger.warning(f"Player ID {player_id} not found in basic stats dataframe")

                # Generate skill grades
                try:
                    skill_grades = generate_skill_grades(player_id, raptor_metrics, basic_stats)
                    logger.info(f"Skill grades for {player_name}: {skill_grades}")

                    # Store skill grades DataFrame if requested
                    if return_dataframe:
                        # Convert skill grades dict to DataFrame
                        skill_grades_df = pd.DataFrame([skill_grades])
                        skill_grades_df['player_id'] = player_id
                        skill_grades_df['player_name'] = player_name
                        skill_grades_df['season'] = season

                        dataframes["skill_grades"] = skill_grades_df

                        # Save to CSV
                        csv_path = _get_csv_path_for_skill_grades(player_id, season)
                        _save_dataframe_to_csv(skill_grades_df, csv_path)
                except Exception as e:
                    logger.error(f"Error generating skill grades for {player_name}: {str(e)}")
                    # Fallback to default skill grades
                    skill_grades = {
                        "perimeter_shooting": "C",
                        "interior_scoring": "C",
                        "playmaking": "C",
                        "perimeter_defense": "C",
                        "interior_defense": "C",
                        "rebounding": "C",
                        "off_ball_movement": "C",
                        "hustle": "C",
                        "versatility": "C"
                    }

                    # Store default skill grades DataFrame if requested
                    if return_dataframe:
                        # Convert default skill grades dict to DataFrame
                        skill_grades_df = pd.DataFrame([skill_grades])
                        skill_grades_df['player_id'] = player_id
                        skill_grades_df['player_name'] = player_name
                        skill_grades_df['season'] = season
                        skill_grades_df['is_default'] = True

                        dataframes["skill_grades"] = skill_grades_df

                # Find similar players
                try:
                    similar_players = find_similar_players(player_id, raptor_metrics, season)

                    # Store similar players DataFrame if requested
                    if return_dataframe and similar_players:
                        similar_players_df = pd.DataFrame(similar_players)
                        dataframes["similar_players"] = similar_players_df

                        # Save to CSV
                        csv_path = _get_csv_path_for_similar_players(player_id, season)
                        _save_dataframe_to_csv(similar_players_df, csv_path)
                except Exception as e:
                    logger.error(f"Error finding similar players for {player_name}: {str(e)}")
                    # Fallback to empty similar players list
                    similar_players = []

                # Ensure the metrics use the correct keys that the frontend expects
                # Map RAPTOR_OFFENSE to RAPTOR_OFFENSE and RAPTOR_OFF for backward compatibility
                if 'RAPTOR_OFFENSE' in raptor_metrics:
                    raptor_metrics['RAPTOR_OFF'] = raptor_metrics['RAPTOR_OFFENSE']

                # Map RAPTOR_DEFENSE to RAPTOR_DEFENSE and RAPTOR_DEF for backward compatibility
                if 'RAPTOR_DEFENSE' in raptor_metrics:
                    raptor_metrics['RAPTOR_DEF'] = raptor_metrics['RAPTOR_DEFENSE']

                # Map RAPTOR_TOTAL to RAPTOR for backward compatibility
                if 'RAPTOR_TOTAL' in raptor_metrics:
                    raptor_metrics['RAPTOR'] = raptor_metrics['RAPTOR_TOTAL']

                # Map WAR to PLAYER_VALUE for backward compatibility
                if 'WAR' in raptor_metrics:
                    raptor_metrics['PLAYER_VALUE'] = raptor_metrics['WAR']

                # Store RAPTOR metrics DataFrame if requested
                if return_dataframe:
                    # Convert RAPTOR metrics dict to DataFrame
                    raptor_df = pd.DataFrame([raptor_metrics])
                    raptor_df['player_id'] = player_id
                    raptor_df['player_name'] = player_name
                    raptor_df['season'] = season

                    dataframes["raptor_metrics"] = raptor_df

                    # Save to CSV
                    csv_path = _get_csv_path_for_raptor_metrics(player_id, season)
                    _save_dataframe_to_csv(raptor_df, csv_path)

                result = {
                    "player_name": player_name,
                    "player_id": player_id,
                    "advanced_metrics": raptor_metrics,
                    "skill_grades": skill_grades,
                    "similar_players": similar_players
                }

                # Add DataFrame info to the response if requested
                if return_dataframe:
                    csv_paths = {}

                    # Add CSV paths for each DataFrame
                    if "raptor_metrics" in dataframes:
                        csv_paths["raptor_metrics"] = get_relative_cache_path(
                            os.path.basename(_get_csv_path_for_raptor_metrics(player_id, season)),
                            "advanced_metrics/raptor"
                        )

                    if "skill_grades" in dataframes:
                        csv_paths["skill_grades"] = get_relative_cache_path(
                            os.path.basename(_get_csv_path_for_skill_grades(player_id, season)),
                            "advanced_metrics/skill_grades"
                        )

                    if "similar_players" in dataframes:
                        csv_paths["similar_players"] = get_relative_cache_path(
                            os.path.basename(_get_csv_path_for_similar_players(player_id, season)),
                            "advanced_metrics/similar_players"
                        )

                    result["dataframe_info"] = {
                        "message": "Advanced metrics data has been converted to DataFrames and saved as CSV files",
                        "dataframes": {
                            name: {
                                "shape": list(df.shape) if not df.empty else [],
                                "columns": df.columns.tolist() if not df.empty else [],
                                "csv_path": csv_paths.get(name, "")
                            } for name, df in dataframes.items() if name in ["raptor_metrics", "skill_grades", "similar_players"]
                        }
                    }

                    return format_response(result), dataframes

                return format_response(result)

            except Exception as e:
                logger.warning(f"Error using RAPTOR metrics for {player_name}: {str(e)}. Falling back to standard metrics.")
                # Fall back to standard metrics if RAPTOR fails

        # Fallback to standard metrics
        # Fetch player estimated metrics
        advanced_metrics = fetch_player_estimated_metrics(player_id, season)

        # Fetch additional advanced stats
        additional_metrics = fetch_player_advanced_stats(player_id, season)

        # Combine metrics
        combined_metrics = {**advanced_metrics, **additional_metrics}

        # Store combined metrics DataFrame if requested
        if return_dataframe:
            # Convert combined metrics dict to DataFrame
            combined_df = pd.DataFrame([combined_metrics])
            combined_df['player_id'] = player_id
            combined_df['player_name'] = player_name
            combined_df['season'] = season

            dataframes["advanced_metrics"] = combined_df

            # Save to CSV
            csv_path = _get_csv_path_for_advanced_metrics(player_id, season)
            _save_dataframe_to_csv(combined_df, csv_path)

        # Generate skill grades based on the metrics
        skill_grades = generate_skill_grades_legacy(combined_metrics)

        # Store skill grades DataFrame if requested
        if return_dataframe:
            # Convert skill grades dict to DataFrame
            skill_grades_df = pd.DataFrame([skill_grades])
            skill_grades_df['player_id'] = player_id
            skill_grades_df['player_name'] = player_name
            skill_grades_df['season'] = season

            dataframes["skill_grades"] = skill_grades_df

            # Save to CSV
            csv_path = _get_csv_path_for_skill_grades(player_id, season)
            _save_dataframe_to_csv(skill_grades_df, csv_path)

        # Find similar players
        similar_players = find_similar_players(player_id, combined_metrics, season)

        # Store similar players DataFrame if requested
        if return_dataframe and similar_players:
            similar_players_df = pd.DataFrame(similar_players)
            dataframes["similar_players"] = similar_players_df

            # Save to CSV
            csv_path = _get_csv_path_for_similar_players(player_id, season)
            _save_dataframe_to_csv(similar_players_df, csv_path)

        result = {
            "player_name": player_name,
            "player_id": player_id,
            "advanced_metrics": combined_metrics,
            "skill_grades": skill_grades,
            "similar_players": similar_players
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_paths = {}

            # Add CSV paths for each DataFrame
            if "advanced_metrics" in dataframes:
                csv_paths["advanced_metrics"] = get_relative_cache_path(
                    os.path.basename(_get_csv_path_for_advanced_metrics(player_id, season)),
                    "advanced_metrics"
                )

            if "skill_grades" in dataframes:
                csv_paths["skill_grades"] = get_relative_cache_path(
                    os.path.basename(_get_csv_path_for_skill_grades(player_id, season)),
                    "advanced_metrics/skill_grades"
                )

            if "similar_players" in dataframes:
                csv_paths["similar_players"] = get_relative_cache_path(
                    os.path.basename(_get_csv_path_for_similar_players(player_id, season)),
                    "advanced_metrics/similar_players"
                )

            result["dataframe_info"] = {
                "message": "Advanced metrics data has been converted to DataFrames and saved as CSV files",
                "dataframes": {
                    name: {
                        "shape": list(df.shape) if not df.empty else [],
                        "columns": df.columns.tolist() if not df.empty else [],
                        "csv_path": csv_paths.get(name, "")
                    } for name, df in dataframes.items() if name in ["advanced_metrics", "skill_grades", "similar_players"]
                }
            }

            return format_response(result), dataframes

        return format_response(result)

    except Exception as e:
        logger.error(f"Error in fetch_player_advanced_analysis_logic for {player_name}: {str(e)}", exc_info=True)
        error_response = {"error": f"Failed to fetch advanced metrics for {player_name}: {str(e)}"}
        if return_dataframe:
            return format_response(error_response), dataframes
        return format_response(error_response)

def fetch_player_estimated_metrics(player_id: int, season: str) -> Dict[str, float]:
    """Fetch player estimated metrics from the NBA API."""
    try:
        def fetch_metrics():
            return playerestimatedmetrics.PlayerEstimatedMetrics(
                league_id='00',
                season=season,
                season_type='Regular Season'
            )

        metrics_data = retry_on_timeout(fetch_metrics)
        metrics_df = metrics_data.get_data_frames()[0]

        # Find the player in the dataframe
        player_metrics = metrics_df[metrics_df['PLAYER_ID'] == player_id]

        # Debug log to check if player is found
        if player_metrics.empty:
            logger.warning(f"Player ID {player_id} not found in playerestimatedmetrics data")
        else:
            logger.info(f"Found player ID {player_id} in playerestimatedmetrics data")

        if player_metrics.empty:
            logger.warning(f"No estimated metrics found for player ID {player_id} in season {season}")
            return {}

        # Extract the metrics we want
        metrics = {
            "E_OFF_RATING": float(player_metrics['E_OFF_RATING'].iloc[0]),
            "E_DEF_RATING": float(player_metrics['E_DEF_RATING'].iloc[0]),
            "E_NET_RATING": float(player_metrics['E_NET_RATING'].iloc[0]),
            "E_AST_RATIO": float(player_metrics['E_AST_RATIO'].iloc[0]),
            "E_OREB_PCT": float(player_metrics['E_OREB_PCT'].iloc[0]),
            "E_DREB_PCT": float(player_metrics['E_DREB_PCT'].iloc[0]),
            "E_REB_PCT": float(player_metrics['E_REB_PCT'].iloc[0]),
            "E_TOV_PCT": float(player_metrics['E_TOV_PCT'].iloc[0]),
            "E_USG_PCT": float(player_metrics['E_USG_PCT'].iloc[0]),
            "E_PACE": float(player_metrics['E_PACE'].iloc[0]),
        }

        # Map to our standardized metric names
        return {
            "ORTG": metrics["E_OFF_RATING"],
            "DRTG": metrics["E_DEF_RATING"],
            "NETRTG": metrics["E_NET_RATING"],
            "AST_PCT": metrics["E_AST_RATIO"],
            "OREB_PCT": metrics["E_OREB_PCT"],
            "DREB_PCT": metrics["E_DREB_PCT"],
            "REB_PCT": metrics["E_REB_PCT"],
            "TOV_PCT": metrics["E_TOV_PCT"],
            "USG_PCT": metrics["E_USG_PCT"],
            "PACE": metrics["E_PACE"],
        }

    except Exception as e:
        logger.error(f"Error fetching estimated metrics for player ID {player_id}: {str(e)}", exc_info=True)
        return {}

def fetch_player_advanced_stats(player_id: int, season: str) -> Dict[str, float]:
    """Fetch advanced stats from the NBA API."""
    try:
        def fetch_advanced_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Advanced',
                per_mode_detailed='PerGame',
                last_n_games=0,
                month=0,
                opponent_team_id=0,
                period=0,
                pace_adjust='N',
                plus_minus='N',
                rank='N'
            )

        advanced_data = retry_on_timeout(fetch_advanced_stats)
        advanced_df = advanced_data.get_data_frames()[0]

        # Find the player in the dataframe
        player_advanced = advanced_df[advanced_df['PLAYER_ID'] == player_id]

        # Debug log to check if player is found
        if player_advanced.empty:
            logger.warning(f"Player ID {player_id} not found in advanced stats data")
        else:
            logger.info(f"Found player ID {player_id} in advanced stats data")

        if advanced_df.empty:
            logger.warning(f"No advanced stats found for player ID {player_id} in season {season}")
            return {}

        # Extract the metrics we want
        # Note: The actual column names may vary, this is an example
        # You'll need to check the actual column names in the dataframe
        metrics = {}

        # Map common advanced metrics if they exist in the dataframe
        metric_mappings = {
            "PER": "PER",
            "TS_PCT": "TS_PCT",
            "USG_PCT": "USG_PCT",
            "BPM": "BPM",
            "VORP": "VORP",
            "WS": "WS",
            "WS_PER_48": "WS_PER_48",
            "PACE": "PACE",
            "PIE": "PIE",
            "POSS": "POSS",
        }

        # Use player-specific data if available
        if not player_advanced.empty:
            for api_name, our_name in metric_mappings.items():
                if api_name in player_advanced.columns:
                    metrics[our_name] = float(player_advanced[api_name].iloc[0])
        else:
            # Fallback to first row if player not found (should be fixed)
            logger.warning(f"Using fallback method for advanced stats for player ID {player_id}")
            for api_name, our_name in metric_mappings.items():
                if api_name in advanced_df.columns:
                    metrics[our_name] = float(advanced_df[api_name].iloc[0])

        # Calculate our own advanced metrics based on real NBA data

        # NBA PLUS - Our own player rating system (similar to 538's RAPTOR but using available NBA stats)
        # Base calculation using Player Impact Estimate (PIE) and other available metrics

        # Offensive Rating (0-100 scale)
        off_rating = 0
        if "ORTG" in metrics:
            # Scale ORTG to a 0-100 rating (league average ORTG is typically around 110-115)
            off_rating = min(100, max(0, (metrics["ORTG"] - 85) * 100 / 40))
        elif "PIE" in metrics:
            # Fallback to PIE for offensive contribution
            off_rating = min(100, max(0, metrics.get("PIE", 0) * 100 * 2))

        # Defensive Rating (0-100 scale)
        def_rating = 0
        if "DRTG" in metrics:
            # Scale DRTG to a 0-100 rating (lower DRTG is better, typical range 100-120)
            def_rating = min(100, max(0, (130 - metrics["DRTG"]) * 100 / 40))
        elif "PIE" in metrics:
            # Fallback to PIE for defensive contribution
            def_rating = min(100, max(0, metrics.get("PIE", 0) * 100 * 1.5))

        # Overall Rating (0-100 scale)
        overall_rating = (off_rating * 0.6) + (def_rating * 0.4)

        # Convert to a +/- scale similar to other advanced metrics
        metrics["NBA_PLUS"] = round((overall_rating - 50) / 5, 1)  # -10 to +10 scale
        metrics["NBA_PLUS_OFF"] = round((off_rating - 50) / 5, 1)
        metrics["NBA_PLUS_DEF"] = round((def_rating - 50) / 5, 1)

        # ELO Rating (1000-2000 scale, similar to 538's player ratings)
        # This incorporates historical data and current season performance
        try:
            # Get historical data for the player
            historical_data = get_historical_player_data(player_id)

            # Base ELO starts at 1500 (average player)
            base_elo = 1500

            # Current season performance (from PIE or overall rating)
            current_season_bonus = 0
            if "PIE" in metrics:
                # PIE typically ranges from 0 to 0.2 for most players
                current_season_bonus = metrics["PIE"] * 1000  # Reduced weight from 2000 to 1000
            else:
                current_season_bonus = (overall_rating - 50) * 5  # Reduced weight from 10 to 5

            # Historical performance factors
            historical_bonus = 0

            if historical_data:
                # Career longevity bonus (max +100 for 15+ years)
                years_played = min(15, historical_data.get("years_played", 0))
                longevity_bonus = years_played * 6.67  # Up to +100 for 15 years

                # Career achievements bonus
                achievements_bonus = historical_data.get("achievements_value", 0)  # Up to +200

                # Career statistical excellence
                career_stats_bonus = 0

                # Career PER bonus (15 is average, 25+ is excellent)
                career_per = historical_data.get("career_per", 0)
                if career_per > 15:
                    career_per_bonus = min(100, (career_per - 15) * 10)  # Up to +100
                    career_stats_bonus += career_per_bonus

                # Career WS bonus
                career_ws = historical_data.get("career_ws", 0)
                career_ws_bonus = min(100, career_ws * 1.5)  # Up to +100
                career_stats_bonus += career_ws_bonus

                # Career VORP bonus
                career_vorp = historical_data.get("career_vorp", 0)
                career_vorp_bonus = min(100, career_vorp * 5)  # Up to +100
                career_stats_bonus += career_vorp_bonus

                # Average the statistical bonuses
                career_stats_bonus = career_stats_bonus / 3

                # Combine all historical factors
                historical_bonus = longevity_bonus + achievements_bonus + career_stats_bonus

                # Scale historical bonus (max +300)
                historical_bonus = min(300, historical_bonus)

            # Calculate final ELO rating
            # 60% current season, 40% historical performance
            metrics["ELO_RATING"] = round(base_elo + (current_season_bonus * 0.6) + (historical_bonus * 0.4), 0)

            # Add historical components for reference
            metrics["ELO_CURRENT"] = round(base_elo + current_season_bonus, 0)
            metrics["ELO_HISTORICAL"] = round(base_elo + historical_bonus, 0)

        except Exception as e:
            logger.error(f"Error calculating ELO rating: {str(e)}")
            # Fallback to simple calculation
            if "PIE" in metrics:
                metrics["ELO_RATING"] = round(base_elo + metrics["PIE"] * 2000, 0)
            else:
                metrics["ELO_RATING"] = round(base_elo + (overall_rating - 50) * 10, 0)

        # Player Value (similar to WAR/VORP concepts)
        if "VORP" in metrics:
            metrics["PLAYER_VALUE"] = round(metrics["VORP"], 1)
        elif "WS" in metrics:
            metrics["PLAYER_VALUE"] = round(metrics["WS"] / 2.5, 1)
        else:
            metrics["PLAYER_VALUE"] = round(metrics.get("PIE", 0) * 10, 1)

        return metrics

    except Exception as e:
        logger.error(f"Error fetching advanced stats for player ID {player_id}: {str(e)}", exc_info=True)
        return {}

def generate_skill_grades_legacy(metrics: Dict[str, float]) -> Dict[str, str]:
    """
    Legacy function to generate skill grades based on actual league-wide statistics and percentiles.
    Uses real NBA data to calculate percentile rankings for each skill.

    Note: This is a fallback method used when the RAPTOR metrics module is not available.
    """
    try:
        # Get league-wide stats for percentile calculations
        league_stats = get_league_stats_for_percentiles()

        # Define grade thresholds based on percentiles
        grade_thresholds = {
            'A+': 0.95,  # 95th percentile and above
            'A': 0.90,   # 90th percentile and above
            'A-': 0.85,  # 85th percentile and above
            'B+': 0.80,  # 80th percentile and above
            'B': 0.70,   # 70th percentile and above
            'B-': 0.60,  # 60th percentile and above
            'C+': 0.55,  # 55th percentile and above
            'C': 0.45,   # 45th percentile and above
            'C-': 0.35,  # 35th percentile and above
            'D+': 0.30,  # 30th percentile and above
            'D': 0.20,   # 20th percentile and above
            'D-': 0.10,  # 10th percentile and above
            'F': 0.0     # Below 10th percentile
        }

        # Initialize grades dictionary
        grades = {}

        # Calculate percentile for each skill
        skill_percentiles = {}

        # 1. Perimeter Shooting
        shooting_metrics = []
        if "TS_PCT" in metrics and "TS_PCT" in league_stats:
            ts_pct_percentile = calculate_percentile(metrics["TS_PCT"], league_stats["TS_PCT"])
            shooting_metrics.append((ts_pct_percentile, 0.5))  # 50% weight

        if "FG3_PCT" in metrics and "FG3_PCT" in league_stats:
            fg3_pct_percentile = calculate_percentile(metrics["FG3_PCT"], league_stats["FG3_PCT"])
            shooting_metrics.append((fg3_pct_percentile, 0.5))  # 50% weight

        if shooting_metrics:
            skill_percentiles["perimeter_shooting"] = sum(pct * weight for pct, weight in shooting_metrics) / sum(weight for _, weight in shooting_metrics)
        else:
            skill_percentiles["perimeter_shooting"] = 0.0

        # 2. Interior Scoring
        interior_metrics = []
        if "FG_PCT" in metrics and "FG_PCT" in league_stats:
            fg_pct_percentile = calculate_percentile(metrics["FG_PCT"], league_stats["FG_PCT"])
            interior_metrics.append((fg_pct_percentile, 0.4))  # 40% weight

        if "FG2_PCT" in metrics and "FG2_PCT" in league_stats:
            fg2_pct_percentile = calculate_percentile(metrics["FG2_PCT"], league_stats["FG2_PCT"])
            interior_metrics.append((fg2_pct_percentile, 0.6))  # 60% weight

        if interior_metrics:
            skill_percentiles["interior_scoring"] = sum(pct * weight for pct, weight in interior_metrics) / sum(weight for _, weight in interior_metrics)
        else:
            skill_percentiles["interior_scoring"] = 0.0

        # 3. Playmaking
        playmaking_metrics = []
        if "AST" in metrics and "AST" in league_stats:
            ast_percentile = calculate_percentile(metrics["AST"], league_stats["AST"])
            playmaking_metrics.append((ast_percentile, 0.4))  # 40% weight

        if "AST_PCT" in metrics and "AST_PCT" in league_stats:
            ast_pct_percentile = calculate_percentile(metrics["AST_PCT"], league_stats["AST_PCT"])
            playmaking_metrics.append((ast_pct_percentile, 0.4))  # 40% weight

        if "AST_TO" in metrics and "AST_TO" in league_stats:
            ast_to_percentile = calculate_percentile(metrics["AST_TO"], league_stats["AST_TO"])
            playmaking_metrics.append((ast_to_percentile, 0.2))  # 20% weight

        if playmaking_metrics:
            skill_percentiles["playmaking"] = sum(pct * weight for pct, weight in playmaking_metrics) / sum(weight for _, weight in playmaking_metrics)
        else:
            skill_percentiles["playmaking"] = 0.0

        # 4. Perimeter Defense
        perimeter_def_metrics = []
        if "STL" in metrics and "STL" in league_stats:
            stl_percentile = calculate_percentile(metrics["STL"], league_stats["STL"])
            perimeter_def_metrics.append((stl_percentile, 0.5))  # 50% weight

        if "DRTG" in metrics and "DRTG" in league_stats:
            # For DRTG, lower is better, so we invert the percentile
            drtg_percentile = 1.0 - calculate_percentile(metrics["DRTG"], league_stats["DRTG"])
            perimeter_def_metrics.append((drtg_percentile, 0.5))  # 50% weight

        if perimeter_def_metrics:
            skill_percentiles["perimeter_defense"] = sum(pct * weight for pct, weight in perimeter_def_metrics) / sum(weight for _, weight in perimeter_def_metrics)
        else:
            skill_percentiles["perimeter_defense"] = 0.0

        # 5. Interior Defense
        interior_def_metrics = []
        if "BLK" in metrics and "BLK" in league_stats:
            blk_percentile = calculate_percentile(metrics["BLK"], league_stats["BLK"])
            interior_def_metrics.append((blk_percentile, 0.5))  # 50% weight

        if "DRTG" in metrics and "DRTG" in league_stats:
            # For DRTG, lower is better, so we invert the percentile
            drtg_percentile = 1.0 - calculate_percentile(metrics["DRTG"], league_stats["DRTG"])
            interior_def_metrics.append((drtg_percentile, 0.5))  # 50% weight

        if interior_def_metrics:
            skill_percentiles["interior_defense"] = sum(pct * weight for pct, weight in interior_def_metrics) / sum(weight for _, weight in interior_def_metrics)
        else:
            skill_percentiles["interior_defense"] = 0.0

        # 6. Rebounding
        rebounding_metrics = []
        if "REB" in metrics and "REB" in league_stats:
            reb_percentile = calculate_percentile(metrics["REB"], league_stats["REB"])
            rebounding_metrics.append((reb_percentile, 0.3))  # 30% weight

        if "REB_PCT" in metrics and "REB_PCT" in league_stats:
            reb_pct_percentile = calculate_percentile(metrics["REB_PCT"], league_stats["REB_PCT"])
            rebounding_metrics.append((reb_pct_percentile, 0.3))  # 30% weight

        if "OREB_PCT" in metrics and "OREB_PCT" in league_stats:
            oreb_pct_percentile = calculate_percentile(metrics["OREB_PCT"], league_stats["OREB_PCT"])
            rebounding_metrics.append((oreb_pct_percentile, 0.2))  # 20% weight

        if "DREB_PCT" in metrics and "DREB_PCT" in league_stats:
            dreb_pct_percentile = calculate_percentile(metrics["DREB_PCT"], league_stats["DREB_PCT"])
            rebounding_metrics.append((dreb_pct_percentile, 0.2))  # 20% weight

        if rebounding_metrics:
            skill_percentiles["rebounding"] = sum(pct * weight for pct, weight in rebounding_metrics) / sum(weight for _, weight in rebounding_metrics)
        else:
            skill_percentiles["rebounding"] = 0.0

        # 7. Off-Ball Movement (harder to measure directly)
        # We'll use a combination of shooting efficiency and offensive metrics
        offball_metrics = []
        if "TS_PCT" in metrics and "TS_PCT" in league_stats:
            ts_pct_percentile = calculate_percentile(metrics["TS_PCT"], league_stats["TS_PCT"])
            offball_metrics.append((ts_pct_percentile, 0.5))  # 50% weight

        if "ORTG" in metrics and "ORTG" in league_stats:
            ortg_percentile = calculate_percentile(metrics["ORTG"], league_stats["ORTG"])
            offball_metrics.append((ortg_percentile, 0.5))  # 50% weight

        if offball_metrics:
            skill_percentiles["off_ball_movement"] = sum(pct * weight for pct, weight in offball_metrics) / sum(weight for _, weight in offball_metrics)
        else:
            skill_percentiles["off_ball_movement"] = 0.0

        # 8. Hustle (combination of steals, blocks, offensive rebounds)
        hustle_metrics = []
        if "STL" in metrics and "STL" in league_stats:
            stl_percentile = calculate_percentile(metrics["STL"], league_stats["STL"])
            hustle_metrics.append((stl_percentile, 0.3))  # 30% weight

        if "BLK" in metrics and "BLK" in league_stats:
            blk_percentile = calculate_percentile(metrics["BLK"], league_stats["BLK"])
            hustle_metrics.append((blk_percentile, 0.3))  # 30% weight

        if "OREB_PCT" in metrics and "OREB_PCT" in league_stats:
            oreb_pct_percentile = calculate_percentile(metrics["OREB_PCT"], league_stats["OREB_PCT"])
            hustle_metrics.append((oreb_pct_percentile, 0.4))  # 40% weight

        if hustle_metrics:
            skill_percentiles["hustle"] = sum(pct * weight for pct, weight in hustle_metrics) / sum(weight for _, weight in hustle_metrics)
        else:
            skill_percentiles["hustle"] = 0.0

        # 9. Versatility (based on all-around contributions)
        versatility_metrics = []
        if "PTS" in metrics and "PTS" in league_stats:
            pts_percentile = calculate_percentile(metrics["PTS"], league_stats["PTS"])
            versatility_metrics.append((pts_percentile, 0.2))  # 20% weight

        if "AST" in metrics and "AST" in league_stats:
            ast_percentile = calculate_percentile(metrics["AST"], league_stats["AST"])
            versatility_metrics.append((ast_percentile, 0.2))  # 20% weight

        if "REB" in metrics and "REB" in league_stats:
            reb_percentile = calculate_percentile(metrics["REB"], league_stats["REB"])
            versatility_metrics.append((reb_percentile, 0.2))  # 20% weight

        if "STL" in metrics and "STL" in league_stats:
            stl_percentile = calculate_percentile(metrics["STL"], league_stats["STL"])
            versatility_metrics.append((stl_percentile, 0.2))  # 20% weight

        if "BLK" in metrics and "BLK" in league_stats:
            blk_percentile = calculate_percentile(metrics["BLK"], league_stats["BLK"])
            versatility_metrics.append((blk_percentile, 0.2))  # 20% weight

        if versatility_metrics:
            # For versatility, we want to reward balanced contributions
            # Calculate the base percentile
            base_percentile = sum(pct * weight for pct, weight in versatility_metrics) / sum(weight for _, weight in versatility_metrics)

            # Calculate the standard deviation of percentiles (lower = more balanced)
            percentiles = [pct for pct, _ in versatility_metrics]
            std_dev = np.std(percentiles) if len(percentiles) > 1 else 0

            # Apply a balance bonus (higher for more balanced players)
            balance_factor = max(0, 1 - std_dev)
            skill_percentiles["versatility"] = base_percentile * (0.7 + 0.3 * balance_factor)
        else:
            skill_percentiles["versatility"] = 0.0

        # Convert percentiles to letter grades
        for skill, percentile in skill_percentiles.items():
            for grade, threshold in sorted(grade_thresholds.items(), key=lambda x: x[1], reverse=True):
                if percentile >= threshold:
                    grades[skill] = grade
                    break

            # Ensure every skill has a grade
            if skill not in grades:
                grades[skill] = 'F'

        return grades

    except Exception as e:
        logger.error(f"Error generating skill grades: {str(e)}", exc_info=True)
        # Return default grades if there's an error
        return {
            "perimeter_shooting": "C",
            "interior_scoring": "C",
            "playmaking": "C",
            "perimeter_defense": "C",
            "interior_defense": "C",
            "rebounding": "C",
            "off_ball_movement": "C",
            "hustle": "C",
            "versatility": "C"
        }

def calculate_percentile(value: float, distribution: List[float]) -> float:
    """
    Calculate the percentile of a value within a distribution.

    Args:
        value: The value to find the percentile for
        distribution: List of values representing the distribution

    Returns:
        Percentile as a float between 0 and 1
    """
    if not distribution:
        return 0.5  # Default to 50th percentile if no distribution

    # Count how many values in the distribution are less than or equal to the given value
    count = sum(1 for x in distribution if x <= value)

    # Calculate percentile
    percentile = count / len(distribution)

    return percentile

def get_league_stats_for_percentiles() -> Dict[str, List[float]]:
    """
    Get league-wide statistics for calculating percentiles.
    Uses cached data or fetches from the NBA API.

    Returns:
        Dictionary mapping stat names to lists of values across the league
    """
    # Check if we have cached league stats
    cache_file = os.path.join(CACHE_DIR, "league_stats.json")

    # Try to load from cache first
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
                # Check if cache is recent enough (1 day)
                cache_time = os.path.getmtime(cache_file)
                if (time.time() - cache_time) < 86400:  # 1 day in seconds
                    return cached_data
        except Exception as e:
            logger.warning(f"Error reading league stats cache: {str(e)}")

    try:
        # Fetch basic stats
        def fetch_basic_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season="2023-24",  # Current season
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Base',
                per_mode_detailed='PerGame',
                last_n_games=0,
                month=0,
                opponent_team_id=0,
                period=0,
                pace_adjust='N',
                plus_minus='N',
                rank='N'
            )

        basic_stats = retry_on_timeout(fetch_basic_stats)
        basic_df = basic_stats.get_data_frames()[0]

        # Fetch advanced stats
        def fetch_advanced_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season="2023-24",  # Current season
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Advanced',
                per_mode_detailed='PerGame',
                last_n_games=0,
                month=0,
                opponent_team_id=0,
                period=0,
                pace_adjust='N',
                plus_minus='N',
                rank='N'
            )

        advanced_stats = retry_on_timeout(fetch_advanced_stats)
        advanced_df = advanced_stats.get_data_frames()[0]

        # Combine the dataframes on PLAYER_ID
        merged_df = pd.merge(basic_df, advanced_df, on='PLAYER_ID', suffixes=('', '_ADV'))

        # Extract the stats we need for percentiles
        league_stats = {}

        # Basic stats
        basic_stat_columns = [
            'PTS', 'AST', 'REB', 'STL', 'BLK', 'FG_PCT', 'FG3_PCT', 'FT_PCT'
        ]

        for col in basic_stat_columns:
            if col in merged_df.columns:
                league_stats[col] = merged_df[col].dropna().tolist()

        # Advanced stats
        advanced_stat_columns = [
            'TS_PCT', 'USG_PCT', 'AST_PCT', 'REB_PCT', 'OREB_PCT', 'DREB_PCT',
            'PACE', 'PIE', 'ORTG', 'DRTG'
        ]

        for col in advanced_stat_columns:
            if col in merged_df.columns:
                league_stats[col] = merged_df[col].dropna().tolist()

        # Calculate additional metrics
        if 'AST' in merged_df.columns and 'TOV' in merged_df.columns:
            # Assist to turnover ratio
            merged_df['AST_TO'] = merged_df.apply(
                lambda row: row['AST'] / row['TOV'] if row['TOV'] > 0 else row['AST'], axis=1
            )
            league_stats['AST_TO'] = merged_df['AST_TO'].dropna().tolist()

        # Cache the league stats
        try:
            with open(cache_file, 'w') as f:
                json.dump(league_stats, f)
        except Exception as e:
            logger.warning(f"Error caching league stats: {str(e)}")

        return league_stats

    except Exception as e:
        logger.error(f"Error fetching league stats: {str(e)}", exc_info=True)
        # Return empty stats if there's an error
        return {}

def get_historical_player_data(player_id: int) -> Dict[str, Any]:
    """
    Get historical data for a player, including career stats and achievements.
    Uses caching to avoid repeated API calls.

    Args:
        player_id: The NBA API player ID

    Returns:
        Dict with historical player data including:
        - years_played: Number of years in the league
        - achievements_value: Numerical value of career achievements
        - career_per: Career PER (Player Efficiency Rating)
        - career_ws: Career Win Shares
        - career_vorp: Career VORP (Value Over Replacement Player)
    """
    # Check if we have cached data
    cache_file = os.path.join(CACHE_DIR, f"player_{player_id}_history.json")

    # Try to load from cache first
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
                # Check if cache is recent enough (1 week)
                cache_time = os.path.getmtime(cache_file)
                if (time.time() - cache_time) < 604800:  # 7 days in seconds
                    return cached_data
        except Exception as e:
            logger.warning(f"Error reading cache for player {player_id}: {str(e)}")

    try:
        # Fetch career stats
        career_stats = retry_on_timeout(lambda: playercareerstats.PlayerCareerStats(player_id=player_id))
        career_totals_df = career_stats.get_data_frames()[1]  # Career totals

        # Get years played
        season_totals_df = career_stats.get_data_frames()[0]  # Season-by-season
        years_played = len(season_totals_df['SEASON_ID'].unique())

        # Get advanced career stats if available
        career_per = 0
        career_ws = 0
        career_vorp = 0

        if not career_totals_df.empty:
            # Some players might not have these stats
            if 'PER' in career_totals_df.columns:
                career_per = float(career_totals_df['PER'].iloc[0])
            if 'WS' in career_totals_df.columns:
                career_ws = float(career_totals_df['WS'].iloc[0])
            if 'VORP' in career_totals_df.columns:
                career_vorp = float(career_totals_df['VORP'].iloc[0])

        # Fetch awards
        awards = retry_on_timeout(lambda: playerawards.PlayerAwards(player_id=player_id))
        awards_df = awards.get_data_frames()[0]

        # Calculate achievements value
        achievements_value = 0

        if not awards_df.empty:
            for _, award in awards_df.iterrows():
                award_type = award['DESCRIPTION']

                # Check for exact matches
                if award_type in AWARD_VALUES:
                    achievements_value += AWARD_VALUES[award_type]
                else:
                    # Check for partial matches
                    for award_key, award_value in AWARD_VALUES.items():
                        if award_key in award_type:
                            achievements_value += award_value
                            break

        # Cap achievements value
        achievements_value = min(200, achievements_value)

        # Compile the data
        historical_data = {
            "years_played": years_played,
            "achievements_value": achievements_value,
            "career_per": career_per,
            "career_ws": career_ws,
            "career_vorp": career_vorp
        }

        # Cache the data
        try:
            with open(cache_file, 'w') as f:
                json.dump(historical_data, f)
        except Exception as e:
            logger.warning(f"Error caching historical data for player {player_id}: {str(e)}")

        return historical_data

    except Exception as e:
        logger.error(f"Error fetching historical data for player {player_id}: {str(e)}", exc_info=True)
        return {}

def find_similar_players(player_id: int, player_metrics: Dict[str, float], season: str) -> List[Dict[str, Any]]:
    """Find players with similar statistical profiles using our NBA_PLUS metrics."""
    try:
        # Fetch all players' advanced stats
        def fetch_all_advanced_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Advanced',
                per_mode_detailed='PerGame',
                last_n_games=0,
                month=0,
                opponent_team_id=0,
                period=0,
                pace_adjust='N',
                plus_minus='N',
                rank='N'
            )

        all_stats = retry_on_timeout(fetch_all_advanced_stats)
        all_stats_df = all_stats.get_data_frames()[0]

        # Also fetch basic stats to get more comparison points
        def fetch_all_basic_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Base',
                per_mode_detailed='PerGame',
                last_n_games=0,
                month=0,
                opponent_team_id=0,
                period=0,
                pace_adjust='N',
                plus_minus='N',
                rank='N'
            )

        basic_stats = retry_on_timeout(fetch_all_basic_stats)
        basic_stats_df = basic_stats.get_data_frames()[0]

        # Merge advanced and basic stats
        merged_stats = pd.merge(
            all_stats_df,
            basic_stats_df[['PLAYER_ID', 'PTS', 'AST', 'REB', 'STL', 'BLK', 'FG_PCT', 'FG3_PCT', 'FT_PCT']],
            on='PLAYER_ID',
            how='left'
        )

        # Filter out the current player and players with minimal minutes
        other_players_df = merged_stats[(merged_stats['PLAYER_ID'] != player_id) & (merged_stats['MIN'] > 15)]

        if other_players_df.empty:
            logger.warning(f"No other players found for comparison in season {season}")
            return []

        # Get the current player's stats
        current_player_stats = merged_stats[merged_stats['PLAYER_ID'] == player_id]

        if current_player_stats.empty:
            logger.warning(f"Current player stats not found for ID {player_id}")
            return []

        # Calculate NBA_PLUS metrics for all players
        all_players_with_metrics = []

        for _, player_row in pd.concat([current_player_stats, other_players_df]).iterrows():
            player_dict = player_row.to_dict()

            # Calculate NBA_PLUS metrics
            player_metrics = {}

            # Copy existing metrics
            for key in player_dict:
                if isinstance(player_dict[key], (int, float)) and not pd.isna(player_dict[key]):
                    player_metrics[key] = float(player_dict[key])

            # Calculate offensive rating (0-100 scale)
            off_rating = 0
            if "OFFRTG" in player_metrics:
                off_rating = min(100, max(0, (player_metrics["OFFRTG"] - 85) * 100 / 40))
            elif "PIE" in player_metrics:
                off_rating = min(100, max(0, player_metrics.get("PIE", 0) * 100 * 2))

            # Calculate defensive rating (0-100 scale)
            def_rating = 0
            if "DEFRTG" in player_metrics:
                def_rating = min(100, max(0, (130 - player_metrics["DEFRTG"]) * 100 / 40))
            elif "PIE" in player_metrics:
                def_rating = min(100, max(0, player_metrics.get("PIE", 0) * 100 * 1.5))

            # Overall Rating (0-100 scale)
            overall_rating = (off_rating * 0.6) + (def_rating * 0.4)

            # Add NBA_PLUS metrics
            player_metrics["NBA_PLUS"] = (overall_rating - 50) / 5  # -10 to +10 scale
            player_metrics["NBA_PLUS_OFF"] = (off_rating - 50) / 5
            player_metrics["NBA_PLUS_DEF"] = (def_rating - 50) / 5

            # Add to list
            all_players_with_metrics.append({
                'player_id': int(player_dict['PLAYER_ID']),
                'player_name': player_dict['PLAYER_NAME'],
                'metrics': player_metrics
            })

        # Get current player metrics
        current_player = next(p for p in all_players_with_metrics if p['player_id'] == player_id)
        current_metrics = current_player['metrics']

        # Define comparison metrics with weights
        comparison_metrics = {
            # Advanced metrics
            'PIE': 3.0,           # Player Impact Estimate (very important)
            'USG_PCT': 2.0,       # Usage percentage
            'TS_PCT': 2.0,        # True shooting percentage
            'AST_PCT': 1.5,       # Assist percentage
            'REB_PCT': 1.5,       # Rebound percentage
            'NBA_PLUS': 3.0,      # Our overall rating (very important)
            'NBA_PLUS_OFF': 1.5,  # Our offensive rating
            'NBA_PLUS_DEF': 1.5,  # Our defensive rating

            # Basic stats
            'PTS': 2.0,           # Points per game
            'AST': 1.5,           # Assists per game
            'REB': 1.5,           # Rebounds per game
            'STL': 1.0,           # Steals per game
            'BLK': 1.0,           # Blocks per game
            'FG_PCT': 1.0,        # Field goal percentage
            'FG3_PCT': 1.0,       # Three-point percentage
            'FT_PCT': 0.5,        # Free throw percentage
        }

        # Calculate similarity scores
        similarity_scores = []
        other_players = [p for p in all_players_with_metrics if p['player_id'] != player_id]

        for player in other_players:
            player_metrics = player['metrics']

            # Calculate weighted Euclidean distance
            weighted_squared_diffs = []
            total_weight = 0

            for metric, weight in comparison_metrics.items():
                if metric in current_metrics and metric in player_metrics:
                    # Get the values
                    current_value = current_metrics[metric]
                    player_value = player_metrics[metric]

                    # Calculate normalized difference
                    # Use all players to get standard deviation
                    all_values = [p['metrics'].get(metric) for p in all_players_with_metrics
                                 if metric in p['metrics']]
                    std_dev = np.std(all_values) if len(all_values) > 1 else 1

                    if std_dev > 0:
                        normalized_diff = (current_value - player_value) / std_dev
                        weighted_squared_diffs.append(normalized_diff ** 2 * weight)
                        total_weight += weight

            # Calculate weighted distance and similarity
            if total_weight > 0:
                weighted_distance = np.sqrt(sum(weighted_squared_diffs) / total_weight)
                similarity = 1 / (1 + weighted_distance)  # Convert distance to similarity (0 to 1)

                similarity_scores.append({
                    'player_id': player['player_id'],
                    'player_name': player['player_name'],
                    'similarity_score': round(similarity, 2)
                })

        # Sort by similarity (highest first) and take top 5
        similarity_scores.sort(key=lambda x: x['similarity_score'], reverse=True)
        return similarity_scores[:5]

    except Exception as e:
        logger.error(f"Error finding similar players for player ID {player_id}: {str(e)}", exc_info=True)
        return []


===== backend\api_tools\advanced_shot_charts.py =====
"""
Advanced shot chart generation module for NBA player shot analysis.
This module provides enhanced visualization capabilities for shot charts,
including heatmaps, animated sequences, and interactive elements.
Provides both JSON and DataFrame outputs with CSV caching.
"""

import logging
import os
import json
import base64
from io import BytesIO
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.colors import LinearSegmentedColormap
from matplotlib.figure import Figure
from matplotlib.patches import Circle, Rectangle, Arc, Polygon
from nba_api.stats.endpoints import shotchartdetail
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, find_player_id_or_error
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
SHOT_CHARTS_CSV_DIR = get_cache_dir("shot_charts")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_shot_charts(
    player_id: int,
    season: str,
    season_type: str,
    chart_type: str
) -> str:
    """
    Generates a file path for saving shot chart DataFrame as CSV.

    Args:
        player_id: Player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        chart_type: The type of chart (e.g., 'scatter', 'heatmap')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"shotchart_{player_id}_{season}_{clean_season_type}_{chart_type}.csv"
    return get_cache_file_path(filename, "shot_charts")

# Module-level constants
COURT_WIDTH = 500
COURT_HEIGHT = 470
THREE_POINT_RADIUS = 237.5
THREE_POINT_SIDE_RADIUS = 220
THREE_POINT_SIDE_HEIGHT = 140
PAINT_WIDTH = 160
PAINT_HEIGHT = 190
BACKBOARD_WIDTH = 60
HOOP_RADIUS = 7.5
RESTRICTED_AREA_RADIUS = 40
FREE_THROW_CIRCLE_RADIUS = 60
CORNER_THREE_SIDE_DIST = 220

# Shot zone positions for annotations
ZONE_POSITIONS = {
    'Above the Break 3': {'offset_x': 0, 'offset_y': 30},
    'Left Corner 3': {'offset_x': -20, 'offset_y': 20},
    'Right Corner 3': {'offset_x': 20, 'offset_y': 20},
    'Mid-Range': {'offset_x': 0, 'offset_y': 25},
    'In The Paint (Non-RA)': {'offset_x': 0, 'offset_y': 20},
    'Restricted Area': {'offset_x': 0, 'offset_y': -30}
}

# Custom color maps
HEATMAP_CMAP = LinearSegmentedColormap.from_list(
    'shot_heatmap',
    [(0, '#0000FF'), (0.5, '#800080'), (1, '#FF0000')],
    N=256
)

def draw_court(ax: plt.Axes, color: str = 'black', lw: float = 2,
               outer_lines: bool = False, court_bg_color: str = '#F2F3F5',
               paint_color: str = '#F9FAFB', three_point_color: str = '#E5E7EB') -> plt.Axes:
    """
    Draw an NBA basketball court with accurate dimensions and styling.

    Args:
        ax: Matplotlib axes object
        color: Color of the court lines
        lw: Line width
        outer_lines: Whether to draw the outer court boundaries
        court_bg_color: Background color of the court
        paint_color: Color of the paint area
        three_point_color: Color of the three-point area

    Returns:
        The matplotlib axes object with the court drawn
    """
    # Court background
    court = Rectangle((-COURT_WIDTH/2, 0), COURT_WIDTH, COURT_HEIGHT,
                     color=court_bg_color, fill=True, zorder=0)
    ax.add_patch(court)

    # Paint area with custom color
    paint = Rectangle((-PAINT_WIDTH/2, 0), PAINT_WIDTH, PAINT_HEIGHT,
                     color=paint_color, fill=True, zorder=1)
    ax.add_patch(paint)

    # Paint outline
    paint_outline = Rectangle((-PAINT_WIDTH/2, 0), PAINT_WIDTH, PAINT_HEIGHT,
                             fill=False, color=color, linewidth=lw, zorder=2)
    ax.add_patch(paint_outline)

    # Three-point line
    three_point_left = Rectangle((-COURT_WIDTH/2, 0),
                               0, THREE_POINT_SIDE_HEIGHT, color=color,
                               linewidth=lw, zorder=2)
    three_point_right = Rectangle((COURT_WIDTH/2, 0),
                                0, THREE_POINT_SIDE_HEIGHT, color=color,
                                linewidth=lw, zorder=2)
    ax.add_patch(three_point_left)
    ax.add_patch(three_point_right)

    # Three point arc
    theta = np.linspace(0, np.pi, 100)
    three_point_arc_x = THREE_POINT_RADIUS * np.cos(theta)
    three_point_arc_y = THREE_POINT_RADIUS * np.sin(theta)
    ax.plot(three_point_arc_x, three_point_arc_y, color=color, linewidth=lw, zorder=2)

    # Backboard
    ax.plot([-BACKBOARD_WIDTH/2, BACKBOARD_WIDTH/2], [0, 0],
           color=color, linewidth=lw*1.5, zorder=2)

    # Hoop
    hoop = Circle((0, 0), HOOP_RADIUS, color=color,
                 fill=False, linewidth=lw, zorder=2)
    ax.add_patch(hoop)

    # Restricted area
    restricted = Circle((0, 0), RESTRICTED_AREA_RADIUS,
                      color=color, fill=False, linewidth=lw, zorder=2)
    ax.add_patch(restricted)

    # Free throw circle
    free_throw = Circle((0, PAINT_HEIGHT), FREE_THROW_CIRCLE_RADIUS,
                       color=color, fill=False, linewidth=lw, zorder=2)
    ax.add_patch(free_throw)

    # Free throw line
    ax.plot([-PAINT_WIDTH/2, PAINT_WIDTH/2], [PAINT_HEIGHT, PAINT_HEIGHT],
           color=color, linewidth=lw, zorder=2)

    # Set aspect ratio and remove axes
    ax.set_aspect('equal')
    ax.set_xticks([])
    ax.set_yticks([])

    # Set axis limits to show only the offensive half-court
    ax.set_xlim(-COURT_WIDTH/2, COURT_WIDTH/2)
    ax.set_ylim(0, COURT_HEIGHT)

    return ax

def create_static_shotchart(
    shot_data: Dict[str, Any],
    chart_type: str = 'scatter',
    output_format: str = 'base64'
) -> Dict[str, Any]:
    """
    Create a static shot chart visualization from the shot data.

    Args:
        shot_data: Dictionary containing shot chart data
        chart_type: Type of chart ('scatter', 'heatmap', 'hexbin')
        output_format: Output format ('base64', 'file')

    Returns:
        Dict containing the visualization data or file path
    """
    # Create figure and axis
    fig = plt.figure(figsize=(12, 11), facecolor='white')
    ax = fig.add_subplot(111)

    # Draw court
    draw_court(ax)

    # Extract shot locations
    shots = []
    for shot in shot_data.get("shot_locations", []):
        shots.append({
            "x": shot.get("x", 0),
            "y": shot.get("y", 0),
            "made": shot.get("made", False),
            "zone": shot.get("zone", ""),
            "value": 3 if "3PT" in shot.get("shot_type", "") else 2
        })

    # Plot shots based on chart type
    if chart_type == 'scatter':
        _plot_scatter_shots(ax, shots)
    elif chart_type == 'heatmap':
        _plot_heatmap_shots(ax, shots)
    elif chart_type == 'hexbin':
        _plot_hexbin_shots(ax, shots)

    # Add title and stats
    stats = shot_data.get("overall_stats", {})
    title = f"{shot_data.get('player_name', 'Player')} Shot Chart {shot_data.get('season', '')}\n"
    title += f"FG: {stats.get('made_shots', 0)}/{stats.get('total_shots', 0)} ({stats.get('field_goal_percentage', 0)}%)"
    plt.title(title, pad=20, size=14, weight='bold')

    # Add zone percentages
    _add_zone_annotations(ax, shot_data.get("zone_breakdown", {}), shots)

    # Add legend
    made_patch = plt.scatter([], [], c='#2ECC71', alpha=0.7, s=50,
                           marker='o', edgecolor='white', label='Made')
    missed_patch = plt.scatter([], [], c='#E74C3C', alpha=0.6, s=40,
                             marker='x', linewidth=1.5, label='Missed')
    ax.legend(handles=[made_patch, missed_patch], loc='upper right',
             bbox_to_anchor=(1, 1), framealpha=1)

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": chart_type
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"shotchart_{shot_data.get('player_name', 'player').replace(' ', '_')}_{shot_data.get('season', '')}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": chart_type
        }

def _plot_scatter_shots(ax: plt.Axes, shots: List[Dict[str, Any]]) -> None:
    """Plot shots as a scatter plot."""
    for shot in shots:
        x = shot["x"]
        y = shot["y"]
        made = shot["made"]

        if made:
            ax.scatter(x, y, c='#2ECC71', alpha=0.7, s=50,
                      marker='o', edgecolor='white', linewidth=0.5, zorder=3)
        else:
            ax.scatter(x, y, c='#E74C3C', alpha=0.6, s=40,
                      marker='x', linewidth=1.5, zorder=3)

def _plot_heatmap_shots(ax: plt.Axes, shots: List[Dict[str, Any]]) -> None:
    """Plot shots as a heatmap."""
    x = [shot["x"] for shot in shots]
    y = [shot["y"] for shot in shots]

    # Create heatmap using kernel density estimation
    if len(x) > 5:  # Need enough points for KDE
        from scipy.stats import gaussian_kde
        xy = np.vstack([x, y])
        z = gaussian_kde(xy)(xy)

        # Sort the points by density, so that the densest points are plotted last
        idx = z.argsort()
        x = np.array(x)[idx]
        y = np.array(y)[idx]
        z = z[idx]

        scatter = ax.scatter(x, y, c=z, cmap=HEATMAP_CMAP, s=30, alpha=0.7, zorder=3)
        plt.colorbar(scatter, ax=ax, label='Shot Frequency')
    else:
        # Fall back to scatter plot if not enough points
        _plot_scatter_shots(ax, shots)

def _plot_hexbin_shots(ax: plt.Axes, shots: List[Dict[str, Any]]) -> None:
    """Plot shots as a hexbin plot."""
    x = [shot["x"] for shot in shots]
    y = [shot["y"] for shot in shots]

    if len(x) > 10:  # Need enough points for hexbin
        hexbin = ax.hexbin(
            x, y,
            gridsize=25,
            cmap=HEATMAP_CMAP,
            alpha=0.7,
            mincnt=1,
            zorder=3
        )
        plt.colorbar(hexbin, ax=ax, label='Shot Frequency')
    else:
        # Fall back to scatter plot if not enough points
        _plot_scatter_shots(ax, shots)

def _add_zone_annotations(ax: plt.Axes, zone_breakdown: Dict[str, Any], shots: List[Dict[str, Any]]) -> None:
    """Add zone percentage annotations to the shot chart."""
    for zone, stats in zone_breakdown.items():
        # Calculate average position for the zone
        zone_shots = [(shot["x"], shot["y"]) for shot in shots if shot.get("zone") == zone]
        if zone_shots:
            avg_x = sum(x for x, _ in zone_shots) / len(zone_shots)
            avg_y = sum(y for _, y in zone_shots) / len(zone_shots)

            # Apply zone-specific positioning
            offset = ZONE_POSITIONS.get(zone, {'offset_x': 0, 'offset_y': 25})
            text_x = avg_x + offset['offset_x']
            text_y = avg_y + offset['offset_y']

            # Add text with zone stats
            text = f"{zone}\n{stats.get('made', 0)}/{stats.get('attempts', 0)}\n{stats.get('percentage', 0)}%"
            ax.text(text_x, text_y, text, ha='center', va='bottom',
                   bbox=dict(facecolor='white', alpha=0.7, edgecolor='#222222',
                            boxstyle='round,pad=0.5'),
                   size=8, weight='bold', zorder=4)

def create_animated_shotchart(
    shot_data: Dict[str, Any],
    output_format: str = 'base64'
) -> Dict[str, Any]:
    """
    Create an animated shot chart visualization showing shots appearing sequentially.

    Args:
        shot_data: Dictionary containing shot chart data
        output_format: Output format ('base64', 'file')

    Returns:
        Dict containing the visualization data or file path
    """
    # Extract shot locations
    shots = []
    for shot in shot_data.get("shot_locations", []):
        shots.append({
            "x": shot.get("x", 0),
            "y": shot.get("y", 0),
            "made": shot.get("made", False),
            "zone": shot.get("zone", ""),
            "value": 3 if "3PT" in shot.get("shot_type", "") else 2,
            "game_date": shot.get("game_date", "")
        })

    # Sort shots by game date if available
    if all("game_date" in shot for shot in shots):
        shots.sort(key=lambda x: x["game_date"])

    # Create figure and axis
    fig = plt.figure(figsize=(12, 11), facecolor='white')
    ax = fig.add_subplot(111)

    # Draw court
    draw_court(ax)

    # Add title and stats
    stats = shot_data.get("overall_stats", {})
    title = f"{shot_data.get('player_name', 'Player')} Shot Chart {shot_data.get('season', '')}\n"
    title += f"FG: {stats.get('made_shots', 0)}/{stats.get('total_shots', 0)} ({stats.get('field_goal_percentage', 0)}%)"
    plt.title(title, pad=20, size=14, weight='bold')

    # Add legend
    made_patch = plt.scatter([], [], c='#2ECC71', alpha=0.7, s=50,
                           marker='o', edgecolor='white', label='Made')
    missed_patch = plt.scatter([], [], c='#E74C3C', alpha=0.6, s=40,
                             marker='x', linewidth=1.5, label='Missed')
    ax.legend(handles=[made_patch, missed_patch], loc='upper right',
             bbox_to_anchor=(1, 1), framealpha=1)

    # Initialize empty scatter plots for made and missed shots
    made_shots = ax.scatter([], [], c='#2ECC71', alpha=0.7, s=50,
                          marker='o', edgecolor='white', linewidth=0.5, zorder=3)
    missed_shots = ax.scatter([], [], c='#E74C3C', alpha=0.6, s=40,
                            marker='x', linewidth=1.5, zorder=3)

    # Animation function
    def update(frame):
        # Get shots up to the current frame
        current_shots = shots[:frame]

        # Separate made and missed shots
        made_x = [shot["x"] for shot in current_shots if shot["made"]]
        made_y = [shot["y"] for shot in current_shots if shot["made"]]
        missed_x = [shot["x"] for shot in current_shots if not shot["made"]]
        missed_y = [shot["y"] for shot in current_shots if not shot["made"]]

        # Update scatter plots
        made_shots.set_offsets(np.c_[made_x, made_y])
        missed_shots.set_offsets(np.c_[missed_x, missed_y])

        return made_shots, missed_shots

    # Create animation
    frames = min(len(shots), 100)  # Limit to 100 frames for performance
    step = max(1, len(shots) // frames)
    anim = animation.FuncAnimation(
        fig, update, frames=range(1, len(shots) + 1, step),
        interval=50, blit=True
    )

    # Return the animation based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        writer = animation.PillowWriter(fps=10)
        anim.save(buffer, writer=writer)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        plt.close(fig)
        return {
            "animation_data": f"data:image/gif;base64,{image_base64}",
            "chart_type": "animated"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"animated_shotchart_{shot_data.get('player_name', 'player').replace(' ', '_')}_{shot_data.get('season', '')}.gif"
        filepath = os.path.join(output_dir, filename)
        writer = animation.PillowWriter(fps=10)
        anim.save(filepath, writer=writer)
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "animated"
        }

def create_shot_frequency_chart(
    shot_data: Dict[str, Any],
    output_format: str = 'base64'
) -> Dict[str, Any]:
    """
    Create a shot frequency chart visualization showing frequency and efficiency.

    Args:
        shot_data: Dictionary containing shot chart data
        output_format: Output format ('base64', 'file')

    Returns:
        Dict containing the visualization data or file path
    """
    # Create figure and axis
    fig = plt.figure(figsize=(12, 11), facecolor='white')
    ax = fig.add_subplot(111)

    # Draw court
    draw_court(ax)

    # Extract shot locations
    shots = []
    for shot in shot_data.get("shot_locations", []):
        shots.append({
            "x": shot.get("x", 0),
            "y": shot.get("y", 0),
            "made": shot.get("made", False),
            "zone": shot.get("zone", ""),
            "value": 3 if "3PT" in shot.get("shot_type", "") else 2
        })

    # Create hexbin plot for frequency
    x = [shot["x"] for shot in shots]
    y = [shot["y"] for shot in shots]

    if len(x) > 10:
        # Create hexbin
        hexbin = ax.hexbin(
            x, y,
            gridsize=25,
            cmap='viridis',
            alpha=0.7,
            mincnt=1,
            zorder=3
        )
        plt.colorbar(hexbin, ax=ax, label='Shot Frequency')

        # Add title and stats
        stats = shot_data.get("overall_stats", {})
        title = f"{shot_data.get('player_name', 'Player')} Shot Frequency {shot_data.get('season', '')}\n"
        title += f"FG: {stats.get('made_shots', 0)}/{stats.get('total_shots', 0)} ({stats.get('field_goal_percentage', 0)}%)"
        plt.title(title, pad=20, size=14, weight='bold')
    else:
        # Fall back to scatter plot if not enough points
        _plot_scatter_shots(ax, shots)

        # Add title and stats
        stats = shot_data.get("overall_stats", {})
        title = f"{shot_data.get('player_name', 'Player')} Shot Chart {shot_data.get('season', '')}\n"
        title += f"FG: {stats.get('made_shots', 0)}/{stats.get('total_shots', 0)} ({stats.get('field_goal_percentage', 0)}%)"
        plt.title(title, pad=20, size=14, weight='bold')

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": "frequency"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"frequency_shotchart_{shot_data.get('player_name', 'player').replace(' ', '_')}_{shot_data.get('season', '')}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "frequency"
        }

def create_shot_distance_chart(
    shot_data: Dict[str, Any],
    output_format: str = 'base64'
) -> Dict[str, Any]:
    """
    Create a shot distance chart visualization showing efficiency by distance.

    Args:
        shot_data: Dictionary containing shot chart data
        output_format: Output format ('base64', 'file')

    Returns:
        Dict containing the visualization data or file path
    """
    # Create figure and axis
    fig = plt.figure(figsize=(12, 8), facecolor='white')
    ax = fig.add_subplot(111)

    # Extract shot locations with distances
    shots = []
    for shot in shot_data.get("shot_locations", []):
        shots.append({
            "distance": shot.get("distance", 0),
            "made": shot.get("made", False),
            "value": 3 if "3PT" in shot.get("shot_type", "") else 2
        })

    # Group shots by distance (rounded to nearest foot)
    distance_groups = {}
    for shot in shots:
        distance = round(shot["distance"])
        if distance not in distance_groups:
            distance_groups[distance] = {"attempts": 0, "made": 0}

        distance_groups[distance]["attempts"] += 1
        if shot["made"]:
            distance_groups[distance]["made"] += 1

    # Calculate percentages and prepare data for plotting
    distances = []
    percentages = []
    sizes = []

    for distance, stats in sorted(distance_groups.items()):
        if stats["attempts"] >= 3:  # Only include distances with enough attempts
            distances.append(distance)
            percentages.append((stats["made"] / stats["attempts"]) * 100)
            sizes.append(stats["attempts"] * 5)  # Scale size by number of attempts

    # Create scatter plot with size representing number of attempts
    scatter = ax.scatter(distances, percentages, s=sizes, alpha=0.7, c=percentages,
                        cmap='RdYlGn', vmin=0, vmax=100, edgecolors='black', linewidths=0.5)

    # Add league average line
    league_avg = 45  # Approximate league average FG%
    ax.axhline(y=league_avg, color='gray', linestyle='--', alpha=0.7, label='League Average')

    # Add trend line
    if len(distances) > 1:
        try:
            import numpy as np
            from scipy import stats

            slope, intercept, r_value, p_value, std_err = stats.linregress(distances, percentages)
            line_x = np.array([min(distances), max(distances)])
            line_y = slope * line_x + intercept
            ax.plot(line_x, line_y, 'b-', alpha=0.5, label=f'Trend (r={r_value:.2f})')
        except:
            # If scipy is not available, skip trend line
            pass

    # Add labels and title
    ax.set_xlabel('Distance (feet)', fontsize=12)
    ax.set_ylabel('Field Goal Percentage (%)', fontsize=12)

    stats = shot_data.get("overall_stats", {})
    title = f"{shot_data.get('player_name', 'Player')} Shooting by Distance {shot_data.get('season', '')}\n"
    title += f"FG: {stats.get('made_shots', 0)}/{stats.get('total_shots', 0)} ({stats.get('field_goal_percentage', 0)}%)"
    plt.title(title, pad=20, size=14, weight='bold')

    # Add colorbar
    cbar = plt.colorbar(scatter)
    cbar.set_label('Field Goal Percentage (%)')

    # Add legend for bubble size
    from matplotlib.lines import Line2D
    legend_elements = [
        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=5, label='5 Attempts'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label='20 Attempts'),
        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=15, label='50 Attempts')
    ]
    ax.legend(handles=legend_elements, loc='upper right')

    # Set y-axis limits
    ax.set_ylim(0, 100)

    # Add grid
    ax.grid(True, alpha=0.3)

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": "distance"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"distance_shotchart_{shot_data.get('player_name', 'player').replace(' ', '_')}_{shot_data.get('season', '')}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "distance"
        }

def process_shot_data_for_visualization(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    chart_type: str = "scatter",
    output_format: str = "base64",
    use_cache: bool = True,
    return_dataframe: bool = False
) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
    """
    Process shot data for a player and create visualizations.
    Provides DataFrame output capabilities.

    Args:
        player_name: Name of the player
        season: NBA season in format YYYY-YY
        season_type: Type of season (Regular Season, Playoffs, etc.)
        chart_type: Type of chart to create (scatter, heatmap, hexbin, animated, frequency)
        output_format: Output format (base64, file)
        use_cache: Whether to use cached visualizations
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            Dict containing the visualization data and metadata
        If return_dataframe=True:
            Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]: A tuple containing the JSON response
                                                          and a dictionary of DataFrames
    """
    from .visualization_cache import VisualizationCache

    # Create cache parameters
    cache_params = {
        "player_name": player_name,
        "season": season,
        "season_type": season_type,
        "chart_type": chart_type,
        "output_format": output_format,
        "return_dataframe": return_dataframe
    }

    # Store DataFrames if requested
    dataframes = {}

    # Check cache first if enabled
    if use_cache:
        cached_result = VisualizationCache.get(cache_params)
        if cached_result:
            logger.info(f"Using cached visualization for {player_name}, {season}, {chart_type}")
            if return_dataframe:
                # If we're returning DataFrames but the cached result doesn't have them,
                # we'll need to fetch the data again
                if isinstance(cached_result, tuple) and len(cached_result) == 2:
                    return cached_result
                # Otherwise, continue with the fetch to get DataFrames
            else:
                return cached_result

    try:
        # Get player ID
        player_id, player_actual_name = find_player_id_or_error(player_name)

        # Fetch shot chart data
        shotchart_endpoint = shotchartdetail.ShotChartDetail(
            player_id=player_id,
            team_id=0,
            season_nullable=season,
            season_type_all_star=season_type,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS,
            context_measure_simple='FGA'
        )

        shots_df = shotchart_endpoint.shot_chart_detail.get_data_frame()
        league_avg_df = shotchart_endpoint.league_averages.get_data_frame()

        if shots_df.empty:
            error_response = {
                "error": f"No shot data found for {player_actual_name} in {season} {season_type}"
            }
            if return_dataframe:
                return error_response, dataframes
            return error_response

        # Process shot data
        shot_locations = []
        for _, row in shots_df.iterrows():
            shot_locations.append({
                "x": float(row['LOC_X']),
                "y": float(row['LOC_Y']),
                "made": bool(row['SHOT_MADE_FLAG']),
                "shot_type": row['ACTION_TYPE'],
                "zone": row['SHOT_ZONE_BASIC'],
                "distance": float(row['SHOT_DISTANCE']),
                "game_date": row['GAME_DATE'],
                "period": int(row['PERIOD'])
            })

        # Calculate overall stats
        total_shots = len(shot_locations)
        made_shots = sum(1 for shot in shot_locations if shot["made"])
        field_goal_percentage = round((made_shots / total_shots) * 100, 1) if total_shots > 0 else 0

        # Calculate zone breakdown
        zone_breakdown = {}
        for zone in set(shot["zone"] for shot in shot_locations):
            zone_shots = [shot for shot in shot_locations if shot["zone"] == zone]
            zone_attempts = len(zone_shots)
            zone_made = sum(1 for shot in zone_shots if shot["made"])
            zone_percentage = round((zone_made / zone_attempts) * 100, 1) if zone_attempts > 0 else 0

            # Get league average for this zone
            league_zone_data = league_avg_df[league_avg_df['SHOT_ZONE_BASIC'] == zone]
            league_percentage = float(league_zone_data['FG_PCT'].iloc[0]) * 100 if not league_zone_data.empty else 0

            zone_breakdown[zone] = {
                "attempts": zone_attempts,
                "made": zone_made,
                "percentage": zone_percentage,
                "league_percentage": league_percentage,
                "relative_percentage": round(zone_percentage - league_percentage, 1)
            }

        # Prepare data for visualization
        shot_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "season": season,
            "season_type": season_type,
            "overall_stats": {
                "total_shots": total_shots,
                "made_shots": made_shots,
                "field_goal_percentage": field_goal_percentage
            },
            "zone_breakdown": zone_breakdown,
            "shot_locations": shot_locations
        }

        # Create visualization based on chart type
        result = None
        if chart_type == "scatter":
            result = create_static_shotchart(shot_data, "scatter", output_format)
        elif chart_type == "heatmap":
            result = create_static_shotchart(shot_data, "heatmap", output_format)
        elif chart_type == "hexbin":
            result = create_static_shotchart(shot_data, "hexbin", output_format)
        elif chart_type == "animated":
            result = create_animated_shotchart(shot_data, output_format)
        elif chart_type == "frequency":
            result = create_shot_frequency_chart(shot_data, output_format)
        elif chart_type == "distance":
            result = create_shot_distance_chart(shot_data, output_format)
        elif chart_type == "comparison":
            # Comparison requires additional parameters
            error_response = {
                "error": "Player comparison requires additional parameters. Use the comparison endpoint instead."
            }
            if return_dataframe:
                return error_response, dataframes
            return error_response
        else:
            error_response = {
                "error": f"Invalid chart type: {chart_type}. Valid types are: scatter, heatmap, hexbin, animated, frequency, distance, comparison"
            }
            if return_dataframe:
                return error_response, dataframes
            return error_response

        # Store DataFrames if requested
        if return_dataframe:
            # Save shots_df and league_avg_df to CSV
            if not shots_df.empty:
                shots_csv_path = _get_csv_path_for_shot_charts(
                    player_id=player_id,
                    season=season,
                    season_type=season_type,
                    chart_type=f"{chart_type}_shots"
                )
                _save_dataframe_to_csv(shots_df, shots_csv_path)

                # Add relative path to result
                shots_relative_path = get_relative_cache_path(
                    os.path.basename(shots_csv_path),
                    "shot_charts"
                )

                if "dataframe_info" not in result:
                    result["dataframe_info"] = {
                        "message": "Shot chart data has been converted to DataFrame and saved as CSV file",
                        "dataframes": {}
                    }

                result["dataframe_info"]["dataframes"]["shots"] = {
                    "shape": list(shots_df.shape),
                    "columns": shots_df.columns.tolist(),
                    "csv_path": shots_relative_path
                }

            if not league_avg_df.empty:
                league_csv_path = _get_csv_path_for_shot_charts(
                    player_id=player_id,
                    season=season,
                    season_type=season_type,
                    chart_type=f"{chart_type}_league_avg"
                )
                _save_dataframe_to_csv(league_avg_df, league_csv_path)

                # Add relative path to result
                league_relative_path = get_relative_cache_path(
                    os.path.basename(league_csv_path),
                    "shot_charts"
                )

                if "dataframe_info" not in result:
                    result["dataframe_info"] = {
                        "message": "Shot chart data has been converted to DataFrame and saved as CSV file",
                        "dataframes": {}
                    }

                result["dataframe_info"]["dataframes"]["league_averages"] = {
                    "shape": list(league_avg_df.shape),
                    "columns": league_avg_df.columns.tolist(),
                    "csv_path": league_relative_path
                }

            # Add DataFrames to the return value
            dataframes["shots"] = shots_df
            dataframes["league_averages"] = league_avg_df

        # Cache the result if successful
        if result and use_cache and "error" not in result:
            if return_dataframe:
                # Cache the tuple with DataFrames
                VisualizationCache.set(cache_params, (result, dataframes))
            else:
                VisualizationCache.set(cache_params, result)

        if return_dataframe:
            return result, dataframes
        return result

    except Exception as e:
        logger.error(f"Error processing shot data for {player_name}: {str(e)}", exc_info=True)
        error_response = {
            "error": f"Failed to process shot data: {str(e)}"
        }
        if return_dataframe:
            return error_response, dataframes
        return error_response


===== backend\api_tools\all_time_leaders_grids.py =====
"""
NBA API tools for accessing all-time statistical leaders data.

This module provides functions to fetch and process data from the NBA's all-time leaders
endpoint, which includes information such as:
- Points leaders
- Rebounds leaders
- Assists leaders
- Steals leaders
- Blocks leaders
- Field goal percentage leaders
- Free throw percentage leaders
- Three-point percentage leaders
- Games played leaders
- And many other statistical categories

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import alltimeleadersgrids
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
ALL_TIME_LEADERS_CACHE_SIZE = 128
ALL_TIME_LEADERS_CSV_DIR = get_cache_dir("all_time_leaders")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

VALID_PER_MODES = {
    "Totals": "Totals",
    "PerGame": "PerGame"
}

VALID_SEASON_TYPES = {
    "Regular Season": "Regular Season",
    "Pre Season": "Pre Season"
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_all_time_leaders(
    league_id: str = "00",
    per_mode: str = "Totals",
    season_type: str = "Regular Season",
    topx: int = 10
) -> str:
    """
    Generates a file path for saving all-time leaders DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode: Per mode (default: "Totals")
        season_type: Season type (default: "Regular Season")
        topx: Number of top players to return (default: 10)

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    # Replace spaces with underscores for season_type
    safe_season_type = season_type.replace(" ", "_")

    # Create a unique filename that includes all parameters
    filename = f"all_time_leaders_league{league_id}_mode{per_mode}_season{safe_season_type}_top{topx}.csv"

    return get_cache_file_path(filename, "all_time_leaders")

# --- Parameter Validation Functions ---
def _validate_all_time_leaders_params(
    league_id: str,
    per_mode: str,
    season_type: str,
    topx: int
) -> Optional[str]:
    """
    Validates parameters for the all-time leaders function.

    Args:
        league_id: League ID (e.g., "00" for NBA)
        per_mode: Per mode (e.g., "Totals", "PerGame")
        season_type: Season type (e.g., "Regular Season", "Pre Season")
        topx: Number of top players to return

    Returns:
        Error message if validation fails, None otherwise
    """
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"

    if per_mode not in VALID_PER_MODES:
        return f"Invalid per_mode: {per_mode}. Valid options are: {', '.join(VALID_PER_MODES.keys())}"

    if season_type not in VALID_SEASON_TYPES:
        return f"Invalid season_type: {season_type}. Valid options are: {', '.join(VALID_SEASON_TYPES.keys())}"

    if not isinstance(topx, int) or topx <= 0:
        return f"Invalid topx: {topx}. Must be a positive integer."

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=ALL_TIME_LEADERS_CACHE_SIZE)
def fetch_all_time_leaders_logic(
    league_id: str = "00",
    per_mode: str = "Totals",
    season_type: str = "Regular Season",
    topx: int = 10,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches all-time statistical leaders data from the NBA API.

    This endpoint provides all-time statistical leaders in various categories:
    - Points leaders
    - Rebounds leaders
    - Assists leaders
    - Steals leaders
    - Blocks leaders
    - Field goal percentage leaders
    - Free throw percentage leaders
    - Three-point percentage leaders
    - Games played leaders
    - And many other statistical categories

    Args:
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        per_mode (str, optional): Per mode. Defaults to "Totals".
        season_type (str, optional): Season type. Defaults to "Regular Season".
        topx (int, optional): Number of top players to return. Defaults to 10.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with all-time leaders data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_all_time_leaders_logic for: League: {league_id}, PerMode: {per_mode}, TopX: {topx}")

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_all_time_leaders_params(
        league_id=league_id,
        per_mode=per_mode,
        season_type=season_type,
        topx=topx
    )

    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_all_time_leaders(
        league_id=league_id,
        per_mode=per_mode,
        season_type=season_type,
        topx=topx
    )

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 10:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading all-time leaders from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "per_mode": per_mode,
                        "season_type": season_type,
                        "topx": topx
                    },
                    "data_sets": {}
                }

                # Split the CSV into multiple DataFrames based on the category
                categories = [
                    "ASTLeaders", "BLKLeaders", "DREBLeaders", "FG3ALeaders", "FG3MLeaders",
                    "FG3_PCTLeaders", "FGALeaders", "FGMLeaders", "FG_PCTLeaders", "FTALeaders",
                    "FTMLeaders", "FT_PCTLeaders", "GPLeaders", "OREBLeaders", "PFLeaders",
                    "PTSLeaders", "REBLeaders", "STLLeaders", "TOVLeaders"
                ]

                # Check if the CSV contains a 'Category' column
                if 'Category' in df.columns:
                    # Split the DataFrame by category
                    for category in categories:
                        category_df = df[df['Category'] == category]
                        if not category_df.empty:
                            # Drop the Category column for the individual dataframes
                            category_df = category_df.drop('Category', axis=1)
                            dataframes[category] = category_df
                            result_dict["data_sets"][category] = _process_dataframe(category_df, single_row=False)
                else:
                    # If no Category column, assume it's a combined CSV with all categories
                    dataframes["AllTimeLeaders"] = df
                    result_dict["data_sets"]["AllTimeLeaders"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "per_mode_simple": per_mode,
        "season_type": season_type,
        "topx": topx
    }

    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling AllTimeLeadersGrids with parameters: {api_params}")
        all_time_leaders_results = alltimeleadersgrids.AllTimeLeadersGrids(**api_params)

        # Get data frames directly
        list_of_dataframes = all_time_leaders_results.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # List of expected data set names for reference
        # These are the categories we expect to see in the API response
        # We'll use this for debugging and documentation purposes
        # "ASTLeaders", "BLKLeaders", "DREBLeaders", "FG3ALeaders", "FG3MLeaders",
        # "FG3_PCTLeaders", "FGALeaders", "FGMLeaders", "FG_PCTLeaders", "FTALeaders",
        # "FTMLeaders", "FT_PCTLeaders", "GPLeaders", "OREBLeaders", "PFLeaders",
        # "PTSLeaders", "REBLeaders", "STLLeaders", "TOVLeaders"

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Try to determine the data set name based on columns
                data_set_name = None

                # Check for specific columns to identify the data set
                if 'PTS' in df.columns:
                    data_set_name = "PTSLeaders"
                elif 'AST' in df.columns:
                    data_set_name = "ASTLeaders"
                elif 'REB' in df.columns:
                    data_set_name = "REBLeaders"
                elif 'BLK' in df.columns:
                    data_set_name = "BLKLeaders"
                elif 'STL' in df.columns:
                    data_set_name = "STLLeaders"
                elif 'FG_PCT' in df.columns:
                    data_set_name = "FG_PCTLeaders"
                elif 'FT_PCT' in df.columns:
                    data_set_name = "FT_PCTLeaders"
                elif 'FG3_PCT' in df.columns:
                    data_set_name = "FG3_PCTLeaders"
                elif 'FGM' in df.columns:
                    data_set_name = "FGMLeaders"
                elif 'FGA' in df.columns:
                    data_set_name = "FGALeaders"
                elif 'FTM' in df.columns:
                    data_set_name = "FTMLeaders"
                elif 'FTA' in df.columns:
                    data_set_name = "FTALeaders"
                elif 'FG3M' in df.columns:
                    data_set_name = "FG3MLeaders"
                elif 'FG3A' in df.columns:
                    data_set_name = "FG3ALeaders"
                elif 'OREB' in df.columns:
                    data_set_name = "OREBLeaders"
                elif 'DREB' in df.columns:
                    data_set_name = "DREBLeaders"
                elif 'GP' in df.columns:
                    data_set_name = "GPLeaders"
                elif 'PF' in df.columns:
                    data_set_name = "PFLeaders"
                elif 'TOV' in df.columns:
                    data_set_name = "TOVLeaders"
                else:
                    # If we can't identify the data set, use a generic name
                    data_set_name = f"DataSet_{idx}"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Add category column and append to combined DataFrame
                    df_with_category = df.copy()
                    df_with_category['Category'] = data_set_name

                    # Make sure all stats columns are present in the combined DataFrame
                    # This ensures that each row has the correct structure for all stats
                    if not combined_df.empty:
                        # Get all columns from both DataFrames
                        all_columns = set(list(combined_df.columns) + list(df_with_category.columns))

                        # Add missing columns to both DataFrames with NaN values
                        for col in all_columns:
                            if col not in combined_df.columns:
                                combined_df[col] = None
                            if col not in df_with_category.columns:
                                df_with_category[col] = None

                    # Now concatenate the DataFrames
                    combined_df = pd.concat([combined_df, df_with_category], ignore_index=True)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV
        if return_dataframe:
            # Even if the DataFrame is empty, save it to indicate we tried to fetch the data
            # This will help with caching and prevent repeated API calls
            _save_dataframe_to_csv(combined_df, csv_path)

        # Return response
        logger.info(f"Successfully fetched all-time leaders data")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(f"API error in fetch_all_time_leaders_logic: {e}", exc_info=True)
        error_msg = f"Error fetching all-time leaders data: {str(e)}"

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_all_time_leaders(
    league_id: str = "00",
    per_mode: str = "Totals",
    season_type: str = "Regular Season",
    topx: int = 10,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets all-time statistical leaders data.

    This function is the main entry point for fetching all-time leaders data.
    It calls the fetch_all_time_leaders_logic function and returns the results.

    Args:
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        per_mode (str, optional): Per mode. Defaults to "Totals".
        season_type (str, optional): Season type. Defaults to "Regular Season".
        topx (int, optional): Number of top players to return. Defaults to 10.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with all-time leaders data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_all_time_leaders_logic(
        league_id=league_id,
        per_mode=per_mode,
        season_type=season_type,
        topx=topx,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\analyze.py =====
"""
Handles fetching NBA player dashboard statistics for analysis.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Dict, Any, Optional, Union, Tuple
import pandas as pd
from nba_api.stats.endpoints import playerdashboardbyyearoveryear
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeDetailed, LeagueID
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_ANALYSIS_CSV_DIR = get_cache_dir("player_analysis")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_analysis(
    player_id: int,
    season: str,
    season_type: str,
    per_mode: str,
    league_id: str
) -> str:
    """
    Generates a file path for saving player analysis DataFrame as CSV.

    Args:
        player_id: Player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        league_id: The league ID

    Returns:
        Path to the CSV file
    """
    # Clean season type and per mode for filename
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"player_analysis_{player_id}_{season}_{clean_season_type}_{clean_per_mode}_{league_id}.csv"
    return get_cache_file_path(filename, "player_analysis")

# Module-level constants for validation
_ANALYZE_VALID_SEASON_TYPES = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.preseason} # Endpoint uses season_type_playoffs
_ANALYZE_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_ANALYZE_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

def analyze_player_stats_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.per_game,
    league_id: str = LeagueID.nba,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches a player's overall dashboard statistics for a specified season and season type
    using the `PlayerDashboardByYearOverYear` endpoint. While the endpoint name suggests
    year-over-year data, this function primarily processes and returns the 'OverallPlayerDashboard'
    dataset, which represents the player's performance for the single specified season.
    Provides DataFrame output capabilities.

    Args:
        player_name: The full name of the player to analyze (e.g., "LeBron James").
        season: The NBA season identifier in YYYY-YY format (e.g., "2023-24").
                Defaults to `CURRENT_SEASON` from config.
        season_type: The type of season. Valid values from `SeasonTypeAllStar`
                     (e.g., "Regular Season", "Playoffs", "Pre Season", "All Star").
                     Defaults to "Regular Season".
        per_mode: The statistical mode. Valid values from `PerModeDetailed`
                  (e.g., "PerGame", "Totals", "Per36", "Per100Possessions").
                  Defaults to "PerGame".
        league_id: The league ID. Valid values from `LeagueID` (e.g., "00" for NBA).
                   Defaults to "00" (NBA).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string containing the player's overall dashboard statistics for the specified season.
                 Expected dictionary structure passed to format_response:
                 {
                     "player_name": str,
                     "player_id": int,
                     "season": str,
                     "season_type": str,
                     "per_mode": str,
                     "league_id": str,
                     "overall_dashboard_stats": { // Stats from OverallPlayerDashboard
                         "GROUP_SET": str, // e.g., "Overall"
                         "PLAYER_ID": int,
                         "PLAYER_NAME": str,
                         "GP": int, "W": int, "L": int, "W_PCT": float, "MIN": float,
                         "FGM": float, "FGA": float, "FG_PCT": float,
                         "FG3M": float, "FG3A": float, "FG3_PCT": float,
                         "FTM": float, "FTA": float, "FT_PCT": float,
                         "OREB": float, "DREB": float, "REB": float, "AST": float, "TOV": float,
                         "STL": float, "BLK": float, "BLKA": float, "PF": float, "PFD": float,
                         "PTS": float, "PLUS_MINUS": float,
                         "NBA_FANTASY_PTS": Optional[float], "DD2": Optional[int], "TD3": Optional[int],
                         // Other fields like GP_RANK, W_RANK, etc., might be present
                         ...
                     }
                 }
                 Returns {"overall_dashboard_stats": {}} if no data is found for the player and criteria.
                 Or an {'error': 'Error message'} object if a critical issue occurs.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing analyze_player_stats_logic for: {player_name}, Season: {season}, PerMode: {per_mode}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _ANALYZE_VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_ANALYZE_VALID_SEASON_TYPES)[:3])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _ANALYZE_VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_ANALYZE_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id not in _ANALYZE_VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_ANALYZE_VALID_LEAGUE_IDS)[:3])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playerdashboardbyyearoveryear for ID: {player_id} ({player_actual_name}), Season: {season}")
        try:
            player_stats_endpoint = playerdashboardbyyearoveryear.PlayerDashboardByYearOverYear(
                player_id=player_id, season=season, season_type_playoffs=season_type, # API uses season_type_playoffs
                per_mode_detailed=per_mode, league_id_nullable=league_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS # Changed
            )
            logger.debug(f"playerdashboardbyyearoveryear API call successful for {player_actual_name}")
        except Exception as api_error:
            logger.error(f"API error fetching analysis stats for {player_actual_name}: {api_error}", exc_info=True)
            error_msg = Errors.PLAYER_ANALYSIS_API.format(identifier=player_actual_name, error=str(api_error)) # Changed name to identifier
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        overall_stats_df = player_stats_endpoint.overall_player_dashboard.get_data_frame()
        overall_stats_dict = _process_dataframe(overall_stats_df, single_row=True)

        if overall_stats_dict is None:
            if overall_stats_df.empty:
                logger.warning(f"No overall dashboard stats found for player {player_actual_name} ({season}).")
                empty_response = {
                    "player_name": player_actual_name, "player_id": player_id,
                    "season": season, "season_type": season_type, "per_mode": per_mode,
                    "league_id": league_id, "overall_dashboard_stats": {}
                }

                if return_dataframe:
                    return format_response(empty_response), dataframes
                return format_response(empty_response)
            else:
                logger.error(f"DataFrame processing failed for analysis stats of {player_actual_name} ({season}).")
                error_msg = Errors.PLAYER_ANALYSIS_PROCESSING.format(identifier=player_actual_name) # Changed name to identifier
                error_response = format_response(error=error_msg)
                if return_dataframe:
                    return error_response, dataframes
                return error_response

        response_payload = {
            "player_name": player_actual_name, "player_id": player_id,
            "season": season, "season_type": season_type, "per_mode": per_mode,
            "league_id": league_id, "overall_dashboard_stats": overall_stats_dict or {}
        }

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["overall_dashboard"] = overall_stats_df

            # Save to CSV if not empty
            if not overall_stats_df.empty:
                csv_path = _get_csv_path_for_player_analysis(
                    player_id=player_id,
                    season=season,
                    season_type=season_type,
                    per_mode=per_mode,
                    league_id=league_id
                )
                _save_dataframe_to_csv(overall_stats_df, csv_path)

                # Add relative path to response
                relative_path = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "player_analysis"
                )

                response_payload["dataframe_info"] = {
                    "message": "Player analysis data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "overall_dashboard": {
                            "shape": list(overall_stats_df.shape),
                            "columns": overall_stats_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

        logger.info(f"analyze_player_stats_logic completed for {player_actual_name}")

        if return_dataframe:
            return format_response(response_payload), dataframes
        return format_response(response_payload)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in analyze_player_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, dataframes
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError (e.g., empty player name) in analyze_player_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, dataframes
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error analyzing stats for player '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_ANALYSIS_UNEXPECTED.format(identifier=player_name, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\assist_leaders.py =====
"""
Handles fetching and processing assist leaders statistics
from the AssistLeaders endpoint.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import assistleaders
from nba_api.stats.library.parameters import SeasonTypeAllStar
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2023-24"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
ASSIST_LEADERS_CACHE_SIZE = 32

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", "20"}  # NBA, WNBA, G-League
VALID_SEASON_TYPES: Set[str] = {
    SeasonTypeAllStar.regular,
    SeasonTypeAllStar.playoffs,
    SeasonTypeAllStar.preseason,
    SeasonTypeAllStar.all_star
}
VALID_PER_MODES: Set[str] = {"Totals", "PerGame"}
VALID_PLAYER_OR_TEAM: Set[str] = {"Team", "Player"}

# --- Cache Directory Setup ---
ASSIST_LEADERS_CSV_DIR = get_cache_dir("assist_leaders")

# Ensure cache directories exist
os.makedirs(ASSIST_LEADERS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_assist_leaders(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = "Totals",
    player_or_team: str = "Team"
) -> str:
    """
    Generates a file path for saving assist leaders DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        season_type: Season type (default: "Regular Season")
        per_mode: Per mode (default: "Totals")
        player_or_team: Player or Team (default: "Team")

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    safe_season_type = season_type.replace(" ", "_")
    filename = f"assist_leaders_league{league_id}_season{season}_type{safe_season_type}_mode{per_mode}_{player_or_team.lower()}.csv"

    return get_cache_file_path(filename, "assist_leaders")

# --- Parameter Validation ---
def _validate_assist_leaders_params(
    league_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    player_or_team: str
) -> Optional[str]:
    """Validates parameters for fetch_assist_leaders_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type not in VALID_SEASON_TYPES:
        return f"Invalid season_type: {season_type}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if per_mode not in VALID_PER_MODES:
        return f"Invalid per_mode: {per_mode}. Valid options: {', '.join(VALID_PER_MODES)}"
    if player_or_team not in VALID_PLAYER_OR_TEAM:
        return f"Invalid player_or_team: {player_or_team}. Valid options: {', '.join(VALID_PLAYER_OR_TEAM)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=ASSIST_LEADERS_CACHE_SIZE)
def fetch_assist_leaders_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = "Totals",
    player_or_team: str = "Team",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches assist leaders statistics using the AssistLeaders endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type: Type of season (default: "Regular Season")
        per_mode: Per mode (default: "Totals")
        player_or_team: Player or Team (default: "Team")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with assist leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_assist_leaders_logic for League: {league_id}, Season: {season}, "
        f"Season Type: {season_type}, Per Mode: {per_mode}, Player/Team: {player_or_team}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_assist_leaders_params(league_id, season, season_type, per_mode, player_or_team)
    if validation_error:
        logger.warning(f"Parameter validation failed for assist leaders: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_assist_leaders(league_id, season, season_type, per_mode, player_or_team)
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 10:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading assist leaders from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season,
                        "season_type_playoffs": season_type,
                        "per_mode_simple": per_mode,
                        "player_or_team": player_or_team
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["AssistLeaders"] = df
                result_dict["data_sets"]["AssistLeaders"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season,
            "season_type_playoffs": season_type,
            "per_mode_simple": per_mode,
            "player_or_team": player_or_team
        }

        logger.debug(f"Calling AssistLeaders with parameters: {api_params}")
        assist_leaders_endpoint = assistleaders.AssistLeaders(**api_params)

        # Get data frames
        list_of_dataframes = assist_leaders_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"AssistLeaders_{idx}" if idx > 0 else "AssistLeaders"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            _save_dataframe_to_csv(combined_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_assist_leaders_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_assist_leaders(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = "Totals",
    player_or_team: str = "Team",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get assist leaders data.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type: Type of season (default: "Regular Season")
        per_mode: Per mode (default: "Totals")
        player_or_team: Player or Team (default: "Team")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with assist leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_assist_leaders_logic(
        league_id=league_id,
        season=season,
        season_type=season_type,
        per_mode=per_mode,
        player_or_team=player_or_team,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing AssistLeaders endpoint...")

    # Test 1: Basic fetch
    json_response = get_assist_leaders()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_assist_leaders(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("AssistLeaders endpoint test completed.")


===== backend\api_tools\comprehensive_analytics.py =====
"""
Comprehensive NBA Analytics Engine
Combines multiple data sources to provide advanced analytics similar to dunksandthrees.com, craftednba.com, etc.
"""

import logging
import json
import asyncio
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np
from functools import lru_cache
from datetime import datetime

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import settings
from core.errors import Errors
from api_tools.utils import format_response
from api_tools.league_dash_player_stats import fetch_league_player_stats_logic
from api_tools.league_dash_team_stats import fetch_league_team_stats_logic
from api_tools.player_estimated_metrics import fetch_player_estimated_metrics_logic
from api_tools.team_estimated_metrics import fetch_team_estimated_metrics_logic
from utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# Cache directory for comprehensive analytics
COMPREHENSIVE_ANALYTICS_CSV_DIR = get_cache_dir("comprehensive_analytics")

class AdvancedAnalyticsEngine:
    """
    Advanced analytics engine that combines multiple NBA data sources
    to create comprehensive player and team evaluations.
    """

    def __init__(self, season: str = settings.CURRENT_NBA_SEASON):
        self.season = season
        self.league_averages = {}
        self.player_data = {}
        self.team_data = {}
        self.estimated_metrics = {}

    async def load_league_data(self) -> Dict[str, Any]:
        """
        Load comprehensive league-wide data for efficient processing.
        This reduces API calls by fetching all data at once.
        """
        try:
            logger.info(f"Loading comprehensive league data for season {self.season}")

            # Fetch all league data in parallel
            tasks = [
                # Basic player stats
                asyncio.to_thread(fetch_league_player_stats_logic,
                                season=self.season, measure_type="Base", per_mode="PerGame"),
                # Advanced player stats
                asyncio.to_thread(fetch_league_player_stats_logic,
                                season=self.season, measure_type="Advanced", per_mode="PerGame"),
                # Player estimated metrics
                asyncio.to_thread(fetch_player_estimated_metrics_logic,
                                season=self.season),
                # Team stats
                asyncio.to_thread(fetch_league_team_stats_logic,
                                season=self.season, measure_type="Advanced", per_mode="PerGame"),
                # Team estimated metrics
                asyncio.to_thread(fetch_team_estimated_metrics_logic,
                                season=self.season)
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Process results
            basic_players_json, advanced_players_json, player_metrics_json, team_stats_json, team_metrics_json = results

            # Parse JSON responses
            basic_players = json.loads(basic_players_json) if isinstance(basic_players_json, str) else basic_players_json
            advanced_players = json.loads(advanced_players_json) if isinstance(advanced_players_json, str) else advanced_players_json
            player_metrics = json.loads(player_metrics_json) if isinstance(player_metrics_json, str) else player_metrics_json
            team_stats = json.loads(team_stats_json) if isinstance(team_stats_json, str) else team_stats_json
            team_metrics = json.loads(team_metrics_json) if isinstance(team_metrics_json, str) else team_metrics_json

            # Store processed data
            self.player_data = {
                'basic': basic_players,
                'advanced': advanced_players,
                'estimated_metrics': player_metrics
            }

            self.team_data = {
                'stats': team_stats,
                'estimated_metrics': team_metrics
            }

            # Calculate league averages
            self._calculate_league_averages()

            logger.info("Successfully loaded comprehensive league data")
            return {
                'status': 'success',
                'season': self.season,
                'players_loaded': len(self._get_player_list()),
                'teams_loaded': len(self._get_team_list()),
                'league_averages': self.league_averages
            }

        except Exception as e:
            logger.error(f"Error loading league data: {e}", exc_info=True)
            return {
                'status': 'error',
                'error': str(e)
            }

    def _calculate_league_averages(self):
        """Calculate league averages for various metrics."""
        try:
            # Extract basic player stats for league averages
            basic_data = self.player_data.get('basic', {})
            if 'data_sets' in basic_data and 'LeagueDashPlayerStats' in basic_data['data_sets']:
                players = basic_data['data_sets']['LeagueDashPlayerStats']

                if players:
                    # Calculate averages for key metrics
                    df = pd.DataFrame(players)

                    # Filter for players with meaningful minutes (>= 10 MPG)
                    if 'MIN' in df.columns:
                        qualified_players = df[df['MIN'] >= 10.0]
                    else:
                        qualified_players = df

                    if not qualified_players.empty:
                        self.league_averages = {
                            'PPG': qualified_players['PTS'].mean() if 'PTS' in qualified_players.columns else 0,
                            'RPG': qualified_players['REB'].mean() if 'REB' in qualified_players.columns else 0,
                            'APG': qualified_players['AST'].mean() if 'AST' in qualified_players.columns else 0,
                            'FG_PCT': qualified_players['FG_PCT'].mean() if 'FG_PCT' in qualified_players.columns else 0,
                            'FG3_PCT': qualified_players['FG3_PCT'].mean() if 'FG3_PCT' in qualified_players.columns else 0,
                            'FT_PCT': qualified_players['FT_PCT'].mean() if 'FT_PCT' in qualified_players.columns else 0,
                            'MPG': qualified_players['MIN'].mean() if 'MIN' in qualified_players.columns else 0,
                            'qualified_players': len(qualified_players)
                        }

                        logger.info(f"Calculated league averages for {len(qualified_players)} qualified players")

        except Exception as e:
            logger.error(f"Error calculating league averages: {e}", exc_info=True)
            # Set default league averages
            self.league_averages = {
                'PPG': 11.2, 'RPG': 4.1, 'APG': 2.4,
                'FG_PCT': 0.462, 'FG3_PCT': 0.349, 'FT_PCT': 0.783,
                'MPG': 20.5, 'qualified_players': 0
            }

    def _get_player_list(self) -> List[Dict[str, Any]]:
        """Get list of all players from loaded data."""
        try:
            basic_data = self.player_data.get('basic', {})
            if 'data_sets' in basic_data and 'LeagueDashPlayerStats' in basic_data['data_sets']:
                return basic_data['data_sets']['LeagueDashPlayerStats']
            return []
        except:
            return []

    def _get_team_list(self) -> List[Dict[str, Any]]:
        """Get list of all teams from loaded data."""
        try:
            team_data = self.team_data.get('stats', {})
            if 'data_sets' in team_data and 'LeagueDashTeamStats' in team_data['data_sets']:
                return team_data['data_sets']['LeagueDashTeamStats']
            return []
        except:
            return []

    def calculate_advanced_player_metrics(self, player_id: int) -> Dict[str, Any]:
        """
        Calculate advanced metrics for a specific player.
        Similar to EPM, RAPTOR, and other advanced metrics.
        """
        try:
            # Find player in basic and advanced stats
            basic_stats = self._find_player_stats(player_id, 'basic')
            advanced_stats = self._find_player_stats(player_id, 'advanced')
            estimated_metrics = self._find_player_estimated_metrics(player_id)

            if not basic_stats:
                return {'error': f'Player {player_id} not found in basic stats'}

            # Calculate custom advanced metrics
            metrics = {}

            # Basic efficiency metrics
            if basic_stats.get('MIN', 0) > 0:
                # Points per minute
                metrics['PPM'] = basic_stats.get('PTS', 0) / basic_stats.get('MIN', 1)
                # Rebounds per minute
                metrics['RPM'] = basic_stats.get('REB', 0) / basic_stats.get('MIN', 1)
                # Assists per minute
                metrics['APM'] = basic_stats.get('AST', 0) / basic_stats.get('MIN', 1)

            # True Shooting Percentage
            pts = basic_stats.get('PTS', 0)
            fga = basic_stats.get('FGA', 0)
            fta = basic_stats.get('FTA', 0)
            if fga > 0 or fta > 0:
                metrics['TS_PCT'] = pts / (2 * (fga + 0.44 * fta)) if (fga + 0.44 * fta) > 0 else 0

            # Effective Field Goal Percentage
            fgm = basic_stats.get('FGM', 0)
            fg3m = basic_stats.get('FG3M', 0)
            if fga > 0:
                metrics['EFG_PCT'] = (fgm + 0.5 * fg3m) / fga

            # Usage Rate (simplified)
            if advanced_stats and 'USG_PCT' in advanced_stats:
                metrics['USG_PCT'] = advanced_stats['USG_PCT']

            # Player Impact Estimate (PIE) - simplified version
            if advanced_stats and 'PIE' in advanced_stats:
                metrics['PIE'] = advanced_stats['PIE']

            # Add estimated metrics if available
            if estimated_metrics:
                metrics.update({
                    'E_OFF_RATING': estimated_metrics.get('E_OFF_RATING', 0),
                    'E_DEF_RATING': estimated_metrics.get('E_DEF_RATING', 0),
                    'E_NET_RATING': estimated_metrics.get('E_NET_RATING', 0),
                    'E_PACE': estimated_metrics.get('E_PACE', 0),
                    'E_USG_PCT': estimated_metrics.get('E_USG_PCT', 0)
                })

            # Calculate percentile rankings vs league
            metrics['percentiles'] = self._calculate_percentiles(basic_stats, advanced_stats)

            return metrics

        except Exception as e:
            logger.error(f"Error calculating advanced metrics for player {player_id}: {e}", exc_info=True)
            return {'error': str(e)}

    def _find_player_stats(self, player_id: int, stat_type: str) -> Optional[Dict[str, Any]]:
        """Find player stats in the loaded data."""
        try:
            data = self.player_data.get(stat_type, {})
            if 'data_sets' in data and 'LeagueDashPlayerStats' in data['data_sets']:
                players = data['data_sets']['LeagueDashPlayerStats']
                for player in players:
                    if player.get('PLAYER_ID') == player_id:
                        return player
            return None
        except:
            return None

    def _find_player_estimated_metrics(self, player_id: int) -> Optional[Dict[str, Any]]:
        """Find player estimated metrics in the loaded data."""
        try:
            data = self.player_data.get('estimated_metrics', {})
            if 'player_estimated_metrics' in data:
                players = data['player_estimated_metrics']
                for player in players:
                    if player.get('PLAYER_ID') == player_id:
                        return player
            return None
        except:
            return None

    def _calculate_percentiles(self, basic_stats: Dict[str, Any], advanced_stats: Optional[Dict[str, Any]]) -> Dict[str, float]:
        """Calculate percentile rankings for key stats."""
        try:
            all_players = self._get_player_list()
            if not all_players:
                return {}

            # Filter for qualified players (>= 10 MPG)
            qualified_players = [p for p in all_players if p.get('MIN', 0) >= 10.0]

            if not qualified_players:
                return {}

            percentiles = {}

            # Key stats to calculate percentiles for
            key_stats = ['PTS', 'REB', 'AST', 'FG_PCT', 'FG3_PCT', 'FT_PCT', 'STL', 'BLK', 'TOV']

            for stat in key_stats:
                if stat in basic_stats:
                    player_value = basic_stats[stat]
                    all_values = [p.get(stat, 0) for p in qualified_players if p.get(stat) is not None]

                    if all_values and player_value is not None:
                        # Calculate percentile (higher is better for most stats, except TOV)
                        if stat == 'TOV':
                            # For turnovers, lower is better
                            percentile = (sum(1 for v in all_values if v > player_value) / len(all_values)) * 100
                        else:
                            # For other stats, higher is better
                            percentile = (sum(1 for v in all_values if v < player_value) / len(all_values)) * 100

                        percentiles[f'{stat}_PERCENTILE'] = round(percentile, 1)

            return percentiles

        except Exception as e:
            logger.error(f"Error calculating percentiles: {e}", exc_info=True)
            return {}

# Global analytics engine instance
analytics_engine = AdvancedAnalyticsEngine()

async def get_comprehensive_league_data(season: str = settings.CURRENT_NBA_SEASON) -> Dict[str, Any]:
    """
    Get comprehensive league data for efficient team/player page loading.
    """
    global analytics_engine

    # Update season if different
    if analytics_engine.season != season:
        analytics_engine = AdvancedAnalyticsEngine(season)

    # Load data if not already loaded
    result = await analytics_engine.load_league_data()

    return {
        'season': season,
        'data_status': result,
        'league_averages': analytics_engine.league_averages,
        'players_count': len(analytics_engine._get_player_list()),
        'teams_count': len(analytics_engine._get_team_list())
    }

async def get_player_advanced_analytics(player_id: int, season: str = settings.CURRENT_NBA_SEASON) -> Dict[str, Any]:
    """
    Get advanced analytics for a specific player.
    """
    global analytics_engine

    # Ensure data is loaded
    if analytics_engine.season != season or not analytics_engine.player_data:
        await analytics_engine.load_league_data()

    # Calculate advanced metrics
    metrics = analytics_engine.calculate_advanced_player_metrics(player_id)

    return {
        'player_id': player_id,
        'season': season,
        'advanced_metrics': metrics,
        'league_averages': analytics_engine.league_averages
    }


===== backend\api_tools\contracts_data.py =====
"""
NBA player contract data API tools.
Fetches data from clean CSV files with NBA API ID mappings.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Any, Dict, Optional, Union, List, Tuple
from functools import lru_cache
import pandas as pd

from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Cache directory setup
CONTRACTS_CSV_DIR = get_cache_dir("contracts")
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
CONTRACTS_CSV = os.path.join(DATA_DIR, "contracts_clean.csv")

def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """Saves a DataFrame to a CSV file."""
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _load_contracts_data() -> pd.DataFrame:
    """Load the clean contracts CSV data."""
    if not os.path.exists(CONTRACTS_CSV):
        raise FileNotFoundError(f"Clean contracts file not found: {CONTRACTS_CSV}")

    df = pd.read_csv(CONTRACTS_CSV)
    logger.info(f"Loaded {len(df)} contract records")
    return df

@lru_cache(maxsize=32)
def fetch_contracts_data_logic(
    player_id: Optional[int] = None,
    team_id: Optional[int] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA player contract data with optional filtering.

    Args:
        player_id: Optional NBA API player ID to filter by
        team_id: Optional NBA API team ID to filter by
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        # Generate cache filename based on filters
        cache_parts = ["contracts"]
        if player_id:
            cache_parts.append(f"player_{player_id}")
        if team_id:
            cache_parts.append(f"team_{team_id}")

        cache_filename = "_".join(cache_parts) + ".csv"
        cache_path = get_cache_file_path(cache_filename, "contracts")

        # Try to load from cache first
        if os.path.exists(cache_path):
            try:
                df = pd.read_csv(cache_path)
                logger.info(f"Loaded cached contracts data: {len(df)} records")
            except Exception as e:
                logger.warning(f"Error reading cache, loading fresh data: {e}")
                df = _load_contracts_data()
        else:
            df = _load_contracts_data()

            # Apply filters
            if player_id:
                df = df[df['nba_player_id'] == player_id]
            if team_id:
                df = df[df['nba_team_id'] == team_id]

            # Cache the filtered results
            _save_dataframe_to_csv(df, cache_path)

        # Process data for JSON response
        data_sets = {
            "contracts": _process_dataframe(df)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "player_id": player_id,
                "team_id": team_id
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"contracts": df}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error fetching contracts data: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

def get_player_contract(player_id: int, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets contract information for a specific player.

    Args:
        player_id: NBA API player ID
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    return fetch_contracts_data_logic(player_id=player_id, return_dataframe=return_dataframe)

def get_team_payroll(team_id: int, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets payroll summary for a team.

    Args:
        team_id: NBA API team ID
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    return fetch_contracts_data_logic(team_id=team_id, return_dataframe=return_dataframe)

def get_highest_paid_players(limit: int = 50, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets the highest paid players by guaranteed money.

    Args:
        limit: Maximum number of players to return
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        cache_filename = f"highest_paid_players_limit_{limit}.csv"
        cache_path = get_cache_file_path(cache_filename, "contracts")

        # Try to load from cache first
        if os.path.exists(cache_path):
            try:
                df = pd.read_csv(cache_path)
                logger.info(f"Loaded cached highest paid players: {len(df)} records")
            except Exception as e:
                logger.warning(f"Error reading cache, loading fresh data: {e}")
                df = _load_contracts_data()
                # Filter and sort
                df = df[df['Guaranteed'].notna()].sort_values('Guaranteed', ascending=False).head(limit)
                _save_dataframe_to_csv(df, cache_path)
        else:
            df = _load_contracts_data()
            # Filter and sort
            df = df[df['Guaranteed'].notna()].sort_values('Guaranteed', ascending=False).head(limit)
            _save_dataframe_to_csv(df, cache_path)

        # Process data for JSON response
        data_sets = {
            "highest_paid_players": _process_dataframe(df)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "limit": limit
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"highest_paid_players": df}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error getting highest paid players: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

def search_player_contracts(player_name: str, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Search for player contracts by name.

    Args:
        player_name: Player name to search for (partial matches allowed)
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        df = _load_contracts_data()

        # Search for players with names containing the search term
        mask = df['Player'].str.contains(player_name, case=False, na=False)
        df_filtered = df[mask]

        # Sort by guaranteed money descending
        df_filtered = df_filtered.sort_values('Guaranteed', ascending=False, na_position='last')

        # Process data for JSON response
        data_sets = {
            "player_contracts": _process_dataframe(df_filtered)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "player_name": player_name
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"player_contracts": df_filtered}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error searching player contracts: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

if __name__ == "__main__":
    # Test basic functionality
    print("Testing Contracts Data endpoint...")

    # Test 1: Basic fetch
    json_response = fetch_contracts_data_logic()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = fetch_contracts_data_logic(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("Contracts Data endpoint test completed.")


===== backend\api_tools\draft_combine_drills.py =====
"""
NBA API tools for accessing draft combine drill results data.

This module provides functions to fetch and process data from the NBA's draft combine
drill results endpoint, which includes physical testing metrics such as:
- Standing vertical leap
- Max vertical leap
- Lane agility time
- Modified lane agility time
- Three-quarter sprint
- Bench press

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import draftcombinedrillresults
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_DRILLS_CACHE_SIZE = 128
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}


def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with data types preserved
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_draft_combine_drills(season_year: str, league_id: str = "00") -> str:
    """
    Generates a file path for saving draft combine drill results DataFrame.

    Args:
        season_year: Season year in YYYY format
        league_id: League ID (default: "00" for NBA)

    Returns:
        Path to the CSV file
    """
    filename = f"draft_combine_drills_{season_year}_{league_id}.csv"
    return get_cache_file_path(filename, "draft_combine")


# --- Parameter Validation Functions ---
def _validate_draft_combine_drills_params(
    season_year: str,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the draft combine drills function.

    Args:
        season_year: Season year in YYYY format
        league_id: League ID (e.g., "00" for NBA)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season_year:
        return "Season year cannot be empty"

    # Validate season format
    season_error = _validate_season_format(season_year)
    if season_error:
        return season_error

    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_DRILLS_CACHE_SIZE)
def fetch_draft_combine_drills_logic(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches draft combine drill results data from the NBA API.

    This endpoint provides physical testing metrics from the NBA Draft Combine:
    - Standing vertical leap
    - Max vertical leap
    - Lane agility time
    - Modified lane agility time
    - Three-quarter sprint
    - Bench press

    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine drill results data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_draft_combine_drills_logic for: Season: {season_year}, League: {league_id}")

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_draft_combine_drills_params(season_year, league_id)
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_draft_combine_drills(season_year, league_id)

    if os.path.exists(csv_path) and return_dataframe:
        try:
            logger.info(f"Loading draft combine drill results from CSV: {csv_path}")
            # Read CSV with appropriate data types
            df = pd.read_csv(csv_path)

            # Convert numeric columns to appropriate types
            numeric_columns = [
                "STANDING_VERTICAL_LEAP", "MAX_VERTICAL_LEAP",
                "LANE_AGILITY_TIME", "MODIFIED_LANE_AGILITY_TIME",
                "THREE_QUARTER_SPRINT", "BENCH_PRESS"
            ]

            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # Store DataFrame
            dataframes["Results"] = df

            # Process for JSON response
            result_dict = {
                "parameters": {
                    "season_year": season_year,
                    "league_id": league_id
                },
                "data_sets": {
                    "Results": _process_dataframe(df, single_row=False)
                }
            }

            if return_dataframe:
                return format_response(result_dict), dataframes
            return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "season_year": season_year
    }

    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling DraftCombineDrillResults with parameters: {api_params}")
        drill_results = draftcombinedrillresults.DraftCombineDrillResults(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = drill_results.get_normalized_dict()

        # Get data frames
        list_of_dataframes = drill_results.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "Results"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched draft combine drill results for {season_year}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(f"API error in fetch_draft_combine_drills_logic: {e}", exc_info=True)
        error_msg = f"Error fetching draft combine drill results for season {season_year}: {str(e)}"

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_draft_combine_drills(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets draft combine drill results data.

    This function is the main entry point for fetching draft combine drill results data.
    It calls the fetch_draft_combine_drills_logic function and returns the results.

    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine drill results data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_draft_combine_drills_logic(
        season_year=season_year,
        league_id=league_id,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\draft_combine_drill_results.py =====
"""
Handles fetching and processing NBA Draft Combine drill results data
from the DraftCombineDrillResults endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The DraftCombineDrillResults endpoint provides comprehensive NBA Draft Combine athletic measurements:
- Player info: ID, names, position (6 columns)
- Athletic measurements: vertical leap, agility times, sprint times, bench press (6 columns)
- Data available for multiple years (2021: 74 players, 2022: 83 players, 2023: 81 players, 2024: 83 players)
- Perfect for pre-draft player analysis and athletic evaluation
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import draftcombinedrillresults
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_year_format(season_year):
    """Validate season year format (YYYY)."""
    if not season_year:
        return False
    
    try:
        year = int(season_year)
        # Check if it's a reasonable year (2000-2030)
        return 2000 <= year <= 2030
    except ValueError:
        return False

# Default current NBA season year
CURRENT_NBA_SEASON_YEAR = "2024"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_CACHE_SIZE = 32

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine_drill_results")

# Ensure cache directories exist
os.makedirs(DRAFT_COMBINE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_draft_combine(
    league_id: str = "00",
    season_year: str = CURRENT_NBA_SEASON_YEAR
) -> str:
    """
    Generates a file path for saving draft combine drill results DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_year: Season year in YYYY format
        
    Returns:
        Path to the CSV file
    """
    filename = f"draft_combine_drill_results_league{league_id}_season{season_year}.csv"
    return get_cache_file_path(filename, "draft_combine_drill_results")

# --- Parameter Validation ---
def _validate_draft_combine_params(
    league_id: str,
    season_year: str
) -> Optional[str]:
    """Validates parameters for fetch_draft_combine_drill_results_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season_year or not _validate_season_year_format(season_year):
        return f"Invalid season_year format: {season_year}. Expected format: YYYY"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_CACHE_SIZE)
def fetch_draft_combine_drill_results_logic(
    league_id: str = "00",
    season_year: str = CURRENT_NBA_SEASON_YEAR,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA Draft Combine drill results data using the DraftCombineDrillResults endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_year: Season year in YYYY format (default: current NBA season year)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine drill results data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_draft_combine_drill_results_logic for League: {league_id}, "
        f"Season Year: {season_year}, return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_draft_combine_params(league_id, season_year)
    if validation_error:
        logger.warning(f"Parameter validation failed for draft combine drill results: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_draft_combine(league_id, season_year)
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading draft combine drill results from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season_year": season_year
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["DraftCombineDrillResults"] = df
                result_dict["data_sets"]["DraftCombineDrillResults"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season_year": season_year
        }
        
        logger.debug(f"Calling DraftCombineDrillResults with parameters: {api_params}")
        draft_combine_endpoint = draftcombinedrillresults.DraftCombineDrillResults(**api_params)
        
        # Get data frames
        list_of_dataframes = draft_combine_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"DraftCombineDrillResults_{idx}" if idx > 0 else "DraftCombineDrillResults"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_draft_combine_drill_results_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_draft_combine_drill_results(
    league_id: str = "00",
    season_year: str = CURRENT_NBA_SEASON_YEAR,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get NBA Draft Combine drill results data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_year: Season year in YYYY format (default: current NBA season year)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine drill results data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_draft_combine_drill_results_logic(
        league_id=league_id,
        season_year=season_year,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing DraftCombineDrillResults endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_draft_combine_drill_results()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_draft_combine_drill_results(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("DraftCombineDrillResults endpoint test completed.")


===== backend\api_tools\draft_combine_nonshooting.py =====
"""
NBA API tools for accessing draft combine non-stationary shooting data.

This module provides functions to fetch and process data from the NBA's draft combine
non-stationary shooting endpoint, which includes shooting metrics such as:
- Off-dribble shooting from different positions (15-foot and college range)
- On-the-move shooting (15-foot and college range)
- Shooting percentages and attempts

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import draftcombinenonstationaryshooting
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_NONSHOOTING_CACHE_SIZE = 128
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}


def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_draft_combine_nonshooting(season_year: str, league_id: str = "00") -> str:
    """
    Generates a file path for saving draft combine non-stationary shooting DataFrame.

    Args:
        season_year: Season year in YYYY format
        league_id: League ID (default: "00" for NBA)

    Returns:
        Path to the CSV file
    """
    filename = f"draft_combine_nonshooting_{season_year}_{league_id}.csv"
    return get_cache_file_path(filename, "draft_combine")


# --- Parameter Validation Functions ---
def _validate_draft_combine_nonshooting_params(
    season_year: str,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the draft combine non-stationary shooting function.

    Args:
        season_year: Season year in YYYY format
        league_id: League ID (e.g., "00" for NBA)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season_year:
        return "Season year cannot be empty"

    # Validate season format
    season_error = _validate_season_format(season_year)
    if season_error:
        return season_error

    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_NONSHOOTING_CACHE_SIZE)
def fetch_draft_combine_nonshooting_logic(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches draft combine non-stationary shooting data from the NBA API.

    This endpoint provides shooting metrics from the NBA Draft Combine:
    - Off-dribble shooting from different positions (15-foot and college range)
    - On-the-move shooting (15-foot and college range)
    - Shooting percentages and attempts

    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine non-stationary shooting data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_draft_combine_nonshooting_logic for: Season: {season_year}, League: {league_id}")

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_draft_combine_nonshooting_params(season_year, league_id)
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_draft_combine_nonshooting(season_year, league_id)

    if os.path.exists(csv_path) and return_dataframe:
        try:
            logger.info(f"Loading draft combine non-stationary shooting from CSV: {csv_path}")
            # Read CSV with appropriate data types
            df = pd.read_csv(csv_path)

            # Convert numeric columns to appropriate types
            numeric_columns = [
                col for col in df.columns
                if any(x in col for x in ["MADE", "ATTEMPT", "PCT"])
            ]

            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # Store DataFrame
            dataframes["Results"] = df

            # Process for JSON response
            result_dict = {
                "parameters": {
                    "season_year": season_year,
                    "league_id": league_id
                },
                "data_sets": {
                    "Results": _process_dataframe(df, single_row=False)
                }
            }

            if return_dataframe:
                return format_response(result_dict), dataframes
            return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "season_year": season_year
    }

    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling DraftCombineNonStationaryShooting with parameters: {api_params}")
        nonshooting_results = draftcombinenonstationaryshooting.DraftCombineNonStationaryShooting(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = nonshooting_results.get_normalized_dict()

        # Get data frames
        list_of_dataframes = nonshooting_results.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "Results"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched draft combine non-stationary shooting for {season_year}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(f"API error in fetch_draft_combine_nonshooting_logic: {e}", exc_info=True)
        error_msg = f"Error fetching draft combine non-stationary shooting for season {season_year}: {str(e)}"

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_draft_combine_nonshooting(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets draft combine non-stationary shooting data.

    This function is the main entry point for fetching draft combine non-stationary shooting data.
    It calls the fetch_draft_combine_nonshooting_logic function and returns the results.

    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine non-stationary shooting data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_draft_combine_nonshooting_logic(
        season_year=season_year,
        league_id=league_id,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\draft_combine_player_anthro.py =====
"""
NBA API tools for accessing draft combine player anthropometric data.

This module provides functions to fetch and process data from the NBA's draft combine
player anthropometric endpoint, which includes physical measurements such as:
- Height (with and without shoes)
- Weight
- Wingspan
- Standing reach
- Body fat percentage
- Hand length and width

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import draftcombineplayeranthro
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []
    
    if single_row:
        return df.iloc[0].to_dict()
    
    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_PLAYER_ANTHRO_CACHE_SIZE = 128
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_draft_combine_player_anthro(season_year: str, league_id: str = "00") -> str:
    """
    Generates a file path for saving draft combine player anthropometric DataFrame.
    
    Args:
        season_year: Season year in YYYY format
        league_id: League ID (default: "00" for NBA)
        
    Returns:
        Path to the CSV file
    """
    filename = f"draft_combine_player_anthro_{season_year}_{league_id}.csv"
    return get_cache_file_path(filename, "draft_combine")

# --- Parameter Validation Functions ---
def _validate_draft_combine_player_anthro_params(
    season_year: str,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the draft combine player anthropometric function.
    
    Args:
        season_year: Season year in YYYY format
        league_id: League ID (e.g., "00" for NBA)
        
    Returns:
        Error message if validation fails, None otherwise
    """
    if not season_year:
        return "Season year cannot be empty"
    
    # Validate season format
    season_error = _validate_season_format(season_year)
    if season_error:
        return season_error
    
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"
    
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_PLAYER_ANTHRO_CACHE_SIZE)
def fetch_draft_combine_player_anthro_logic(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches draft combine player anthropometric data from the NBA API.
    
    This endpoint provides physical measurements from the NBA Draft Combine:
    - Height (with and without shoes)
    - Weight
    - Wingspan
    - Standing reach
    - Body fat percentage
    - Hand length and width
    
    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine player anthropometric data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_draft_combine_player_anthro_logic for: Season: {season_year}, League: {league_id}")
    
    dataframes: Dict[str, pd.DataFrame] = {}
    
    # Validate parameters
    validation_error = _validate_draft_combine_player_anthro_params(season_year, league_id)
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)
    
    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_draft_combine_player_anthro(season_year, league_id)
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            logger.info(f"Loading draft combine player anthropometric data from CSV: {csv_path}")
            # Read CSV with appropriate data types
            df = pd.read_csv(csv_path)
            
            # Convert numeric columns to appropriate types
            numeric_columns = [
                "HEIGHT_WO_SHOES", "HEIGHT_W_SHOES", "WEIGHT", 
                "WINGSPAN", "STANDING_REACH", "BODY_FAT_PCT",
                "HAND_LENGTH", "HAND_WIDTH"
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Store DataFrame
            dataframes["Results"] = df
            
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "season_year": season_year,
                    "league_id": league_id
                },
                "data_sets": {
                    "Results": _process_dataframe(df, single_row=False)
                }
            }
            
            if return_dataframe:
                return format_response(result_dict), dataframes
            return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "season_year": season_year
    }
    
    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling DraftCombinePlayerAnthro with parameters: {api_params}")
        player_anthro_results = draftcombineplayeranthro.DraftCombinePlayerAnthro(**api_params)
        
        # Get normalized dictionary for data set names
        normalized_dict = player_anthro_results.get_normalized_dict()
        
        # Get data frames
        list_of_dataframes = player_anthro_results.get_data_frames()
        
        # Expected data set name based on documentation
        expected_data_set_name = "Results"
        
        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save to CSV if not empty
                    if not df.empty:
                        _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Return response
        logger.info(f"Successfully fetched draft combine player anthropometric data for {season_year}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)
        
    except Exception as e:
        logger.error(f"API error in fetch_draft_combine_player_anthro_logic: {e}", exc_info=True)
        error_msg = f"Error fetching draft combine player anthropometric data for season {season_year}: {str(e)}"
        
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_draft_combine_player_anthro(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets draft combine player anthropometric data.
    
    This function is the main entry point for fetching draft combine player anthropometric data.
    It calls the fetch_draft_combine_player_anthro_logic function and returns the results.
    
    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine player anthropometric data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_draft_combine_player_anthro_logic(
        season_year=season_year,
        league_id=league_id,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\draft_combine_spot_shooting.py =====
"""
NBA API tools for accessing draft combine spot shooting data.

This module provides functions to fetch and process data from the NBA's draft combine
spot shooting endpoint, which includes shooting metrics such as:
- 15-foot shooting from different spots (corner, break, top key)
- College range shooting from different spots
- NBA range shooting from different spots
- Shooting percentages and attempts

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import draftcombinespotshooting
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []
    
    if single_row:
        return df.iloc[0].to_dict()
    
    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_SPOT_SHOOTING_CACHE_SIZE = 128
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_draft_combine_spot_shooting(season_year: str, league_id: str = "00") -> str:
    """
    Generates a file path for saving draft combine spot shooting DataFrame.
    
    Args:
        season_year: Season year in YYYY format
        league_id: League ID (default: "00" for NBA)
        
    Returns:
        Path to the CSV file
    """
    filename = f"draft_combine_spot_shooting_{season_year}_{league_id}.csv"
    return get_cache_file_path(filename, "draft_combine")

# --- Parameter Validation Functions ---
def _validate_draft_combine_spot_shooting_params(
    season_year: str,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the draft combine spot shooting function.
    
    Args:
        season_year: Season year in YYYY format
        league_id: League ID (e.g., "00" for NBA)
        
    Returns:
        Error message if validation fails, None otherwise
    """
    if not season_year:
        return "Season year cannot be empty"
    
    # Validate season format
    season_error = _validate_season_format(season_year)
    if season_error:
        return season_error
    
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"
    
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_SPOT_SHOOTING_CACHE_SIZE)
def fetch_draft_combine_spot_shooting_logic(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches draft combine spot shooting data from the NBA API.
    
    This endpoint provides shooting metrics from the NBA Draft Combine:
    - 15-foot shooting from different spots (corner, break, top key)
    - College range shooting from different spots
    - NBA range shooting from different spots
    - Shooting percentages and attempts
    
    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine spot shooting data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_draft_combine_spot_shooting_logic for: Season: {season_year}, League: {league_id}")
    
    dataframes: Dict[str, pd.DataFrame] = {}
    
    # Validate parameters
    validation_error = _validate_draft_combine_spot_shooting_params(season_year, league_id)
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)
    
    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_draft_combine_spot_shooting(season_year, league_id)
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            logger.info(f"Loading draft combine spot shooting from CSV: {csv_path}")
            # Read CSV with appropriate data types
            df = pd.read_csv(csv_path)
            
            # Convert numeric columns to appropriate types
            numeric_columns = [
                col for col in df.columns 
                if any(x in col for x in ["MADE", "ATTEMPT", "PCT"])
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Store DataFrame
            dataframes["Results"] = df
            
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "season_year": season_year,
                    "league_id": league_id
                },
                "data_sets": {
                    "Results": _process_dataframe(df, single_row=False)
                }
            }
            
            if return_dataframe:
                return format_response(result_dict), dataframes
            return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "season_year": season_year
    }
    
    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling DraftCombineSpotShooting with parameters: {api_params}")
        spot_shooting_results = draftcombinespotshooting.DraftCombineSpotShooting(**api_params)
        
        # Get normalized dictionary for data set names
        normalized_dict = spot_shooting_results.get_normalized_dict()
        
        # Get data frames
        list_of_dataframes = spot_shooting_results.get_data_frames()
        
        # Expected data set name based on documentation
        expected_data_set_name = "Results"
        
        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save to CSV if not empty
                    if not df.empty:
                        _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Return response
        logger.info(f"Successfully fetched draft combine spot shooting for {season_year}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)
        
    except Exception as e:
        logger.error(f"API error in fetch_draft_combine_spot_shooting_logic: {e}", exc_info=True)
        error_msg = f"Error fetching draft combine spot shooting for season {season_year}: {str(e)}"
        
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_draft_combine_spot_shooting(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets draft combine spot shooting data.
    
    This function is the main entry point for fetching draft combine spot shooting data.
    It calls the fetch_draft_combine_spot_shooting_logic function and returns the results.
    
    Args:
        season_year (str): Season year in YYYY format (e.g., "2023")
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine spot shooting data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_draft_combine_spot_shooting_logic(
        season_year=season_year,
        league_id=league_id,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\draft_combine_stats.py =====
"""
NBA API tools for accessing comprehensive draft combine statistics.

This module provides functions to fetch and process data from the NBA's draft combine
stats endpoint, which includes comprehensive data such as:
- Anthropometric measurements (height, weight, wingspan, etc.)
- Physical testing results (vertical leap, agility, sprint, bench press)
- Shooting statistics (spot shooting, off-dribble shooting, on-move shooting)

The data is returned as pandas DataFrames and can be cached as CSV files for faster access.
"""

import os
import logging
import json
import re
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import draftcombinestats
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []
    
    if single_row:
        return df.iloc[0].to_dict()
    
    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Set up logging
logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
DRAFT_COMBINE_STATS_CACHE_SIZE = 128
DRAFT_COMBINE_CSV_DIR = get_cache_dir("draft_combine")

# Valid parameter values
VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)


def _get_csv_path_for_draft_combine_stats(season_year: str, league_id: str = "00") -> str:
    """
    Generates a file path for saving draft combine stats DataFrame.
    
    Args:
        season_year: Season year in YYYY-YY format (e.g., "2022-23") or "All Time"
        league_id: League ID (default: "00" for NBA)
        
    Returns:
        Path to the CSV file
    """
    # Sanitize season_year for filename (replace special characters)
    safe_season = season_year.replace("-", "_").replace(" ", "_")
    filename = f"draft_combine_stats_{safe_season}_{league_id}.csv"
    return get_cache_file_path(filename, "draft_combine")

# --- Parameter Validation Functions ---
def _validate_draft_combine_stats_params(
    season_year: str,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the draft combine stats function.
    
    Args:
        season_year: Season year in YYYY-YY format (e.g., "2022-23") or "All Time"
        league_id: League ID (e.g., "00" for NBA)
        
    Returns:
        Error message if validation fails, None otherwise
    """
    if not season_year:
        return "Season year cannot be empty"
    
    # Validate season format (YYYY-YY or "All Time")
    if season_year != "All Time" and not re.match(r"^\d{4}-\d{2}$", season_year):
        return f"Invalid season_year format: {season_year}. Expected format: YYYY-YY (e.g., '2022-23') or 'All Time'"
    
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options are: {', '.join(VALID_LEAGUE_IDS.keys())}"
    
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=DRAFT_COMBINE_STATS_CACHE_SIZE)
def fetch_draft_combine_stats_logic(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches comprehensive draft combine statistics from the NBA API.
    
    This endpoint provides comprehensive data from the NBA Draft Combine:
    - Anthropometric measurements (height, weight, wingspan, etc.)
    - Physical testing results (vertical leap, agility, sprint, bench press)
    - Shooting statistics (spot shooting, off-dribble shooting, on-move shooting)
    
    Args:
        season_year (str): Season year in YYYY-YY format (e.g., "2022-23") or "All Time"
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine stats or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_draft_combine_stats_logic for: Season: {season_year}, League: {league_id}")
    
    dataframes: Dict[str, pd.DataFrame] = {}
    
    # Validate parameters
    validation_error = _validate_draft_combine_stats_params(season_year, league_id)
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)
    
    # Check if cached CSV file exists
    csv_path = _get_csv_path_for_draft_combine_stats(season_year, league_id)
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            logger.info(f"Loading draft combine stats from CSV: {csv_path}")
            # Read CSV with appropriate data types
            df = pd.read_csv(csv_path)
            
            # Convert numeric columns to appropriate types
            numeric_columns = [
                "HEIGHT_WO_SHOES", "HEIGHT_W_SHOES", "WEIGHT", 
                "WINGSPAN", "STANDING_REACH", "BODY_FAT_PCT",
                "HAND_LENGTH", "HAND_WIDTH", "STANDING_VERTICAL_LEAP",
                "MAX_VERTICAL_LEAP", "LANE_AGILITY_TIME", 
                "MODIFIED_LANE_AGILITY_TIME", "THREE_QUARTER_SPRINT",
                "BENCH_PRESS"
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Store DataFrame
            dataframes["DraftCombineStats"] = df
            
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "season_year": season_year,
                    "league_id": league_id
                },
                "data_sets": {
                    "DraftCombineStats": _process_dataframe(df, single_row=False)
                }
            }
            
            if return_dataframe:
                return format_response(result_dict), dataframes
            return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    # Prepare API parameters
    api_params = {
        "league_id": league_id,
        "season_all_time": season_year  # Note: The parameter name is different in this endpoint
    }
    
    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling DraftCombineStats with parameters: {api_params}")
        combine_stats_results = draftcombinestats.DraftCombineStats(**api_params)
        
        # Get normalized dictionary for data set names
        normalized_dict = combine_stats_results.get_normalized_dict()
        
        # Get data frames
        list_of_dataframes = combine_stats_results.get_data_frames()
        
        # Expected data set name based on documentation
        expected_data_set_name = "DraftCombineStats"
        
        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save to CSV if not empty
                    if not df.empty:
                        _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Return response
        logger.info(f"Successfully fetched draft combine stats for {season_year}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)
        
    except Exception as e:
        logger.error(f"API error in fetch_draft_combine_stats_logic: {e}", exc_info=True)
        error_msg = f"Error fetching draft combine stats for season {season_year}: {str(e)}"
        
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


def get_draft_combine_stats(
    season_year: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets comprehensive draft combine statistics.
    
    This function is the main entry point for fetching draft combine statistics.
    It calls the fetch_draft_combine_stats_logic function and returns the results.
    
    Args:
        season_year (str): Season year in YYYY-YY format (e.g., "2022-23") or "All Time"
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
        
    Returns:
        If return_dataframe=False:
            str: JSON string with draft combine stats or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    return fetch_draft_combine_stats_logic(
        season_year=season_year,
        league_id=league_id,
        return_dataframe=return_dataframe
    )


===== backend\api_tools\fantasy_widget.py =====
"""
Handles fetching and processing fantasy basketball widget data
from the FantasyWidget endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The FantasyWidget endpoint provides comprehensive fantasy basketball data (20 columns):
- Player info: ID, name, position, team (5 columns)
- Game stats: games played, minutes (2 columns)
- Fantasy points: FanDuel points, NBA fantasy points (2 columns)
- Traditional stats: PTS, REB, AST, BLK, STL, TOV, FG3M, FGA, FG_PCT, FTA, FT_PCT (11 columns)
- Multi-season data: 2021-22: 605 players, 2022-23: 539 players, 2023-24: 572 players, 2024-25: 569 players
- Perfect for fantasy basketball analysis, player evaluation, and DFS optimization
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import fantasywidget
from nba_api.stats.library.parameters import SeasonTypeAllStar
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
FANTASY_WIDGET_CACHE_SIZE = 32

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_ACTIVE_PLAYERS: Set[str] = {"Y", "N"}
VALID_TODAYS_PLAYERS: Set[str] = {"Y", "N"}
VALID_TODAYS_OPPONENT: Set[str] = {"0", "1"}
VALID_POSITIONS: Set[str] = {"F", "C", "G", "F-C", "F-G", "G-F", "C-F"}

# --- Cache Directory Setup ---
FANTASY_WIDGET_CSV_DIR = get_cache_dir("fantasy_widget")

# Ensure cache directories exist
os.makedirs(FANTASY_WIDGET_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_fantasy_widget(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    active_players: Optional[str] = None,
    last_n_games: Optional[int] = None,
    team_id: Optional[str] = None,
    position: Optional[str] = None
) -> str:
    """
    Generates a file path for saving fantasy widget DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        season_type: Season type (default: Regular Season)
        active_players: Active players filter (optional)
        last_n_games: Last N games filter (optional)
        team_id: Team ID filter (optional)
        position: Position filter (optional)

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"fantasy_widget_league{league_id}",
        f"season{season}",
        f"type{season_type.replace(' ', '_')}"
    ]

    # Add optional filters to filename
    if active_players:
        filename_parts.append(f"active{active_players}")
    if last_n_games and last_n_games > 0:
        filename_parts.append(f"lastN{last_n_games}")
    if team_id:
        filename_parts.append(f"team{team_id}")
    if position:
        filename_parts.append(f"pos{position}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "fantasy_widget")

# --- Parameter Validation ---
def _validate_fantasy_widget_params(
    league_id: str,
    season: str,
    season_type: str,
    active_players: Optional[str] = None,
    position: Optional[str] = None
) -> Optional[str]:
    """Validates parameters for fetch_fantasy_widget_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if active_players and active_players not in VALID_ACTIVE_PLAYERS:
        return f"Invalid active_players: {active_players}. Valid options: {', '.join(VALID_ACTIVE_PLAYERS)}"
    if position and position not in VALID_POSITIONS:
        return f"Invalid position: {position}. Valid options: {', '.join(VALID_POSITIONS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=FANTASY_WIDGET_CACHE_SIZE)
def fetch_fantasy_widget_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    active_players: str = "N",
    last_n_games: int = 0,
    todays_opponent: str = "0",
    todays_players: str = "N",
    team_id: Optional[str] = None,
    position: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches fantasy widget data using the FantasyWidget endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type: Season type (default: Regular Season)
        active_players: Active players only flag (default: "N")
        last_n_games: Last N games filter (default: 0)
        todays_opponent: Today's opponent filter (default: "0")
        todays_players: Today's players filter (default: "N")
        team_id: Team ID filter (optional)
        position: Position filter (optional)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with fantasy widget data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_fantasy_widget_logic for League: {league_id}, Season: {season}, "
        f"Season Type: {season_type}, Active: {active_players}, Last N Games: {last_n_games}, "
        f"Team: {team_id}, Position: {position}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_fantasy_widget_params(league_id, season, season_type, active_players, position)
    if validation_error:
        logger.warning(f"Parameter validation failed for fantasy widget: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_fantasy_widget(
        league_id=league_id,
        season=season,
        season_type=season_type,
        active_players=active_players if active_players != "N" else None,
        last_n_games=last_n_games if last_n_games > 0 else None,
        team_id=team_id,
        position=position
    )
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading fantasy widget from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season,
                        "season_type_all_star": season_type,
                        "active_players": active_players,
                        "last_n_games": last_n_games,
                        "todays_opponent": todays_opponent,
                        "todays_players": todays_players,
                        "team_id_nullable": team_id,
                        "position_nullable": position
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["FantasyWidget"] = df
                result_dict["data_sets"]["FantasyWidget"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters (only include non-None values)
        api_params = {
            "league_id": league_id,
            "season": season,
            "season_type_all_star": season_type,
            "active_players": active_players,
            "last_n_games": last_n_games,
            "todays_opponent": todays_opponent,
            "todays_players": todays_players
        }

        # Add optional parameters if provided
        if team_id:
            api_params["team_id_nullable"] = team_id
        if position:
            api_params["position_nullable"] = position

        logger.debug(f"Calling FantasyWidget with parameters: {api_params}")
        fantasy_widget_endpoint = fantasywidget.FantasyWidget(**api_params)

        # Get data frames
        list_of_dataframes = fantasy_widget_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"FantasyWidget_{idx}" if idx > 0 else "FantasyWidget"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            # Recalculate CSV path to ensure consistency
            save_csv_path = _get_csv_path_for_fantasy_widget(
                league_id=league_id,
                season=season,
                season_type=season_type,
                active_players=active_players if active_players != "N" else None,
                last_n_games=last_n_games if last_n_games > 0 else None,
                team_id=team_id,
                position=position
            )
            _save_dataframe_to_csv(combined_df, save_csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_fantasy_widget_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_fantasy_widget(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    active_players: str = "N",
    last_n_games: int = 0,
    todays_opponent: str = "0",
    todays_players: str = "N",
    team_id: Optional[str] = None,
    position: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get fantasy widget data.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type: Season type (default: Regular Season)
        active_players: Active players only flag (default: "N")
        last_n_games: Last N games filter (default: 0)
        todays_opponent: Today's opponent filter (default: "0")
        todays_players: Today's players filter (default: "N")
        team_id: Team ID filter (optional)
        position: Position filter (optional)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with fantasy widget data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_fantasy_widget_logic(
        league_id=league_id,
        season=season,
        season_type=season_type,
        active_players=active_players,
        last_n_games=last_n_games,
        todays_opponent=todays_opponent,
        todays_players=todays_players,
        team_id=team_id,
        position=position,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing FantasyWidget endpoint...")

    # Test 1: Basic fetch
    json_response = get_fantasy_widget()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_fantasy_widget(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("FantasyWidget endpoint test completed.")


===== backend\api_tools\franchise_history.py =====
"""
Handles fetching and processing franchise history data
from the FranchiseHistory endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The FranchiseHistory endpoint provides comprehensive franchise history data (15 columns):
- Team info: LEAGUE_ID, TEAM_ID, TEAM_CITY, TEAM_NAME (4 columns)
- Time period: START_YEAR, END_YEAR, YEARS (3 columns)
- Performance: GAMES, WINS, LOSSES, WIN_PCT (4 columns)
- Achievements: PO_APPEARANCES, DIV_TITLES, CONF_TITLES, LEAGUE_TITLES (4 columns)
- NBA: 74 franchise records (current and historical teams)
- WNBA: 22 franchise records
- Perfect for franchise analysis, team comparisons, and historical context
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import franchisehistory
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
FRANCHISE_HISTORY_CACHE_SIZE = 8

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
FRANCHISE_HISTORY_CSV_DIR = get_cache_dir("franchise_history")

# Ensure cache directories exist
os.makedirs(FRANCHISE_HISTORY_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_franchise_history(
    league_id: str = "00",
    data_set_name: str = "FranchiseHistory"
) -> str:
    """
    Generates a file path for saving franchise history DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    filename = f"franchise_history_league{league_id}_{data_set_name}.csv"
    return get_cache_file_path(filename, "franchise_history")

# --- Parameter Validation ---
def _validate_franchise_history_params(
    league_id: str
) -> Optional[str]:
    """Validates parameters for fetch_franchise_history_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=FRANCHISE_HISTORY_CACHE_SIZE)
def fetch_franchise_history_logic(
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches franchise history data using the FranchiseHistory endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise history data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_franchise_history_logic for League: {league_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_franchise_history_params(league_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for franchise history: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for both possible data sets
        data_set_names = ["FranchiseHistory", "DefunctTeams"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_franchise_history(league_id, data_set_name)
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading franchise history from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id
        }
        
        logger.debug(f"Calling FranchiseHistory with parameters: {api_params}")
        franchise_history_endpoint = franchisehistory.FranchiseHistory(**api_params)
        
        # Get data frames
        list_of_dataframes = franchise_history_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["FranchiseHistory", "DefunctTeams"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"FranchiseHistory_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_franchise_history(league_id, data_set_name)
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_franchise_history_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_franchise_history(
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get franchise history data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise history data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_franchise_history_logic(
        league_id=league_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing FranchiseHistory endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_franchise_history()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_franchise_history(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("FranchiseHistory endpoint test completed.")


===== backend\api_tools\franchise_leaders.py =====
"""
Handles fetching and processing franchise statistical leaders data
from the FranchiseLeaders endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The FranchiseLeaders endpoint provides comprehensive franchise statistical leaders data (16 columns):
- Team info: TEAM_ID (1 column)
- Points leaders: PTS, PTS_PERSON_ID, PTS_PLAYER (3 columns)
- Assists leaders: AST, AST_PERSON_ID, AST_PLAYER (3 columns)
- Rebounds leaders: REB, REB_PERSON_ID, REB_PLAYER (3 columns)
- Blocks leaders: BLK, BLK_PERSON_ID, BLK_PLAYER (3 columns)
- Steals leaders: STL, STL_PERSON_ID, STL_PLAYER (3 columns)
- Team-specific data: Each team has their all-time statistical leaders
- Perfect for franchise legends, historical context, and team comparisons
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import franchiseleaders
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_team_id(team_id):
    """Validate team ID format."""
    if not team_id:
        return False
    
    try:
        # Check if it's a valid team ID (should be a number)
        int(team_id)
        return True
    except ValueError:
        return False

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
FRANCHISE_LEADERS_CACHE_SIZE = 64

# Valid NBA team IDs (current teams)
VALID_TEAM_IDS: Set[str] = {
    "1610612737",  # Atlanta Hawks
    "1610612738",  # Boston Celtics
    "1610612751",  # Brooklyn Nets
    "1610612766",  # Charlotte Hornets
    "1610612741",  # Chicago Bulls
    "1610612739",  # Cleveland Cavaliers
    "1610612742",  # Dallas Mavericks
    "1610612743",  # Denver Nuggets
    "1610612765",  # Detroit Pistons
    "1610612744",  # Golden State Warriors
    "1610612745",  # Houston Rockets
    "1610612754",  # Indiana Pacers
    "1610612746",  # LA Clippers
    "1610612747",  # Los Angeles Lakers
    "1610612763",  # Memphis Grizzlies
    "1610612748",  # Miami Heat
    "1610612749",  # Milwaukee Bucks
    "1610612750",  # Minnesota Timberwolves
    "1610612740",  # New Orleans Pelicans
    "1610612752",  # New York Knicks
    "1610612760",  # Oklahoma City Thunder
    "1610612753",  # Orlando Magic
    "1610612755",  # Philadelphia 76ers
    "1610612756",  # Phoenix Suns
    "1610612757",  # Portland Trail Blazers
    "1610612758",  # Sacramento Kings
    "1610612759",  # San Antonio Spurs
    "1610612761",  # Toronto Raptors
    "1610612762",  # Utah Jazz
    "1610612764",  # Washington Wizards
}

# --- Cache Directory Setup ---
FRANCHISE_LEADERS_CSV_DIR = get_cache_dir("franchise_leaders")

# Ensure cache directories exist
os.makedirs(FRANCHISE_LEADERS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_franchise_leaders(
    team_id: str
) -> str:
    """
    Generates a file path for saving franchise leaders DataFrame.
    
    Args:
        team_id: Team ID
        
    Returns:
        Path to the CSV file
    """
    filename = f"franchise_leaders_team{team_id}.csv"
    return get_cache_file_path(filename, "franchise_leaders")

# --- Parameter Validation ---
def _validate_franchise_leaders_params(
    team_id: str
) -> Optional[str]:
    """Validates parameters for fetch_franchise_leaders_logic."""
    if not team_id:
        return "team_id is required"
    if not _validate_team_id(team_id):
        return f"Invalid team_id format: {team_id}. Expected numeric team ID"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=FRANCHISE_LEADERS_CACHE_SIZE)
def fetch_franchise_leaders_logic(
    team_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches franchise statistical leaders data using the FranchiseLeaders endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        team_id: Team ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_franchise_leaders_logic for Team: {team_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_franchise_leaders_params(team_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for franchise leaders: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_franchise_leaders(team_id)
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading franchise leaders from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "team_id": team_id
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["FranchiseLeaders"] = df
                result_dict["data_sets"]["FranchiseLeaders"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "team_id": team_id
        }
        
        logger.debug(f"Calling FranchiseLeaders with parameters: {api_params}")
        franchise_leaders_endpoint = franchiseleaders.FranchiseLeaders(**api_params)
        
        # Get data frames
        list_of_dataframes = franchise_leaders_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"FranchiseLeaders_{idx}" if idx > 0 else "FranchiseLeaders"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_franchise_leaders_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_franchise_leaders(
    team_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get franchise statistical leaders data.
    
    Args:
        team_id: Team ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_franchise_leaders_logic(
        team_id=team_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing FranchiseLeaders endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_franchise_leaders("1610612747")  # Lakers
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_franchise_leaders("1610612747", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("FranchiseLeaders endpoint test completed.")


===== backend\api_tools\franchise_players.py =====
"""
Handles fetching and processing franchise players data
from the FranchisePlayers endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The FranchisePlayers endpoint provides comprehensive franchise player roster history (26 columns):
- Team info: LEAGUE_ID, TEAM_ID, TEAM (3 columns)
- Player info: PERSON_ID, PLAYER, SEASON_TYPE, ACTIVE_WITH_TEAM (4 columns)
- Game stats: GP (games played) (1 column)
- Shooting stats: FGM, FGA, FG_PCT, FG3M, FG3A, FG3_PCT, FTM, FTA, FT_PCT (9 columns)
- Other stats: OREB, DREB, REB, AST, PF, STL, TOV, BLK, PTS (9 columns)
- Rich historical data: Cleveland: 459 players, Golden State: 565 players, Lakers: 500 players
- Perfect for franchise roster history, player statistics, and historical analysis
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import franchiseplayers
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeDetailed
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_team_id(team_id):
    """Validate team ID format."""
    if not team_id:
        return False
    
    try:
        # Check if it's a valid team ID (should be a number)
        int(team_id)
        return True
    except ValueError:
        return False

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
FRANCHISE_PLAYERS_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PER_MODES: Set[str] = {PerModeDetailed.totals, PerModeDetailed.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# Valid NBA team IDs (current teams)
VALID_TEAM_IDS: Set[str] = {
    "1610612737",  # Atlanta Hawks
    "1610612738",  # Boston Celtics
    "1610612751",  # Brooklyn Nets
    "1610612766",  # Charlotte Hornets
    "1610612741",  # Chicago Bulls
    "1610612739",  # Cleveland Cavaliers
    "1610612742",  # Dallas Mavericks
    "1610612743",  # Denver Nuggets
    "1610612765",  # Detroit Pistons
    "1610612744",  # Golden State Warriors
    "1610612745",  # Houston Rockets
    "1610612754",  # Indiana Pacers
    "1610612746",  # LA Clippers
    "1610612747",  # Los Angeles Lakers
    "1610612763",  # Memphis Grizzlies
    "1610612748",  # Miami Heat
    "1610612749",  # Milwaukee Bucks
    "1610612750",  # Minnesota Timberwolves
    "1610612740",  # New Orleans Pelicans
    "1610612752",  # New York Knicks
    "1610612760",  # Oklahoma City Thunder
    "1610612753",  # Orlando Magic
    "1610612755",  # Philadelphia 76ers
    "1610612756",  # Phoenix Suns
    "1610612757",  # Portland Trail Blazers
    "1610612758",  # Sacramento Kings
    "1610612759",  # San Antonio Spurs
    "1610612761",  # Toronto Raptors
    "1610612762",  # Utah Jazz
    "1610612764",  # Washington Wizards
}

# --- Cache Directory Setup ---
FRANCHISE_PLAYERS_CSV_DIR = get_cache_dir("franchise_players")

# Ensure cache directories exist
os.makedirs(FRANCHISE_PLAYERS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_franchise_players(
    team_id: str,
    league_id: str = "00",
    per_mode_detailed: str = PerModeDetailed.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular
) -> str:
    """
    Generates a file path for saving franchise players DataFrame.
    
    Args:
        team_id: Team ID
        league_id: League ID (default: "00" for NBA)
        per_mode_detailed: Per mode detailed (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"franchise_players_team{team_id}",
        f"league{league_id}",
        f"mode{per_mode_detailed.replace(' ', '_')}",
        f"type{season_type_all_star.replace(' ', '_')}"
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "franchise_players")

# --- Parameter Validation ---
def _validate_franchise_players_params(
    team_id: str,
    league_id: str,
    per_mode_detailed: str,
    season_type_all_star: str
) -> Optional[str]:
    """Validates parameters for fetch_franchise_players_logic."""
    if not team_id:
        return "team_id is required"
    if not _validate_team_id(team_id):
        return f"Invalid team_id format: {team_id}. Expected numeric team ID"
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if per_mode_detailed not in VALID_PER_MODES:
        return f"Invalid per_mode_detailed: {per_mode_detailed}. Valid options: {', '.join(VALID_PER_MODES)}"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=FRANCHISE_PLAYERS_CACHE_SIZE)
def fetch_franchise_players_logic(
    team_id: str,
    league_id: str = "00",
    per_mode_detailed: str = PerModeDetailed.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches franchise players data using the FranchisePlayers endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        team_id: Team ID (required)
        league_id: League ID (default: "00" for NBA)
        per_mode_detailed: Per mode detailed (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise players data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_franchise_players_logic for Team: {team_id}, League: {league_id}, "
        f"Per Mode: {per_mode_detailed}, Season Type: {season_type_all_star}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_franchise_players_params(team_id, league_id, per_mode_detailed, season_type_all_star)
    if validation_error:
        logger.warning(f"Parameter validation failed for franchise players: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_franchise_players(team_id, league_id, per_mode_detailed, season_type_all_star)
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading franchise players from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "team_id": team_id,
                        "league_id": league_id,
                        "per_mode_detailed": per_mode_detailed,
                        "season_type_all_star": season_type_all_star
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["FranchisePlayers"] = df
                result_dict["data_sets"]["FranchisePlayers"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "team_id": team_id,
            "league_id": league_id,
            "per_mode_detailed": per_mode_detailed,
            "season_type_all_star": season_type_all_star
        }
        
        logger.debug(f"Calling FranchisePlayers with parameters: {api_params}")
        franchise_players_endpoint = franchiseplayers.FranchisePlayers(**api_params)
        
        # Get data frames
        list_of_dataframes = franchise_players_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"FranchisePlayers_{idx}" if idx > 0 else "FranchisePlayers"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_franchise_players_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_franchise_players(
    team_id: str,
    league_id: str = "00",
    per_mode_detailed: str = PerModeDetailed.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get franchise players data.
    
    Args:
        team_id: Team ID (required)
        league_id: League ID (default: "00" for NBA)
        per_mode_detailed: Per mode detailed (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with franchise players data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_franchise_players_logic(
        team_id=team_id,
        league_id=league_id,
        per_mode_detailed=per_mode_detailed,
        season_type_all_star=season_type_all_star,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing FranchisePlayers endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_franchise_players("1610612747")  # Lakers
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_franchise_players("1610612747", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("FranchisePlayers endpoint test completed.")


===== backend\api_tools\free_agents_data.py =====
"""
NBA free agent data API tools.
Fetches data from clean CSV files with NBA API ID mappings.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Any, Dict, Optional, Union, List, Tuple
from functools import lru_cache
import pandas as pd

from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

# Cache directory setup
FREE_AGENTS_CSV_DIR = get_cache_dir("free_agents")
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
FREE_AGENTS_CSV = os.path.join(DATA_DIR, "free_agents_clean.csv")

def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """Saves a DataFrame to a CSV file."""
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _load_free_agents_data() -> pd.DataFrame:
    """Load the clean free agents CSV data."""
    if not os.path.exists(FREE_AGENTS_CSV):
        raise FileNotFoundError(f"Clean free agents file not found: {FREE_AGENTS_CSV}")

    df = pd.read_csv(FREE_AGENTS_CSV)
    logger.info(f"Loaded {len(df)} free agent records")
    return df

@lru_cache(maxsize=32)
def fetch_free_agents_data_logic(
    player_id: Optional[int] = None,
    team_id: Optional[int] = None,
    position: Optional[str] = None,
    free_agent_type: Optional[str] = None,
    min_ppg: Optional[float] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA free agent data with optional filtering.

    Args:
        player_id: Optional NBA API player ID to filter by
        team_id: Optional NBA API team ID to filter by (old team)
        position: Optional position to filter by (G, F, C, etc.)
        free_agent_type: Optional free agent type (ufa, rfa)
        min_ppg: Optional minimum PPG to filter by
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        # Generate cache filename based on filters
        cache_parts = ["free_agents"]
        if player_id:
            cache_parts.append(f"player_{player_id}")
        if team_id:
            cache_parts.append(f"team_{team_id}")
        if position:
            cache_parts.append(f"pos_{position}")
        if free_agent_type:
            cache_parts.append(f"type_{free_agent_type}")
        if min_ppg:
            cache_parts.append(f"minppg_{min_ppg}")

        cache_filename = "_".join(cache_parts) + ".csv"
        cache_path = get_cache_file_path(cache_filename, "free_agents")

        # Try to load from cache first
        if os.path.exists(cache_path):
            try:
                df = pd.read_csv(cache_path)
                logger.info(f"Loaded cached free agents data: {len(df)} records")
            except Exception as e:
                logger.warning(f"Error reading cache, loading fresh data: {e}")
                df = _load_free_agents_data()
        else:
            df = _load_free_agents_data()

            # Apply filters
            if player_id:
                df = df[df['nba_player_id'] == player_id]
            if team_id:
                df = df[df['nba_old_team_id'] == team_id]
            if position:
                df = df[df['position'].str.contains(position, case=False, na=False)]
            if free_agent_type:
                df = df[df['type'] == free_agent_type]
            if min_ppg is not None:
                df = df[df['PPG'] >= min_ppg]

            # Cache the filtered results
            _save_dataframe_to_csv(df, cache_path)

        # Process data for JSON response
        data_sets = {
            "free_agents": _process_dataframe(df)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "player_id": player_id,
                "team_id": team_id,
                "position": position,
                "free_agent_type": free_agent_type,
                "min_ppg": min_ppg
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"free_agents": df}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error fetching free agents data: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

def get_free_agent_info(player_id: int, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets free agent information for a specific player.

    Args:
        player_id: NBA API player ID
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    return fetch_free_agents_data_logic(player_id=player_id, return_dataframe=return_dataframe)

def get_team_free_agents(team_id: int, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets all free agents who previously played for a team.

    Args:
        team_id: NBA API team ID
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    return fetch_free_agents_data_logic(team_id=team_id, return_dataframe=return_dataframe)

def get_top_free_agents(
    position: Optional[str] = None,
    free_agent_type: Optional[str] = None,
    limit: int = 50,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Gets top free agents ranked by PPG.

    Args:
        position: Optional position filter
        free_agent_type: Optional free agent type filter
        limit: Maximum number of results to return
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        cache_parts = ["top_free_agents"]
        if position:
            cache_parts.append(f"pos_{position}")
        if free_agent_type:
            cache_parts.append(f"type_{free_agent_type}")
        cache_parts.append(f"limit_{limit}")

        cache_filename = "_".join(cache_parts) + ".csv"
        cache_path = get_cache_file_path(cache_filename, "free_agents")

        # Try to load from cache first
        if os.path.exists(cache_path):
            try:
                df = pd.read_csv(cache_path)
                logger.info(f"Loaded cached top free agents: {len(df)} records")
            except Exception as e:
                logger.warning(f"Error reading cache, loading fresh data: {e}")
                df = _load_free_agents_data()
                # Apply filters and sort
                if position:
                    df = df[df['position'].str.contains(position, case=False, na=False)]
                if free_agent_type:
                    df = df[df['type'] == free_agent_type]
                df = df.sort_values('PPG', ascending=False).head(limit)
                _save_dataframe_to_csv(df, cache_path)
        else:
            df = _load_free_agents_data()
            # Apply filters and sort
            if position:
                df = df[df['position'].str.contains(position, case=False, na=False)]
            if free_agent_type:
                df = df[df['type'] == free_agent_type]
            df = df.sort_values('PPG', ascending=False).head(limit)
            _save_dataframe_to_csv(df, cache_path)

        # Process data for JSON response
        data_sets = {
            "top_free_agents": _process_dataframe(df)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "position": position,
                "free_agent_type": free_agent_type,
                "limit": limit
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"top_free_agents": df}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error getting top free agents: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

def search_free_agents(player_name: str, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Search for free agents by name.

    Args:
        player_name: Player name to search for (partial matches allowed)
        return_dataframe: Whether to return DataFrame alongside JSON

    Returns:
        JSON string or tuple of (JSON string, DataFrames dict)
    """
    try:
        df = _load_free_agents_data()

        # Search for players with names containing the search term
        mask = df['playerDisplayName'].str.contains(player_name, case=False, na=False)
        df_filtered = df[mask]

        # Sort by PPG descending
        df_filtered = df_filtered.sort_values('PPG', ascending=False, na_position='last')

        # Process data for JSON response
        data_sets = {
            "free_agent_search": _process_dataframe(df_filtered)
        }

        response_data = {
            "data_sets": data_sets,
            "parameters": {
                "player_name": player_name
            }
        }

        json_response = format_response(response_data)

        if return_dataframe:
            dataframes = {"free_agent_search": df_filtered}
            return json_response, dataframes

        return json_response

    except Exception as e:
        error_msg = f"Error searching free agents: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return format_response(error=error_msg)

if __name__ == "__main__":
    # Test basic functionality
    print("Testing Free Agents Data endpoint...")

    # Test 1: Basic fetch
    json_response = fetch_free_agents_data_logic()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = fetch_free_agents_data_logic(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("Free Agents Data endpoint test completed.")


===== backend\api_tools\game_boxscores.py =====
"""
Handles fetching and processing various types of game box score data
(Traditional, Advanced, Four Factors, Usage, Defensive, Summary)
using a generic helper function.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
import pandas as pd
from typing import Any, Dict, Optional, Type, Union, Tuple, List

from nba_api.stats.endpoints import (
    BoxScoreAdvancedV3,
    BoxScoreTraditionalV3,
    BoxScoreFourFactorsV3,
    BoxScoreUsageV3,
    BoxScoreDefensiveV2,
    BoxScoreSummaryV2,
    BoxScoreMiscV3,
    BoxScorePlayerTrackV3,
    BoxScoreScoringV3,
    BoxScoreHustleV2
)
from ..config import settings
from ..core.errors import Errors
from nba_api.stats.library.parameters import EndPeriod, EndRange, RangeType, StartPeriod, StartRange
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_game_id_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_BOXSCORE_CACHE_SIZE = 128
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
BOXSCORE_CSV_DIR = os.path.join(CSV_CACHE_DIR, "boxscores")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(BOXSCORE_CSV_DIR, exist_ok=True)

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_boxscore(game_id: str, boxscore_type: str, **kwargs) -> str:
    """
    Generates a file path for saving a boxscore DataFrame as CSV.

    Args:
        game_id: The game ID
        boxscore_type: The type of boxscore (e.g., 'traditional', 'advanced')
        **kwargs: Additional parameters to include in the filename

    Returns:
        Path to the CSV file
    """
    # Create a string with the parameters
    params_str = ""
    if kwargs:
        params_str = "_" + "_".join(f"{k}_{v}" for k, v in kwargs.items() if v != 0)

    filename = f"{game_id}_{boxscore_type}{params_str}.csv"
    return os.path.join(BOXSCORE_CSV_DIR, filename)

def _get_dataframes_from_endpoint(
    endpoint_instance: Any,
    dataset_mapping: Dict[str, str]
) -> Dict[str, pd.DataFrame]:
    """
    Extracts DataFrames from an endpoint instance based on the dataset mapping.

    Args:
        endpoint_instance: The NBA API endpoint instance
        dataset_mapping: Mapping of output keys to dataset attribute names

    Returns:
        Dictionary of DataFrames with output keys as keys
    """
    dataframes = {}
    for output_key, dataset_attr_name in dataset_mapping.items():
        if not hasattr(endpoint_instance, dataset_attr_name):
            logger.error(f"Dataset attribute '{dataset_attr_name}' not found on endpoint instance.")
            dataframes[output_key] = pd.DataFrame()
            continue

        dataset_obj = getattr(endpoint_instance, dataset_attr_name)
        if not hasattr(dataset_obj, 'get_data_frame'):
            logger.error(f"Dataset attribute '{dataset_attr_name}' does not have 'get_data_frame' method.")
            dataframes[output_key] = pd.DataFrame()
            continue

        df = dataset_obj.get_data_frame()
        dataframes[output_key] = df

    return dataframes

# --- Generic Helper Function ---
def _fetch_boxscore_data_generic(
    game_id: str,
    endpoint_class: Type[Any], # Using Type[Any] as a placeholder for specific endpoint types
    dataset_mapping: Dict[str, str], # e.g., {"players": "player_stats", "teams": "team_stats"}
    error_constants: Dict[str, str], # e.g., {"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR}
    endpoint_name_for_logging: str,
    additional_params_for_response: Optional[Dict[str, Any]] = None,
    return_dataframe: bool = False,
    **kwargs: Any # Parameters for the endpoint_class constructor
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Generic helper to fetch and process box score data from various nba_api endpoints.

    Args:
        game_id: The ID of the game to fetch data for
        endpoint_class: The NBA API endpoint class to use
        dataset_mapping: Mapping of output keys to dataset attribute names
        error_constants: Error message constants for different error types
        endpoint_name_for_logging: Name of the endpoint for logging purposes
        additional_params_for_response: Additional parameters to include in the response
        return_dataframe: Whether to return DataFrames along with the JSON response
        **kwargs: Additional parameters for the endpoint class constructor

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    logger.info(f"Executing generic boxscore fetch for {endpoint_name_for_logging}, game ID: {game_id}, params: {kwargs}, return_dataframe: {return_dataframe}")

    if not game_id:
        error_response = format_response(error=Errors.GAME_ID_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not validate_game_id_format(game_id):
        error_response = format_response(error=Errors.INVALID_GAME_ID_FORMAT.format(game_id=game_id))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        endpoint_instance = endpoint_class(game_id=game_id, **kwargs, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        logger.debug(f"{endpoint_name_for_logging} API call successful for {game_id}")

        # Extract DataFrames from the endpoint
        dataframes = _get_dataframes_from_endpoint(endpoint_instance, dataset_mapping)

        # Process data for JSON response
        processed_data: Dict[str, Any] = {}
        all_datasets_valid = True

        for output_key, df in dataframes.items():
            if df.empty:
                logger.warning(f"Empty DataFrame for '{output_key}' in {endpoint_name_for_logging} of game {game_id}.")
                processed_data[output_key] = []
                continue

            # Process DataFrame for JSON response
            data_list = _process_dataframe(df, single_row=False)

            if data_list is None:  # _process_dataframe returns None on internal error
                logger.error(f"DataFrame processing failed for '{output_key}' in {endpoint_name_for_logging} of game {game_id}.")
                all_datasets_valid = False
                processed_data[output_key] = []
            else:
                processed_data[output_key] = data_list

        # Check if essential datasets failed
        if not all_datasets_valid and any(val == [] for key, val in processed_data.items()
                                         if dataset_mapping.get(key) != "team_starter_bench_stats"):
            error_msg = error_constants["processing"].format(error=f"{endpoint_name_for_logging} data for game {game_id}")
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        # Prepare the result
        result = {"game_id": game_id, **processed_data}
        if additional_params_for_response:
            result["parameters"] = additional_params_for_response

        # Save DataFrames to CSV if requested
        if return_dataframe:
            boxscore_type = endpoint_name_for_logging.replace("BoxScore", "").replace("V2", "").replace("V3", "").lower()
            for output_key, df in dataframes.items():
                if not df.empty:
                    csv_path = _get_csv_path_for_boxscore(game_id, f"{boxscore_type}_{output_key}", **kwargs)
                    _save_dataframe_to_csv(df, csv_path)

        logger.info(f"Generic boxscore fetch for {endpoint_name_for_logging} completed for game {game_id}")
        json_response = format_response(result)

        if return_dataframe:
            return json_response, dataframes
        return json_response

    except IndexError as ie:  # Specific catch for endpoints that might return empty data sets causing index errors
        logger.warning(f"IndexError during {endpoint_name_for_logging} processing for game {game_id}: {ie}. Data likely unavailable.", exc_info=False)
        error_msg = Errors.DATA_NOT_FOUND + f" ({endpoint_name_for_logging} data might be unavailable for game {game_id} with current filters)"
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    except Exception as e:
        logger.error(f"Error fetching {endpoint_name_for_logging} for game {game_id}: {str(e)}", exc_info=True)
        error_msg = error_constants["api"].format(game_id=game_id, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response


# --- Public Fetch Logic Functions ---

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_traditional_logic(
    game_id: str,
    start_period: int = StartPeriod.default,
    end_period: int = EndPeriod.default,
    start_range: int = StartRange.default,
    end_range: int = EndRange.default,
    range_type: int = RangeType.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Traditional Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        start_period: Starting period number (0 for full game)
        end_period: Ending period number (0 for full game)
        start_range: Starting range in seconds
        end_range: Ending range in seconds
        range_type: Type of range (0 for full game)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreTraditionalV3,
        dataset_mapping={
            "teams": "team_stats",
            "players": "player_stats",
            "starters_bench": "team_starter_bench_stats"
        },
        error_constants={"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreTraditionalV3",
        additional_params_for_response={
            "start_period": start_period, "end_period": end_period,
            "start_range": start_range, "end_range": end_range,
            "range_type": range_type, "note": "Using BoxScoreTraditionalV3"
        },
        return_dataframe=return_dataframe,
        # Kwargs for BoxScoreTraditionalV3 constructor
        start_period=start_period, end_period=end_period,
        start_range=start_range, end_range=end_range, range_type=range_type
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_advanced_logic(
    game_id: str,
    start_period: int = StartPeriod.default,
    end_period: int = EndPeriod.default,
    start_range: int = StartRange.default,
    end_range: int = EndRange.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Advanced Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        start_period: Starting period number (0 for full game)
        end_period: Ending period number (0 for full game)
        start_range: Starting range in seconds
        end_range: Ending range in seconds
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreAdvancedV3,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_ADVANCED_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreAdvancedV3",
        additional_params_for_response={
            "start_period": start_period, "end_period": end_period,
            "start_range": start_range, "end_range": end_range
        },
        return_dataframe=return_dataframe,
        start_period=start_period, end_period=end_period,
        start_range=start_range, end_range=end_range
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_four_factors_logic(
    game_id: str,
    start_period: int = StartPeriod.default,
    end_period: int = EndPeriod.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Four Factors Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        start_period: Starting period number (0 for full game)
        end_period: Ending period number (0 for full game)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreFourFactorsV3,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_FOURFACTORS_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreFourFactorsV3",
        additional_params_for_response={"start_period": start_period, "end_period": end_period},
        return_dataframe=return_dataframe,
        start_period=start_period, end_period=end_period
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_usage_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Usage Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreUsageV3,
        dataset_mapping={"player_usage_stats": "player_stats", "team_usage_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_USAGE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreUsageV3",
        return_dataframe=return_dataframe
        # No additional constructor kwargs beyond game_id for BoxScoreUsageV3 apart from defaults
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_defensive_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Defensive Box Score data (V2) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreDefensiveV2,
        dataset_mapping={"player_defensive_stats": "player_stats", "team_defensive_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_DEFENSIVE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreDefensiveV2",
        return_dataframe=return_dataframe
        # No additional constructor kwargs beyond game_id for BoxScoreDefensiveV2
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_summary_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches the comprehensive summary box score (V2) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreSummaryV2,
        dataset_mapping={
            "available_video": "available_video",
            "game_info": "game_info",
            "game_summary": "game_summary",
            "inactive_players": "inactive_players",
            "last_meeting": "last_meeting",
            "line_score": "line_score",
            "officials": "officials",
            "other_stats": "other_stats",
            "season_series": "season_series"
        },
        error_constants={"api": Errors.BOXSCORE_SUMMARY_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreSummaryV2",
        return_dataframe=return_dataframe
        # No additional constructor kwargs beyond game_id for BoxScoreSummaryV2
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_misc_logic(
    game_id: str,
    start_period: int = StartPeriod.default,
    end_period: int = EndPeriod.default,
    start_range: int = StartRange.default,
    end_range: int = EndRange.default,
    range_type: int = RangeType.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Miscellaneous Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        start_period: Starting period number (0 for full game)
        end_period: Ending period number (0 for full game)
        start_range: Starting range in seconds
        end_range: Ending range in seconds
        range_type: Type of range (0 for full game)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreMiscV3,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreMiscV3",
        additional_params_for_response={
            "start_period": start_period, "end_period": end_period,
            "start_range": start_range, "end_range": end_range,
            "range_type": range_type
        },
        return_dataframe=return_dataframe,
        start_period=start_period, end_period=end_period,
        start_range=start_range, end_range=end_range, range_type=range_type
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_playertrack_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Player Tracking Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScorePlayerTrackV3,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScorePlayerTrackV3",
        return_dataframe=return_dataframe
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_scoring_logic(
    game_id: str,
    start_period: int = StartPeriod.default,
    end_period: int = EndPeriod.default,
    start_range: int = StartRange.default,
    end_range: int = EndRange.default,
    range_type: int = RangeType.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Scoring Box Score data (V3) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        start_period: Starting period number (0 for full game)
        end_period: Ending period number (0 for full game)
        start_range: Starting range in seconds
        end_range: Ending range in seconds
        range_type: Type of range (0 for full game)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreScoringV3,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreScoringV3",
        additional_params_for_response={
            "start_period": start_period, "end_period": end_period,
            "start_range": start_range, "end_range": end_range,
            "range_type": range_type
        },
        return_dataframe=return_dataframe,
        start_period=start_period, end_period=end_period,
        start_range=start_range, end_range=end_range, range_type=range_type
    )

@lru_cache(maxsize=GAME_BOXSCORE_CACHE_SIZE)
def fetch_boxscore_hustle_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Hustle Box Score data (V2) for a given game_id.

    Args:
        game_id: The ID of the game to fetch data for
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False: JSON string containing the boxscore data
        If return_dataframe=True: Tuple of (JSON string, Dictionary of DataFrames)
    """
    return _fetch_boxscore_data_generic(
        game_id=game_id,
        endpoint_class=BoxScoreHustleV2,
        dataset_mapping={"player_stats": "player_stats", "team_stats": "team_stats"},
        error_constants={"api": Errors.BOXSCORE_API, "processing": Errors.PROCESSING_ERROR},
        endpoint_name_for_logging="BoxScoreHustleV2",
        return_dataframe=return_dataframe
    )

===== backend\api_tools\game_boxscore_matchups.py =====
"""
Handles fetching and processing player matchup data from NBA games
using the BoxScoreMatchupsV3 endpoint.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
import pandas as pd
from typing import Dict, Optional, Union, Tuple

from nba_api.stats.endpoints import BoxScoreMatchupsV3
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_game_id_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_BOXSCORE_MATCHUPS_CACHE_SIZE = 128
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
BOXSCORE_MATCHUPS_CSV_DIR = os.path.join(CSV_CACHE_DIR, "boxscore_matchups")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(BOXSCORE_MATCHUPS_CSV_DIR, exist_ok=True)

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_matchups(game_id: str) -> str:
    """
    Generates a file path for saving a matchups DataFrame as CSV.

    Args:
        game_id: The game ID

    Returns:
        Path to the CSV file
    """
    filename = f"{game_id}_matchups.csv"
    return os.path.join(BOXSCORE_MATCHUPS_CSV_DIR, filename)

@lru_cache(maxsize=GAME_BOXSCORE_MATCHUPS_CACHE_SIZE)
def fetch_game_boxscore_matchups_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player matchup data for a given game using BoxScoreMatchupsV3 endpoint.

    This endpoint provides detailed player-vs-player matchup statistics including:
    - Time matched up against each other
    - Points scored in the matchup
    - Field goal percentages
    - Assists, turnovers, and blocks in the matchup

    Args:
        game_id (str): The ID of the game to fetch matchup data for
        return_dataframe (bool, optional): Whether to return DataFrames along with the JSON response.
                                          Defaults to False.

    Returns:
        Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
            If return_dataframe is False, a JSON string with the matchup data or an error.
            If return_dataframe is True, a tuple containing the JSON response string and a dictionary
            of DataFrames.
    """
    logger.info(f"Executing fetch_game_boxscore_matchups_logic for game ID: {game_id}")
    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not game_id:
        error_msg = Errors.GAME_ID_EMPTY
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if not validate_game_id_format(game_id):
        error_msg = Errors.INVALID_GAME_ID_FORMAT.format(game_id=game_id)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        # Call the NBA API endpoint
        logger.debug(f"Calling BoxScoreMatchupsV3 API with game_id: {game_id}")
        matchups = BoxScoreMatchupsV3(
            game_id=game_id,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        # Get DataFrame from the endpoint
        matchups_df = matchups.player_stats.get_data_frame()

        # Handle DataFrame if requested
        if return_dataframe:
            dataframes["matchups"] = matchups_df

            # Save to CSV if not empty
            if not matchups_df.empty:
                csv_path = _get_csv_path_for_matchups(game_id)
                _save_dataframe_to_csv(matchups_df, csv_path)

        # Process data for JSON response
        if matchups_df.empty:
            logger.warning(f"No matchup data returned by API for game {game_id}")
            processed_matchups = []
        else:
            processed_matchups = _process_dataframe(matchups_df, single_row=False)

            if processed_matchups is None:  # _process_dataframe returns None on internal error
                logger.error(f"Error processing matchups data for game {game_id}")
                error_msg = Errors.PROCESSING_ERROR.format(error=f"BoxScoreMatchupsV3 data for game {game_id}")
                if return_dataframe:
                    return format_response(error=error_msg), dataframes
                return format_response(error=error_msg)

        # Create response
        response_data = {
            "game_id": game_id,
            "matchups": processed_matchups,
            "parameters": {
                "note": "Using BoxScoreMatchupsV3"
            }
        }

        # Return response
        logger.info(f"Successfully fetched BoxScoreMatchupsV3 for game {game_id}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"API error in fetch_game_boxscore_matchups_logic for game {game_id}: {e}",
            exc_info=True
        )
        error_msg = Errors.BOXSCORE_MATCHUPS_API.format(game_id=game_id, error=str(e))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\game_finder.py =====
"""
Handles fetching league game data using the LeagueGameFinder endpoint.
Includes logic for parameter validation, post-fetch date filtering (due to API instability
with date range parameters), and result limiting for broad queries.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import pandas as pd
from typing import Optional, List, Dict, Any, Set, Union, Tuple
from datetime import datetime
from functools import lru_cache
from nba_api.stats.endpoints import leaguegamefinder
from nba_api.stats.library.parameters import LeagueID, SeasonTypeAllStar
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_FINDER_CACHE_SIZE = 32
VALID_PLAYER_TEAM_ABBREVIATIONS = ['P', 'T']
MAX_GAME_FINDER_RESULTS = 200
DATE_FORMAT_YYYY_MM_DD = '%Y-%m-%d'

_VALID_GAME_FINDER_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str) and attr != 'default'}
_VALID_GAME_FINDER_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Cache Directory Setup ---
GAME_FINDER_CSV_DIR = get_cache_dir("game_finder")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_game_finder(
    player_or_team_abbreviation: str,
    player_id_nullable: Optional[int] = None,
    team_id_nullable: Optional[int] = None,
    season_nullable: Optional[str] = None,
    season_type_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving game finder DataFrame as CSV.

    Args:
        player_or_team_abbreviation: 'P' for player or 'T' for team
        player_id_nullable: Player ID
        team_id_nullable: Team ID
        season_nullable: Season in 'YYYY-YY' format
        season_type_nullable: Season type (e.g., 'Regular Season')
        league_id_nullable: League ID
        date_from_nullable: Start date 'YYYY-MM-DD'
        date_to_nullable: End date 'YYYY-MM-DD'

    Returns:
        Path to the CSV file
    """
    # Create a string with the parameters
    params = []

    if player_or_team_abbreviation:
        params.append(f"pt_{player_or_team_abbreviation}")

    if player_id_nullable:
        params.append(f"pid_{player_id_nullable}")

    if team_id_nullable:
        params.append(f"tid_{team_id_nullable}")

    if season_nullable:
        params.append(f"season_{season_nullable}")

    if season_type_nullable:
        clean_season_type = season_type_nullable.replace(" ", "_").lower()
        params.append(f"type_{clean_season_type}")

    if league_id_nullable:
        params.append(f"lid_{league_id_nullable}")

    if date_from_nullable:
        params.append(f"from_{date_from_nullable}")

    if date_to_nullable:
        params.append(f"to_{date_to_nullable}")

    # Join parameters with underscores
    filename = "_".join(params) + ".csv"

    return get_cache_file_path(filename, "game_finder")

# --- Helper Functions ---
def _validate_game_finder_params(
    player_or_team_abbreviation: str,
    player_id_nullable: Optional[int],
    team_id_nullable: Optional[int],
    season_nullable: Optional[str],
    season_type_nullable: Optional[str],
    league_id_nullable: Optional[str],
    date_from_nullable: Optional[str],
    date_to_nullable: Optional[str]
) -> Optional[str]:
    """Validates parameters for fetch_league_games_logic."""
    if player_or_team_abbreviation not in VALID_PLAYER_TEAM_ABBREVIATIONS:
        return Errors.INVALID_PLAYER_OR_TEAM_ABBREVIATION.format(value=player_or_team_abbreviation)
    if player_or_team_abbreviation == 'P' and player_id_nullable is None:
        return Errors.PLAYER_ID_REQUIRED
    if date_from_nullable and date_to_nullable and not season_nullable and not player_id_nullable and not team_id_nullable:
        logger.warning("Attempted date-only query for leaguegamefinder, which is unsupported.")
        return Errors.DATE_ONLY_GAME_FINDER_UNSUPPORTED
    if season_nullable and not _validate_season_format(season_nullable, league_id=league_id_nullable):
        return Errors.INVALID_SEASON_FORMAT.format(season=season_nullable)
    if date_from_nullable and not validate_date_format(date_from_nullable):
        return Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
    if date_to_nullable and not validate_date_format(date_to_nullable):
        return Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
    if season_type_nullable is not None and season_type_nullable not in _VALID_GAME_FINDER_SEASON_TYPES and season_type_nullable != SeasonTypeAllStar.default:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type_nullable, options=", ".join(list(_VALID_GAME_FINDER_SEASON_TYPES)[:5]))
    if league_id_nullable is not None and league_id_nullable not in _VALID_GAME_FINDER_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_GAME_FINDER_LEAGUE_IDS)[:5]))
    return None

def _apply_date_filters_to_df(
    games_df: pd.DataFrame,
    date_from_nullable: Optional[str],
    date_to_nullable: Optional[str]
) -> pd.DataFrame:
    """Applies date filtering to the DataFrame after initial fetch."""
    if not date_from_nullable and not date_to_nullable:
        return games_df
    if games_df.empty:
        return games_df

    logger.info(f"Applying post-fetch date filtering: From {date_from_nullable}, To {date_to_nullable}")
    try:
        games_df['GAME_DATE_DT'] = pd.to_datetime(games_df['GAME_DATE']).dt.date
        if date_from_nullable:
            from_date = datetime.strptime(date_from_nullable, DATE_FORMAT_YYYY_MM_DD).date()
            games_df = games_df[games_df['GAME_DATE_DT'] >= from_date]
        if date_to_nullable:
            to_date = datetime.strptime(date_to_nullable, DATE_FORMAT_YYYY_MM_DD).date()
            games_df = games_df[games_df['GAME_DATE_DT'] <= to_date]
        games_df = games_df.drop(columns=['GAME_DATE_DT'])
    except Exception as e:
        logger.error(f"Error during post-fetch date filtering: {str(e)}", exc_info=True)
        # Decide if to raise, return empty, or return original. Returning original for now.
        # Consider raising a specific processing error to be caught by the main function.
        raise ValueError(f"Date filtering failed: {str(e)}")
    return games_df

def _limit_results_if_broad(
    games_df: pd.DataFrame,
    player_id_nullable: Optional[int],
    team_id_nullable: Optional[int],
    season_nullable: Optional[str],
    season_type_nullable: Optional[str], # Added for more precise broad query check
    original_date_from: Optional[str], # Use original dates for this check
    original_date_to: Optional[str]
) -> pd.DataFrame:
    """Limits the number of results for broad queries."""
    if games_df.empty:
        return games_df

    # A query is considered broad if it's not filtered by player, team, or season,
    # but might have date filters applied post-fetch.
    is_broad_query_by_params = all(param is None for param in [player_id_nullable, team_id_nullable, season_nullable])

    # A query is very general if no specific filters (player, team, season, type, or original dates) were applied.
    is_very_general_query = all(param is None for param in [
        player_id_nullable, team_id_nullable, season_nullable, season_type_nullable,
        original_date_from, original_date_to
    ])

    if is_broad_query_by_params and len(games_df) > MAX_GAME_FINDER_RESULTS:
        logger.info(f"Limiting broad league game list to the top {MAX_GAME_FINDER_RESULTS} games. Original count: {len(games_df)}")
        return games_df.head(MAX_GAME_FINDER_RESULTS)
    elif is_very_general_query and len(games_df) > MAX_GAME_FINDER_RESULTS:
        logger.info(f"Limiting very general league game list to the top {MAX_GAME_FINDER_RESULTS} games. Original count: {len(games_df)}")
        return games_df.head(MAX_GAME_FINDER_RESULTS)
    return games_df

def _format_game_dates(games_list: List[Dict[str, Any]]) -> None:
    """Formats GAME_DATE to YYYY-MM-DD in place."""
    for game_item in games_list:
        if 'GAME_DATE' in game_item and isinstance(game_item['GAME_DATE'], str):
            try:
                parsed_date = datetime.fromisoformat(game_item['GAME_DATE'].split('T')[0])
                game_item['GAME_DATE_FORMATTED'] = parsed_date.strftime(DATE_FORMAT_YYYY_MM_DD)
            except ValueError:
                game_item['GAME_DATE_FORMATTED'] = game_item['GAME_DATE'].split('T')[0] # Fallback

# --- Main Logic Function ---
def fetch_league_games_logic(
    player_or_team_abbreviation: str = 'T',
    player_id_nullable: Optional[int] = None,
    team_id_nullable: Optional[int] = None,
    season_nullable: Optional[str] = None,
    season_type_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = LeagueID.nba,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA league games using LeagueGameFinder with common filters.
    Applies date filtering post-API call due to API instability with date ranges.
    Provides DataFrame output capabilities.

    Args:
        player_or_team_abbreviation: 'P' for player or 'T' for team. Defaults to 'T'.
        player_id_nullable: Player ID.
        team_id_nullable: Team ID.
        season_nullable: Season in 'YYYY-YY' format.
        season_type_nullable: Season type (e.g., 'Regular Season').
        league_id_nullable: League ID. Defaults to NBA.
        date_from_nullable: Start date 'YYYY-MM-DD'.
        date_to_nullable: End date 'YYYY-MM-DD'.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string of games list or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_games_logic with P/T: {player_or_team_abbreviation}, TeamID: {team_id_nullable}, PlayerID: {player_id_nullable}, Season: {season_nullable}, League: {league_id_nullable}, Dates: {date_from_nullable}-{date_to_nullable}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    param_error = _validate_game_finder_params(
        player_or_team_abbreviation, player_id_nullable, team_id_nullable,
        season_nullable, season_type_nullable, league_id_nullable,
        date_from_nullable, date_to_nullable
    )
    if param_error:
        error_response = format_response(error=param_error)
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # API call is made without date filters initially due to potential instability
        logger.debug(f"Calling LeagueGameFinder with P/T: {player_or_team_abbreviation}, PlayerID: {player_id_nullable}, TeamID: {team_id_nullable}, Season: {season_nullable}, SeasonType: {season_type_nullable}, League: {league_id_nullable}")
        game_finder_endpoint = leaguegamefinder.LeagueGameFinder(
            player_or_team_abbreviation=player_or_team_abbreviation,
            player_id_nullable=player_id_nullable,
            team_id_nullable=team_id_nullable,
            season_nullable=season_nullable,
            season_type_nullable=season_type_nullable,
            league_id_nullable=league_id_nullable,
            date_from_nullable=None, # Fetch all, then filter
            date_to_nullable=None,   # Fetch all, then filter
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        games_df = game_finder_endpoint.league_game_finder_results.get_data_frame()
        logger.debug("LeagueGameFinder API call successful.")

        if games_df.empty:
            logger.warning("No league games found from initial API call.")
            empty_response = format_response({"games": []})
            if return_dataframe:
                return empty_response, dataframes
            return empty_response

        # Apply date filtering post-fetch
        try:
            games_df = _apply_date_filters_to_df(games_df, date_from_nullable, date_to_nullable)
        except ValueError as filter_error: # Catch specific error from helper
             logger.error(f"Error during post-fetch date filtering: {filter_error}", exc_info=True)
             error_response = format_response(error=Errors.PROCESSING_ERROR.format(error=str(filter_error)))
             if return_dataframe:
                 return error_response, dataframes
             return error_response

        if games_df.empty:
            logger.warning("No league games found after post-fetch date filtering.")
            empty_response = format_response({"games": []})
            if return_dataframe:
                return empty_response, dataframes
            return empty_response

        # Limit results for broad queries
        games_df = _limit_results_if_broad(
            games_df, player_id_nullable, team_id_nullable, season_nullable,
            season_type_nullable, date_from_nullable, date_to_nullable
        )

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["games"] = games_df

            # Save to CSV if not empty
            if not games_df.empty:
                csv_path = _get_csv_path_for_game_finder(
                    player_or_team_abbreviation=player_or_team_abbreviation,
                    player_id_nullable=player_id_nullable,
                    team_id_nullable=team_id_nullable,
                    season_nullable=season_nullable,
                    season_type_nullable=season_type_nullable,
                    league_id_nullable=league_id_nullable,
                    date_from_nullable=date_from_nullable,
                    date_to_nullable=date_to_nullable
                )
                _save_dataframe_to_csv(games_df, csv_path)

        games_list = _process_dataframe(games_df, single_row=False)
        if games_list is None:
            logger.error("Failed to process league games DataFrame after all filters.")
            error_response = format_response(error=Errors.PROCESSING_ERROR.format(error="league games data"))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        _format_game_dates(games_list)

        result = {"games": games_list}

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_game_finder(
                player_or_team_abbreviation=player_or_team_abbreviation,
                player_id_nullable=player_id_nullable,
                team_id_nullable=team_id_nullable,
                season_nullable=season_nullable,
                season_type_nullable=season_type_nullable,
                league_id_nullable=league_id_nullable,
                date_from_nullable=date_from_nullable,
                date_to_nullable=date_to_nullable
            )
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "game_finder"
            )

            result["dataframe_info"] = {
                "message": "League games data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "games": {
                        "shape": list(games_df.shape) if not games_df.empty else [],
                        "columns": games_df.columns.tolist() if not games_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"fetch_league_games_logic found {len(games_list)} games matching criteria.")

        json_response = format_response(result)
        if return_dataframe:
            return json_response, dataframes
        return json_response

    except Exception as e:
        if "Expecting value: line 1 column 1 (char 0)" in str(e): # Handle potential JSONDecodeError from API
            logger.error(f"JSONDecodeError from NBA API (LeagueGameFinder): {str(e)}", exc_info=True)
            error_msg = Errors.JSON_PROCESSING_ERROR
        else:
            logger.error(f"Error fetching league games: {str(e)}", exc_info=True)
            error_msg = Errors.LEAGUE_GAMES_API.format(error=str(e))

        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\game_playbyplay.py =====
"""
Handles fetching and processing game play-by-play (PBP) data.
It attempts to fetch live PBP data first and falls back to historical PBP data (PlayByPlayV3)
if live data is unavailable or if specific period filters are applied.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import json
import logging
import os
from typing import Dict, Any, Optional, List, Union, Tuple
from datetime import datetime
import re
import pandas as pd
from functools import lru_cache

from nba_api.stats.endpoints import playbyplayv3
from nba_api.live.nba.endpoints import PlayByPlay as LivePlayByPlay
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_game_id_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_PBP_CACHE_SIZE = 64
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PBP_CSV_DIR = os.path.join(CSV_CACHE_DIR, "playbyplay")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PBP_CSV_DIR, exist_ok=True)

# --- Helper Functions ---
# Note: Previous helper functions like _get_event_type and _determine_team_from_tricode
# were removed as PlayByPlayV3 provides richer data directly (actionType, subType, teamTricode).

def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_playbyplay(game_id: str, source: str, start_period: int = 0, end_period: int = 0) -> str:
    """
    Generates a file path for saving a play-by-play DataFrame as CSV.

    Args:
        game_id: The game ID
        source: The source of the data ('live' or 'historical_v3')
        start_period: Starting period filter
        end_period: Ending period filter

    Returns:
        Path to the CSV file
    """
    # Create a string with the period filters
    period_str = ""
    if start_period > 0 or end_period > 0:
        period_str = f"_periods_{start_period}_{end_period}"

    filename = f"{game_id}_{source}{period_str}.csv"
    return os.path.join(PBP_CSV_DIR, filename)

def _filter_plays_by_event_type(plays_df: pd.DataFrame, event_types: List[str] = None) -> pd.DataFrame:
    """
    Filters plays by event type.

    Args:
        plays_df: DataFrame containing play-by-play data
        event_types: List of event types to include (e.g., ['SHOT', 'REBOUND', 'TURNOVER'])
                    If None, all plays are returned

    Returns:
        Filtered DataFrame
    """
    if event_types is None or len(event_types) == 0 or plays_df.empty:
        return plays_df

    # Convert event types to uppercase for case-insensitive matching
    event_types = [et.upper() for et in event_types]

    # Check if actionType column exists (for V3 data)
    if 'actionType' in plays_df.columns:
        return plays_df[plays_df['actionType'].str.upper().isin(event_types)]

    # For live data, check the description for event types
    filtered_df = pd.DataFrame()
    for event_type in event_types:
        event_matches = plays_df[plays_df['description'].str.contains(event_type, case=False, na=False)]
        filtered_df = pd.concat([filtered_df, event_matches])

    return filtered_df.drop_duplicates().reset_index(drop=True)

def _filter_plays_by_player(plays_df: pd.DataFrame, player_name: str = None, person_id: int = None) -> pd.DataFrame:
    """
    Filters plays by player name or ID.

    Args:
        plays_df: DataFrame containing play-by-play data
        player_name: Player name to filter by
        person_id: Player ID to filter by

    Returns:
        Filtered DataFrame
    """
    if (player_name is None and person_id is None) or plays_df.empty:
        return plays_df

    filtered_df = plays_df.copy()

    # Filter by player ID if provided
    if person_id is not None:
        if 'personId' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['personId'] == person_id]
        else:
            # If personId column doesn't exist, return empty DataFrame
            return pd.DataFrame(columns=filtered_df.columns)

    # Filter by player name if provided
    if player_name is not None:
        player_name = player_name.lower()

        # Check for playerName or playerNameI columns
        if 'playerName' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['playerName'].str.lower().str.contains(player_name, na=False)]
        elif 'playerNameI' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['playerNameI'].str.lower().str.contains(player_name, na=False)]
        else:
            # If neither column exists, check description
            filtered_df = filtered_df[filtered_df['description'].str.lower().str.contains(player_name, na=False)]

    return filtered_df.reset_index(drop=True)

def _filter_plays_by_team(plays_df: pd.DataFrame, team_id: int = None, team_tricode: str = None) -> pd.DataFrame:
    """
    Filters plays by team ID or tricode.

    Args:
        plays_df: DataFrame containing play-by-play data
        team_id: Team ID to filter by
        team_tricode: Team tricode to filter by (e.g., 'LAL', 'BOS')

    Returns:
        Filtered DataFrame
    """
    if (team_id is None and team_tricode is None) or plays_df.empty:
        return plays_df

    filtered_df = plays_df.copy()

    # Filter by team ID if provided
    if team_id is not None:
        if 'teamId' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['teamId'] == team_id]
        else:
            # If teamId column doesn't exist, return empty DataFrame
            return pd.DataFrame(columns=filtered_df.columns)

    # Filter by team tricode if provided
    if team_tricode is not None:
        team_tricode = team_tricode.upper()

        if 'teamTricode' in filtered_df.columns:
            filtered_df = filtered_df[filtered_df['teamTricode'] == team_tricode]
        else:
            # If teamTricode column doesn't exist, check description
            filtered_df = filtered_df[filtered_df['description'].str.upper().str.contains(team_tricode, na=False)]

    return filtered_df.reset_index(drop=True)

def _format_historical_pbp_dataframe(pbp_df: pd.DataFrame) -> pd.DataFrame:
    """
    Formats the historical play-by-play DataFrame for better usability.

    Args:
        pbp_df: Raw DataFrame from PlayByPlayV3

    Returns:
        Formatted DataFrame with standardized column names and values
    """
    if pbp_df.empty:
        return pbp_df

    # Create a copy to avoid modifying the original
    formatted_df = pbp_df.copy()

    # Format the clock column (e.g., PT11M58.00S to 11:58)
    if 'clock' in formatted_df.columns:
        formatted_df['clock'] = formatted_df['clock'].apply(
            lambda x: str(x).replace("PT", "").replace("M", ":").replace("S", "").split(".")[0] if pd.notna(x) else ""
        )

    # Create a score column combining home and away scores
    if 'scoreHome' in formatted_df.columns and 'scoreAway' in formatted_df.columns:
        formatted_df['score'] = formatted_df.apply(
            lambda row: f"{row['scoreHome']}-{row['scoreAway']}"
            if pd.notna(row['scoreHome']) and pd.notna(row['scoreAway']) else None,
            axis=1
        )

    # Create a standardized event_type column
    if 'actionType' in formatted_df.columns and 'subType' in formatted_df.columns:
        formatted_df['event_type'] = formatted_df.apply(
            lambda row: f"{str(row['actionType']).upper()}_{str(row['subType']).upper()}"
            if pd.notna(row['actionType']) else "",
            axis=1
        )

    return formatted_df

def _format_live_pbp_dataframe(live_data_dict: Dict[str, Any]) -> pd.DataFrame:
    """
    Converts live play-by-play data dictionary to a DataFrame and formats it.

    Args:
        live_data_dict: Dictionary from LivePlayByPlay endpoint

    Returns:
        Formatted DataFrame with standardized columns
    """
    raw_actions_list = live_data_dict.get('game', {}).get('actions', [])

    if not raw_actions_list:
        return pd.DataFrame()

    # Convert list of dictionaries to DataFrame
    df = pd.DataFrame(raw_actions_list)

    if df.empty:
        return df

    # Get team information for mapping team IDs to home/away
    game_details_dict = live_data_dict.get('game', {})
    home_team_id_live = game_details_dict.get('homeTeam', {}).get('teamId')
    away_team_id_live = game_details_dict.get('awayTeam', {}).get('teamId')

    # Add team information
    if 'teamId' in df.columns:
        df['team'] = df['teamId'].apply(
            lambda x: "home" if x == home_team_id_live else
                     "away" if x == away_team_id_live else "neutral"
        )

    # Format the clock column
    if 'clock' in df.columns:
        df['clock'] = df['clock'].apply(
            lambda x: re.match(r"PT(\d+)M(\d+\.?\d*)S", str(x)).group(1) + ":" +
                     re.match(r"PT(\d+)M(\d+\.?\d*)S", str(x)).group(2).split('.')[0].zfill(2)
                     if re.match(r"PT(\d+)M(\d+\.?\d*)S", str(x)) else x
        )

    # Create a score column
    if 'scoreHome' in df.columns and 'scoreAway' in df.columns:
        df['score'] = df.apply(
            lambda row: f"{row['scoreHome']}-{row['scoreAway']}"
            if pd.notna(row['scoreHome']) and pd.notna(row['scoreAway']) else None,
            axis=1
        )

    # Create a standardized event_type column
    if 'actionType' in df.columns and 'subType' in df.columns:
        df['event_type'] = df.apply(
            lambda row: f"{str(row['actionType']).upper()}_{str(row['subType']).upper()}"
            if pd.notna(row['actionType']) else "",
            axis=1
        )

    return df

# --- Play-by-Play Logic Functions ---

def _fetch_historical_playbyplay_logic(
    game_id: str,
    start_period: int = 0,
    end_period: int = 0,
    event_types: List[str] = None,
    player_name: str = None,
    person_id: int = None,
    team_id: int = None,
    team_tricode: str = None,
    return_dataframe: bool = False
) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
    """
    Fetches historical play-by-play data using PlayByPlayV3.

    Args:
        game_id: The ID of the game.
        start_period: The starting period to fetch. Defaults to 0 (all).
        end_period: The ending period to fetch. Defaults to 0 (all).
        event_types: List of event types to filter by (e.g., ['SHOT', 'REBOUND', 'TURNOVER']).
        player_name: Filter plays by player name.
        person_id: Filter plays by player ID.
        team_id: Filter plays by team ID.
        team_tricode: Filter plays by team tricode (e.g., 'LAL', 'BOS').
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            Dict[str, Any]: A dictionary containing formatted PBP data.
                            Includes 'periods' list, 'has_video' flag, and 'source'.
        If return_dataframe=True:
            Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]: A tuple containing the formatted PBP data
                                                           and a dictionary of DataFrames.

    Raises:
        ValueError: If DataFrame processing fails for non-empty data.
        Exception: For NBA API call failures.
    """
    logger.info(f"Executing _fetch_historical_playbyplay_logic (V3) for game ID: {game_id}, periods {start_period}-{end_period}, return_dataframe={return_dataframe}")

    # Fetch data from the API
    pbp_endpoint = playbyplayv3.PlayByPlayV3(
        game_id=game_id, start_period=start_period, end_period=end_period, timeout=settings.DEFAULT_TIMEOUT_SECONDS
    )

    # Get DataFrames
    pbp_df = pbp_endpoint.play_by_play.get_data_frame()  # V3 uses camelCase
    video_df = pbp_endpoint.available_video.get_data_frame()  # V3 uses camelCase 'videoAvailable'

    # Format the DataFrame for better usability
    formatted_pbp_df = _format_historical_pbp_dataframe(pbp_df)

    # Apply filters if provided
    if event_types:
        formatted_pbp_df = _filter_plays_by_event_type(formatted_pbp_df, event_types)

    if player_name or person_id:
        formatted_pbp_df = _filter_plays_by_player(formatted_pbp_df, player_name, person_id)

    if team_id or team_tricode:
        formatted_pbp_df = _filter_plays_by_team(formatted_pbp_df, team_id, team_tricode)

    # Save to CSV if returning DataFrame
    if return_dataframe:
        csv_path = _get_csv_path_for_playbyplay(game_id, "historical_v3", start_period, end_period)
        _save_dataframe_to_csv(formatted_pbp_df, csv_path)

    # Process the DataFrame for JSON response
    processed_plays = _process_dataframe(pbp_df, single_row=False)
    if processed_plays is None:
        if pbp_df.empty:
            logger.warning(f"No historical PBP data found for game {game_id} via API (V3).")
            processed_plays = []
        else:
            raise ValueError(f"Failed to process non-empty historical PBP (V3) DataFrame for game {game_id}")

    # Organize plays by period
    periods_data = {}
    for play_item in processed_plays:  # play_item keys are now camelCase
        period_num = play_item.get("period", 0)
        if period_num not in periods_data:
            periods_data[period_num] = []

        # V3 provides scoreHome and scoreAway
        score_home_v3 = play_item.get('scoreHome')
        score_away_v3 = play_item.get('scoreAway')
        score_str_v3 = f"{score_home_v3}-{score_away_v3}" if score_home_v3 is not None and score_away_v3 is not None else None

        # Format the play data
        formatted_play = {
            "event_num": play_item.get("actionNumber"),
            "clock": play_item.get("clock", "").replace("PT", "").replace("M", ":").replace("S", "").split(".")[0],  # Format PT11M58.00S to 11:58
            "score": score_str_v3,
            "team_tricode": play_item.get("teamTricode"),  # Team performing action
            "person_id": play_item.get("personId"),
            "player_name": play_item.get("playerNameI"),  # Or playerName
            "description": play_item.get("description"),  # Main description
            "action_type": play_item.get("actionType"),
            "sub_type": play_item.get("subType"),
            "event_type": f"{play_item.get('actionType', '').upper()}_{play_item.get('subType', '').upper()}",
            "video_available": play_item.get("videoAvailable", False)  # V3 has this per play
        }

        # Add to the appropriate period
        periods_data[period_num].append(formatted_play)

    # Create the final periods list
    periods_list_final = [{"period": p_num, "plays": plays_list} for p_num, plays_list in sorted(periods_data.items())]

    # Check if video is available
    has_overall_video = bool(video_df.iloc[0]['videoAvailable'] == 1) if not video_df.empty and 'videoAvailable' in video_df.columns else False

    # Create the result dictionary
    result_dict = {
        "game_id": game_id,
        "has_video": has_overall_video,
        "source": "historical_v3",
        "filtered_periods": {"start": start_period, "end": end_period} if start_period > 0 or end_period > 0 else None,
        "periods": periods_list_final
    }

    # Add filter information if any filters were applied
    filters_applied = {}
    if event_types:
        filters_applied["event_types"] = event_types
    if player_name:
        filters_applied["player_name"] = player_name
    if person_id:
        filters_applied["person_id"] = person_id
    if team_id:
        filters_applied["team_id"] = team_id
    if team_tricode:
        filters_applied["team_tricode"] = team_tricode

    if filters_applied:
        result_dict["filters_applied"] = filters_applied

    # Return the appropriate result based on return_dataframe
    if return_dataframe:
        dataframes = {
            "play_by_play": formatted_pbp_df,
            "available_video": video_df
        }
        return result_dict, dataframes

    return result_dict

def _fetch_live_playbyplay_logic(
    game_id: str,
    event_types: List[str] = None,
    player_name: str = None,
    person_id: int = None,
    team_id: int = None,
    team_tricode: str = None,
    return_dataframe: bool = False
) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
    """
    Fetches live play-by-play data.

    Args:
        game_id: The ID of the game.
        event_types: List of event types to filter by (e.g., ['SHOT', 'REBOUND', 'TURNOVER']).
        player_name: Filter plays by player name.
        person_id: Filter plays by player ID.
        team_id: Filter plays by team ID.
        team_tricode: Filter plays by team tricode (e.g., 'LAL', 'BOS').
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            Dict[str, Any]: A dictionary containing formatted live PBP data.
                            Includes 'periods' list and 'source'. 'has_video' is False for live.
        If return_dataframe=True:
            Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]: A tuple containing the formatted live PBP data
                                                           and a dictionary of DataFrames.

    Raises:
        ValueError: If no live actions are found (game not live or recently concluded).
        Exception: For NBA API call failures or unexpected data structure.
    """
    logger.info(f"Executing _fetch_live_playbyplay_logic for game ID: {game_id}, return_dataframe={return_dataframe}")

    # Fetch data from the API
    live_pbp_endpoint = LivePlayByPlay(game_id=game_id)
    live_data_dict = live_pbp_endpoint.get_dict()
    raw_actions_list = live_data_dict.get('game', {}).get('actions', [])

    if not raw_actions_list:
        logger.warning(f"No live actions found for game {game_id}. Game might not be live or recently concluded.")
        raise ValueError("No live actions found, game may not be live.")

    # Convert to DataFrame and format
    formatted_pbp_df = _format_live_pbp_dataframe(live_data_dict)

    # Apply filters if provided
    if event_types:
        formatted_pbp_df = _filter_plays_by_event_type(formatted_pbp_df, event_types)

    if player_name or person_id:
        formatted_pbp_df = _filter_plays_by_player(formatted_pbp_df, player_name, person_id)

    if team_id or team_tricode:
        formatted_pbp_df = _filter_plays_by_team(formatted_pbp_df, team_id, team_tricode)

    # Save to CSV if returning DataFrame
    if return_dataframe:
        csv_path = _get_csv_path_for_playbyplay(game_id, "live")
        _save_dataframe_to_csv(formatted_pbp_df, csv_path)

    # Process for JSON response
    periods_data = {}
    game_details_dict = live_data_dict.get('game', {})
    home_team_id_live = game_details_dict.get('homeTeam', {}).get('teamId')
    away_team_id_live = game_details_dict.get('awayTeam', {}).get('teamId')

    # Use the filtered DataFrame if filters were applied
    actions_to_process = formatted_pbp_df.to_dict('records') if not formatted_pbp_df.empty else raw_actions_list

    for action_item in actions_to_process:
        period_num = action_item.get("period", 0)
        if period_num not in periods_data:
            periods_data[period_num] = []

        # Format clock string
        clock_raw_str = action_item.get("clock", "")
        match_obj = re.match(r"PT(\d+)M(\d+\.?\d*)S", str(clock_raw_str))
        clock_formatted_str = f"{match_obj.group(1)}:{match_obj.group(2).split('.')[0].zfill(2)}" if match_obj else clock_raw_str

        # Format score string
        score_home = action_item.get('scoreHome')
        score_away = action_item.get('scoreAway')
        score_str_live = f"{score_home}-{score_away}" if score_home is not None and score_away is not None else None

        # Determine team (home/away/neutral)
        action_team_id_live = action_item.get('teamId')
        team_str_live = "home" if action_team_id_live == home_team_id_live else "away" if action_team_id_live == away_team_id_live else "neutral"

        # Create formatted play dictionary
        formatted_play = {
            "event_num": action_item.get("actionNumber"),
            "clock": clock_formatted_str,
            "score": score_str_live,
            "team": team_str_live,  # home, away, or neutral
            "team_tricode": action_item.get("teamTricode"),
            "description": action_item.get("description"),
            "person_id": action_item.get("personId"),
            "player_name": action_item.get("playerNameI") or action_item.get("playerName"),  # Prefer playerNameI if available
            "action_type": action_item.get("actionType"),
            "sub_type": action_item.get("subType"),
            "event_type": f"{str(action_item.get('actionType', '')).upper()}_{str(action_item.get('subType', '')).upper()}"
        }

        # Add to the appropriate period
        periods_data[period_num].append(formatted_play)

    # Create the final periods list
    periods_list_final = [{"period": p_num, "plays": plays_list} for p_num, plays_list in sorted(periods_data.items())]

    # Create the result dictionary
    result_dict = {
        "game_id": game_id,
        "has_video": False,
        "source": "live",
        "filtered_periods": None,
        "periods": periods_list_final
    }

    # Add filter information if any filters were applied
    filters_applied = {}
    if event_types:
        filters_applied["event_types"] = event_types
    if player_name:
        filters_applied["player_name"] = player_name
    if person_id:
        filters_applied["person_id"] = person_id
    if team_id:
        filters_applied["team_id"] = team_id
    if team_tricode:
        filters_applied["team_tricode"] = team_tricode

    if filters_applied:
        result_dict["filters_applied"] = filters_applied

    # Return the appropriate result based on return_dataframe
    if return_dataframe:
        dataframes = {
            "play_by_play": formatted_pbp_df
        }
        return result_dict, dataframes

    return result_dict

# --- Main Public Function ---
def fetch_playbyplay_logic(
    game_id: str,
    start_period: int = 0,
    end_period: int = 0,
    event_types: List[str] = None,
    player_name: str = None,
    person_id: int = None,
    team_id: int = None,
    team_tricode: str = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches play-by-play data for a game. Attempts live data first if no period filters
    are applied, otherwise falls back to or directly uses historical data (PlayByPlayV3).

    Provides granular filtering options and DataFrame output capabilities.

    Args:
        game_id: The ID of the game.
        start_period: The starting period filter. Defaults to 0 (no filter).
        end_period: The ending period filter. Defaults to 0 (no filter).
        event_types: List of event types to filter by (e.g., ['SHOT', 'REBOUND', 'TURNOVER']).
                    Common event types include: 'SHOT', 'REBOUND', 'TURNOVER', 'FOUL', 'FREE_THROW',
                    'SUBSTITUTION', 'TIMEOUT', 'JUMP_BALL', 'BLOCK', 'STEAL', 'VIOLATION'.
        player_name: Filter plays by player name.
        person_id: Filter plays by player ID.
        team_id: Filter plays by team ID.
        team_tricode: Filter plays by team tricode (e.g., 'LAL', 'BOS').
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing the play-by-play data or an error message.
                 The response includes a 'source' field ("live" or "historical_v3")
                 and a 'periods' list, each containing plays for that period.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_playbyplay_logic for game ID: {game_id}, StartPeriod: {start_period}, EndPeriod: {end_period}, return_dataframe: {return_dataframe}")

    # Validate input parameters
    if not game_id:
        error_response = format_response(error=Errors.GAME_ID_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not validate_game_id_format(game_id):
        error_response = format_response(error=Errors.INVALID_GAME_ID_FORMAT.format(game_id=game_id))
        if return_dataframe:
            return error_response, {}
        return error_response

    # If specific periods are requested, directly fetch historical data as live PBP typically doesn't support period filtering.
    if start_period != 0 or end_period != 0:
        logger.info(f"Period filters active ({start_period}-{end_period}), fetching historical PBP directly for game {game_id}.")
        try:
            # Call the historical PBP logic with all parameters
            result = _fetch_historical_playbyplay_logic(
                game_id,
                start_period,
                end_period,
                event_types,
                player_name,
                person_id,
                team_id,
                team_tricode,
                return_dataframe
            )

            # Handle the result based on return_dataframe
            if return_dataframe:
                result_dict, dataframes = result
                result_dict["parameters"] = {
                    "start_period": start_period,
                    "end_period": end_period,
                    "note": "Filtered historical data with DataFrame output."
                }
                json_response = format_response(data=result_dict)
                return json_response, dataframes
            else:
                result_dict = result
                result_dict["parameters"] = {
                    "start_period": start_period,
                    "end_period": end_period,
                    "note": "Filtered historical data."
                }
                return format_response(data=result_dict)

        except Exception as hist_fetch_error:
            logger.error(f"Historical PBP fetch failed for game {game_id} (with period filters): {hist_fetch_error}", exc_info=True)
            error_msg = Errors.PLAYBYPLAY_API.format(game_id=game_id, error=str(hist_fetch_error))
            error_response = format_response(error=error_msg)

            if return_dataframe:
                return error_response, {}
            return error_response
    else:
        # No period filters, attempt live PBP first, then fallback to historical.
        try:
            logger.debug(f"Attempting live PBP fetch for game {game_id} (no period filters).")

            # Call the live PBP logic with all parameters
            result = _fetch_live_playbyplay_logic(
                game_id,
                event_types,
                player_name,
                person_id,
                team_id,
                team_tricode,
                return_dataframe
            )

            # Handle the result based on return_dataframe
            if return_dataframe:
                result_dict, dataframes = result
                result_dict["parameters"] = {
                    "start_period": 0,
                    "end_period": 0,
                    "note": "Live data with DataFrame output; period filters not applied."
                }
                json_response = format_response(data=result_dict)
                return json_response, dataframes
            else:
                result_dict = result
                result_dict["parameters"] = {
                    "start_period": 0,
                    "end_period": 0,
                    "note": "Live data; period filters not applied."
                }
                return format_response(data=result_dict)

        except ValueError as live_fetch_error:  # Raised by _fetch_live_playbyplay_logic if no live actions
            logger.warning(f"Live PBP fetch failed for game {game_id} ({live_fetch_error}), attempting historical (no period filters).")
            try:
                # Call the historical PBP logic with all parameters
                result = _fetch_historical_playbyplay_logic(
                    game_id,
                    0,
                    0,
                    event_types,
                    player_name,
                    person_id,
                    team_id,
                    team_tricode,
                    return_dataframe
                )

                # Handle the result based on return_dataframe
                if return_dataframe:
                    result_dict, dataframes = result
                    result_dict["parameters"] = {
                        "start_period": 0,
                        "end_period": 0,
                        "note": "Full historical data with DataFrame output after live fallback."
                    }
                    json_response = format_response(data=result_dict)
                    return json_response, dataframes
                else:
                    result_dict = result
                    result_dict["parameters"] = {
                        "start_period": 0,
                        "end_period": 0,
                        "note": "Full historical data after live fallback."
                    }
                    return format_response(data=result_dict)

            except Exception as hist_fetch_error:
                logger.error(f"Historical PBP fetch (fallback) also failed for game {game_id}: {hist_fetch_error}", exc_info=True)
                error_msg = Errors.PLAYBYPLAY_API.format(game_id=game_id, error=str(hist_fetch_error))
                error_response = format_response(error=error_msg)

                if return_dataframe:
                    return error_response, {}
                return error_response

        except Exception as e:  # Catch other unexpected errors during the live attempt phase
            logger.error(f"Unexpected error during initial live PBP fetch attempt for game {game_id}: {e}", exc_info=True)
            error_msg = Errors.PLAYBYPLAY_API.format(game_id=game_id, error=str(e))
            error_response = format_response(error=error_msg)

            if return_dataframe:
                return error_response, {}
            return error_response

===== backend\api_tools\game_rotation.py =====
"""
Handles fetching and processing game rotation data
from the GameRotation endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The GameRotation endpoint provides comprehensive game rotation data (12 columns):
- Game info: GAME_ID (1 column)
- Team info: TEAM_ID, TEAM_CITY, TEAM_NAME (3 columns)
- Player info: PERSON_ID, PLAYER_FIRST, PLAYER_LAST (3 columns)
- Rotation data: IN_TIME_REAL, OUT_TIME_REAL (2 columns)
- Performance: PLAYER_PTS, PT_DIFF, USG_PCT (3 columns)
- Game-specific data: Each game has detailed rotation information (33-44 records per game)
- Perfect for rotation analysis, performance impact, and coaching insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import gamerotation
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_game_id(game_id):
    """Validate game ID format."""
    if not game_id:
        return False
    
    try:
        # Check if it's a valid game ID (should be a 10-digit number)
        if len(game_id) == 10 and game_id.isdigit():
            return True
        return False
    except (ValueError, AttributeError):
        return False

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_ROTATION_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
GAME_ROTATION_CSV_DIR = get_cache_dir("game_rotation")

# Ensure cache directories exist
os.makedirs(GAME_ROTATION_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_game_rotation(
    game_id: str,
    league_id: str = "00",
    data_set_name: str = "GameRotation"
) -> str:
    """
    Generates a file path for saving game rotation DataFrame.
    
    Args:
        game_id: Game ID
        league_id: League ID (default: "00" for NBA)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    filename = f"game_rotation_game{game_id}_league{league_id}_{data_set_name}.csv"
    return get_cache_file_path(filename, "game_rotation")

# --- Parameter Validation ---
def _validate_game_rotation_params(
    game_id: str,
    league_id: str
) -> Optional[str]:
    """Validates parameters for fetch_game_rotation_logic."""
    if not game_id:
        return "game_id is required"
    if not _validate_game_id(game_id):
        return f"Invalid game_id format: {game_id}. Expected 10-digit game ID"
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=GAME_ROTATION_CACHE_SIZE)
def fetch_game_rotation_logic(
    game_id: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches game rotation data using the GameRotation endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        game_id: Game ID (required)
        league_id: League ID (default: "00" for NBA)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with game rotation data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_game_rotation_logic for Game: {game_id}, League: {league_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_game_rotation_params(game_id, league_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for game rotation: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for both possible data sets
        data_set_names = ["GameRotation", "AvailableRotation"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_game_rotation(game_id, league_id, data_set_name)
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading game rotation from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "game_id": game_id,
                    "league_id": league_id
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "game_id": game_id,
            "league_id": league_id
        }
        
        logger.debug(f"Calling GameRotation with parameters: {api_params}")
        game_rotation_endpoint = gamerotation.GameRotation(**api_params)
        
        # Get data frames
        list_of_dataframes = game_rotation_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["GameRotation", "AvailableRotation"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"GameRotation_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_game_rotation(game_id, league_id, data_set_name)
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_game_rotation_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_game_rotation(
    game_id: str,
    league_id: str = "00",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get game rotation data.
    
    Args:
        game_id: Game ID (required)
        league_id: League ID (default: "00" for NBA)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with game rotation data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_game_rotation_logic(
        game_id=game_id,
        league_id=league_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing GameRotation endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_game_rotation("0022400001")
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_game_rotation("0022400001", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("GameRotation endpoint test completed.")


===== backend\api_tools\game_visuals_analytics.py =====
"""
Handles fetching game-level visual and analytical data, specifically
game-wide shot charts and win probability play-by-play.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
import pandas as pd
from typing import Set, Dict, Any, List, Union, Tuple, Optional

from nba_api.stats.endpoints import (
    shotchartdetail,
    WinProbabilityPBP
)
from nba_api.stats.library.parameters import RunType
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_game_id_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
GAME_SHOTCHART_CACHE_SIZE = 64
GAME_WIN_PROB_CACHE_SIZE = 64
SHOTCHART_CONTEXT_MEASURE_GAME = "FGA"

_VALID_WP_RUN_TYPES: Set[str] = {"each play", "each second", "each poss"}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
SHOTCHART_CSV_DIR = os.path.join(CSV_CACHE_DIR, "shotcharts")
WIN_PROB_CSV_DIR = os.path.join(CSV_CACHE_DIR, "win_probability")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(SHOTCHART_CSV_DIR, exist_ok=True)
os.makedirs(WIN_PROB_CSV_DIR, exist_ok=True)

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _filter_shots_by_team(shots_df: pd.DataFrame, team_id: int = None, team_name: str = None) -> pd.DataFrame:
    """
    Filters shot chart data by team ID or name.

    Args:
        shots_df: DataFrame containing shot chart data
        team_id: Team ID to filter by
        team_name: Team name to filter by

    Returns:
        Filtered DataFrame
    """
    if (team_id is None and team_name is None) or shots_df.empty:
        return shots_df

    filtered_df = shots_df.copy()

    if team_id is not None and 'TEAM_ID' in filtered_df.columns:
        filtered_df = filtered_df[filtered_df['TEAM_ID'] == team_id]

    if team_name is not None and 'TEAM_NAME' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['TEAM_NAME'].str.upper().str.contains(team_name.upper(), na=False)]

    return filtered_df.reset_index(drop=True)

def _filter_shots_by_player(shots_df: pd.DataFrame, player_id: int = None, player_name: str = None) -> pd.DataFrame:
    """
    Filters shot chart data by player ID or name.

    Args:
        shots_df: DataFrame containing shot chart data
        player_id: Player ID to filter by
        player_name: Player name to filter by

    Returns:
        Filtered DataFrame
    """
    if (player_id is None and player_name is None) or shots_df.empty:
        return shots_df

    filtered_df = shots_df.copy()

    if player_id is not None and 'PLAYER_ID' in filtered_df.columns:
        filtered_df = filtered_df[filtered_df['PLAYER_ID'] == player_id]

    if player_name is not None and 'PLAYER_NAME' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['PLAYER_NAME'].str.upper().str.contains(player_name.upper(), na=False)]

    return filtered_df.reset_index(drop=True)

def _filter_shots_by_zone(shots_df: pd.DataFrame, zone_basic: str = None, zone_area: str = None, zone_range: str = None) -> pd.DataFrame:
    """
    Filters shot chart data by shot zone.

    Args:
        shots_df: DataFrame containing shot chart data
        zone_basic: Basic shot zone (e.g., 'Restricted Area', 'Mid-Range', '3PT Field Goal')
        zone_area: Shot zone area (e.g., 'Center', 'Left Side', 'Right Side')
        zone_range: Shot zone range (e.g., 'Less Than 8 ft.', '16-24 ft.')

    Returns:
        Filtered DataFrame
    """
    if (zone_basic is None and zone_area is None and zone_range is None) or shots_df.empty:
        return shots_df

    filtered_df = shots_df.copy()

    if zone_basic is not None and 'SHOT_ZONE_BASIC' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['SHOT_ZONE_BASIC'].str.upper().str.contains(zone_basic.upper(), na=False)]

    if zone_area is not None and 'SHOT_ZONE_AREA' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['SHOT_ZONE_AREA'].str.upper().str.contains(zone_area.upper(), na=False)]

    if zone_range is not None and 'SHOT_ZONE_RANGE' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['SHOT_ZONE_RANGE'].str.upper().str.contains(zone_range.upper(), na=False)]

    return filtered_df.reset_index(drop=True)

def _filter_shots_by_period(shots_df: pd.DataFrame, period: int = None) -> pd.DataFrame:
    """
    Filters shot chart data by period.

    Args:
        shots_df: DataFrame containing shot chart data
        period: Period number to filter by (1-4 for quarters, 5+ for overtime)

    Returns:
        Filtered DataFrame
    """
    if period is None or shots_df.empty or 'PERIOD' not in shots_df.columns:
        return shots_df

    filtered_df = shots_df[shots_df['PERIOD'] == period]
    return filtered_df.reset_index(drop=True)

def _filter_shots_by_shot_type(shots_df: pd.DataFrame, shot_type: str = None, shot_made: bool = None) -> pd.DataFrame:
    """
    Filters shot chart data by shot type and whether the shot was made.

    Args:
        shots_df: DataFrame containing shot chart data
        shot_type: Shot type to filter by (e.g., '2PT Field Goal', '3PT Field Goal')
        shot_made: If True, only include made shots; if False, only include missed shots

    Returns:
        Filtered DataFrame
    """
    if (shot_type is None and shot_made is None) or shots_df.empty:
        return shots_df

    filtered_df = shots_df.copy()

    if shot_type is not None and 'SHOT_TYPE' in filtered_df.columns:
        # Case-insensitive partial match
        filtered_df = filtered_df[filtered_df['SHOT_TYPE'].str.upper().str.contains(shot_type.upper(), na=False)]

    if shot_made is not None and 'SHOT_MADE_FLAG' in filtered_df.columns:
        filtered_df = filtered_df[filtered_df['SHOT_MADE_FLAG'] == (1 if shot_made else 0)]

    return filtered_df.reset_index(drop=True)

# --- Logic Functions ---
def fetch_shotchart_logic(
    game_id: str,
    team_id: int = None,
    team_name: str = None,
    player_id: int = None,
    player_name: str = None,
    period: int = None,
    shot_type: str = None,
    shot_made: bool = None,
    zone_basic: str = None,
    zone_area: str = None,
    zone_range: str = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches shot chart data for all players in a specific NBA game.
    Data is sourced from the nba_api's ShotChartDetail endpoint.

    Provides granular filtering options and DataFrame output capabilities.

    Args:
        game_id: NBA game ID (10-digit string).
        team_id: Filter shots by team ID.
        team_name: Filter shots by team name (case-insensitive partial match).
        player_id: Filter shots by player ID.
        player_name: Filter shots by player name (case-insensitive partial match).
        period: Filter shots by period number (1-4 for quarters, 5+ for overtime).
        shot_type: Filter shots by shot type (e.g., '2PT Field Goal', '3PT Field Goal').
        shot_made: If True, only include made shots; if False, only include missed shots.
        zone_basic: Filter shots by basic shot zone (e.g., 'Restricted Area', 'Mid-Range', '3PT Field Goal').
        zone_area: Filter shots by shot zone area (e.g., 'Center', 'Left Side', 'Right Side').
        zone_range: Filter shots by shot zone range (e.g., 'Less Than 8 ft.', '16-24 ft.').
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON-formatted string containing shot chart data grouped by team and league averages, or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.

    Notes:
        - Returns an error if the game_id is empty or invalid.
        - Returns an empty list for teams if no shot data is found.
        - Each shot includes player, period, time, type, made/missed, coordinates, and event info.
    """
    logger.info(f"Executing fetch_shotchart_logic for game ID: {game_id}, return_dataframe={return_dataframe}")

    if not game_id:
        error_response = format_response(error=Errors.GAME_ID_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not validate_game_id_format(game_id):
        error_response = format_response(error=Errors.INVALID_GAME_ID_FORMAT.format(game_id=game_id))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        # Fetch data from the API
        shotchart_endpoint = shotchartdetail.ShotChartDetail(
            game_id_nullable=game_id,
            team_id=0,
            player_id=0,  # team_id=0 and player_id=0 for game-wide chart
            context_measure_simple=SHOTCHART_CONTEXT_MEASURE_GAME,
            season_nullable=None,  # Season not needed for game-specific chart
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"shotchartdetail API call successful for game {game_id}")

        # Get DataFrames
        shots_df = shotchart_endpoint.shot_chart_detail.get_data_frame()
        league_avg_df = shotchart_endpoint.league_averages.get_data_frame()

        # Apply filters if provided
        filtered_shots_df = shots_df.copy()

        if team_id is not None or team_name is not None:
            filtered_shots_df = _filter_shots_by_team(filtered_shots_df, team_id, team_name)

        if player_id is not None or player_name is not None:
            filtered_shots_df = _filter_shots_by_player(filtered_shots_df, player_id, player_name)

        if period is not None:
            filtered_shots_df = _filter_shots_by_period(filtered_shots_df, period)

        if shot_type is not None or shot_made is not None:
            filtered_shots_df = _filter_shots_by_shot_type(filtered_shots_df, shot_type, shot_made)

        if zone_basic is not None or zone_area is not None or zone_range is not None:
            filtered_shots_df = _filter_shots_by_zone(filtered_shots_df, zone_basic, zone_area, zone_range)

        # Save to CSV if returning DataFrame
        if return_dataframe:
            # Create a descriptive filename based on filters
            filename_parts = [game_id]
            if team_id:
                filename_parts.append(f"team_{team_id}")
            if team_name:
                filename_parts.append(f"team_{team_name.replace(' ', '_')}")
            if player_id:
                filename_parts.append(f"player_{player_id}")
            if player_name:
                filename_parts.append(f"player_{player_name.replace(' ', '_')}")
            if period:
                filename_parts.append(f"period_{period}")
            if shot_type:
                filename_parts.append(f"shottype_{shot_type.replace(' ', '_')}")
            if shot_made is not None:
                filename_parts.append(f"made_{shot_made}")
            if zone_basic:
                filename_parts.append(f"zone_{zone_basic.replace(' ', '_')}")

            shots_csv_path = os.path.join(SHOTCHART_CSV_DIR, f"{'_'.join(filename_parts)}_shots.csv")
            league_avg_csv_path = os.path.join(SHOTCHART_CSV_DIR, f"{'_'.join(filename_parts)}_league_avg.csv")

            _save_dataframe_to_csv(filtered_shots_df, shots_csv_path)
            _save_dataframe_to_csv(league_avg_df, league_avg_csv_path)

        # Process DataFrames for JSON response
        shots_list = _process_dataframe(filtered_shots_df, single_row=False)
        league_avgs_list = _process_dataframe(league_avg_df, single_row=False)

        if shots_list is None or league_avgs_list is None:
            if filtered_shots_df.empty:
                logger.warning(f"No shot chart data found for game {game_id} from API or after filtering.")
                result_dict = {"game_id": game_id, "teams": [], "league_averages": league_avgs_list or []}

                if return_dataframe:
                    dataframes = {
                        "shots": filtered_shots_df,
                        "league_averages": league_avg_df
                    }
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            else:
                logger.error(f"DataFrame processing failed for shot chart of game {game_id}")
                error_msg = Errors.SHOTCHART_PROCESSING.format(game_id=game_id)
                error_response = format_response(error=error_msg)

                if return_dataframe:
                    return error_response, {}
                return error_response

        if not shots_list:
            logger.warning(f"No shot data processed for game {game_id}.")
            result_dict = {"game_id": game_id, "teams": [], "league_averages": league_avgs_list or []}

            if return_dataframe:
                dataframes = {
                    "shots": filtered_shots_df,
                    "league_averages": league_avg_df
                }
                return format_response(result_dict), dataframes
            return format_response(result_dict)

        # Organize shots by team
        teams_data = {}
        for shot_item in shots_list:
            team_id_shot = shot_item.get("TEAM_ID")
            if team_id_shot is None:
                continue

            if team_id_shot not in teams_data:
                teams_data[team_id_shot] = {
                    "team_name": shot_item.get("TEAM_NAME"),
                    "team_id": team_id_shot,
                    "shots": []
                }

            # Format the shot data
            teams_data[team_id_shot]["shots"].append({
                "player": {"id": shot_item.get("PLAYER_ID"), "name": shot_item.get("PLAYER_NAME")},
                "period": shot_item.get("PERIOD"),
                "time_remaining": f"{shot_item.get('MINUTES_REMAINING', 0)}:{str(shot_item.get('SECONDS_REMAINING', 0)).zfill(2)}",
                "shot_type": shot_item.get("SHOT_TYPE"),
                "made": shot_item.get("SHOT_MADE_FLAG") == 1,
                "coordinates": {"x": shot_item.get("LOC_X"), "y": shot_item.get("LOC_Y")},
                "action_type": shot_item.get("ACTION_TYPE"),
                "shot_zone_basic": shot_item.get("SHOT_ZONE_BASIC"),
                "shot_zone_area": shot_item.get("SHOT_ZONE_AREA"),
                "shot_zone_range": shot_item.get("SHOT_ZONE_RANGE"),
                "shot_distance": shot_item.get("SHOT_DISTANCE"),
                "event_num": shot_item.get("GAME_EVENT_ID")
            })

        # Create the result dictionary
        result_dict = {
            "game_id": game_id,
            "teams": list(teams_data.values()),
            "league_averages": league_avgs_list or []
        }

        # Add filter information if any filters were applied
        filters_applied = {}
        if team_id:
            filters_applied["team_id"] = team_id
        if team_name:
            filters_applied["team_name"] = team_name
        if player_id:
            filters_applied["player_id"] = player_id
        if player_name:
            filters_applied["player_name"] = player_name
        if period:
            filters_applied["period"] = period
        if shot_type:
            filters_applied["shot_type"] = shot_type
        if shot_made is not None:
            filters_applied["shot_made"] = shot_made
        if zone_basic:
            filters_applied["zone_basic"] = zone_basic
        if zone_area:
            filters_applied["zone_area"] = zone_area
        if zone_range:
            filters_applied["zone_range"] = zone_range

        if filters_applied:
            result_dict["filters_applied"] = filters_applied

        logger.info(f"fetch_shotchart_logic completed for game {game_id}")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "shots": filtered_shots_df,
                "league_averages": league_avg_df
            }
            return format_response(result_dict), dataframes

        return format_response(result_dict)

    except Exception as e:
        logger.error(f"Error fetching shot chart for game {game_id}: {str(e)}", exc_info=True)
        error_msg = Errors.SHOTCHART_API.format(game_id=game_id, error=str(e))
        error_response = format_response(error=error_msg)

        if return_dataframe:
            return error_response, {}
        return error_response

def fetch_win_probability_logic(
    game_id: str,
    run_type: str = RunType.default,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches win probability data for a specific NBA game.
    Data is sourced from the nba_api's WinProbabilityPBP endpoint.

    Provides DataFrame output capabilities.

    Args:
        game_id: NBA game ID (10-digit string).
        run_type: Run type for win probability calculation (e.g., 'each play', 'each second', 'each poss').
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON-formatted string containing game info and win probability PBP data, or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.

    Notes:
        - Returns an error if the game_id is empty or invalid, or if run_type is not supported.
        - Filters out win probability rows with missing EVENT_NUM.
        - Returns empty lists if no valid data is found.
    """
    logger.info(f"Executing fetch_win_probability_logic for game {game_id}, RunType: {run_type}, return_dataframe={return_dataframe}")

    if not game_id:
        error_response = format_response(error=Errors.GAME_ID_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not validate_game_id_format(game_id):
        error_response = format_response(error=Errors.INVALID_GAME_ID_FORMAT.format(game_id=game_id))
        if return_dataframe:
            return error_response, {}
        return error_response

    if run_type not in _VALID_WP_RUN_TYPES:
        error_response = format_response(error=Errors.INVALID_RUN_TYPE.format(value=run_type, options=", ".join(list(_VALID_WP_RUN_TYPES)[:5])))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        # Fetch data from the API
        wp_endpoint = WinProbabilityPBP(game_id=game_id, run_type=run_type, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        logger.debug(f"WinProbabilityPBP API call successful for {game_id}")

        # Get DataFrames
        game_info_df = wp_endpoint.game_info.get_data_frame()
        win_prob_df = wp_endpoint.win_prob_p_bp.get_data_frame()

        # Filter win probability data
        win_prob_df_filtered = win_prob_df
        if not win_prob_df.empty and 'EVENT_NUM' in win_prob_df.columns:
            # Filter out rows where EVENT_NUM might be NaN or None, ensuring they are actual PBP events
            win_prob_df_filtered = win_prob_df[pd.notna(win_prob_df['EVENT_NUM'])]
            if win_prob_df_filtered.empty:
                logger.warning(f"Win probability PBP data for game {game_id} had EVENT_NUM column but all values were NaN/None.")
        elif not win_prob_df.empty:
            logger.warning(f"Win probability PBP data for game {game_id} is missing EVENT_NUM column.")

        # Save to CSV if returning DataFrame
        if return_dataframe:
            # Create descriptive filenames
            game_info_csv_path = os.path.join(WIN_PROB_CSV_DIR, f"{game_id}_run_{run_type.replace(' ', '_')}_game_info.csv")
            win_prob_csv_path = os.path.join(WIN_PROB_CSV_DIR, f"{game_id}_run_{run_type.replace(' ', '_')}_win_prob.csv")

            _save_dataframe_to_csv(game_info_df, game_info_csv_path)
            _save_dataframe_to_csv(win_prob_df_filtered, win_prob_csv_path)

        # Process DataFrames for JSON response
        game_info_dict = _process_dataframe(game_info_df, single_row=True)
        win_probs_list = _process_dataframe(win_prob_df_filtered, single_row=False) if not win_prob_df_filtered.empty else []

        # Check if essential data processing failed
        if game_info_dict is None:  # _process_dataframe failed for game_info
            logger.error(f"DataFrame processing failed for game_info of win probability for game {game_id}.")
            error_msg = Errors.PROCESSING_ERROR.format(error=f"game_info for win probability data for game {game_id}")
            error_response = format_response(error=error_msg)

            if return_dataframe:
                return error_response, {}
            return error_response

        # Create the result dictionary
        result_dict = {
            "game_id": game_id,
            "game_info": game_info_dict or {},
            "win_probability": win_probs_list or [],
            "run_type": run_type
        }

        logger.info(f"Successfully fetched win probability for game {game_id}")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "game_info": game_info_df,
                "win_probability": win_prob_df_filtered
            }
            return format_response(result_dict), dataframes

        return format_response(result_dict)

    except Exception as e:
        logger.error(f"Error fetching win probability for {game_id}: {e}", exc_info=True)
        error_msg = Errors.WINPROBABILITY_API.format(game_id=game_id, error=str(e))
        error_response = format_response(error=error_msg)

        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\homepage_leaders.py =====
"""
Handles fetching and processing homepage leaders data
from the HomePageLeaders endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The HomePageLeaders endpoint provides comprehensive homepage leaders data (3 DataFrames):
- Team Leaders: Top 5 teams with rankings and detailed stats (11 columns)
- League Averages: League-wide averages for all statistical categories (7 columns)
- League Highs: League-wide highest values for all statistical categories (7 columns)
- Rich current data: Cleveland Cavaliers leading with 121.9 PPG, Memphis Grizzlies 121.7 PPG, Denver Nuggets 120.8 PPG
- Perfect for homepage showcase, team rankings, and league context
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import homepageleaders
from nba_api.stats.library.parameters import SeasonTypePlayoffs, PlayerOrTeam, PlayerScope, StatCategory, GameScopeDetailed
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
HOMEPAGE_LEADERS_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PLAYER_OR_TEAM: Set[str] = {PlayerOrTeam.team, PlayerOrTeam.player}
VALID_PLAYER_SCOPE: Set[str] = {PlayerScope.all_players, PlayerScope.rookies}
VALID_SEASON_TYPE: Set[str] = {SeasonTypePlayoffs.regular, SeasonTypePlayoffs.playoffs}
VALID_STAT_CATEGORY: Set[str] = {StatCategory.points, StatCategory.rebounds, StatCategory.assists}
VALID_GAME_SCOPE: Set[str] = {GameScopeDetailed.season, GameScopeDetailed.last_10}

# --- Cache Directory Setup ---
HOMEPAGE_LEADERS_CSV_DIR = get_cache_dir("homepage_leaders")

# Ensure cache directories exist
os.makedirs(HOMEPAGE_LEADERS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_homepage_leaders(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_category: str = StatCategory.points,
    game_scope_detailed: str = GameScopeDetailed.season,
    data_set_name: str = "HomePageLeaders"
) -> str:
    """
    Generates a file path for saving homepage leaders DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_category: Statistical category (default: Points)
        game_scope_detailed: Game scope (default: Season)
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"homepage_leaders_league{league_id}",
        f"season{season}",
        f"type{season_type_playoffs.replace(' ', '_')}",
        f"scope{player_or_team}",
        f"players{player_scope.replace(' ', '_')}",
        f"stat{stat_category.replace(' ', '_')}",
        f"game{game_scope_detailed.replace(' ', '_')}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "homepage_leaders")

# --- Parameter Validation ---
def _validate_homepage_leaders_params(
    league_id: str,
    season: str,
    season_type_playoffs: str,
    player_or_team: str,
    player_scope: str,
    stat_category: str,
    game_scope_detailed: str
) -> Optional[str]:
    """Validates parameters for fetch_homepage_leaders_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_playoffs not in VALID_SEASON_TYPE:
        return f"Invalid season_type_playoffs: {season_type_playoffs}. Valid options: {', '.join(VALID_SEASON_TYPE)}"
    if player_or_team not in VALID_PLAYER_OR_TEAM:
        return f"Invalid player_or_team: {player_or_team}. Valid options: {', '.join(VALID_PLAYER_OR_TEAM)}"
    if player_scope not in VALID_PLAYER_SCOPE:
        return f"Invalid player_scope: {player_scope}. Valid options: {', '.join(VALID_PLAYER_SCOPE)}"
    if stat_category not in VALID_STAT_CATEGORY:
        return f"Invalid stat_category: {stat_category}. Valid options: {', '.join(VALID_STAT_CATEGORY)}"
    if game_scope_detailed not in VALID_GAME_SCOPE:
        return f"Invalid game_scope_detailed: {game_scope_detailed}. Valid options: {', '.join(VALID_GAME_SCOPE)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=HOMEPAGE_LEADERS_CACHE_SIZE)
def fetch_homepage_leaders_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_category: str = StatCategory.points,
    game_scope_detailed: str = GameScopeDetailed.season,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches homepage leaders data using the HomePageLeaders endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_category: Statistical category (default: Points)
        game_scope_detailed: Game scope (default: Season)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with homepage leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_homepage_leaders_logic for League: {league_id}, Season: {season}, "
        f"Type: {season_type_playoffs}, Scope: {player_or_team}, Players: {player_scope}, "
        f"Stat: {stat_category}, Game: {game_scope_detailed}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_homepage_leaders_params(
        league_id, season, season_type_playoffs, player_or_team, player_scope, stat_category, game_scope_detailed
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for homepage leaders: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV files
    dataframes = {}

    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["HomePageLeaders", "LeagueAverage", "LeagueHigh"]
        all_cached = True

        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_homepage_leaders(
                league_id, season, season_type_playoffs, player_or_team, player_scope,
                stat_category, game_scope_detailed, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading homepage leaders from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break

        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id,
                    "season": season,
                    "season_type_playoffs": season_type_playoffs,
                    "player_or_team": player_or_team,
                    "player_scope": player_scope,
                    "stat_category": stat_category,
                    "game_scope_detailed": game_scope_detailed
                },
                "data_sets": {}
            }

            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)

            return format_response(result_dict), dataframes

    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season,
            "season_type_playoffs": season_type_playoffs,
            "player_or_team": player_or_team,
            "player_scope": player_scope,
            "stat_category": stat_category,
            "game_scope_detailed": game_scope_detailed
        }

        logger.debug(f"Calling HomePageLeaders with parameters: {api_params}")
        homepage_leaders_endpoint = homepageleaders.HomePageLeaders(**api_params)

        # Get data frames
        list_of_dataframes = homepage_leaders_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Process each data frame
        data_set_names = ["HomePageLeaders", "LeagueAverage", "LeagueHigh"]

        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"HomePageLeaders_{idx}"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_homepage_leaders(
                        league_id, season, season_type_playoffs, player_or_team, player_scope,
                        stat_category, game_scope_detailed, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_homepage_leaders_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_homepage_leaders(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_category: str = StatCategory.points,
    game_scope_detailed: str = GameScopeDetailed.season,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get homepage leaders data.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_category: Statistical category (default: Points)
        game_scope_detailed: Game scope (default: Season)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with homepage leaders data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_homepage_leaders_logic(
        league_id=league_id,
        season=season,
        season_type_playoffs=season_type_playoffs,
        player_or_team=player_or_team,
        player_scope=player_scope,
        stat_category=stat_category,
        game_scope_detailed=game_scope_detailed,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing HomePageLeaders endpoint...")

    # Test 1: Basic fetch
    json_response = get_homepage_leaders()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_homepage_leaders(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("HomePageLeaders endpoint test completed.")


===== backend\api_tools\homepage_v2.py =====
"""
Handles fetching and processing homepage version 2 data
from the HomePageV2 endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The HomePageV2 endpoint provides enhanced homepage data (8 DataFrames):
- Points Leaders: Top 5 teams in scoring (5 columns)
- Rebounds Leaders: Top 5 teams in rebounding (5 columns)
- Assists Leaders: Top 5 teams in assists (5 columns)
- Steals Leaders: Top 5 teams in steals (5 columns)
- Field Goal % Leaders: Top 5 teams in shooting percentage (5 columns)
- Free Throw % Leaders: Top 5 teams in free throw percentage (5 columns)
- 3-Point % Leaders: Top 5 teams in three-point percentage (5 columns)
- Blocks Leaders: Top 5 teams in blocks (5 columns)
- Perfect for enhanced homepage, category-specific leaders, and visual dashboards
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import homepagev2
from nba_api.stats.library.parameters import SeasonTypePlayoffs, PlayerOrTeam, PlayerScope, GameScopeDetailed
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
HOMEPAGE_V2_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PLAYER_OR_TEAM: Set[str] = {PlayerOrTeam.team, PlayerOrTeam.player}
VALID_PLAYER_SCOPE: Set[str] = {PlayerScope.all_players, PlayerScope.rookies}
VALID_SEASON_TYPE: Set[str] = {SeasonTypePlayoffs.regular, SeasonTypePlayoffs.playoffs}
VALID_STAT_TYPE: Set[str] = {"Traditional", "Advanced"}
VALID_GAME_SCOPE: Set[str] = {GameScopeDetailed.season, GameScopeDetailed.last_10}

# --- Cache Directory Setup ---
HOMEPAGE_V2_CSV_DIR = get_cache_dir("homepage_v2")

# Ensure cache directories exist
os.makedirs(HOMEPAGE_V2_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_homepage_v2(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_type: str = "Traditional",
    game_scope_detailed: str = GameScopeDetailed.season,
    data_set_name: str = "HomePageV2"
) -> str:
    """
    Generates a file path for saving homepage v2 DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_type: Statistical type (default: Traditional)
        game_scope_detailed: Game scope (default: Season)
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"homepage_v2_league{league_id}",
        f"season{season}",
        f"type{season_type_playoffs.replace(' ', '_')}",
        f"scope{player_or_team}",
        f"players{player_scope.replace(' ', '_')}",
        f"stat{stat_type.replace(' ', '_')}",
        f"game{game_scope_detailed.replace(' ', '_')}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "homepage_v2")

# --- Parameter Validation ---
def _validate_homepage_v2_params(
    league_id: str,
    season: str,
    season_type_playoffs: str,
    player_or_team: str,
    player_scope: str,
    stat_type: str,
    game_scope_detailed: str
) -> Optional[str]:
    """Validates parameters for fetch_homepage_v2_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_playoffs not in VALID_SEASON_TYPE:
        return f"Invalid season_type_playoffs: {season_type_playoffs}. Valid options: {', '.join(VALID_SEASON_TYPE)}"
    if player_or_team not in VALID_PLAYER_OR_TEAM:
        return f"Invalid player_or_team: {player_or_team}. Valid options: {', '.join(VALID_PLAYER_OR_TEAM)}"
    if player_scope not in VALID_PLAYER_SCOPE:
        return f"Invalid player_scope: {player_scope}. Valid options: {', '.join(VALID_PLAYER_SCOPE)}"
    if stat_type not in VALID_STAT_TYPE:
        return f"Invalid stat_type: {stat_type}. Valid options: {', '.join(VALID_STAT_TYPE)}"
    if game_scope_detailed not in VALID_GAME_SCOPE:
        return f"Invalid game_scope_detailed: {game_scope_detailed}. Valid options: {', '.join(VALID_GAME_SCOPE)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=HOMEPAGE_V2_CACHE_SIZE)
def fetch_homepage_v2_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_type: str = "Traditional",
    game_scope_detailed: str = GameScopeDetailed.season,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches homepage v2 data using the HomePageV2 endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_type: Statistical type (default: Traditional)
        game_scope_detailed: Game scope (default: Season)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with homepage v2 data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_homepage_v2_logic for League: {league_id}, Season: {season}, "
        f"Type: {season_type_playoffs}, Scope: {player_or_team}, Players: {player_scope}, "
        f"Stat: {stat_type}, Game: {game_scope_detailed}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_homepage_v2_params(
        league_id, season, season_type_playoffs, player_or_team, player_scope, stat_type, game_scope_detailed
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for homepage v2: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV files
    dataframes = {}

    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets (8 statistical categories)
        data_set_names = ["Points", "Rebounds", "Assists", "Steals", "FieldGoalPct", "FreeThrowPct", "ThreePointPct", "Blocks"]
        all_cached = True

        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_homepage_v2(
                league_id, season, season_type_playoffs, player_or_team, player_scope,
                stat_type, game_scope_detailed, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading homepage v2 from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break

        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id,
                    "season": season,
                    "season_type_playoffs": season_type_playoffs,
                    "player_or_team": player_or_team,
                    "player_scope": player_scope,
                    "stat_type": stat_type,
                    "game_scope_detailed": game_scope_detailed
                },
                "data_sets": {}
            }

            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)

            return format_response(result_dict), dataframes

    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season,
            "season_type_playoffs": season_type_playoffs,
            "player_or_team": player_or_team,
            "player_scope": player_scope,
            "stat_type": stat_type,
            "game_scope_detailed": game_scope_detailed
        }

        logger.debug(f"Calling HomePageV2 with parameters: {api_params}")
        homepage_v2_endpoint = homepagev2.HomePageV2(**api_params)

        # Get data frames
        list_of_dataframes = homepage_v2_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Process each data frame
        data_set_names = ["Points", "Rebounds", "Assists", "Steals", "FieldGoalPct", "FreeThrowPct", "ThreePointPct", "Blocks"]

        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"HomePageV2_{idx}"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_homepage_v2(
                        league_id, season, season_type_playoffs, player_or_team, player_scope,
                        stat_type, game_scope_detailed, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_homepage_v2_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_homepage_v2(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    stat_type: str = "Traditional",
    game_scope_detailed: str = GameScopeDetailed.season,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get homepage v2 data.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        stat_type: Statistical type (default: Traditional)
        game_scope_detailed: Game scope (default: Season)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with homepage v2 data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_homepage_v2_logic(
        league_id=league_id,
        season=season,
        season_type_playoffs=season_type_playoffs,
        player_or_team=player_or_team,
        player_scope=player_scope,
        stat_type=stat_type,
        game_scope_detailed=game_scope_detailed,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing HomePageV2 endpoint...")

    # Test 1: Basic fetch
    json_response = get_homepage_v2()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_homepage_v2(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("HomePageV2 endpoint test completed.")


===== backend\api_tools\http_client.py =====
from nba_api.stats.static import players
from nba_api.stats import endpoints
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging
from nba_api.stats.library.http import NBAStatsHTTP
from config import settings
from typing import Optional

logger = logging.getLogger(__name__)

# Constants for HTTP client configuration
DEFAULT_MAX_RETRIES = 3
DEFAULT_BACKOFF_FACTOR = 1
RETRY_STATUS_CODES = [500, 502, 503, 504]
DEFAULT_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

def configure_nba_api_client(timeout: Optional[int] = None):
    """Configure the NBA API client with custom settings for better reliability.

    Args:
        timeout (Optional[int]): Timeout in seconds for API requests. Defaults to settings.DEFAULT_TIMEOUT_SECONDS.
    """
    try:
        session = requests.Session()
        retries = Retry(
            total=DEFAULT_MAX_RETRIES,
            backoff_factor=DEFAULT_BACKOFF_FACTOR,
            status_forcelist=RETRY_STATUS_CODES,
        )

        # Mount the retry adapter to both HTTP and HTTPS requests
        adapter = HTTPAdapter(max_retries=retries)
        session.mount('http://', adapter)
        session.mount('https://', adapter)

        # Create NBA stats client
        nba_session = NBAStatsHTTP()

        # Configure the session
        nba_session.session = session
        nba_session.timeout = timeout or settings.DEFAULT_TIMEOUT_SECONDS
        nba_session.headers = {
            'User-Agent': DEFAULT_USER_AGENT
        }

        logger.info(f"NBA API client configured successfully with timeout {nba_session.timeout}s")
        return nba_session

    except Exception as e:
        logger.error(f"Failed to configure NBA API client: {str(e)}")
        raise

# Configure the NBA API client
nba_session = configure_nba_api_client()

# Patch the NBA API's internal requests session
players.requests = nba_session
endpoints.requests = nba_session

===== backend\api_tools\hustle_stats_boxscore.py =====
"""
Handles fetching and processing hustle stats box score data
from the HustleStatsBoxScore endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The HustleStatsBoxScore endpoint provides comprehensive hustle stats data (3 DataFrames):
- Game Status: Game ID and hustle status (2 columns)
- Player Stats: Individual player hustle statistics (19-22 players, 25 columns)
- Team Stats: Team totals for all hustle metrics (2 teams, 22 columns)
- Rich hustle metrics: Contested shots, deflections, charges drawn, screen assists, loose balls recovered, box outs
- Perfect for advanced analytics, player evaluation, and performance tracking
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import hustlestatsboxscore
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_game_id(game_id):
    """Validate game ID format."""
    if not game_id:
        return False
    
    try:
        # Check if it's a valid game ID (should be a 10-digit number)
        if len(game_id) == 10 and game_id.isdigit():
            return True
        return False
    except (ValueError, AttributeError):
        return False

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
HUSTLE_STATS_CACHE_SIZE = 128

# --- Cache Directory Setup ---
HUSTLE_STATS_CSV_DIR = get_cache_dir("hustle_stats")

# Ensure cache directories exist
os.makedirs(HUSTLE_STATS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_hustle_stats(
    game_id: str,
    data_set_name: str = "HustleStats"
) -> str:
    """
    Generates a file path for saving hustle stats DataFrame.
    
    Args:
        game_id: Game ID
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    filename = f"hustle_stats_game{game_id}_{data_set_name}.csv"
    return get_cache_file_path(filename, "hustle_stats")

# --- Parameter Validation ---
def _validate_hustle_stats_params(game_id: str) -> Optional[str]:
    """Validates parameters for fetch_hustle_stats_logic."""
    if not game_id:
        return "game_id is required"
    if not _validate_game_id(game_id):
        return f"Invalid game_id format: {game_id}. Expected 10-digit game ID"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=HUSTLE_STATS_CACHE_SIZE)
def fetch_hustle_stats_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches hustle stats box score data using the HustleStatsBoxScore endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        game_id: Game ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with hustle stats data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_hustle_stats_logic for Game: {game_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_hustle_stats_params(game_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for hustle stats: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["GameStatus", "PlayerStats", "TeamStats"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_hustle_stats(game_id, data_set_name)
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 50:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading hustle stats from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "game_id": game_id
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "game_id": game_id
        }
        
        logger.debug(f"Calling HustleStatsBoxScore with parameters: {api_params}")
        hustle_stats_endpoint = hustlestatsboxscore.HustleStatsBoxScore(**api_params)
        
        # Get data frames
        list_of_dataframes = hustle_stats_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["GameStatus", "PlayerStats", "TeamStats"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"HustleStats_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_hustle_stats(game_id, data_set_name)
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_hustle_stats_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_hustle_stats_boxscore(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get hustle stats box score data.
    
    Args:
        game_id: Game ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with hustle stats data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_hustle_stats_logic(
        game_id=game_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing HustleStatsBoxScore endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_hustle_stats_boxscore("0022400001")
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_hustle_stats_boxscore("0022400001", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("HustleStatsBoxScore endpoint test completed.")


===== backend\api_tools\infographic_fanduel_player.py =====
"""
Handles fetching and processing FanDuel player infographic data
from the InfographicFanDuelPlayer endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The InfographicFanDuelPlayer endpoint provides comprehensive FanDuel fantasy data (1 DataFrame):
- Player Info: PLAYER_ID, PLAYER_NAME, TEAM_ID, TEAM_NAME, TEAM_ABBREVIATION, JERSEY_NUM, PLAYER_POSITION, LOCATION (8 columns)
- Fantasy Scoring: FAN_DUEL_PTS, NBA_FANTASY_PTS, USG_PCT (3 columns)
- Traditional Stats: Complete box score with 22 statistical categories
- Rich fantasy data: 19-26 players per game with detailed fantasy statistics (33 columns total)
- Perfect for fantasy analysis, DFS optimization, and player evaluation
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import infographicfanduelplayer
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_game_id(game_id):
    """Validate game ID format."""
    if not game_id:
        return False
    
    try:
        # Check if it's a valid game ID (should be a 10-digit number)
        if len(game_id) == 10 and game_id.isdigit():
            return True
        return False
    except (ValueError, AttributeError):
        return False

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
INFOGRAPHIC_FANDUEL_CACHE_SIZE = 128

# --- Cache Directory Setup ---
INFOGRAPHIC_FANDUEL_CSV_DIR = get_cache_dir("infographic_fanduel")

# Ensure cache directories exist
os.makedirs(INFOGRAPHIC_FANDUEL_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_infographic_fanduel(
    game_id: str,
    data_set_name: str = "FanDuelPlayers"
) -> str:
    """
    Generates a file path for saving infographic FanDuel DataFrame.
    
    Args:
        game_id: Game ID
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    filename = f"infographic_fanduel_game{game_id}_{data_set_name}.csv"
    return get_cache_file_path(filename, "infographic_fanduel")

# --- Parameter Validation ---
def _validate_infographic_fanduel_params(game_id: str) -> Optional[str]:
    """Validates parameters for fetch_infographic_fanduel_logic."""
    if not game_id:
        return "game_id is required"
    if not _validate_game_id(game_id):
        return f"Invalid game_id format: {game_id}. Expected 10-digit game ID"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=INFOGRAPHIC_FANDUEL_CACHE_SIZE)
def fetch_infographic_fanduel_logic(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches FanDuel player infographic data using the InfographicFanDuelPlayer endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        game_id: Game ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with FanDuel player data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_infographic_fanduel_logic for Game: {game_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_infographic_fanduel_params(game_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for infographic FanDuel: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_infographic_fanduel(game_id, "FanDuelPlayers")
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading infographic FanDuel from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "game_id": game_id
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["FanDuelPlayers"] = df
                result_dict["data_sets"]["FanDuelPlayers"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "game_id": game_id
        }
        
        logger.debug(f"Calling InfographicFanDuelPlayer with parameters: {api_params}")
        infographic_endpoint = infographicfanduelplayer.InfographicFanDuelPlayer(**api_params)
        
        # Get data frames
        list_of_dataframes = infographic_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"FanDuelPlayers_{idx}" if idx > 0 else "FanDuelPlayers"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_infographic_fanduel_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_infographic_fanduel_player(
    game_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get FanDuel player infographic data.
    
    Args:
        game_id: Game ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with FanDuel player data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_infographic_fanduel_logic(
        game_id=game_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing InfographicFanDuelPlayer endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_infographic_fanduel_player("0022400001")
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_infographic_fanduel_player("0022400001", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("InfographicFanDuelPlayer endpoint test completed.")


===== backend\api_tools\ist_standings.py =====
"""
Handles fetching and processing In-Season Tournament standings data
from the ISTStandings endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The ISTStandings endpoint provides comprehensive IST standings data (1 DataFrame):
- Team Info: leagueId, seasonYear, teamId, teamCity, teamName, teamAbbreviation, teamSlug, conference (8 columns)
- Tournament Structure: istGroup, clinchIndicator, clinchedIstKnockout, clinchedIstGroup, clinchedIstWildcard (5 columns)
- Rankings: istWildcardRank, istGroupRank, istKnockoutRank (3 columns)
- Record: wins, losses, pct, istGroupGb, istWildcardGb, diff, pts, oppPts (8 columns)
- Game Details: 4 games × 6 details each = 24 columns (gameId, opponent, location, status, outcome)
- Rich tournament data: 30 teams with detailed tournament statistics (48 columns total)
- Perfect for tournament tracking, group analysis, and historical data
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import iststandings
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
IST_STANDINGS_CACHE_SIZE = 32

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00"}  # Only NBA has IST
VALID_SECTIONS: Set[str] = {"group", "knockout"}

# --- Cache Directory Setup ---
IST_STANDINGS_CSV_DIR = get_cache_dir("ist_standings")

# Ensure cache directories exist
os.makedirs(IST_STANDINGS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_ist_standings(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    section: str = "group",
    data_set_name: str = "ISTStandings"
) -> str:
    """
    Generates a file path for saving IST standings DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        section: Section type (default: "group")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    filename = f"ist_standings_league{league_id}_season{season}_section{section}_{data_set_name}.csv"
    return get_cache_file_path(filename, "ist_standings")

# --- Parameter Validation ---
def _validate_ist_standings_params(
    league_id: str,
    season: str,
    section: str
) -> Optional[str]:
    """Validates parameters for fetch_ist_standings_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if section not in VALID_SECTIONS:
        return f"Invalid section: {section}. Valid options: {', '.join(VALID_SECTIONS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=IST_STANDINGS_CACHE_SIZE)
def fetch_ist_standings_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    section: str = "group",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches In-Season Tournament standings data using the ISTStandings endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        section: Section type (default: "group")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with IST standings data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_ist_standings_logic for League: {league_id}, Season: {season}, "
        f"Section: {section}, return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_ist_standings_params(league_id, season, section)
    if validation_error:
        logger.warning(f"Parameter validation failed for IST standings: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_ist_standings(league_id, season, section, "ISTStandings")
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading IST standings from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season,
                        "section": section
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["ISTStandings"] = df
                result_dict["data_sets"]["ISTStandings"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season,
            "section": section
        }
        
        logger.debug(f"Calling ISTStandings with parameters: {api_params}")
        ist_standings_endpoint = iststandings.ISTStandings(**api_params)
        
        # Get data frames
        list_of_dataframes = ist_standings_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"ISTStandings_{idx}" if idx > 0 else "ISTStandings"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_ist_standings_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_ist_standings(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    section: str = "group",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get In-Season Tournament standings data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        section: Section type (default: "group")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with IST standings data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_ist_standings_logic(
        league_id=league_id,
        season=season,
        section=section,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing ISTStandings endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_ist_standings()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_ist_standings(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("ISTStandings endpoint test completed.")


===== backend\api_tools\leaders_tiles.py =====
"""
Handles fetching and processing statistical leaders tiles data
from the LeadersTiles endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The LeadersTiles endpoint provides comprehensive statistical leaders data (4 DataFrames):
- Current Season Leaders: Top 5 teams in current season (5 columns)
- All-Time High Record: Historical best performance (1 team, 5 columns)
- Last Season Leaders: Top 5 teams from previous season (5 columns)
- All-Time Low Record: Historical worst performance (1 team, 5 columns)
- Rich leaders data: RANK, TEAM_ID, TEAM_ABBREVIATION, TEAM_NAME, [STAT]
- Perfect for visual leader boards, historical context, and statistical insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import leaderstiles
from nba_api.stats.library.parameters import SeasonTypePlayoffs, PlayerOrTeam, PlayerScope, GameScopeDetailed
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEADERS_TILES_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00"}  # Only NBA supported
VALID_PLAYER_OR_TEAM: Set[str] = {PlayerOrTeam.team, PlayerOrTeam.player}
VALID_PLAYER_SCOPE: Set[str] = {PlayerScope.all_players, PlayerScope.rookies}
VALID_SEASON_TYPE: Set[str] = {SeasonTypePlayoffs.regular, SeasonTypePlayoffs.playoffs}
VALID_GAME_SCOPE: Set[str] = {GameScopeDetailed.season, GameScopeDetailed.last_10}
VALID_STATS: Set[str] = {"PTS", "REB", "AST", "STL", "BLK", "FG_PCT", "FG3_PCT", "FT_PCT"}

# --- Cache Directory Setup ---
LEADERS_TILES_CSV_DIR = get_cache_dir("leaders_tiles")

# Ensure cache directories exist
os.makedirs(LEADERS_TILES_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_leaders_tiles(
    game_scope_detailed: str = GameScopeDetailed.season,
    league_id: str = "00",
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    stat: str = "PTS",
    data_set_name: str = "LeadersTiles"
) -> str:
    """
    Generates a file path for saving leaders tiles DataFrame.
    
    Args:
        game_scope_detailed: Game scope (default: Season)
        league_id: League ID (default: "00" for NBA)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        season: Season in YYYY-YY format
        season_type_playoffs: Season type (default: Regular Season)
        stat: Statistical category (default: PTS)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"leaders_tiles_league{league_id}",
        f"season{season}",
        f"type{season_type_playoffs.replace(' ', '_')}",
        f"scope{player_or_team}",
        f"players{player_scope.replace(' ', '_')}",
        f"stat{stat}",
        f"game{game_scope_detailed.replace(' ', '_')}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "leaders_tiles")

# --- Parameter Validation ---
def _validate_leaders_tiles_params(
    game_scope_detailed: str,
    league_id: str,
    player_or_team: str,
    player_scope: str,
    season: str,
    season_type_playoffs: str,
    stat: str
) -> Optional[str]:
    """Validates parameters for fetch_leaders_tiles_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_playoffs not in VALID_SEASON_TYPE:
        return f"Invalid season_type_playoffs: {season_type_playoffs}. Valid options: {', '.join(VALID_SEASON_TYPE)}"
    if player_or_team not in VALID_PLAYER_OR_TEAM:
        return f"Invalid player_or_team: {player_or_team}. Valid options: {', '.join(VALID_PLAYER_OR_TEAM)}"
    if player_scope not in VALID_PLAYER_SCOPE:
        return f"Invalid player_scope: {player_scope}. Valid options: {', '.join(VALID_PLAYER_SCOPE)}"
    if game_scope_detailed not in VALID_GAME_SCOPE:
        return f"Invalid game_scope_detailed: {game_scope_detailed}. Valid options: {', '.join(VALID_GAME_SCOPE)}"
    if stat not in VALID_STATS:
        return f"Invalid stat: {stat}. Valid options: {', '.join(VALID_STATS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEADERS_TILES_CACHE_SIZE)
def fetch_leaders_tiles_logic(
    game_scope_detailed: str = GameScopeDetailed.season,
    league_id: str = "00",
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    stat: str = "PTS",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches statistical leaders tiles data using the LeadersTiles endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        game_scope_detailed: Game scope (default: Season)
        league_id: League ID (default: "00" for NBA)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        stat: Statistical category (default: PTS)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with leaders tiles data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_leaders_tiles_logic for League: {league_id}, Season: {season}, "
        f"Type: {season_type_playoffs}, Scope: {player_or_team}, Players: {player_scope}, "
        f"Stat: {stat}, Game: {game_scope_detailed}, return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_leaders_tiles_params(
        game_scope_detailed, league_id, player_or_team, player_scope, season, season_type_playoffs, stat
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for leaders tiles: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["CurrentSeasonLeaders", "AllTimeHigh", "LastSeasonLeaders", "AllTimeLow"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_leaders_tiles(
                game_scope_detailed, league_id, player_or_team, player_scope, 
                season, season_type_playoffs, stat, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 50:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading leaders tiles from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "game_scope_detailed": game_scope_detailed,
                    "league_id": league_id,
                    "player_or_team": player_or_team,
                    "player_scope": player_scope,
                    "season": season,
                    "season_type_playoffs": season_type_playoffs,
                    "stat": stat
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "game_scope_detailed": game_scope_detailed,
            "league_id": league_id,
            "player_or_team": player_or_team,
            "player_scope": player_scope,
            "season": season,
            "season_type_playoffs": season_type_playoffs,
            "stat": stat
        }
        
        logger.debug(f"Calling LeadersTiles with parameters: {api_params}")
        leaders_tiles_endpoint = leaderstiles.LeadersTiles(**api_params)
        
        # Get data frames
        list_of_dataframes = leaders_tiles_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["CurrentSeasonLeaders", "AllTimeHigh", "LastSeasonLeaders", "AllTimeLow"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"LeadersTiles_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_leaders_tiles(
                        game_scope_detailed, league_id, player_or_team, player_scope,
                        season, season_type_playoffs, stat, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_leaders_tiles_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_leaders_tiles(
    game_scope_detailed: str = GameScopeDetailed.season,
    league_id: str = "00",
    player_or_team: str = PlayerOrTeam.team,
    player_scope: str = PlayerScope.all_players,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    stat: str = "PTS",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get statistical leaders tiles data.
    
    Args:
        game_scope_detailed: Game scope (default: Season)
        league_id: League ID (default: "00" for NBA)
        player_or_team: Player or Team (default: Team)
        player_scope: Player scope (default: All Players)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_playoffs: Season type (default: Regular Season)
        stat: Statistical category (default: PTS)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with leaders tiles data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_leaders_tiles_logic(
        game_scope_detailed=game_scope_detailed,
        league_id=league_id,
        player_or_team=player_or_team,
        player_scope=player_scope,
        season=season,
        season_type_playoffs=season_type_playoffs,
        stat=stat,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing LeadersTiles endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_leaders_tiles()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_leaders_tiles(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("LeadersTiles endpoint test completed.")


===== backend\api_tools\league_dash_player_bio.py =====
"""
Handles fetching player biographical statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPlayerBioStats endpoint, which provides
detailed player biographical and statistical information:
- Player demographics (age, height, weight)
- College and country information
- Draft information
- Basic and advanced statistics
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashplayerbiostats
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeSimple, LeagueIDNullable, PlayerPositionAbbreviationNullable,
    PlayerExperienceNullable, StarterBenchNullable
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PLAYER_BIO_CACHE_SIZE = 128
LEAGUE_DASH_PLAYER_BIO_CSV_DIR = get_cache_dir("league_dash_player_bio")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_PER_MODES = {
    "Totals": PerModeSimple.totals,
    "PerGame": PerModeSimple.per_game
}

VALID_PLAYER_POSITIONS = {
    "Forward": PlayerPositionAbbreviationNullable.forward,
    "Center": PlayerPositionAbbreviationNullable.center,
    "Guard": PlayerPositionAbbreviationNullable.guard,
    "Center-Forward": PlayerPositionAbbreviationNullable.center_forward,
    "Forward-Center": PlayerPositionAbbreviationNullable.forward_center,
    "Forward-Guard": PlayerPositionAbbreviationNullable.forward_guard,
    "Guard-Forward": PlayerPositionAbbreviationNullable.guard_forward
}

VALID_PLAYER_EXPERIENCES = {
    "Rookie": PlayerExperienceNullable.rookie,
    "Sophomore": PlayerExperienceNullable.sophomore,
    "Veteran": PlayerExperienceNullable.veteran
}

VALID_STARTER_BENCH = {
    "Starters": StarterBenchNullable.starters,
    "Bench": StarterBenchNullable.bench
}

VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_bio(
    season: str,
    season_type: str,
    per_mode: str,
    league_id: str,
    team_id: Optional[str] = None,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a player bio DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        league_id: The league ID (e.g., 00 for NBA)
        team_id: Optional team ID filter
        player_position: Optional player position filter
        player_experience: Optional player experience filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"bio_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{league_id}"
    ]

    if team_id:
        filename_parts.append(f"team_{team_id}")

    if player_position:
        pos_clean = player_position.replace("-", "_").lower()
        filename_parts.append(f"pos_{pos_clean}")

    if player_experience:
        exp_clean = player_experience.lower()
        filename_parts.append(f"exp_{exp_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_player_bio")

# --- Parameter Validation Functions ---
def _validate_player_bio_params(
    season: str,
    season_type: str,
    per_mode: str,
    league_id: str,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the player bio stats function.

    Args:
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        league_id: League ID (e.g., 00 for NBA, 10 for WNBA, 20 for G-League)
        player_position: Optional player position filter
        player_experience: Optional player experience filter
        starter_bench: Optional starter/bench filter

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if league_id not in VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(
            value=league_id,
            options=", ".join(list(VALID_LEAGUE_IDS.keys()))
        )

    if player_position and player_position not in VALID_PLAYER_POSITIONS:
        return Errors.INVALID_PLAYER_POSITION.format(
            value=player_position,
            options=", ".join(list(VALID_PLAYER_POSITIONS.keys()))
        )

    if player_experience and player_experience not in VALID_PLAYER_EXPERIENCES:
        return Errors.INVALID_PLAYER_EXPERIENCE.format(
            value=player_experience,
            options=", ".join(list(VALID_PLAYER_EXPERIENCES.keys()))
        )

    if starter_bench and starter_bench not in VALID_STARTER_BENCH:
        return Errors.INVALID_STARTER_BENCH.format(
            value=starter_bench,
            options=", ".join(list(VALID_STARTER_BENCH.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_PLAYER_BIO_CACHE_SIZE)
def fetch_league_player_bio_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    league_id: str = "00",  # NBA
    team_id: Optional[str] = None,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None,
    college: Optional[str] = None,
    country: Optional[str] = None,
    draft_year: Optional[str] = None,
    height: Optional[str] = None,
    weight: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player biographical statistics using the LeagueDashPlayerBioStats endpoint.

    This endpoint provides detailed player biographical and statistical information:
    - Player demographics (age, height, weight)
    - College and country information
    - Draft information
    - Basic and advanced statistics

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        league_id (str, optional): League ID. Defaults to "NBA".
        team_id (str, optional): Team ID filter. Defaults to None.
        player_position (str, optional): Player position filter. Defaults to None.
        player_experience (str, optional): Player experience filter. Defaults to None.
        starter_bench (str, optional): Starter/bench filter. Defaults to None.
        college (str, optional): College filter. Defaults to None.
        country (str, optional): Country filter. Defaults to None.
        draft_year (str, optional): Draft year filter. Defaults to None.
        height (str, optional): Height filter. Defaults to None.
        weight (str, optional): Weight filter. Defaults to None.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player bio stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_player_bio_stats_logic for: "
        f"Season: {season}, Type: {season_type}, PerMode: {per_mode}, LeagueID: {league_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_player_bio_params(
        season, season_type, per_mode, league_id, player_position, player_experience, starter_bench
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "league_id": league_id,
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "per_mode_simple": VALID_PER_MODES[per_mode],
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if team_id:
        api_params["team_id_nullable"] = team_id

    if player_position:
        api_params["player_position_abbreviation_nullable"] = VALID_PLAYER_POSITIONS[player_position]

    if player_experience:
        api_params["player_experience_nullable"] = VALID_PLAYER_EXPERIENCES[player_experience]

    if starter_bench:
        api_params["starter_bench_nullable"] = VALID_STARTER_BENCH[starter_bench]

    if college:
        api_params["college_nullable"] = college

    if country:
        api_params["country_nullable"] = country

    if draft_year:
        api_params["draft_year_nullable"] = draft_year

    if height:
        api_params["height_nullable"] = height

    if weight:
        api_params["weight_nullable"] = weight

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling LeagueDashPlayerBioStats with parameters: {filtered_api_params}")
        bio_stats_endpoint = leaguedashplayerbiostats.LeagueDashPlayerBioStats(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = bio_stats_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = bio_stats_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "LeagueDashPlayerBioStats"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_player_bio(
                            season, season_type, per_mode, league_id,
                            team_id, player_position, player_experience
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched player bio stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_player_bio_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.LEAGUE_DASH_PLAYER_BIO_API.format(
            season=season, season_type=season_type, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_player_clutch.py =====
"""
Handles fetching player clutch statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPlayerClutch endpoint, which provides
detailed player statistics in clutch situations:
- Performance in close games
- Statistics in the final minutes of games
- Performance when ahead or behind
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashplayerclutch
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense,
    PlayerPositionAbbreviationNullable, PlayerExperienceNullable, StarterBenchNullable
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PLAYER_CLUTCH_CACHE_SIZE = 128
LEAGUE_DASH_PLAYER_CLUTCH_CSV_DIR = get_cache_dir("league_dash_player_clutch")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_PLAYER_POSITIONS = {
    "Forward": PlayerPositionAbbreviationNullable.forward,
    "Center": PlayerPositionAbbreviationNullable.center,
    "Guard": PlayerPositionAbbreviationNullable.guard,
    "Center-Forward": PlayerPositionAbbreviationNullable.center_forward,
    "Forward-Center": PlayerPositionAbbreviationNullable.forward_center,
    "Forward-Guard": PlayerPositionAbbreviationNullable.forward_guard,
    "Guard-Forward": PlayerPositionAbbreviationNullable.guard_forward
}

VALID_PLAYER_EXPERIENCES = {
    "Rookie": PlayerExperienceNullable.rookie,
    "Sophomore": PlayerExperienceNullable.sophomore,
    "Veteran": PlayerExperienceNullable.veteran
}

VALID_STARTER_BENCH = {
    "Starters": StarterBenchNullable.starters,
    "Bench": StarterBenchNullable.bench
}

VALID_CLUTCH_TIMES = {
    "Last 5 Minutes": "Last 5 Minutes",
    "Last 4 Minutes": "Last 4 Minutes",
    "Last 3 Minutes": "Last 3 Minutes",
    "Last 2 Minutes": "Last 2 Minutes",
    "Last 1 Minute": "Last 1 Minute",
    "Last 30 Seconds": "Last 30 Seconds",
    "Last 10 Seconds": "Last 10 Seconds"
}

VALID_AHEAD_BEHIND = {
    "Ahead or Behind": "Ahead or Behind",
    "Behind or Tied": "Behind or Tied",
    "Ahead or Tied": "Ahead or Tied"
}

VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_clutch(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    clutch_time: str,
    ahead_behind: str,
    point_diff: int,
    team_id: Optional[str] = None,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a player clutch DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        clutch_time: The clutch time definition (e.g., Last 5 Minutes)
        ahead_behind: The ahead/behind filter (e.g., Ahead or Behind)
        point_diff: The point differential
        team_id: Optional team ID filter
        player_position: Optional player position filter
        player_experience: Optional player experience filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()
    clutch_time_clean = clutch_time.replace(" ", "_").lower()
    ahead_behind_clean = ahead_behind.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"clutch_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}",
        f"clutch_{clutch_time_clean}_{ahead_behind_clean}_pt{point_diff}"
    ]

    if team_id:
        filename_parts.append(f"team_{team_id}")

    if player_position:
        pos_clean = player_position.replace("-", "_").lower()
        filename_parts.append(f"pos_{pos_clean}")

    if player_experience:
        exp_clean = player_experience.lower()
        filename_parts.append(f"exp_{exp_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_player_clutch")

# --- Parameter Validation Functions ---
def _validate_player_clutch_params(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    clutch_time: str,
    ahead_behind: str,
    point_diff: int,
    league_id: str,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the player clutch stats function.

    Args:
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        clutch_time: Clutch time definition (e.g., Last 5 Minutes)
        ahead_behind: Ahead/behind filter (e.g., Ahead or Behind)
        point_diff: Point differential
        league_id: League ID (e.g., 00 for NBA, 10 for WNBA, 20 for G-League)
        player_position: Optional player position filter
        player_experience: Optional player experience filter
        starter_bench: Optional starter/bench filter

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if clutch_time not in VALID_CLUTCH_TIMES:
        return Errors.INVALID_CLUTCH_TIME.format(
            value=clutch_time,
            options=", ".join(list(VALID_CLUTCH_TIMES.keys()))
        )

    if ahead_behind not in VALID_AHEAD_BEHIND:
        return Errors.INVALID_AHEAD_BEHIND.format(
            value=ahead_behind,
            options=", ".join(list(VALID_AHEAD_BEHIND.keys()))
        )

    if not isinstance(point_diff, int) or point_diff < 0:
        return Errors.INVALID_POINT_DIFF.format(value=point_diff)

    if league_id not in VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(
            value=league_id,
            options=", ".join(list(VALID_LEAGUE_IDS.keys()))
        )

    if player_position and player_position not in VALID_PLAYER_POSITIONS:
        return Errors.INVALID_PLAYER_POSITION.format(
            value=player_position,
            options=", ".join(list(VALID_PLAYER_POSITIONS.keys()))
        )

    if player_experience and player_experience not in VALID_PLAYER_EXPERIENCES:
        return Errors.INVALID_PLAYER_EXPERIENCE.format(
            value=player_experience,
            options=", ".join(list(VALID_PLAYER_EXPERIENCES.keys()))
        )

    if starter_bench and starter_bench not in VALID_STARTER_BENCH:
        return Errors.INVALID_STARTER_BENCH.format(
            value=starter_bench,
            options=", ".join(list(VALID_STARTER_BENCH.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_PLAYER_CLUTCH_CACHE_SIZE)
def fetch_league_player_clutch_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    clutch_time: str = "Last 5 Minutes",
    ahead_behind: str = "Ahead or Behind",
    point_diff: int = 5,
    league_id: str = "00",  # NBA
    team_id: Optional[str] = None,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player clutch statistics using the LeagueDashPlayerClutch endpoint.

    This endpoint provides detailed player statistics in clutch situations:
    - Performance in close games
    - Statistics in the final minutes of games
    - Performance when ahead or behind

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        clutch_time (str, optional): Clutch time definition. Defaults to "Last 5 Minutes".
        ahead_behind (str, optional): Ahead/behind filter. Defaults to "Ahead or Behind".
        point_diff (int, optional): Point differential. Defaults to 5.
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        team_id (str, optional): Team ID filter. Defaults to None.
        player_position (str, optional): Player position filter. Defaults to None.
        player_experience (str, optional): Player experience filter. Defaults to None.
        starter_bench (str, optional): Starter/bench filter. Defaults to None.
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int, optional): Opponent team ID filter. Defaults to 0 (all teams).
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player clutch stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_player_clutch_stats_logic for: "
        f"Season: {season}, Type: {season_type}, ClutchTime: {clutch_time}, PointDiff: {point_diff}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_player_clutch_params(
        season, season_type, per_mode, measure_type, clutch_time, ahead_behind, point_diff,
        league_id, player_position, player_experience, starter_bench
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "league_id_nullable": league_id,
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "clutch_time": clutch_time,
        "ahead_behind": ahead_behind,
        "point_diff": str(point_diff),
        "last_n_games": str(last_n_games),
        "month": str(month),
        "opponent_team_id": opponent_team_id,
        "period": str(period),
        "pace_adjust": "N",
        "plus_minus": "N",
        "rank": "N",
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if team_id:
        api_params["team_id_nullable"] = team_id

    if player_position:
        api_params["player_position_abbreviation_nullable"] = VALID_PLAYER_POSITIONS[player_position]

    if player_experience:
        api_params["player_experience_nullable"] = VALID_PLAYER_EXPERIENCES[player_experience]

    if starter_bench:
        api_params["starter_bench_nullable"] = VALID_STARTER_BENCH[starter_bench]

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling LeagueDashPlayerClutch with parameters: {filtered_api_params}")
        clutch_stats_endpoint = leaguedashplayerclutch.LeagueDashPlayerClutch(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = clutch_stats_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = clutch_stats_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "LeagueDashPlayerClutch"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_player_clutch(
                            season, season_type, per_mode, measure_type, clutch_time, ahead_behind, point_diff,
                            team_id, player_position, player_experience
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched player clutch stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_player_clutch_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.LEAGUE_DASH_PLAYER_CLUTCH_API.format(
            season=season, season_type=season_type, clutch_time=clutch_time, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_player_pt_shot.py =====
"""
Handles fetching player shooting statistics across the league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPlayerPtShot endpoint, which provides
comprehensive shooting statistics for players across the league.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd

from nba_api.stats.endpoints import leaguedashplayerptshot
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeSimple, LeagueID
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PLAYER_PT_SHOT_CACHE_SIZE = 128
LEAGUE_DASH_PLAYER_PT_SHOT_CSV_DIR = get_cache_dir("league_dash_player_pt_shot")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple)
                            if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID)
                             if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_pt_shot(
    season: str,
    season_type: str,
    per_mode: str,
    league_id: str,
    team_id_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league player shot DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        league_id: The league ID (e.g., 00 for NBA)
        team_id_nullable: Optional team ID filter
        player_position_nullable: Optional player position filter
        conference_nullable: Optional conference filter
        division_nullable: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"player_pt_shot_{season_clean}_{season_type_clean}_{per_mode_clean}_{league_id}"
    ]

    if team_id_nullable:
        filename_parts.append(f"team_{team_id_nullable}")

    if player_position_nullable:
        position_clean = player_position_nullable.replace(" ", "_").lower()
        filename_parts.append(f"pos_{position_clean}")

    if conference_nullable:
        conference_clean = conference_nullable.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division_nullable:
        division_clean = division_nullable.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_player_pt_shot")

# --- Main Logic Function ---
def fetch_league_dash_player_pt_shot_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    league_id: str = LeagueID.nba,
    close_def_dist_range_nullable: Optional[str] = None,
    college_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    country_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None,
    draft_pick_nullable: Optional[str] = None,
    draft_year_nullable: Optional[str] = None,
    dribble_range_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    general_range_nullable: Optional[str] = None,
    height_nullable: Optional[str] = None,
    last_n_games_nullable: Optional[int] = None,
    location_nullable: Optional[str] = None,
    month_nullable: Optional[int] = None,
    opponent_team_id_nullable: Optional[int] = None,
    outcome_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    period_nullable: Optional[int] = None,
    player_experience_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    shot_clock_range_nullable: Optional[str] = None,
    shot_dist_range_nullable: Optional[str] = None,
    starter_bench_nullable: Optional[str] = None,
    team_id_nullable: Optional[str] = None,
    touch_time_range_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    weight_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player shooting statistics across the league using the LeagueDashPlayerPtShot endpoint.

    This endpoint provides comprehensive shooting statistics for players across the league.

    Args:
        season: Season in YYYY-YY format. Defaults to current season.
        season_type: Season type. Defaults to Regular Season.
        per_mode: Per mode for stats. Defaults to PerGame.
        league_id: League ID. Defaults to "00" (NBA).
        close_def_dist_range_nullable: Filter by defender distance.
        college_nullable: Filter by college.
        conference_nullable: Filter by conference.
        country_nullable: Filter by country.
        date_from_nullable: Start date filter (YYYY-MM-DD).
        date_to_nullable: End date filter (YYYY-MM-DD).
        division_nullable: Filter by division.
        draft_pick_nullable: Filter by draft pick.
        draft_year_nullable: Filter by draft year.
        dribble_range_nullable: Filter by dribble range.
        game_segment_nullable: Filter by game segment.
        general_range_nullable: Filter by general range.
        height_nullable: Filter by player height.
        last_n_games_nullable: Filter by last N games.
        location_nullable: Filter by location (Home/Road).
        month_nullable: Filter by month.
        opponent_team_id_nullable: Filter by opponent team ID.
        outcome_nullable: Filter by game outcome (W/L).
        po_round_nullable: Filter by playoff round.
        period_nullable: Filter by period.
        player_experience_nullable: Filter by player experience.
        player_position_nullable: Filter by player position.
        season_segment_nullable: Filter by season segment.
        shot_clock_range_nullable: Filter by shot clock range.
        shot_dist_range_nullable: Filter by shot distance range.
        starter_bench_nullable: Filter by starter/bench.
        team_id_nullable: Filter by team ID.
        touch_time_range_nullable: Filter by touch time range.
        vs_conference_nullable: Filter by opponent conference.
        vs_division_nullable: Filter by opponent division.
        weight_nullable: Filter by player weight.
        return_dataframe: Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player shooting stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_dash_player_pt_shot_logic for: "
        f"Season: {season}, Type: {season_type}, Per Mode: {per_mode}, League ID: {league_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(
            value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id not in _VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(
            value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "per_mode_simple": per_mode,
            "league_id": league_id,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if close_def_dist_range_nullable:
            api_params["close_def_dist_range_nullable"] = close_def_dist_range_nullable
        if college_nullable:
            api_params["college_nullable"] = college_nullable
        if conference_nullable:
            api_params["conference_nullable"] = conference_nullable
        if country_nullable:
            api_params["country_nullable"] = country_nullable
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        if division_nullable:
            api_params["division_nullable"] = division_nullable
        if draft_pick_nullable:
            api_params["draft_pick_nullable"] = draft_pick_nullable
        if draft_year_nullable:
            api_params["draft_year_nullable"] = draft_year_nullable
        if dribble_range_nullable:
            api_params["dribble_range_nullable"] = dribble_range_nullable
        if game_segment_nullable:
            api_params["game_segment_nullable"] = game_segment_nullable
        if general_range_nullable:
            api_params["general_range_nullable"] = general_range_nullable
        if height_nullable:
            api_params["height_nullable"] = height_nullable
        if last_n_games_nullable is not None:
            api_params["last_n_games_nullable"] = last_n_games_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if month_nullable is not None:
            api_params["month_nullable"] = month_nullable
        if opponent_team_id_nullable is not None:
            api_params["opponent_team_id_nullable"] = opponent_team_id_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if po_round_nullable:
            api_params["po_round_nullable"] = po_round_nullable
        if period_nullable is not None:
            api_params["period_nullable"] = period_nullable
        if player_experience_nullable:
            api_params["player_experience_nullable"] = player_experience_nullable
        if player_position_nullable:
            api_params["player_position_nullable"] = player_position_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if shot_clock_range_nullable:
            api_params["shot_clock_range_nullable"] = shot_clock_range_nullable
        if shot_dist_range_nullable:
            api_params["shot_dist_range_nullable"] = shot_dist_range_nullable
        if starter_bench_nullable:
            api_params["starter_bench_nullable"] = starter_bench_nullable
        if team_id_nullable:
            api_params["team_id_nullable"] = team_id_nullable
        if touch_time_range_nullable:
            api_params["touch_time_range_nullable"] = touch_time_range_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable
        if weight_nullable:
            api_params["weight_nullable"] = weight_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueDashPlayerPtShot with parameters: {filtered_api_params}")
        player_pt_shot_endpoint = leaguedashplayerptshot.LeagueDashPlayerPtShot(**api_params)

        # Get data frames
        player_pt_shot_df = player_pt_shot_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["LeagueDashPTShots"] = player_pt_shot_df

            # Save to CSV if not empty
            if not player_pt_shot_df.empty:
                csv_path = _get_csv_path_for_player_pt_shot(
                    season, season_type, per_mode, league_id,
                    team_id_nullable, player_position_nullable, conference_nullable, division_nullable
                )
                _save_dataframe_to_csv(player_pt_shot_df, csv_path)

        # Process for JSON response
        processed_data = _process_dataframe(player_pt_shot_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "player_pt_shots": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched player shooting stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_dash_player_pt_shot_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching player shooting stats: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_player_shot_locations.py =====
"""
Handles fetching and processing league dashboard player shot locations data
from the LeagueDashPlayerShotLocations endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The LeagueDashPlayerShotLocations endpoint provides comprehensive player shot locations data (1 DataFrame):
- Player Info: PLAYER_ID, PLAYER_NAME, TEAM_ID, TEAM_ABBREVIATION, AGE, NICKNAME (6 columns)
- Shot Zones: Restricted Area, In The Paint (Non-RA), Mid-Range, Left Corner 3, Right Corner 3, Above the Break 3, Backcourt, Corner 3 (8 zones)
- Shooting Metrics: FGM, FGA, FG_PCT for each shot zone (3 metrics × 8 zones = 24 columns)
- Rich shooting data: 569 players with detailed shot location statistics (30 columns total)
- Perfect for shot chart analysis, player evaluation, and scouting insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import leaguedashplayershotlocations
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeDetailed, MeasureTypeSimple
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]

    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PLAYER_SHOT_LOCATIONS_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_DISTANCE_RANGES: Set[str] = {"5ft Range", "8ft Range", "By Zone"}
VALID_MEASURE_TYPES: Set[str] = {MeasureTypeSimple.base}
VALID_PER_MODES: Set[str] = {PerModeDetailed.totals, PerModeDetailed.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# --- Cache Directory Setup ---
LEAGUE_DASH_PLAYER_SHOT_LOCATIONS_CSV_DIR = get_cache_dir("league_dash_player_shot_locations")

# Ensure cache directories exist
os.makedirs(LEAGUE_DASH_PLAYER_SHOT_LOCATIONS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_dash_player_shot_locations(
    distance_range: str = "By Zone",
    last_n_games: int = 0,
    measure_type_simple: str = MeasureTypeSimple.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    data_set_name: str = "PlayerShotLocations"
) -> str:
    """
    Generates a file path for saving league dash player shot locations DataFrame.

    Args:
        distance_range: Distance range (default: "By Zone")
        last_n_games: Last N games (default: 0)
        measure_type_simple: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"league_dash_player_shot_locations",
        f"season{season}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"measure{measure_type_simple.replace(' ', '_')}",
        f"per{per_mode_detailed.replace(' ', '_')}",
        f"distance{distance_range.replace(' ', '_')}",
        f"games{last_n_games}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_player_shot_locations")

# --- Parameter Validation ---
def _validate_league_dash_player_shot_locations_params(
    distance_range: str,
    last_n_games: int,
    measure_type_simple: str,
    per_mode_detailed: str,
    season: str,
    season_type_all_star: str,
    league_id_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_league_dash_player_shot_locations_logic."""
    if distance_range not in VALID_DISTANCE_RANGES:
        return f"Invalid distance_range: {distance_range}. Valid options: {', '.join(VALID_DISTANCE_RANGES)}"
    if last_n_games < 0:
        return f"Invalid last_n_games: {last_n_games}. Must be >= 0"
    if measure_type_simple not in VALID_MEASURE_TYPES:
        return f"Invalid measure_type_simple: {measure_type_simple}. Valid options: {', '.join(VALID_MEASURE_TYPES)}"
    if per_mode_detailed not in VALID_PER_MODES:
        return f"Invalid per_mode_detailed: {per_mode_detailed}. Valid options: {', '.join(VALID_PER_MODES)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if league_id_nullable and league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_PLAYER_SHOT_LOCATIONS_CACHE_SIZE)
def fetch_league_dash_player_shot_locations_logic(
    distance_range: str = "By Zone",
    last_n_games: int = 0,
    measure_type_simple: str = MeasureTypeSimple.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league dashboard player shot locations data using the LeagueDashPlayerShotLocations endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        distance_range: Distance range (default: "By Zone")
        last_n_games: Last N games (default: 0)
        measure_type_simple: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player shot locations data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_league_dash_player_shot_locations_logic for Season: {season}, "
        f"Type: {season_type_all_star}, Measure: {measure_type_simple}, Per: {per_mode_detailed}, "
        f"Distance: {distance_range}, Games: {last_n_games}, League: {league_id_nullable}, "
        f"return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_league_dash_player_shot_locations_params(
        distance_range, last_n_games, measure_type_simple, per_mode_detailed,
        season, season_type_all_star, league_id_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for league dash player shot locations: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_league_dash_player_shot_locations(
        distance_range, last_n_games, measure_type_simple, per_mode_detailed,
        season, season_type_all_star, league_id_nullable, "PlayerShotLocations"
    )
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 1000:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading league dash player shot locations from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "distance_range": distance_range,
                        "last_n_games": last_n_games,
                        "measure_type_simple": measure_type_simple,
                        "per_mode_detailed": per_mode_detailed,
                        "season": season,
                        "season_type_all_star": season_type_all_star,
                        "league_id_nullable": league_id_nullable
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["PlayerShotLocations"] = df
                result_dict["data_sets"]["PlayerShotLocations"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters
        api_params = {
            "distance_range": distance_range,
            "last_n_games": last_n_games,
            "measure_type_simple": measure_type_simple,
            "per_mode_detailed": per_mode_detailed,
            "season": season,
            "season_type_all_star": season_type_all_star
        }

        # Add league_id_nullable only if it's not empty
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable

        logger.debug(f"Calling LeagueDashPlayerShotLocations with parameters: {api_params}")
        shot_locations_endpoint = leaguedashplayershotlocations.LeagueDashPlayerShotLocations(**api_params)

        # Get data frames
        list_of_dataframes = shot_locations_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "distance_range": distance_range,
                "last_n_games": last_n_games,
                "measure_type_simple": measure_type_simple,
                "per_mode_detailed": per_mode_detailed,
                "season": season,
                "season_type_all_star": season_type_all_star,
                "league_id_nullable": league_id_nullable
            },
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"PlayerShotLocations_{idx}" if idx > 0 else "PlayerShotLocations"

            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df

                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()

            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_league_dash_player_shot_locations_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_league_dash_player_shot_locations(
    distance_range: str = "By Zone",
    last_n_games: int = 0,
    measure_type_simple: str = MeasureTypeSimple.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get league dashboard player shot locations data.

    Args:
        distance_range: Distance range (default: "By Zone")
        last_n_games: Last N games (default: 0)
        measure_type_simple: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player shot locations data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_league_dash_player_shot_locations_logic(
        distance_range=distance_range,
        last_n_games=last_n_games,
        measure_type_simple=measure_type_simple,
        per_mode_detailed=per_mode_detailed,
        season=season,
        season_type_all_star=season_type_all_star,
        league_id_nullable=league_id_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing LeagueDashPlayerShotLocations endpoint...")

    # Test 1: Basic fetch
    json_response = get_league_dash_player_shot_locations()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_league_dash_player_shot_locations(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("LeagueDashPlayerShotLocations endpoint test completed.")


===== backend\api_tools\league_dash_player_stats.py =====
"""
Handles fetching league player statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPlayerStats endpoint, which provides
comprehensive player statistics across the league:
- Basic and advanced statistics
- Scoring and defensive metrics
- Player rankings
- Filtering by team, position, experience, etc.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashplayerstats
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
)
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response
)
from utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PLAYER_STATS_CACHE_SIZE = 128
LEAGUE_DASH_PLAYER_STATS_CSV_DIR = get_cache_dir("league_dash_player_stats")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_PLAYER_POSITIONS = {
    "F": "F",  # Forward
    "C": "C",  # Center
    "G": "G",  # Guard
    "C-F": "C-F",  # Center-Forward
    "F-C": "F-C",  # Forward-Center
    "F-G": "F-G",  # Forward-Guard
    "G-F": "G-F"   # Guard-Forward
}

VALID_PLAYER_EXPERIENCES = {
    "Rookie": "Rookie",
    "Sophomore": "Sophomore",
    "Veteran": "Veteran"
}

VALID_STARTER_BENCH = {
    "Starters": "Starters",
    "Bench": "Bench"
}

VALID_GAME_SEGMENTS = {
    "First Half": "First Half",
    "Second Half": "Second Half",
    "Overtime": "Overtime"
}

VALID_LOCATIONS = {
    "Home": "Home",
    "Road": "Road"
}

VALID_OUTCOMES = {
    "W": "W",
    "L": "L"
}

VALID_SEASON_SEGMENTS = {
    "Post All-Star": "Post All-Star",
    "Pre All-Star": "Pre All-Star"
}

VALID_CONFERENCES = {
    "East": "East",
    "West": "West"
}

VALID_DIVISIONS = {
    "Atlantic": "Atlantic",
    "Central": "Central",
    "Southeast": "Southeast",
    "Northwest": "Northwest",
    "Pacific": "Pacific",
    "Southwest": "Southwest",
    "East": "East",
    "West": "West"
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_player_stats(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    team_id: Optional[str] = None,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league player stats DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        team_id: Optional team ID filter
        player_position: Optional player position filter
        player_experience: Optional player experience filter
        starter_bench: Optional starter/bench filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"league_player_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}"
    ]

    if team_id:
        filename_parts.append(f"team_{team_id}")

    if player_position:
        position_clean = player_position.replace("-", "_")
        filename_parts.append(f"pos_{position_clean}")

    if player_experience:
        experience_clean = player_experience.lower()
        filename_parts.append(f"exp_{experience_clean}")

    if starter_bench:
        starter_bench_clean = starter_bench.lower()
        filename_parts.append(f"role_{starter_bench_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_player_stats")

# --- Parameter Validation Functions ---
def _validate_league_player_stats_params(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    player_position: Optional[str] = None,
    player_experience: Optional[str] = None,
    starter_bench: Optional[str] = None,
    game_segment: Optional[str] = None,
    location: Optional[str] = None,
    outcome: Optional[str] = None,
    season_segment: Optional[str] = None,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the league player stats function.

    Args:
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        player_position: Optional player position filter
        player_experience: Optional player experience filter
        starter_bench: Optional starter/bench filter
        game_segment: Optional game segment filter
        location: Optional location filter
        outcome: Optional outcome filter
        season_segment: Optional season segment filter
        vs_conference: Optional conference filter
        vs_division: Optional division filter

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if player_position and player_position not in VALID_PLAYER_POSITIONS:
        return Errors.INVALID_PLAYER_POSITION.format(
            value=player_position,
            options=", ".join(list(VALID_PLAYER_POSITIONS.keys()))
        )

    if player_experience and player_experience not in VALID_PLAYER_EXPERIENCES:
        return Errors.INVALID_PLAYER_EXPERIENCE.format(
            value=player_experience,
            options=", ".join(list(VALID_PLAYER_EXPERIENCES.keys()))
        )

    if starter_bench and starter_bench not in VALID_STARTER_BENCH:
        return Errors.INVALID_STARTER_BENCH.format(
            value=starter_bench,
            options=", ".join(list(VALID_STARTER_BENCH.keys()))
        )

    if game_segment and game_segment not in VALID_GAME_SEGMENTS:
        return f"Invalid game_segment: '{game_segment}'. Valid options: {', '.join(list(VALID_GAME_SEGMENTS.keys()))}"

    if location and location not in VALID_LOCATIONS:
        return f"Invalid location: '{location}'. Valid options: {', '.join(list(VALID_LOCATIONS.keys()))}"

    if outcome and outcome not in VALID_OUTCOMES:
        return f"Invalid outcome: '{outcome}'. Valid options: {', '.join(list(VALID_OUTCOMES.keys()))}"

    if season_segment and season_segment not in VALID_SEASON_SEGMENTS:
        return f"Invalid season_segment: '{season_segment}'. Valid options: {', '.join(list(VALID_SEASON_SEGMENTS.keys()))}"

    if vs_conference and vs_conference not in VALID_CONFERENCES:
        return f"Invalid vs_conference: '{vs_conference}'. Valid options: {', '.join(list(VALID_CONFERENCES.keys()))}"

    if vs_division and vs_division not in VALID_DIVISIONS:
        return f"Invalid vs_division: '{vs_division}'. Valid options: {', '.join(list(VALID_DIVISIONS.keys()))}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_PLAYER_STATS_CACHE_SIZE)
def fetch_league_player_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    team_id: str = "",
    player_position: str = "",
    player_experience: str = "",
    starter_bench: str = "",
    date_from: str = "",
    date_to: str = "",
    game_segment: str = "",
    last_n_games: int = 0,
    league_id: str = "00",  # NBA
    location: str = "",
    month: int = 0,
    opponent_team_id: int = 0,
    outcome: str = "",
    period: int = 0,
    season_segment: str = "",
    vs_conference: str = "",
    vs_division: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league player statistics using the LeagueDashPlayerStats endpoint.

    This endpoint provides comprehensive player statistics across the league:
    - Basic and advanced statistics
    - Scoring and defensive metrics
    - Player rankings
    - Filtering by team, position, experience, etc.

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        team_id (str, optional): Team ID filter. Defaults to "".
        player_position (str, optional): Player position filter. Defaults to "".
        player_experience (str, optional): Player experience filter. Defaults to "".
        starter_bench (str, optional): Starter/bench filter. Defaults to "".
        date_from (str, optional): Start date filter (MM/DD/YYYY). Defaults to "".
        date_to (str, optional): End date filter (MM/DD/YYYY). Defaults to "".
        game_segment (str, optional): Game segment filter. Defaults to "".
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        location (str, optional): Location filter (Home/Road). Defaults to "".
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int, optional): Opponent team ID filter. Defaults to 0 (all teams).
        outcome (str, optional): Outcome filter (W/L). Defaults to "".
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        season_segment (str, optional): Season segment filter. Defaults to "".
        vs_conference (str, optional): Conference filter. Defaults to "".
        vs_division (str, optional): Division filter. Defaults to "".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with league player stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_player_stats_logic for: "
        f"Season: {season}, Type: {season_type}, Measure Type: {measure_type}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_league_player_stats_params(
        season, season_type, per_mode, measure_type, player_position, player_experience,
        starter_bench, game_segment, location, outcome, season_segment, vs_conference, vs_division
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "last_n_games": str(last_n_games),
        "month": str(month),
        "opponent_team_id": opponent_team_id,
        "period": str(period),
        "pace_adjust": "N",
        "plus_minus": "N",
        "rank": "N",
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if team_id:
        api_params["team_id_nullable"] = team_id

    if player_position:
        api_params["player_position_abbreviation_nullable"] = player_position

    if player_experience:
        api_params["player_experience_nullable"] = player_experience

    if starter_bench:
        api_params["starter_bench_nullable"] = starter_bench

    if date_from:
        api_params["date_from_nullable"] = date_from

    if date_to:
        api_params["date_to_nullable"] = date_to

    if game_segment:
        api_params["game_segment_nullable"] = game_segment

    if location:
        api_params["location_nullable"] = location

    if outcome:
        api_params["outcome_nullable"] = outcome

    if season_segment:
        api_params["season_segment_nullable"] = season_segment

    if vs_conference:
        api_params["vs_conference_nullable"] = vs_conference

    if vs_division:
        api_params["vs_division_nullable"] = vs_division

    if league_id:
        api_params["league_id_nullable"] = league_id

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling LeagueDashPlayerStats with parameters: {filtered_api_params}")
        player_stats_endpoint = leaguedashplayerstats.LeagueDashPlayerStats(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = player_stats_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = player_stats_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "LeagueDashPlayerStats"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_league_player_stats(
                            season, season_type, per_mode, measure_type, team_id, player_position, player_experience, starter_bench
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched league player stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_player_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.LEAGUE_DASH_PLAYER_STATS_API.format(
            season=season, season_type=season_type, measure_type=measure_type, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_pt_defend.py =====
"""
Handles fetching defensive player tracking statistics across the league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPtDefend endpoint, which provides
comprehensive defensive statistics for players across the league.
The endpoint offers various defensive categories including:

1. Overall - All defensive possessions
2. 3 Pointers - Defense against 3-point shots
3. 2 Pointers - Defense against 2-point shots
4. Less Than 6Ft - Defense against shots less than 6 feet from the basket
5. Less Than 10Ft - Defense against shots less than 10 feet from the basket
6. Greater Than 15Ft - Defense against shots greater than 15 feet from the basket

The data includes metrics like defended field goal percentage, normal field goal
percentage, and the difference between them (PCT_PLUSMINUS) which indicates
defensive impact.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd

from nba_api.stats.endpoints import leaguedashptdefend
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeSimple, LeagueID, DefenseCategory
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PT_DEFEND_CACHE_SIZE = 128
LEAGUE_DASH_PT_DEFEND_CSV_DIR = get_cache_dir("league_dash_pt_defend")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple)
                            if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID)
                             if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

_VALID_DEFENSE_CATEGORIES: Set[str] = {getattr(DefenseCategory, attr) for attr in dir(DefenseCategory)
                                     if not attr.startswith('_') and isinstance(getattr(DefenseCategory, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_pt_defend(
    season: str,
    season_type: str,
    per_mode: str,
    defense_category: str,
    league_id: str,
    team_id_nullable: Optional[str] = None,
    player_id_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league defensive player tracking stats DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        defense_category: The defense category (e.g., Overall, 3 Pointers)
        league_id: The league ID (e.g., 00 for NBA)
        team_id_nullable: Optional team ID filter
        player_id_nullable: Optional player ID filter
        player_position_nullable: Optional player position filter
        conference_nullable: Optional conference filter
        division_nullable: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    defense_category_clean = defense_category.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"pt_defend_{season_clean}_{season_type_clean}_{per_mode_clean}_{defense_category_clean}_{league_id}"
    ]

    if team_id_nullable:
        filename_parts.append(f"team_{team_id_nullable}")

    if player_id_nullable:
        filename_parts.append(f"player_{player_id_nullable}")

    if player_position_nullable:
        position_clean = player_position_nullable.replace("-", "_").lower()
        filename_parts.append(f"pos_{position_clean}")

    if conference_nullable:
        conference_clean = conference_nullable.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division_nullable:
        division_clean = division_nullable.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_pt_defend")

# --- Main Logic Function ---
def fetch_league_dash_pt_defend_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    defense_category: str = DefenseCategory.overall,
    league_id: str = LeagueID.nba,
    college_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    country_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None,
    draft_pick_nullable: Optional[str] = None,
    draft_year_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    height_nullable: Optional[str] = None,
    last_n_games_nullable: Optional[int] = None,
    location_nullable: Optional[str] = None,
    month_nullable: Optional[int] = None,
    opponent_team_id_nullable: Optional[int] = None,
    outcome_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    period_nullable: Optional[int] = None,
    player_experience_nullable: Optional[str] = None,
    player_id_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    starter_bench_nullable: Optional[str] = None,
    team_id_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    weight_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches defensive player tracking statistics across the league using the LeagueDashPtDefend endpoint.

    This endpoint provides comprehensive defensive statistics for players across the league,
    showing how players perform when defending shots in different categories.

    The data includes:
    - Player identification (ID, name, team, position)
    - Age and games played
    - Frequency of defensive matchups (FREQ)
    - Defended field goal makes and attempts (D_FGM, D_FGA)
    - Defended field goal percentage (D_FG_PCT)
    - Normal field goal percentage of opponents (NORMAL_FG_PCT)
    - Difference between normal and defended FG% (PCT_PLUSMINUS) - negative values indicate good defense

    Args:
        season (str): Season in YYYY-YY format (e.g., "2023-24"). Defaults to current season.
        season_type (str): Season type from SeasonTypeAllStar (e.g., "Regular Season", "Playoffs").
            Defaults to "Regular Season".
        per_mode (str): Per mode from PerModeSimple (e.g., "PerGame", "Totals").
            Defaults to "PerGame".
        defense_category (str): Defense category from DefenseCategory (e.g., "Overall", "3 Pointers").
            Defaults to "Overall".
        league_id (str): League ID from LeagueID (e.g., "00" for NBA). Defaults to "00".
        college_nullable (Optional[str]): College filter.
        conference_nullable (Optional[str]): Conference filter for teams (e.g., "East", "West").
        country_nullable (Optional[str]): Country filter.
        date_from_nullable (Optional[str]): Start date filter in YYYY-MM-DD format.
        date_to_nullable (Optional[str]): End date filter in YYYY-MM-DD format.
        division_nullable (Optional[str]): Division filter for teams (e.g., "Atlantic", "Pacific").
        draft_pick_nullable (Optional[str]): Draft pick filter.
        draft_year_nullable (Optional[str]): Draft year filter.
        game_segment_nullable (Optional[str]): Game segment filter (e.g., "First Half", "Second Half").
        height_nullable (Optional[str]): Height filter.
        last_n_games_nullable (Optional[int]): Last N games filter.
        location_nullable (Optional[str]): Location filter ("Home" or "Road").
        month_nullable (Optional[int]): Month filter (0-12).
        opponent_team_id_nullable (Optional[int]): Opponent team ID filter.
        outcome_nullable (Optional[str]): Outcome filter ("W" or "L").
        po_round_nullable (Optional[str]): Playoff round filter.
        period_nullable (Optional[int]): Period filter.
        player_experience_nullable (Optional[str]): Player experience filter (e.g., "Rookie", "Sophomore").
        player_id_nullable (Optional[str]): Player ID filter.
        player_position_nullable (Optional[str]): Player position filter (e.g., "F", "C", "G").
        season_segment_nullable (Optional[str]): Season segment filter (e.g., "Pre All-Star", "Post All-Star").
        starter_bench_nullable (Optional[str]): Starter/bench filter (e.g., "Starters", "Bench").
        team_id_nullable (Optional[str]): Team ID filter.
        vs_conference_nullable (Optional[str]): Conference filter for opponents (e.g., "East", "West").
        vs_division_nullable (Optional[str]): Division filter for opponents (e.g., "Atlantic", "Pacific").
        weight_nullable (Optional[str]): Weight filter.
        return_dataframe (bool): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with defensive player tracking stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_dash_pt_defend_logic for: "
        f"Season: {season}, Type: {season_type}, Per Mode: {per_mode}, "
        f"Defense Category: {defense_category}, League ID: {league_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(
            value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id not in _VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(
            value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if defense_category not in _VALID_DEFENSE_CATEGORIES:
        error_response = format_response(error=f"Invalid defense_category: '{defense_category}'. Valid options: {', '.join(list(_VALID_DEFENSE_CATEGORIES))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "per_mode_simple": per_mode,
            "defense_category": defense_category,
            "league_id": league_id,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if college_nullable:
            api_params["college_nullable"] = college_nullable
        if conference_nullable:
            api_params["conference_nullable"] = conference_nullable
        if country_nullable:
            api_params["country_nullable"] = country_nullable
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        if division_nullable:
            api_params["division_nullable"] = division_nullable
        if draft_pick_nullable:
            api_params["draft_pick_nullable"] = draft_pick_nullable
        if draft_year_nullable:
            api_params["draft_year_nullable"] = draft_year_nullable
        if game_segment_nullable:
            api_params["game_segment_nullable"] = game_segment_nullable
        if height_nullable:
            api_params["height_nullable"] = height_nullable
        if last_n_games_nullable is not None:
            api_params["last_n_games_nullable"] = last_n_games_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if month_nullable is not None:
            api_params["month_nullable"] = month_nullable
        if opponent_team_id_nullable is not None:
            api_params["opponent_team_id_nullable"] = opponent_team_id_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if po_round_nullable:
            api_params["po_round_nullable"] = po_round_nullable
        if period_nullable is not None:
            api_params["period_nullable"] = period_nullable
        if player_experience_nullable:
            api_params["player_experience_nullable"] = player_experience_nullable
        if player_id_nullable:
            api_params["player_id_nullable"] = player_id_nullable
        if player_position_nullable:
            api_params["player_position_nullable"] = player_position_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if starter_bench_nullable:
            api_params["starter_bench_nullable"] = starter_bench_nullable
        if team_id_nullable:
            api_params["team_id_nullable"] = team_id_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable
        if weight_nullable:
            api_params["weight_nullable"] = weight_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueDashPtDefend with parameters: {filtered_api_params}")
        pt_defend_endpoint = leaguedashptdefend.LeagueDashPtDefend(**api_params)

        # Get data frames
        pt_defend_df = pt_defend_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["LeagueDashPTDefend"] = pt_defend_df

            # Save to CSV if not empty
            if not pt_defend_df.empty:
                csv_path = _get_csv_path_for_pt_defend(
                    season, season_type, per_mode, defense_category, league_id,
                    team_id_nullable, player_id_nullable, player_position_nullable,
                    conference_nullable, division_nullable
                )
                _save_dataframe_to_csv(pt_defend_df, csv_path)

        # Process for JSON response
        processed_data = _process_dataframe(pt_defend_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "pt_defend": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched defensive player tracking stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_dash_pt_defend_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching defensive player tracking stats: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_pt_stats.py =====
"""
Handles fetching player tracking statistics across the league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPtStats endpoint, which provides
comprehensive player tracking statistics for players or teams across the league.
The endpoint offers various player tracking measure types including:

1. SpeedDistance - Distance covered and speed metrics
2. Rebounding - Rebounding opportunities, contested rebounds, etc.
3. Possessions - Time of possession, touches, etc.
4. CatchShoot - Catch and shoot statistics
5. PullUpShot - Pull-up shot statistics
6. Defense - Defensive impact metrics
7. Drives - Drive statistics (frequency, points, assists)
8. Passing - Passing statistics (assists, potential assists)
9. ElbowTouch - Elbow touch statistics
10. PostTouch - Post touch statistics
11. PaintTouch - Paint touch statistics
12. Efficiency - Scoring efficiency metrics

The data includes metrics like distance traveled, average speed, and various
specialized statistics based on the selected measure type.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd

from nba_api.stats.endpoints import leaguedashptstats
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeSimple, PlayerOrTeam, PtMeasureType
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PT_STATS_CACHE_SIZE = 128
LEAGUE_DASH_PT_STATS_CSV_DIR = get_cache_dir("league_dash_pt_stats")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple)
                            if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

_VALID_PLAYER_OR_TEAM: Set[str] = {getattr(PlayerOrTeam, attr) for attr in dir(PlayerOrTeam)
                                 if not attr.startswith('_') and isinstance(getattr(PlayerOrTeam, attr), str)}

_VALID_PT_MEASURE_TYPES: Set[str] = {getattr(PtMeasureType, attr) for attr in dir(PtMeasureType)
                                   if not attr.startswith('_') and isinstance(getattr(PtMeasureType, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_pt_stats(
    season: str,
    season_type: str,
    per_mode: str,
    player_or_team: str,
    pt_measure_type: str,
    team_id_nullable: Optional[str] = None,
    player_position_abbreviation_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    division_simple_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league player tracking stats DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        player_or_team: Player or Team filter (e.g., Player, Team)
        pt_measure_type: Player tracking measure type (e.g., SpeedDistance, Rebounding)
        team_id_nullable: Optional team ID filter
        player_position_abbreviation_nullable: Optional player position filter
        conference_nullable: Optional conference filter
        division_simple_nullable: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    player_or_team_clean = player_or_team.lower()
    pt_measure_type_clean = pt_measure_type.lower()

    # Create filename with optional filters
    filename_parts = [
        f"pt_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{player_or_team_clean}_{pt_measure_type_clean}"
    ]

    if team_id_nullable:
        filename_parts.append(f"team_{team_id_nullable}")

    if player_position_abbreviation_nullable:
        position_clean = player_position_abbreviation_nullable.replace("-", "_").lower()
        filename_parts.append(f"pos_{position_clean}")

    if conference_nullable:
        conference_clean = conference_nullable.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division_simple_nullable:
        division_clean = division_simple_nullable.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_pt_stats")

# --- Main Logic Function ---
def fetch_league_dash_pt_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    player_or_team: str = PlayerOrTeam.team,
    pt_measure_type: str = PtMeasureType.speed_distance,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    game_scope_simple_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    player_experience_nullable: Optional[str] = None,
    player_position_abbreviation_nullable: Optional[str] = None,
    starter_bench_nullable: Optional[str] = None,
    team_id_nullable: Optional[str] = None,
    college_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    country_nullable: Optional[str] = None,
    division_simple_nullable: Optional[str] = None,
    draft_pick_nullable: Optional[str] = None,
    draft_year_nullable: Optional[str] = None,
    height_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    weight_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player tracking statistics across the league using the LeagueDashPtStats endpoint.

    This endpoint provides comprehensive player tracking statistics for players or teams across the league.
    The data returned varies based on the selected pt_measure_type, but generally includes metrics like
    distance traveled, average speed, and specialized statistics for the chosen tracking category.

    For SpeedDistance measure type (default), the data includes:
    - Team/player identification (ID, name, abbreviation)
    - Games played (GP), wins (W), and losses (L)
    - Minutes played (MIN)
    - Distance metrics (DIST_FEET, DIST_MILES, DIST_MILES_OFF, DIST_MILES_DEF)
    - Speed metrics (AVG_SPEED, AVG_SPEED_OFF, AVG_SPEED_DEF)

    Other measure types provide specialized statistics:
    - Rebounding: Rebounding opportunities, contested rebounds, box outs
    - Possessions: Time of possession, touches, passes, points per touch
    - CatchShoot: Catch and shoot frequency, points, FG%
    - PullUpShot: Pull-up shot frequency, points, FG%
    - Defense: Defensive impact metrics like DFGM, DFGA, DFG%
    - Drives: Drive frequency, points, assists, FG% on drives
    - Passing: Passing statistics like assists, potential assists, assist points
    - ElbowTouch: Elbow touch frequency, points, FG%
    - PostTouch: Post touch frequency, points, FG%
    - PaintTouch: Paint touch frequency, points, FG%
    - Efficiency: Scoring efficiency metrics

    Args:
        season (str): Season in YYYY-YY format (e.g., "2023-24"). Defaults to current season.
        season_type (str): Season type from SeasonTypeAllStar (e.g., "Regular Season", "Playoffs").
            Defaults to "Regular Season".
        per_mode (str): Per mode from PerModeSimple (e.g., "PerGame", "Totals").
            Defaults to "PerGame".
        player_or_team (str): Player or Team filter from PlayerOrTeam (e.g., "Player", "Team").
            Defaults to "Team".
        pt_measure_type (str): Player tracking measure type from PtMeasureType (e.g., "SpeedDistance",
            "Rebounding", "Drives"). Defaults to "SpeedDistance".
        last_n_games (int): Last N games filter. Defaults to 0 (all games).
        month (int): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int): Opponent team ID filter. Defaults to 0 (all teams).
        date_from_nullable (Optional[str]): Start date filter in YYYY-MM-DD format.
        date_to_nullable (Optional[str]): End date filter in YYYY-MM-DD format.
        game_scope_simple_nullable (Optional[str]): Game scope filter (e.g., "Yesterday", "Last 10").
        location_nullable (Optional[str]): Location filter ("Home" or "Road").
        outcome_nullable (Optional[str]): Outcome filter ("W" or "L").
        season_segment_nullable (Optional[str]): Season segment filter (e.g., "Pre All-Star", "Post All-Star").
        vs_conference_nullable (Optional[str]): Conference filter for opponents (e.g., "East", "West").
        vs_division_nullable (Optional[str]): Division filter for opponents (e.g., "Atlantic", "Pacific").
        player_experience_nullable (Optional[str]): Player experience filter (e.g., "Rookie", "Sophomore", "Veteran").
        player_position_abbreviation_nullable (Optional[str]): Player position filter (e.g., "G", "F", "C", "G-F").
        starter_bench_nullable (Optional[str]): Starter/bench filter ("Starters" or "Bench").
        team_id_nullable (Optional[str]): Team ID filter.
        college_nullable (Optional[str]): College filter.
        conference_nullable (Optional[str]): Conference filter for teams (e.g., "East", "West").
        country_nullable (Optional[str]): Country filter.
        division_simple_nullable (Optional[str]): Division filter for teams (e.g., "Atlantic", "Pacific").
        draft_pick_nullable (Optional[str]): Draft pick filter.
        draft_year_nullable (Optional[str]): Draft year filter.
        height_nullable (Optional[str]): Height filter.
        league_id_nullable (Optional[str]): League ID filter.
        po_round_nullable (Optional[str]): Playoff round filter.
        weight_nullable (Optional[str]): Weight filter.
        return_dataframe (bool): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player tracking stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_dash_pt_stats_logic for: "
        f"Season: {season}, Type: {season_type}, Per Mode: {per_mode}, "
        f"Player/Team: {player_or_team}, Measure Type: {pt_measure_type}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(
            value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if player_or_team not in _VALID_PLAYER_OR_TEAM:
        error_response = format_response(error=f"Invalid player_or_team: '{player_or_team}'. Valid options: {', '.join(list(_VALID_PLAYER_OR_TEAM))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if pt_measure_type not in _VALID_PT_MEASURE_TYPES:
        error_response = format_response(error=f"Invalid pt_measure_type: '{pt_measure_type}'. Valid options: {', '.join(list(_VALID_PT_MEASURE_TYPES))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "per_mode_simple": per_mode,
            "player_or_team": player_or_team,
            "pt_measure_type": pt_measure_type,
            "last_n_games": last_n_games,
            "month": month,
            "opponent_team_id": opponent_team_id,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        if game_scope_simple_nullable:
            api_params["game_scope_simple_nullable"] = game_scope_simple_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable
        if player_experience_nullable:
            api_params["player_experience_nullable"] = player_experience_nullable
        if player_position_abbreviation_nullable:
            api_params["player_position_abbreviation_nullable"] = player_position_abbreviation_nullable
        if starter_bench_nullable:
            api_params["starter_bench_nullable"] = starter_bench_nullable
        if team_id_nullable:
            api_params["team_id_nullable"] = team_id_nullable
        if college_nullable:
            api_params["college_nullable"] = college_nullable
        if conference_nullable:
            api_params["conference_nullable"] = conference_nullable
        if country_nullable:
            api_params["country_nullable"] = country_nullable
        if division_simple_nullable:
            api_params["division_simple_nullable"] = division_simple_nullable
        if draft_pick_nullable:
            api_params["draft_pick_nullable"] = draft_pick_nullable
        if draft_year_nullable:
            api_params["draft_year_nullable"] = draft_year_nullable
        if height_nullable:
            api_params["height_nullable"] = height_nullable
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable
        if po_round_nullable:
            api_params["po_round_nullable"] = po_round_nullable
        if weight_nullable:
            api_params["weight_nullable"] = weight_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueDashPtStats with parameters: {filtered_api_params}")
        pt_stats_endpoint = leaguedashptstats.LeagueDashPtStats(**api_params)

        # Get data frames
        pt_stats_df = pt_stats_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["LeagueDashPtStats"] = pt_stats_df

            # Save to CSV if not empty
            if not pt_stats_df.empty:
                csv_path = _get_csv_path_for_pt_stats(
                    season, season_type, per_mode, player_or_team, pt_measure_type,
                    team_id_nullable, player_position_abbreviation_nullable,
                    conference_nullable, division_simple_nullable
                )
                _save_dataframe_to_csv(pt_stats_df, csv_path)

        # Process for JSON response
        processed_data = _process_dataframe(pt_stats_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "pt_stats": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched player tracking stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_dash_pt_stats_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching player tracking stats: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_pt_team_defend.py =====
"""
Handles fetching defensive team tracking statistics across the league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashPtTeamDefend endpoint, which provides
comprehensive defensive statistics for teams across the league.
The endpoint offers various defensive categories including:

1. Overall - All defensive possessions
2. 3 Pointers - Defense against 3-point shots
3. 2 Pointers - Defense against 2-point shots
4. Less Than 6Ft - Defense against shots less than 6 feet from the basket
5. Less Than 10Ft - Defense against shots less than 10 feet from the basket
6. Greater Than 15Ft - Defense against shots greater than 15 feet from the basket

The data includes metrics like defended field goal percentage, normal field goal
percentage, and the difference between them (PCT_PLUSMINUS) which indicates
defensive impact at the team level.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd

from nba_api.stats.endpoints import leaguedashptteamdefend
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeSimple, LeagueID, DefenseCategory
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_PT_TEAM_DEFEND_CACHE_SIZE = 128
LEAGUE_DASH_PT_TEAM_DEFEND_CSV_DIR = get_cache_dir("league_dash_pt_team_defend")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple)
                            if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID)
                             if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

_VALID_DEFENSE_CATEGORIES: Set[str] = {getattr(DefenseCategory, attr) for attr in dir(DefenseCategory)
                                     if not attr.startswith('_') and isinstance(getattr(DefenseCategory, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_pt_team_defend(
    season: str,
    season_type: str,
    per_mode: str,
    defense_category: str,
    league_id: str,
    team_id_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league defensive team tracking stats DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        defense_category: The defense category (e.g., Overall, 3 Pointers)
        league_id: The league ID (e.g., 00 for NBA)
        team_id_nullable: Optional team ID filter
        conference_nullable: Optional conference filter
        division_nullable: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    defense_category_clean = defense_category.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"pt_team_defend_{season_clean}_{season_type_clean}_{per_mode_clean}_{defense_category_clean}_{league_id}"
    ]

    if team_id_nullable:
        filename_parts.append(f"team_{team_id_nullable}")

    if conference_nullable:
        conference_clean = conference_nullable.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division_nullable:
        division_clean = division_nullable.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_pt_team_defend")

# --- Main Logic Function ---
def fetch_league_dash_pt_team_defend_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    defense_category: str = DefenseCategory.overall,
    league_id: str = LeagueID.nba,
    conference_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    last_n_games_nullable: Optional[int] = None,
    location_nullable: Optional[str] = None,
    month_nullable: Optional[int] = None,
    opponent_team_id_nullable: Optional[int] = None,
    outcome_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    period_nullable: Optional[int] = None,
    season_segment_nullable: Optional[str] = None,
    team_id_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches defensive team tracking statistics across the league using the LeagueDashPtTeamDefend endpoint.

    This endpoint provides comprehensive defensive statistics for teams across the league,
    showing how teams perform when defending shots in different categories.

    The data includes:
    - Team identification (ID, name, abbreviation)
    - Games played (GP, G)
    - Frequency of defensive matchups (FREQ)
    - Defended field goal makes and attempts (D_FGM, D_FGA)
    - Defended field goal percentage (D_FG_PCT)
    - Normal field goal percentage of opponents (NORMAL_FG_PCT)
    - Difference between normal and defended FG% (PCT_PLUSMINUS) - negative values indicate good defense

    Args:
        season (str): Season in YYYY-YY format (e.g., "2023-24"). Defaults to current season.
        season_type (str): Season type from SeasonTypeAllStar (e.g., "Regular Season", "Playoffs").
            Defaults to "Regular Season".
        per_mode (str): Per mode from PerModeSimple (e.g., "PerGame", "Totals").
            Defaults to "PerGame".
        defense_category (str): Defense category from DefenseCategory (e.g., "Overall", "3 Pointers").
            Defaults to "Overall".
        league_id (str): League ID from LeagueID (e.g., "00" for NBA). Defaults to "00".
        conference_nullable (Optional[str]): Conference filter for teams (e.g., "East", "West").
        date_from_nullable (Optional[str]): Start date filter in YYYY-MM-DD format.
        date_to_nullable (Optional[str]): End date filter in YYYY-MM-DD format.
        division_nullable (Optional[str]): Division filter for teams (e.g., "Atlantic", "Pacific").
        game_segment_nullable (Optional[str]): Game segment filter (e.g., "First Half", "Second Half").
        last_n_games_nullable (Optional[int]): Last N games filter.
        location_nullable (Optional[str]): Location filter ("Home" or "Road").
        month_nullable (Optional[int]): Month filter (0-12).
        opponent_team_id_nullable (Optional[int]): Opponent team ID filter.
        outcome_nullable (Optional[str]): Outcome filter ("W" or "L").
        po_round_nullable (Optional[str]): Playoff round filter.
        period_nullable (Optional[int]): Period filter.
        season_segment_nullable (Optional[str]): Season segment filter (e.g., "Pre All-Star", "Post All-Star").
        team_id_nullable (Optional[str]): Team ID filter.
        vs_conference_nullable (Optional[str]): Conference filter for opponents (e.g., "East", "West").
        vs_division_nullable (Optional[str]): Division filter for opponents (e.g., "Atlantic", "Pacific").
        return_dataframe (bool): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with defensive team tracking stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_dash_pt_team_defend_logic for: "
        f"Season: {season}, Type: {season_type}, Per Mode: {per_mode}, "
        f"Defense Category: {defense_category}, League ID: {league_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(
            value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id not in _VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(
            value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if defense_category not in _VALID_DEFENSE_CATEGORIES:
        error_response = format_response(error=f"Invalid defense_category: '{defense_category}'. Valid options: {', '.join(list(_VALID_DEFENSE_CATEGORIES))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "per_mode_simple": per_mode,
            "defense_category": defense_category,
            "league_id": league_id,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if conference_nullable:
            api_params["conference_nullable"] = conference_nullable
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        if division_nullable:
            api_params["division_nullable"] = division_nullable
        if game_segment_nullable:
            api_params["game_segment_nullable"] = game_segment_nullable
        if last_n_games_nullable is not None:
            api_params["last_n_games_nullable"] = last_n_games_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if month_nullable is not None:
            api_params["month_nullable"] = month_nullable
        if opponent_team_id_nullable is not None:
            api_params["opponent_team_id_nullable"] = opponent_team_id_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        # Only add po_round_nullable for playoff data
        if po_round_nullable and season_type == SeasonTypeAllStar.playoffs:
            api_params["po_round_nullable"] = po_round_nullable
        if period_nullable is not None:
            api_params["period_nullable"] = period_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if team_id_nullable:
            api_params["team_id_nullable"] = team_id_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueDashPtTeamDefend with parameters: {filtered_api_params}")
        pt_team_defend_endpoint = leaguedashptteamdefend.LeagueDashPtTeamDefend(**api_params)

        # Get data frames
        pt_team_defend_df = pt_team_defend_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["LeagueDashPtTeamDefend"] = pt_team_defend_df

            # Save to CSV if not empty
            if not pt_team_defend_df.empty:
                csv_path = _get_csv_path_for_pt_team_defend(
                    season, season_type, per_mode, defense_category, league_id,
                    team_id_nullable, conference_nullable, division_nullable
                )
                _save_dataframe_to_csv(pt_team_defend_df, csv_path)

        # Process for JSON response
        processed_data = _process_dataframe(pt_team_defend_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "pt_team_defend": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched defensive team tracking stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_dash_pt_team_defend_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching defensive team tracking stats: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_team_clutch.py =====
"""
Handles fetching team clutch statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashTeamClutch endpoint, which provides
detailed team statistics in clutch situations:
- Performance in close games
- Statistics in the final minutes of games
- Performance when ahead or behind
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashteamclutch
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_TEAM_CLUTCH_CACHE_SIZE = 128
LEAGUE_DASH_TEAM_CLUTCH_CSV_DIR = get_cache_dir("league_dash_team_clutch")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_CLUTCH_TIMES = {
    "Last 5 Minutes": "Last 5 Minutes",
    "Last 4 Minutes": "Last 4 Minutes",
    "Last 3 Minutes": "Last 3 Minutes",
    "Last 2 Minutes": "Last 2 Minutes",
    "Last 1 Minute": "Last 1 Minute",
    "Last 30 Seconds": "Last 30 Seconds",
    "Last 10 Seconds": "Last 10 Seconds"
}

VALID_AHEAD_BEHIND = {
    "Ahead or Behind": "Ahead or Behind",
    "Behind or Tied": "Behind or Tied",
    "Ahead or Tied": "Ahead or Tied"
}

VALID_LEAGUE_IDS = {
    "00": "00",  # NBA
    "10": "10",  # WNBA
    "20": "20"   # G-League
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_clutch(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    clutch_time: str,
    ahead_behind: str,
    point_diff: int,
    team_id: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a team clutch DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        clutch_time: The clutch time definition (e.g., Last 5 Minutes)
        ahead_behind: The ahead/behind filter (e.g., Ahead or Behind)
        point_diff: The point differential
        team_id: Optional team ID filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()
    clutch_time_clean = clutch_time.replace(" ", "_").lower()
    ahead_behind_clean = ahead_behind.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"team_clutch_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}",
        f"clutch_{clutch_time_clean}_{ahead_behind_clean}_pt{point_diff}"
    ]

    if team_id:
        filename_parts.append(f"team_{team_id}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_team_clutch")

# --- Parameter Validation Functions ---
def _validate_team_clutch_params(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    clutch_time: str,
    ahead_behind: str,
    point_diff: int,
    league_id: str
) -> Optional[str]:
    """
    Validates parameters for the team clutch stats function.

    Args:
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        clutch_time: Clutch time definition (e.g., Last 5 Minutes)
        ahead_behind: Ahead/behind filter (e.g., Ahead or Behind)
        point_diff: Point differential
        league_id: League ID (e.g., 00 for NBA, 10 for WNBA, 20 for G-League)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if clutch_time not in VALID_CLUTCH_TIMES:
        return Errors.INVALID_CLUTCH_TIME.format(
            value=clutch_time,
            options=", ".join(list(VALID_CLUTCH_TIMES.keys()))
        )

    if ahead_behind not in VALID_AHEAD_BEHIND:
        return Errors.INVALID_AHEAD_BEHIND.format(
            value=ahead_behind,
            options=", ".join(list(VALID_AHEAD_BEHIND.keys()))
        )

    if not isinstance(point_diff, int) or point_diff < 0:
        return Errors.INVALID_POINT_DIFF.format(value=point_diff)

    if league_id not in VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(
            value=league_id,
            options=", ".join(list(VALID_LEAGUE_IDS.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_TEAM_CLUTCH_CACHE_SIZE)
def fetch_league_team_clutch_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    clutch_time: str = "Last 5 Minutes",
    ahead_behind: str = "Ahead or Behind",
    point_diff: int = 5,
    league_id: str = "00",  # NBA
    team_id: Optional[str] = None,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team clutch statistics using the LeagueDashTeamClutch endpoint.

    This endpoint provides detailed team statistics in clutch situations:
    - Performance in close games
    - Statistics in the final minutes of games
    - Performance when ahead or behind

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        clutch_time (str, optional): Clutch time definition. Defaults to "Last 5 Minutes".
        ahead_behind (str, optional): Ahead/behind filter. Defaults to "Ahead or Behind".
        point_diff (int, optional): Point differential. Defaults to 5.
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        team_id (str, optional): Team ID filter. Defaults to None.
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int, optional): Opponent team ID filter. Defaults to 0 (all teams).
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with team clutch stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_team_clutch_stats_logic for: "
        f"Season: {season}, Type: {season_type}, ClutchTime: {clutch_time}, PointDiff: {point_diff}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_team_clutch_params(
        season, season_type, per_mode, measure_type, clutch_time, ahead_behind, point_diff, league_id
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "league_id_nullable": league_id,
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "clutch_time": clutch_time,
        "ahead_behind": ahead_behind,
        "point_diff": str(point_diff),
        "last_n_games": str(last_n_games),
        "month": str(month),
        "opponent_team_id": opponent_team_id,
        "period": str(period),
        "pace_adjust": "N",
        "plus_minus": "N",
        "rank": "N",
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if team_id:
        api_params["team_id_nullable"] = team_id

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling LeagueDashTeamClutch with parameters: {filtered_api_params}")
        clutch_stats_endpoint = leaguedashteamclutch.LeagueDashTeamClutch(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = clutch_stats_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = clutch_stats_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "LeagueDashTeamClutch"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_team_clutch(
                            season, season_type, per_mode, measure_type, clutch_time, ahead_behind, point_diff, team_id
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched team clutch stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_team_clutch_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.LEAGUE_DASH_TEAM_CLUTCH_API.format(
            season=season, season_type=season_type, clutch_time=clutch_time, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_team_pt_shot.py =====
"""
Handles fetching and processing league dashboard team player tracking shot data
from the LeagueDashTeamPtShot endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The LeagueDashTeamPtShot endpoint provides comprehensive team shooting data (1 DataFrame):
- Team Info: TEAM_ID, TEAM_NAME, TEAM_ABBREVIATION, GP, G (5 columns)
- Overall Shooting: FGA_FREQUENCY, FGM, FGA, FG_PCT, EFG_PCT (5 columns)
- 2-Point Shooting: FG2A_FREQUENCY, FG2M, FG2A, FG2_PCT (4 columns)
- 3-Point Shooting: FG3A_FREQUENCY, FG3M, FG3A, FG3_PCT (4 columns)
- Rich shooting data: 30 teams with detailed shooting statistics (18 columns total)
- Perfect for team shooting analysis, shot selection insights, and strategic analysis
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import leaguedashteamptshot
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_TEAM_PT_SHOT_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PER_MODES: Set[str] = {PerModeSimple.totals, PerModeSimple.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# --- Cache Directory Setup ---
LEAGUE_DASH_TEAM_PT_SHOT_CSV_DIR = get_cache_dir("league_dash_team_pt_shot")

# Ensure cache directories exist
os.makedirs(LEAGUE_DASH_TEAM_PT_SHOT_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_dash_team_pt_shot(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    data_set_name: str = "TeamPtShot"
) -> str:
    """
    Generates a file path for saving league dash team pt shot DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season: Season in YYYY-YY format
        season_type_all_star: Season type (default: Regular Season)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"league_dash_team_pt_shot_league{league_id}",
        f"season{season}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"per{per_mode_simple.replace(' ', '_')}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "league_dash_team_pt_shot")

# --- Parameter Validation ---
def _validate_league_dash_team_pt_shot_params(
    league_id: str,
    per_mode_simple: str,
    season: str,
    season_type_all_star: str
) -> Optional[str]:
    """Validates parameters for fetch_league_dash_team_pt_shot_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if per_mode_simple not in VALID_PER_MODES:
        return f"Invalid per_mode_simple: {per_mode_simple}. Valid options: {', '.join(VALID_PER_MODES)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_TEAM_PT_SHOT_CACHE_SIZE)
def fetch_league_dash_team_pt_shot_logic(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league dashboard team player tracking shot data using the LeagueDashTeamPtShot endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team pt shot data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_league_dash_team_pt_shot_logic for League: {league_id}, Season: {season}, "
        f"Type: {season_type_all_star}, Per: {per_mode_simple}, return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_league_dash_team_pt_shot_params(
        league_id, per_mode_simple, season, season_type_all_star
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for league dash team pt shot: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_league_dash_team_pt_shot(
        league_id, per_mode_simple, season, season_type_all_star, "TeamPtShot"
    )
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 500:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading league dash team pt shot from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "per_mode_simple": per_mode_simple,
                        "season": season,
                        "season_type_all_star": season_type_all_star
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["TeamPtShot"] = df
                result_dict["data_sets"]["TeamPtShot"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "per_mode_simple": per_mode_simple,
            "season": season,
            "season_type_all_star": season_type_all_star
        }
        
        logger.debug(f"Calling LeagueDashTeamPtShot with parameters: {api_params}")
        team_pt_shot_endpoint = leaguedashteamptshot.LeagueDashTeamPtShot(**api_params)
        
        # Get data frames
        list_of_dataframes = team_pt_shot_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"TeamPtShot_{idx}" if idx > 0 else "TeamPtShot"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_league_dash_team_pt_shot_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_league_dash_team_pt_shot(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get league dashboard team player tracking shot data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team pt shot data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_league_dash_team_pt_shot_logic(
        league_id=league_id,
        per_mode_simple=per_mode_simple,
        season=season,
        season_type_all_star=season_type_all_star,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing LeagueDashTeamPtShot endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_league_dash_team_pt_shot()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_league_dash_team_pt_shot(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("LeagueDashTeamPtShot endpoint test completed.")


===== backend\api_tools\league_dash_team_shot_locations.py =====
"""
Handles fetching team shot location statistics across the league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashTeamShotLocations endpoint, which provides
comprehensive shot location statistics for teams across the league:
- Restricted Area
- In The Paint (Non-RA)
- Mid-Range
- Left Corner 3
- Right Corner 3
- Above the Break 3
- Backcourt
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashteamshotlocations
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeSimple, DistanceRange
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_TEAM_SHOT_LOCATIONS_CACHE_SIZE = 128
LEAGUE_DASH_TEAM_SHOT_LOCATIONS_CSV_DIR = get_cache_dir("league_dash_team_shot_locations")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_PER_MODES: Set[str] = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed)
                            if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}

_VALID_MEASURE_TYPES: Set[str] = {getattr(MeasureTypeSimple, attr) for attr in dir(MeasureTypeSimple)
                                if not attr.startswith('_') and isinstance(getattr(MeasureTypeSimple, attr), str)}

_VALID_DISTANCE_RANGES: Set[str] = {getattr(DistanceRange, attr) for attr in dir(DistanceRange)
                                  if not attr.startswith('_') and isinstance(getattr(DistanceRange, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_shot_locations(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    distance_range: str,
    conference: Optional[str] = None,
    division: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league team shot locations DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Opponent)
        distance_range: The distance range (e.g., By Zone, 5ft Range)
        conference: Optional conference filter
        division: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()
    distance_range_clean = distance_range.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"team_shot_locations_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{distance_range_clean}"
    ]

    if conference:
        conference_clean = conference.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division:
        division_clean = division.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_team_shot_locations")

# --- Main Logic Function ---
def fetch_league_team_shot_locations_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.per_game,
    measure_type: str = MeasureTypeSimple.base,
    distance_range: str = DistanceRange.by_zone,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    period: int = 0,
    team_id_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    shot_clock_range_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    conference_nullable: Optional[str] = None,
    division_nullable: Optional[str] = None,
    game_scope_nullable: Optional[str] = None,
    player_experience_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    starter_bench_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team shot location statistics across the league using the LeagueDashTeamShotLocations endpoint.

    This endpoint provides comprehensive shot location statistics for teams:
    - Restricted Area
    - In The Paint (Non-RA)
    - Mid-Range
    - Left Corner 3
    - Right Corner 3
    - Above the Break 3
    - Backcourt

    Args:
        season: Season in YYYY-YY format. Defaults to current season.
        season_type: Season type. Defaults to Regular Season.
        per_mode: Per mode for stats. Defaults to PerGame.
        measure_type: Statistical category. Defaults to Base.
        distance_range: Shot distance range. Defaults to By Zone.
        last_n_games: Last N games filter. Defaults to 0 (all games).
        month: Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id: Opponent team ID filter. Defaults to 0 (all teams).
        pace_adjust: Whether to adjust for pace. Defaults to "N".
        plus_minus: Whether to include plus/minus. Defaults to "N".
        rank: Whether to include rankings. Defaults to "N".
        period: Period filter (0-4). Defaults to 0 (all periods).
        team_id_nullable: Team ID filter. Defaults to None.
        date_from_nullable: Start date filter (YYYY-MM-DD). Defaults to None.
        date_to_nullable: End date filter (YYYY-MM-DD). Defaults to None.
        game_segment_nullable: Game segment filter. Defaults to None.
        league_id_nullable: League ID. Defaults to None.
        location_nullable: Location filter (Home/Road). Defaults to None.
        outcome_nullable: Outcome filter (W/L). Defaults to None.
        po_round_nullable: Playoff round filter. Defaults to None.
        season_segment_nullable: Season segment filter. Defaults to None.
        shot_clock_range_nullable: Shot clock range filter. Defaults to None.
        vs_conference_nullable: Conference filter for opponents. Defaults to None.
        vs_division_nullable: Division filter for opponents. Defaults to None.
        conference_nullable: Conference filter for teams. Defaults to None.
        division_nullable: Division filter for teams. Defaults to None.
        game_scope_nullable: Game scope filter. Defaults to None.
        player_experience_nullable: Player experience filter. Defaults to None.
        player_position_nullable: Player position filter. Defaults to None.
        starter_bench_nullable: Starter/bench filter. Defaults to None.
        return_dataframe: Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with team shot location stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_team_shot_locations_logic for: "
        f"Season: {season}, Type: {season_type}, Measure Type: {measure_type}, Distance Range: {distance_range}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(
            value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if measure_type not in _VALID_MEASURE_TYPES:
        error_response = format_response(error=Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type, options=", ".join(list(_VALID_MEASURE_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if distance_range not in _VALID_DISTANCE_RANGES:
        error_response = format_response(error=f"Invalid distance_range: '{distance_range}'. Valid options: {', '.join(list(_VALID_DISTANCE_RANGES))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "per_mode_detailed": per_mode,
            "measure_type_simple": measure_type,
            "distance_range": distance_range,
            "last_n_games": last_n_games,
            "month": month,
            "opponent_team_id": opponent_team_id,
            "pace_adjust": pace_adjust,
            "plus_minus": plus_minus,
            "rank": rank,
            "period": period,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if team_id_nullable:
            api_params["team_id_nullable"] = team_id_nullable
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        if game_segment_nullable:
            api_params["game_segment_nullable"] = game_segment_nullable
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if po_round_nullable:
            api_params["po_round_nullable"] = po_round_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if shot_clock_range_nullable:
            api_params["shot_clock_range_nullable"] = shot_clock_range_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable
        if conference_nullable:
            api_params["conference_nullable"] = conference_nullable
        if division_nullable:
            api_params["division_nullable"] = division_nullable
        if game_scope_nullable:
            api_params["game_scope_nullable"] = game_scope_nullable
        if player_experience_nullable:
            api_params["player_experience_nullable"] = player_experience_nullable
        if player_position_nullable:
            api_params["player_position_nullable"] = player_position_nullable
        if starter_bench_nullable:
            api_params["starter_bench_nullable"] = starter_bench_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueDashTeamShotLocations with parameters: {filtered_api_params}")
        shot_locations_endpoint = leaguedashteamshotlocations.LeagueDashTeamShotLocations(**api_params)

        # Get data frames
        shot_locations_df = shot_locations_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["ShotLocations"] = shot_locations_df

            # Save to CSV if not empty
            if not shot_locations_df.empty:
                csv_path = _get_csv_path_for_team_shot_locations(
                    season, season_type, per_mode, measure_type, distance_range,
                    conference_nullable, division_nullable
                )
                _save_dataframe_to_csv(shot_locations_df, csv_path)

        # The shot locations DataFrame has multi-level columns which need special handling
        # First, flatten the column names to make them JSON serializable
        flattened_df = shot_locations_df.copy()

        # Create flattened column names
        flattened_columns = []
        for col in shot_locations_df.columns:
            if isinstance(col, tuple):
                # Join the tuple elements with an underscore, but skip empty strings
                parts = [str(part) for part in col if part]
                flattened_columns.append("_".join(parts))
            else:
                flattened_columns.append(str(col))

        # Set the new column names
        flattened_df.columns = flattened_columns

        # Process for JSON response
        processed_data = _process_dataframe(flattened_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "shot_locations": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched team shot locations for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_team_shot_locations_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching team shot locations: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_dash_team_stats.py =====
"""
Handles fetching league team statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueDashTeamStats endpoint, which provides
comprehensive team statistics across the league:
- Basic and advanced statistics
- Scoring and defensive metrics
- Team rankings
- Filtering by conference, division, etc.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import leaguedashteamstats
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
)
from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response
)
from utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DASH_TEAM_STATS_CACHE_SIZE = 128
LEAGUE_DASH_TEAM_STATS_CSV_DIR = get_cache_dir("league_dash_team_stats")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_GAME_SEGMENTS = {
    "First Half": "First Half",
    "Second Half": "Second Half",
    "Overtime": "Overtime"
}

VALID_LOCATIONS = {
    "Home": "Home",
    "Road": "Road"
}

VALID_OUTCOMES = {
    "W": "W",
    "L": "L"
}

VALID_SEASON_SEGMENTS = {
    "Post All-Star": "Post All-Star",
    "Pre All-Star": "Pre All-Star"
}

VALID_CONFERENCES = {
    "East": "East",
    "West": "West"
}

VALID_DIVISIONS = {
    "Atlantic": "Atlantic",
    "Central": "Central",
    "Southeast": "Southeast",
    "Northwest": "Northwest",
    "Pacific": "Pacific",
    "Southwest": "Southwest",
    "East": "East",
    "West": "West"
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_team_stats(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    conference: Optional[str] = None,
    division: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league team stats DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        conference: Optional conference filter
        division: Optional division filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"league_team_stats_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}"
    ]

    if conference:
        conference_clean = conference.lower()
        filename_parts.append(f"conf_{conference_clean}")

    if division:
        division_clean = division.replace(" ", "_").lower()
        filename_parts.append(f"div_{division_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_dash_team_stats")

# --- Parameter Validation Functions ---
def _validate_league_team_stats_params(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    game_segment: Optional[str] = None,
    location: Optional[str] = None,
    outcome: Optional[str] = None,
    season_segment: Optional[str] = None,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None,
    conference: Optional[str] = None,
    division: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the league team stats function.

    Args:
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        game_segment: Optional game segment filter
        location: Optional location filter
        outcome: Optional outcome filter
        season_segment: Optional season segment filter
        vs_conference: Optional conference filter for opponents
        vs_division: Optional division filter for opponents
        conference: Optional conference filter for teams
        division: Optional division filter for teams

    Returns:
        Error message if validation fails, None otherwise
    """
    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if game_segment and game_segment not in VALID_GAME_SEGMENTS:
        return f"Invalid game_segment: '{game_segment}'. Valid options: {', '.join(list(VALID_GAME_SEGMENTS.keys()))}"

    if location and location not in VALID_LOCATIONS:
        return f"Invalid location: '{location}'. Valid options: {', '.join(list(VALID_LOCATIONS.keys()))}"

    if outcome and outcome not in VALID_OUTCOMES:
        return f"Invalid outcome: '{outcome}'. Valid options: {', '.join(list(VALID_OUTCOMES.keys()))}"

    if season_segment and season_segment not in VALID_SEASON_SEGMENTS:
        return f"Invalid season_segment: '{season_segment}'. Valid options: {', '.join(list(VALID_SEASON_SEGMENTS.keys()))}"

    if vs_conference and vs_conference not in VALID_CONFERENCES:
        return f"Invalid vs_conference: '{vs_conference}'. Valid options: {', '.join(list(VALID_CONFERENCES.keys()))}"

    if vs_division and vs_division not in VALID_DIVISIONS:
        return f"Invalid vs_division: '{vs_division}'. Valid options: {', '.join(list(VALID_DIVISIONS.keys()))}"

    if conference and conference not in VALID_CONFERENCES:
        return f"Invalid conference: '{conference}'. Valid options: {', '.join(list(VALID_CONFERENCES.keys()))}"

    if division and division not in VALID_DIVISIONS:
        return f"Invalid division: '{division}'. Valid options: {', '.join(list(VALID_DIVISIONS.keys()))}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_DASH_TEAM_STATS_CACHE_SIZE)
def fetch_league_team_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    period: int = 0,
    team_id: str = "",
    date_from: str = "",
    date_to: str = "",
    game_segment: str = "",
    league_id: str = "00",  # NBA
    location: str = "",
    outcome: str = "",
    po_round: str = "",
    season_segment: str = "",
    shot_clock_range: str = "",
    vs_conference: str = "",
    vs_division: str = "",
    conference: str = "",
    division: str = "",
    game_scope: str = "",
    player_experience: str = "",
    player_position: str = "",
    starter_bench: str = "",
    two_way: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league team statistics using the LeagueDashTeamStats endpoint.

    This endpoint provides comprehensive team statistics across the league:
    - Basic and advanced statistics
    - Scoring and defensive metrics
    - Team rankings
    - Filtering by conference, division, etc.

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int, optional): Opponent team ID filter. Defaults to 0 (all teams).
        pace_adjust (str, optional): Whether to adjust for pace. Defaults to "N".
        plus_minus (str, optional): Whether to include plus/minus. Defaults to "N".
        rank (str, optional): Whether to include rankings. Defaults to "N".
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        team_id (str, optional): Team ID filter. Defaults to "".
        date_from (str, optional): Start date filter (MM/DD/YYYY). Defaults to "".
        date_to (str, optional): End date filter (MM/DD/YYYY). Defaults to "".
        game_segment (str, optional): Game segment filter. Defaults to "".
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        location (str, optional): Location filter (Home/Road). Defaults to "".
        outcome (str, optional): Outcome filter (W/L). Defaults to "".
        po_round (str, optional): Playoff round filter. Defaults to "".
        season_segment (str, optional): Season segment filter. Defaults to "".
        shot_clock_range (str, optional): Shot clock range filter. Defaults to "".
        vs_conference (str, optional): Conference filter for opponents. Defaults to "".
        vs_division (str, optional): Division filter for opponents. Defaults to "".
        conference (str, optional): Conference filter for teams. Defaults to "".
        division (str, optional): Division filter for teams. Defaults to "".
        game_scope (str, optional): Game scope filter. Defaults to "".
        player_experience (str, optional): Player experience filter. Defaults to "".
        player_position (str, optional): Player position filter. Defaults to "".
        starter_bench (str, optional): Starter/bench filter. Defaults to "".
        two_way (str, optional): Two-way player filter. Defaults to "".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with league team stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_team_stats_logic for: "
        f"Season: {season}, Type: {season_type}, Measure Type: {measure_type}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_league_team_stats_params(
        season, season_type, per_mode, measure_type, game_segment, location, outcome,
        season_segment, vs_conference, vs_division, conference, division
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "last_n_games": last_n_games,
        "month": month,
        "opponent_team_id": opponent_team_id,
        "pace_adjust": pace_adjust,
        "plus_minus": plus_minus,
        "rank": rank,
        "period": period,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if team_id:
        api_params["team_id_nullable"] = team_id

    if date_from:
        api_params["date_from_nullable"] = date_from

    if date_to:
        api_params["date_to_nullable"] = date_to

    if game_segment:
        api_params["game_segment_nullable"] = game_segment

    if league_id:
        api_params["league_id_nullable"] = league_id

    if location:
        api_params["location_nullable"] = location

    if outcome:
        api_params["outcome_nullable"] = outcome

    if po_round:
        api_params["po_round_nullable"] = po_round

    if season_segment:
        api_params["season_segment_nullable"] = season_segment

    if shot_clock_range:
        api_params["shot_clock_range_nullable"] = shot_clock_range

    if vs_conference:
        api_params["vs_conference_nullable"] = vs_conference

    if vs_division:
        api_params["vs_division_nullable"] = vs_division

    if conference:
        api_params["conference_nullable"] = conference

    if division:
        api_params["division_simple_nullable"] = division

    if game_scope:
        api_params["game_scope_simple_nullable"] = game_scope

    if player_experience:
        api_params["player_experience_nullable"] = player_experience

    if player_position:
        api_params["player_position_abbreviation_nullable"] = player_position

    if starter_bench:
        api_params["starter_bench_nullable"] = starter_bench

    if two_way:
        api_params["two_way_nullable"] = two_way

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling LeagueDashTeamStats with parameters: {filtered_api_params}")
        team_stats_endpoint = leaguedashteamstats.LeagueDashTeamStats(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = team_stats_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = team_stats_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "LeagueDashTeamStats"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_league_team_stats(
                            season, season_type, per_mode, measure_type, conference, division
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched league team stats for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_team_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.LEAGUE_DASH_TEAM_STATS_API.format(
            season=season, season_type=season_type, measure_type=measure_type, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_draft.py =====
"""
Handles fetching NBA draft history data using the drafthistory endpoint.
Allows filtering by various parameters like season year, league, team, round, and pick.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json # Not explicitly used, but format_response returns JSON string. Good to keep for context.
from typing import Optional, Dict, Any, List, Set, Union, Tuple
import pandas as pd
from functools import lru_cache

from nba_api.stats.endpoints import drafthistory
from nba_api.stats.library.parameters import LeagueID
from .utils import _process_dataframe, format_response
from ..config import settings
from ..core.errors import Errors
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_DRAFT_CACHE_SIZE = 32

_VALID_DRAFT_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Cache Directory Setup ---
DRAFT_HISTORY_CSV_DIR = get_cache_dir("draft_history")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_draft_history(
    season_year_nullable: Optional[str] = None,
    league_id_nullable: str = LeagueID.nba,
    team_id_nullable: Optional[int] = None,
    round_num_nullable: Optional[int] = None,
    overall_pick_nullable: Optional[int] = None
) -> str:
    """
    Generates a file path for saving draft history DataFrame as CSV.

    Args:
        season_year_nullable: Four-digit draft year (e.g., '2023')
        league_id_nullable: League ID
        team_id_nullable: NBA team ID to filter by team
        round_num_nullable: Draft round number to filter by round
        overall_pick_nullable: Overall pick number to filter by pick

    Returns:
        Path to the CSV file
    """
    # Create a string with the parameters
    params = []

    if season_year_nullable:
        params.append(f"year_{season_year_nullable}")
    else:
        params.append("year_all")

    if league_id_nullable:
        params.append(f"league_{league_id_nullable}")

    if team_id_nullable:
        params.append(f"team_{team_id_nullable}")

    if round_num_nullable:
        params.append(f"round_{round_num_nullable}")

    if overall_pick_nullable:
        params.append(f"pick_{overall_pick_nullable}")

    # Join parameters with underscores
    filename = "_".join(params) + ".csv"

    return get_cache_file_path(filename, "draft_history")

# --- Logic Function ---
def fetch_draft_history_logic(
    season_year_nullable: Optional[str] = None,
    league_id_nullable: str = LeagueID.nba,
    team_id_nullable: Optional[int] = None,
    round_num_nullable: Optional[int] = None,
    overall_pick_nullable: Optional[int] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA draft history using nba_api's DraftHistory endpoint, optionally filtered by season year, league, team, round number, or overall pick.
    Provides DataFrame output capabilities.

    Args:
        season_year_nullable: Four-digit draft year (e.g., '2023'). If None, returns all years.
        league_id_nullable: League ID (default: NBA).
        team_id_nullable: NBA team ID to filter by team.
        round_num_nullable: Draft round number to filter by round.
        overall_pick_nullable: Overall pick number to filter by pick.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON-formatted string containing a list of draft picks or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.

    Notes:
        - Returns an error for invalid year format or league ID.
        - Returns an empty list if no draft picks are found for the filters.
        - Each draft pick includes player, year, round, pick, team, and organization info.
    """
    year_log_display = season_year_nullable or "All"
    logger.info(f"Executing fetch_draft_history_logic for SeasonYear: {year_log_display}, League: {league_id_nullable}, Team: {team_id_nullable}, Round: {round_num_nullable}, Pick: {overall_pick_nullable}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if season_year_nullable and (not season_year_nullable.isdigit() or len(season_year_nullable) != 4):
        error_msg = Errors.INVALID_DRAFT_YEAR_FORMAT.format(year=season_year_nullable)
        logger.error(error_msg)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id_nullable not in _VALID_DRAFT_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(_VALID_DRAFT_LEAGUE_IDS)))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        logger.debug(f"Fetching drafthistory for SeasonYear: {year_log_display}, League: {league_id_nullable}")
        draft_endpoint = drafthistory.DraftHistory(
            league_id=league_id_nullable,
            season_year_nullable=season_year_nullable,
            team_id_nullable=team_id_nullable,
            round_num_nullable=round_num_nullable,
            overall_pick_nullable=overall_pick_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"drafthistory API call successful for SeasonYear: {year_log_display}")
        draft_df = draft_endpoint.draft_history.get_data_frame()

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["draft_history"] = draft_df

            # Save to CSV if not empty
            if not draft_df.empty:
                csv_path = _get_csv_path_for_draft_history(
                    season_year_nullable=season_year_nullable,
                    league_id_nullable=league_id_nullable,
                    team_id_nullable=team_id_nullable,
                    round_num_nullable=round_num_nullable,
                    overall_pick_nullable=overall_pick_nullable
                )
                _save_dataframe_to_csv(draft_df, csv_path)

        draft_list = _process_dataframe(draft_df, single_row=False)

        if draft_list is None:
            if draft_df.empty:
                logger.warning(f"No draft history data found for year {year_log_display} with specified filters.")
                empty_result = {
                    "season_year_requested": year_log_display, "league_id": league_id_nullable,
                    "team_id_filter": team_id_nullable, "round_num_filter": round_num_nullable,
                    "overall_pick_filter": overall_pick_nullable, "draft_picks": []
                }

                if return_dataframe:
                    return format_response(empty_result), dataframes
                return format_response(empty_result)
            else:
                logger.error(f"DataFrame processing failed for draft history ({year_log_display}).")
                error_msg = Errors.DRAFT_HISTORY_PROCESSING.format(year=year_log_display)
                error_response = format_response(error=error_msg)

                if return_dataframe:
                    return error_response, dataframes
                return error_response

        result = {
            "season_year_requested": year_log_display, "league_id": league_id_nullable,
            "team_id_filter": team_id_nullable, "round_num_filter": round_num_nullable,
            "overall_pick_filter": overall_pick_nullable, "draft_picks": draft_list or []
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_draft_history(
                season_year_nullable=season_year_nullable,
                league_id_nullable=league_id_nullable,
                team_id_nullable=team_id_nullable,
                round_num_nullable=round_num_nullable,
                overall_pick_nullable=overall_pick_nullable
            )
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "draft_history"
            )

            result["dataframe_info"] = {
                "message": "Draft history data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "draft_history": {
                        "shape": list(draft_df.shape) if not draft_df.empty else [],
                        "columns": draft_df.columns.tolist() if not draft_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

            logger.info(f"fetch_draft_history_logic completed for SeasonYear: {year_log_display}")
            return format_response(result), dataframes

        logger.info(f"fetch_draft_history_logic completed for SeasonYear: {year_log_display}")
        return format_response(result)

    except Exception as e:
        logger.critical(f"Unexpected error in fetch_draft_history_logic for SeasonYear '{year_log_display}': {e}", exc_info=True)
        error_msg = Errors.DRAFT_HISTORY_UNEXPECTED.format(year=year_log_display, error=str(e))
        error_response = format_response(error=error_msg)

        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\league_game_log.py =====
"""
Handles fetching game logs for the entire league.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the LeagueGameLog endpoint, which provides
comprehensive game-by-game statistics for all teams or players across the league.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd

from nba_api.stats.endpoints import leaguegamelog
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, LeagueID, Direction, PlayerOrTeamAbbreviation, Sorter
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_GAME_LOG_CACHE_SIZE = 128
LEAGUE_GAME_LOG_CSV_DIR = get_cache_dir("league_game_log")

# Valid parameter values
_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar)
                               if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID)
                             if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

_VALID_DIRECTIONS: Set[str] = {getattr(Direction, attr) for attr in dir(Direction)
                             if not attr.startswith('_') and isinstance(getattr(Direction, attr), str)}

_VALID_PLAYER_OR_TEAM: Set[str] = {getattr(PlayerOrTeamAbbreviation, attr) for attr in dir(PlayerOrTeamAbbreviation)
                                 if not attr.startswith('_') and isinstance(getattr(PlayerOrTeamAbbreviation, attr), str)}

_VALID_SORTERS: Set[str] = {getattr(Sorter, attr) for attr in dir(Sorter)
                          if not attr.startswith('_') and isinstance(getattr(Sorter, attr), str)}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_game_log(
    season: str,
    season_type: str,
    player_or_team: str,
    league_id: str,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a league game log DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        player_or_team: Whether to get player or team game logs (P or T)
        league_id: The league ID (e.g., 00 for NBA)
        date_from_nullable: Optional start date filter
        date_to_nullable: Optional end date filter

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    player_or_team_clean = "player" if player_or_team == PlayerOrTeamAbbreviation.player else "team"

    # Create filename with optional filters
    filename_parts = [
        f"game_log_{season_clean}_{season_type_clean}_{player_or_team_clean}_{league_id}"
    ]

    if date_from_nullable:
        date_from_clean = date_from_nullable.replace("-", "")
        filename_parts.append(f"from_{date_from_clean}")

    if date_to_nullable:
        date_to_clean = date_to_nullable.replace("-", "")
        filename_parts.append(f"to_{date_to_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_game_log")

# --- Main Logic Function ---
def fetch_league_game_log_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    player_or_team: str = PlayerOrTeamAbbreviation.team,
    league_id: str = LeagueID.nba,
    direction: str = Direction.asc,
    sorter: str = Sorter.date,
    counter: int = 0,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches game logs for the entire league using the LeagueGameLog endpoint.

    This endpoint provides comprehensive game-by-game statistics for all teams or players
    across the league, with options to filter by date range and sort the results.

    The data includes:
    - Game information (date, matchup, win/loss)
    - Team or player identification (ID, name, abbreviation)
    - Basic statistics (points, rebounds, assists, etc.)
    - Shooting percentages (FG%, 3P%, FT%)
    - Advanced metrics (plus/minus)

    Args:
        season (str): Season in YYYY-YY format (e.g., "2023-24"). Defaults to current season.
        season_type (str): Season type from SeasonTypeAllStar (e.g., "Regular Season", "Playoffs").
            Defaults to "Regular Season".
        player_or_team (str): Whether to get player or team game logs from PlayerOrTeam (e.g., "P", "T").
            Defaults to "T" (team).
        league_id (str): League ID from LeagueID (e.g., "00" for NBA). Defaults to "00".
        direction (str): Sort direction from Direction (e.g., "ASC", "DESC").
            Defaults to "ASC".
        sorter (str): Field to sort by from Sorter (e.g., "DATE", "PTS").
            Defaults to "DATE".
        counter (int): Counter parameter required by the API. Defaults to 0.
        date_from_nullable (Optional[str]): Start date filter in YYYY-MM-DD format.
        date_to_nullable (Optional[str]): End date filter in YYYY-MM-DD format.
        return_dataframe (bool): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with game log data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_league_game_log_logic for: "
        f"Season: {season}, Type: {season_type}, Player/Team: {player_or_team}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Parameter validation
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(
            value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id not in _VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(
            value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if player_or_team not in _VALID_PLAYER_OR_TEAM:
        error_response = format_response(error=f"Invalid player_or_team: '{player_or_team}'. Valid options: {', '.join(list(_VALID_PLAYER_OR_TEAM))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if direction not in _VALID_DIRECTIONS:
        error_response = format_response(error=f"Invalid direction: '{direction}'. Valid options: {', '.join(list(_VALID_DIRECTIONS))}")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if sorter not in _VALID_SORTERS:
        error_response = format_response(error=f"Invalid sorter: '{sorter}'. Valid options: {', '.join(list(_VALID_SORTERS)[:5])}...")
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Prepare API parameters
        api_params = {
            "season": season,
            "season_type_all_star": season_type,
            "player_or_team_abbreviation": player_or_team,  # This is the parameter name in the Python API
            "league_id": league_id,
            "direction": direction,
            "sorter": sorter,
            "counter": counter,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }

        # Add optional parameters if provided
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable

        # Filter out None values for cleaner logging
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

        logger.debug(f"Calling LeagueGameLog with parameters: {filtered_api_params}")
        game_log_endpoint = leaguegamelog.LeagueGameLog(**api_params)

        # Get data frames
        game_log_df = game_log_endpoint.get_data_frames()[0]

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["LeagueGameLog"] = game_log_df

            # Save to CSV if not empty
            if not game_log_df.empty:
                csv_path = _get_csv_path_for_game_log(
                    season, season_type, player_or_team, league_id,
                    date_from_nullable, date_to_nullable
                )
                _save_dataframe_to_csv(game_log_df, csv_path)

        # Process for JSON response
        processed_data = _process_dataframe(game_log_df, single_row=False)

        # Create result dictionary
        result_dict = {
            "parameters": filtered_api_params,
            "game_log": processed_data or []
        }

        # Return response
        logger.info(f"Successfully fetched game logs for {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_league_game_log_logic: {e}",
            exc_info=True
        )
        error_msg = f"Error fetching game logs: {str(e)}"
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\league_hustle_stats_team.py =====
"""
Handles fetching and processing league hustle stats team data
from the LeagueHustleStatsTeam endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The LeagueHustleStatsTeam endpoint provides comprehensive team hustle stats data (1 DataFrame):
- Team Info: TEAM_ID, TEAM_NAME, MIN (3 columns)
- Contested Shots: CONTESTED_SHOTS, CONTESTED_SHOTS_2PT, CONTESTED_SHOTS_3PT (3 columns)
- Defensive Hustle: DEFLECTIONS, CHARGES_DRAWN (2 columns)
- Screen Assists: SCREEN_ASSISTS, SCREEN_AST_PTS (2 columns)
- Loose Balls: OFF_LOOSE_BALLS_RECOVERED, DEF_LOOSE_BALLS_RECOVERED, LOOSE_BALLS_RECOVERED, PCT_LOOSE_BALLS_RECOVERED_OFF, PCT_LOOSE_BALLS_RECOVERED_DEF (5 columns)
- Box Outs: OFF_BOXOUTS, DEF_BOXOUTS, BOX_OUTS, PCT_BOX_OUTS_OFF, PCT_BOX_OUTS_DEF (5 columns)
- Rich hustle data: 30 teams with detailed hustle statistics (20 columns total)
- Perfect for effort analysis, defensive intensity tracking, and team chemistry insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import leaguehustlestatsteam
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeTime
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_HUSTLE_STATS_TEAM_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PER_MODES: Set[str] = {PerModeTime.totals, PerModeTime.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# --- Cache Directory Setup ---
LEAGUE_HUSTLE_STATS_TEAM_CSV_DIR = get_cache_dir("league_hustle_stats_team")

# Ensure cache directories exist
os.makedirs(LEAGUE_HUSTLE_STATS_TEAM_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_hustle_stats_team(
    per_mode_time: str = PerModeTime.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    data_set_name: str = "TeamHustleStats"
) -> str:
    """
    Generates a file path for saving league hustle stats team DataFrame.
    
    Args:
        per_mode_time: Per mode (default: Totals)
        season: Season in YYYY-YY format
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"league_hustle_stats_team",
        f"season{season}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"per{per_mode_time.replace(' ', '_')}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "league_hustle_stats_team")

# --- Parameter Validation ---
def _validate_league_hustle_stats_team_params(
    per_mode_time: str,
    season: str,
    season_type_all_star: str,
    league_id_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_league_hustle_stats_team_logic."""
    if per_mode_time not in VALID_PER_MODES:
        return f"Invalid per_mode_time: {per_mode_time}. Valid options: {', '.join(VALID_PER_MODES)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if league_id_nullable and league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_HUSTLE_STATS_TEAM_CACHE_SIZE)
def fetch_league_hustle_stats_team_logic(
    per_mode_time: str = PerModeTime.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league hustle stats team data using the LeagueHustleStatsTeam endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        per_mode_time: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team hustle stats data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_league_hustle_stats_team_logic for Season: {season}, "
        f"Type: {season_type_all_star}, Per: {per_mode_time}, League: {league_id_nullable}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_league_hustle_stats_team_params(
        per_mode_time, season, season_type_all_star, league_id_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for league hustle stats team: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_league_hustle_stats_team(
        per_mode_time, season, season_type_all_star, league_id_nullable, "TeamHustleStats"
    )
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 500:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading league hustle stats team from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "per_mode_time": per_mode_time,
                        "season": season,
                        "season_type_all_star": season_type_all_star,
                        "league_id_nullable": league_id_nullable
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["TeamHustleStats"] = df
                result_dict["data_sets"]["TeamHustleStats"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "per_mode_time": per_mode_time,
            "season": season,
            "season_type_all_star": season_type_all_star
        }
        
        # Add league_id_nullable only if it's not empty
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable
        
        logger.debug(f"Calling LeagueHustleStatsTeam with parameters: {api_params}")
        hustle_stats_endpoint = leaguehustlestatsteam.LeagueHustleStatsTeam(**api_params)
        
        # Get data frames
        list_of_dataframes = hustle_stats_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "per_mode_time": per_mode_time,
                "season": season,
                "season_type_all_star": season_type_all_star,
                "league_id_nullable": league_id_nullable
            },
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"TeamHustleStats_{idx}" if idx > 0 else "TeamHustleStats"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_league_hustle_stats_team_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_league_hustle_stats_team(
    per_mode_time: str = PerModeTime.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get league hustle stats team data.
    
    Args:
        per_mode_time: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team hustle stats data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_league_hustle_stats_team_logic(
        per_mode_time=per_mode_time,
        season=season,
        season_type_all_star=season_type_all_star,
        league_id_nullable=league_id_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing LeagueHustleStatsTeam endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_league_hustle_stats_team()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_league_hustle_stats_team(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("LeagueHustleStatsTeam endpoint test completed.")


===== backend\api_tools\league_leaders_data.py =====
"""
Handles fetching league leaders data for various statistical categories.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import os
import logging
from functools import lru_cache
import requests # For direct HTTP debug request
import json
import pandas as pd
from typing import Optional, Dict, Any, List, Set, Union, Tuple

from nba_api.stats.endpoints import leagueleaders
from nba_api.stats.library.parameters import LeagueID, SeasonTypeAllStar, PerMode48, Scope, StatCategoryAbbreviation
from .utils import _process_dataframe, format_response
from ..utils.validation import _validate_season_format
from ..config import settings
from ..core.errors import Errors

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_LEADERS_CACHE_SIZE = 128
DEFAULT_TOP_N_LEADERS = 10
LEAGUE_LEADERS_STATS_URL = "https://stats.nba.com/stats/leagueleaders"
DEFAULT_LEAGUE_LEADERS_HTTP_HEADERS = {'User-Agent': 'Mozilla/5.0', 'Referer': 'https://stats.nba.com/', 'Accept': 'application/json'}
DEBUG_HTTP_REQUEST_TIMEOUT = 5

_VALID_STAT_CATEGORIES_LEADERS: Set[str] = {getattr(StatCategoryAbbreviation, attr) for attr in dir(StatCategoryAbbreviation) if not attr.startswith('_')}
_VALID_SEASON_TYPES_LEADERS: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES_LEADERS: Set[str] = {getattr(PerMode48, attr) for attr in dir(PerMode48) if not attr.startswith('_') and isinstance(getattr(PerMode48, attr), str)}
_VALID_LEAGUE_IDS_LEADERS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}
_VALID_SCOPES_LEADERS: Set[str] = {getattr(Scope, attr) for attr in dir(Scope) if not attr.startswith('_') and isinstance(getattr(Scope, attr), str)}

_EXPECTED_LEADER_COLS = ['PLAYER_ID', 'RANK', 'PLAYER', 'TEAM_ID', 'TEAM', 'GP', 'MIN']

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
LEAGUE_LEADERS_CSV_DIR = os.path.join(CSV_CACHE_DIR, "league_leaders")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(LEAGUE_LEADERS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_leaders(
    season: str,
    stat_category: str,
    season_type: str,
    per_mode: str,
    league_id: str,
    scope: str
) -> str:
    """
    Generates a file path for saving league leaders DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        stat_category: The statistical category (e.g., 'PTS', 'REB')
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        league_id: The league ID (e.g., '00' for NBA)
        scope: The scope (e.g., 'S' for season, 'RS' for rookies)

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    # Clean per mode for filename
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"leaders_{season}_{stat_category}_{clean_season_type}_{clean_per_mode}_{league_id}_{scope}.csv"
    return os.path.join(LEAGUE_LEADERS_CSV_DIR, filename)

# --- Helper for Parameter Validation ---
def _validate_league_leaders_params(
    season: str, stat_category: str, season_type: str, per_mode: str,
    league_id: str, scope: str, top_n: int
) -> Optional[str]:
    """Validates parameters for fetch_league_leaders_logic."""
    if not _validate_season_format(season):
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if not isinstance(top_n, int) or top_n <= 0: # top_n already defaulted if invalid by main func
        # This specific log/defaulting is handled in main func, here just check type for safety
        pass # Assuming top_n is already validated/defaulted by caller
    if stat_category not in _VALID_STAT_CATEGORIES_LEADERS:
        return Errors.INVALID_STAT_CATEGORY.format(value=stat_category, options=", ".join(list(_VALID_STAT_CATEGORIES_LEADERS)[:5]))
    if season_type not in _VALID_SEASON_TYPES_LEADERS:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SEASON_TYPES_LEADERS)[:5]))
    if per_mode not in _VALID_PER_MODES_LEADERS:
        return Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES_LEADERS)[:5]))
    if league_id not in _VALID_LEAGUE_IDS_LEADERS:
        return Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS_LEADERS)[:5]))
    if scope not in _VALID_SCOPES_LEADERS:
        return Errors.INVALID_SCOPE.format(value=scope, options=", ".join(list(_VALID_SCOPES_LEADERS)[:5]))
    return None

# --- Main Logic Function ---
def fetch_league_leaders_logic(
    season: str,
    stat_category: str = StatCategoryAbbreviation.pts,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerMode48.per_game,
    league_id: str = LeagueID.nba,
    scope: str = Scope.s,
    top_n: int = DEFAULT_TOP_N_LEADERS,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league leaders for a specific statistical category and criteria.
    Provides DataFrame output capabilities.

    Args:
        season: NBA season in 'YYYY-YY' format.
        stat_category: Statistical category abbreviation (e.g., 'PTS').
        season_type: Season type (e.g., 'Regular Season').
        per_mode: Per mode (e.g., 'PerGame').
        league_id: League ID. Defaults to NBA.
        scope: Scope of leaders (e.g., 'S' for season).
        top_n: Number of top leaders to return. Defaults to 10.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string of league leaders list or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_leaders_logic for season: {season}, category: {stat_category}, type: {season_type}, per_mode: {per_mode}, scope: {scope}, top_n: {top_n}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    # Validate top_n separately as it has a default and specific logging
    if not isinstance(top_n, int) or top_n <= 0:
        logger.warning(f"Invalid top_n value '{top_n}'. Must be a positive integer. Defaulting to {DEFAULT_TOP_N_LEADERS}.")
        top_n = DEFAULT_TOP_N_LEADERS

    param_error = _validate_league_leaders_params(season, stat_category, season_type, per_mode, league_id, scope, top_n)
    if param_error:
        if return_dataframe:
            return format_response(error=param_error), dataframes
        return format_response(error=param_error)

    http_params = {
        "LeagueID": league_id, "PerMode": per_mode, "Scope": scope,
        "Season": season, "SeasonType": season_type, "StatCategory": stat_category,
    }
    raw_response_status_for_log = None

    try:
        logger.debug(f"Calling leagueleaders.LeagueLeaders with params: {http_params}")
        leaders_endpoint = leagueleaders.LeagueLeaders(
            league_id=league_id, per_mode48=per_mode, scope=scope, season=season,
            season_type_all_star=season_type, stat_category_abbreviation=stat_category,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        leaders_df = leaders_endpoint.league_leaders.get_data_frame()
        logger.debug(f"nba_api leagueleaders call successful for {stat_category} ({season})")

    except KeyError as ke:
        if 'resultSet' in str(ke): # Specific handling for NBA API malformed JSON
            logger.error(f"KeyError 'resultSet' using nba_api for {stat_category} ({season}). NBA API response likely malformed.", exc_info=True)
            try:
                response = requests.get(LEAGUE_LEADERS_STATS_URL, headers=DEFAULT_LEAGUE_LEADERS_HTTP_HEADERS, params=http_params, timeout=DEBUG_HTTP_REQUEST_TIMEOUT)
                raw_response_status_for_log = response.status_code
                logger.debug(f"Direct HTTP debug request status: {raw_response_status_for_log}, Response (first 500): {response.text[:500]}")
            except Exception as direct_req_err:
                logger.error(f"Direct HTTP debug request failed: {direct_req_err}")
            error_msg = Errors.LEAGUE_LEADERS_API_KEY_ERROR.format(stat=stat_category, season=season) + f" (Direct status: {raw_response_status_for_log or 'N/A'})"

            if return_dataframe:
                return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)
        else: # Re-raise other KeyErrors
            logger.error(f"Unexpected KeyError in leagueleaders for {stat_category} ({season}): {ke}", exc_info=True)
            raise # Re-raise to be caught by general Exception handler
    except Exception as api_error:
        logger.error(f"nba_api leagueleaders failed for {stat_category} ({season}): {api_error}", exc_info=True)
        error_msg = Errors.LEAGUE_LEADERS_API.format(stat_category=stat_category, season=season, error=str(api_error))

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if leaders_df.empty:
        logger.warning(f"No league leaders data found via nba_api for {stat_category} ({season}).")

        if return_dataframe:
            return format_response({"leaders": []}), dataframes
        return format_response({"leaders": []})

    if stat_category not in leaders_df.columns:
        logger.error(f"Stat category '{stat_category}' not found in league leaders DataFrame columns: {list(leaders_df.columns)}")
        error_msg = Errors.LEAGUE_LEADERS_PROCESSING.format(stat_category=stat_category, season=season) + f" - Stat column '{stat_category}' missing."

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Save DataFrame to CSV if requested
    if return_dataframe:
        dataframes["leaders"] = leaders_df

        # Save to CSV if not empty
        if not leaders_df.empty:
            csv_path = _get_csv_path_for_league_leaders(
                season, stat_category, season_type, per_mode, league_id, scope
            )
            _save_dataframe_to_csv(leaders_df, csv_path)

    # Ensure all expected columns and the stat_category column are present for processing
    cols_for_processing = list(dict.fromkeys([col for col in _EXPECTED_LEADER_COLS if col in leaders_df.columns] + [stat_category]))

    leaders_list = _process_dataframe(leaders_df.loc[:, cols_for_processing], single_row=False)
    if leaders_list is None:
        logger.error(f"DataFrame processing failed for league leaders {stat_category} ({season}).")
        error_msg = Errors.LEAGUE_LEADERS_PROCESSING.format(stat_category=stat_category, season=season)

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if top_n > 0: # top_n is already validated to be positive integer
        leaders_list = leaders_list[:top_n]

    logger.info(f"fetch_league_leaders_logic completed for {stat_category} ({season}), found {len(leaders_list)} leaders.")

    if return_dataframe:
        return format_response({"leaders": leaders_list}), dataframes
    return format_response({"leaders": leaders_list})

===== backend\api_tools\league_lineups.py =====
"""
Handles fetching league lineup statistics with extensive filtering options.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from functools import lru_cache
from typing import Optional, Dict, Union, Tuple

import pandas as pd
from nba_api.stats.endpoints import LeagueDashLineups
from nba_api.stats.library.parameters import (
    GroupQuantity,
    LastNGames,
    MeasureTypeDetailedDefense,
    Month,
    PaceAdjust,
    PerModeDetailed,
    Period,
    PlusMinus,
    Rank,
    SeasonTypeAllStar,
    LeagueIDNullable,
    LocationNullable,
    OutcomeNullable,
    SeasonSegmentNullable,
    ConferenceNullable,
    DivisionNullable,
    GameSegmentNullable,
    ShotClockRangeNullable
)

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_season_format, validate_date_format, validate_team_id
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
LEAGUE_LINEUPS_CSV_DIR = get_cache_dir("league_lineups")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_lineups(
    season: str,
    group_quantity: int,
    measure_type: str,
    per_mode: str,
    season_type: str,
    team_id_nullable: Optional[int] = None
) -> str:
    """
    Generates a file path for saving league lineups DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        group_quantity: Number of players in the lineup
        measure_type: Type of stats (Base, Advanced, Misc, etc.)
        per_mode: Stat mode (Totals, PerGame, Per100Possessions, etc.)
        season_type: Type of season (Regular Season, Playoffs, etc.)
        team_id_nullable: Filter by a specific team ID

    Returns:
        Path to the CSV file
    """
    # Clean measure type and per mode for filename
    clean_measure_type = measure_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()
    clean_season_type = season_type.replace(" ", "_").lower()

    # Add team ID to filename if provided
    team_part = f"_team_{team_id_nullable}" if team_id_nullable else ""

    filename = f"lineups_{season}_{group_quantity}_{clean_measure_type}_{clean_per_mode}_{clean_season_type}{team_part}.csv"
    return get_cache_file_path(filename, "league_lineups")

def fetch_league_dash_lineups_logic(
    season: str,
    group_quantity: int = 5,
    last_n_games: int = LastNGames.default,
    measure_type: str = MeasureTypeDetailedDefense.base,
    month: int = Month.default,
    opponent_team_id: int = 0, # Defaults to 0 (all teams)
    pace_adjust: str = PaceAdjust.no,
    per_mode: str = PerModeDetailed.totals,
    period: int = Period.default,
    plus_minus: str = PlusMinus.no,
    rank: str = Rank.no,
    season_type: str = "Regular Season",
    conference_nullable: Optional[str] = ConferenceNullable.default,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    division_nullable: Optional[str] = DivisionNullable.default,
    game_segment_nullable: Optional[str] = GameSegmentNullable.default,
    league_id_nullable: Optional[str] = LeagueIDNullable.default, # Typically "00" for NBA
    location_nullable: Optional[str] = LocationNullable.default,
    outcome_nullable: Optional[str] = OutcomeNullable.default,
    po_round_nullable: Optional[str] = None, # NBA API docs show no enum, pass as string e.g., "1", "2"
    season_segment_nullable: Optional[str] = SeasonSegmentNullable.default,
    shot_clock_range_nullable: Optional[str] = ShotClockRangeNullable.default,
    team_id_nullable: Optional[int] = None, # Specific team ID, defaults to all
    vs_conference_nullable: Optional[str] = ConferenceNullable.default, # Yes, ConferenceNullable again for VsConference
    vs_division_nullable: Optional[str] = DivisionNullable.default, # Yes, DivisionNullable again for VsDivision
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league-wide lineup statistics with extensive filtering options.
    Provides DataFrame output capabilities.

    Args:
        season: YYYY-YY format (e.g., "2023-24").
        group_quantity: Number of players in the lineup (e.g., 2, 3, 4, 5). Defaults to 5.
        last_n_games: Filter by last N games. Defaults to 0 (all games).
        measure_type: Type of stats (Base, Advanced, Misc, etc.). Defaults to "Base".
        month: Filter by month (1-12). Defaults to 0 (all months).
        opponent_team_id: Filter by opponent team ID. Defaults to 0 (all opponents).
        pace_adjust: Pace adjust stats (Y/N). Defaults to "N".
        per_mode: Stat mode (Totals, PerGame, Per100Possessions, etc.). Defaults to "Totals".
        period: Filter by period (1-4 for quarters, 0 for full game). Defaults to 0.
        plus_minus: Include plus/minus (Y/N). Defaults to "N".
        rank: Include rank (Y/N). Defaults to "N".
        season_type: Type of season (Regular Season, Playoffs, etc.). Defaults to "Regular Season".
        conference_nullable: Filter by conference (East, West).
        date_from_nullable: Start date (YYYY-MM-DD).
        date_to_nullable: End date (YYYY-MM-DD).
        division_nullable: Filter by division.
        game_segment_nullable: Filter by game segment (First Half, Second Half, Overtime).
        league_id_nullable: League ID (e.g., "00" for NBA). Defaults to "00".
        location_nullable: Filter by location (Home, Road).
        outcome_nullable: Filter by outcome (W, L).
        po_round_nullable: Playoff round.
        season_segment_nullable: Filter by season segment (Pre All-Star, Post All-Star).
        shot_clock_range_nullable: Filter by shot clock range.
        team_id_nullable: Filter by a specific team ID.
        vs_conference_nullable: Filter by opponent conference.
        vs_division_nullable: Filter by opponent division.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with lineup stats or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_dash_lineups_logic for Season: {season}, GroupQty: {group_quantity}, Measure: {measure_type}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if team_id_nullable is not None and not validate_team_id(team_id_nullable):
        error_response = format_response(error=Errors.INVALID_TEAM_ID_VALUE.format(team_id=team_id_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if opponent_team_id != 0 and not validate_team_id(opponent_team_id):
        error_response = format_response(error=Errors.INVALID_TEAM_ID_VALUE.format(team_id=opponent_team_id))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    # Input type validation for enums can be added if necessary, but nba_api often handles string versions.

    try:
        lineups_endpoint = LeagueDashLineups(
            season=season,
            group_quantity=group_quantity,
            last_n_games=last_n_games,
            measure_type_detailed_defense=measure_type, # Maps to library's naming
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            per_mode_detailed=per_mode, # Maps to library's naming
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            season_type_all_star=season_type, # Maps to library's naming
            conference_nullable=conference_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            division_simple_nullable=division_nullable,
            game_segment_nullable=game_segment_nullable,
            league_id_nullable=league_id_nullable,
            location_nullable=location_nullable,
            outcome_nullable=outcome_nullable,
            po_round_nullable=po_round_nullable,
            season_segment_nullable=season_segment_nullable,
            shot_clock_range_nullable=shot_clock_range_nullable,
            team_id_nullable=team_id_nullable,
            vs_conference_nullable=vs_conference_nullable,
            vs_division_nullable=vs_division_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"LeagueDashLineups API call successful for Season: {season}")

        lineups_df = lineups_endpoint.lineups.get_data_frame()

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["lineups"] = lineups_df

            # Save to CSV if not empty
            if not lineups_df.empty:
                csv_path = _get_csv_path_for_league_lineups(
                    season=season,
                    group_quantity=group_quantity,
                    measure_type=measure_type,
                    per_mode=per_mode,
                    season_type=season_type,
                    team_id_nullable=team_id_nullable
                )
                _save_dataframe_to_csv(lineups_df, csv_path)

        lineups_list = _process_dataframe(lineups_df, single_row=False)

        if lineups_list is None:
            logger.error("DataFrame processing failed for LeagueDashLineups.")
            error_response = format_response(error=Errors.LEAGUE_DASH_LINEUPS_API_ERROR.format(error="DataFrame processing returned None unexpectedly."))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        response_data = {
            "parameters": {
                "season": season,
                "group_quantity": group_quantity,
                "measure_type": measure_type,
                "per_mode": per_mode,
                "season_type": season_type,
                "team_id": team_id_nullable,
                "opponent_team_id": opponent_team_id,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "lineups": lineups_list
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_league_lineups(
                season=season,
                group_quantity=group_quantity,
                measure_type=measure_type,
                per_mode=per_mode,
                season_type=season_type,
                team_id_nullable=team_id_nullable
            )
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "league_lineups"
            )

            response_data["dataframe_info"] = {
                "message": "League lineups data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "lineups": {
                        "shape": list(lineups_df.shape) if not lineups_df.empty else [],
                        "columns": lineups_df.columns.tolist() if not lineups_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"Successfully fetched {len(lineups_list)} lineup entries for Season: {season}")

        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"Error in fetch_league_dash_lineups_logic for Season {season}: {str(e)}",
            exc_info=True
        )
        error_response = format_response(error=Errors.LEAGUE_DASH_LINEUPS_API_ERROR.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\league_lineup_viz.py =====
"""
Handles fetching and processing league lineup visualization data
from the LeagueLineupViz endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The LeagueLineupViz endpoint provides comprehensive lineup data (1 DataFrame):
- Lineup Info: GROUP_ID, GROUP_NAME, TEAM_ID, TEAM_ABBREVIATION, MIN (5 columns)
- Performance Ratings: OFF_RATING, DEF_RATING, NET_RATING, PACE (4 columns)
- Shooting Efficiency: TS_PCT, FTA_RATE, PCT_FGA_2PT, PCT_FGA_3PT (4 columns)
- Scoring Breakdown: PCT_PTS_2PT_MR, PCT_PTS_FB, PCT_PTS_FT, PCT_PTS_PAINT (4 columns)
- Team Play: TM_AST_PCT, PCT_AST_FGM, PCT_UAST_FGM (3 columns)
- Opponent Stats: OPP_FG3_PCT, OPP_EFG_PCT, OPP_FTA_RATE, OPP_TOV_PCT (4 columns)
- Rich lineup data: 5000+ lineups with detailed performance statistics (24 columns total)
- Perfect for lineup analysis, performance tracking, and strategic insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import leaguelineupviz
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]

    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_LINEUP_VIZ_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_GROUP_QUANTITIES: Set[int] = {2, 3, 4, 5}  # 2-5 player lineups
VALID_MEASURE_TYPES: Set[str] = {MeasureTypeDetailedDefense.base}
VALID_PER_MODES: Set[str] = {PerModeDetailed.totals, PerModeDetailed.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}
VALID_PACE_ADJUST: Set[str] = {"Y", "N"}
VALID_PLUS_MINUS: Set[str] = {"Y", "N"}
VALID_RANK: Set[str] = {"Y", "N"}

# --- Cache Directory Setup ---
LEAGUE_LINEUP_VIZ_CSV_DIR = get_cache_dir("league_lineup_viz")

# Ensure cache directories exist
os.makedirs(LEAGUE_LINEUP_VIZ_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_lineup_viz(
    minutes_min: int = 5,
    group_quantity: int = 5,
    last_n_games: int = 0,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    data_set_name: str = "LineupViz"
) -> str:
    """
    Generates a file path for saving league lineup viz DataFrame.

    Args:
        minutes_min: Minimum minutes played (default: 5)
        group_quantity: Number of players in lineup (default: 5)
        last_n_games: Last N games (default: 0)
        measure_type_detailed_defense: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"league_lineup_viz",
        f"season{season}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"measure{measure_type_detailed_defense.replace(' ', '_')}",
        f"per{per_mode_detailed.replace(' ', '_')}",
        f"min{minutes_min}",
        f"group{group_quantity}",
        f"games{last_n_games}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "league_lineup_viz")

# --- Parameter Validation ---
def _validate_league_lineup_viz_params(
    minutes_min: int,
    group_quantity: int,
    last_n_games: int,
    measure_type_detailed_defense: str,
    per_mode_detailed: str,
    season: str,
    season_type_all_star: str,
    league_id_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_league_lineup_viz_logic."""
    if minutes_min < 0:
        return f"Invalid minutes_min: {minutes_min}. Must be >= 0"
    if group_quantity not in VALID_GROUP_QUANTITIES:
        return f"Invalid group_quantity: {group_quantity}. Valid options: {', '.join(map(str, VALID_GROUP_QUANTITIES))}"
    if last_n_games < 0:
        return f"Invalid last_n_games: {last_n_games}. Must be >= 0"
    if measure_type_detailed_defense not in VALID_MEASURE_TYPES:
        return f"Invalid measure_type_detailed_defense: {measure_type_detailed_defense}. Valid options: {', '.join(VALID_MEASURE_TYPES)}"
    if per_mode_detailed not in VALID_PER_MODES:
        return f"Invalid per_mode_detailed: {per_mode_detailed}. Valid options: {', '.join(VALID_PER_MODES)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if league_id_nullable and league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=LEAGUE_LINEUP_VIZ_CACHE_SIZE)
def fetch_league_lineup_viz_logic(
    minutes_min: int = 5,
    group_quantity: int = 5,
    last_n_games: int = 0,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league lineup visualization data using the LeagueLineupViz endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        minutes_min: Minimum minutes played (default: 5)
        group_quantity: Number of players in lineup (default: 5)
        last_n_games: Last N games (default: 0)
        measure_type_detailed_defense: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with lineup viz data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_league_lineup_viz_logic for Season: {season}, "
        f"Type: {season_type_all_star}, Measure: {measure_type_detailed_defense}, Per: {per_mode_detailed}, "
        f"Min: {minutes_min}, Group: {group_quantity}, Games: {last_n_games}, League: {league_id_nullable}, "
        f"return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_league_lineup_viz_params(
        minutes_min, group_quantity, last_n_games, measure_type_detailed_defense,
        per_mode_detailed, season, season_type_all_star, league_id_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for league lineup viz: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_league_lineup_viz(
        minutes_min, group_quantity, last_n_games, measure_type_detailed_defense,
        per_mode_detailed, season, season_type_all_star, league_id_nullable, "LineupViz"
    )
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 1000:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading league lineup viz from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "minutes_min": minutes_min,
                        "group_quantity": group_quantity,
                        "last_n_games": last_n_games,
                        "measure_type_detailed_defense": measure_type_detailed_defense,
                        "per_mode_detailed": per_mode_detailed,
                        "season": season,
                        "season_type_all_star": season_type_all_star,
                        "league_id_nullable": league_id_nullable
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["LineupViz"] = df
                result_dict["data_sets"]["LineupViz"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters
        api_params = {
            "minutes_min": minutes_min,
            "group_quantity": group_quantity,
            "last_n_games": last_n_games,
            "measure_type_detailed_defense": measure_type_detailed_defense,
            "per_mode_detailed": per_mode_detailed,
            "season": season,
            "season_type_all_star": season_type_all_star
        }

        # Add league_id_nullable only if it's not empty
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable

        logger.debug(f"Calling LeagueLineupViz with parameters: {api_params}")
        lineup_viz_endpoint = leaguelineupviz.LeagueLineupViz(**api_params)

        # Get data frames
        list_of_dataframes = lineup_viz_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "minutes_min": minutes_min,
                "group_quantity": group_quantity,
                "last_n_games": last_n_games,
                "measure_type_detailed_defense": measure_type_detailed_defense,
                "per_mode_detailed": per_mode_detailed,
                "season": season,
                "season_type_all_star": season_type_all_star,
                "league_id_nullable": league_id_nullable
            },
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"LineupViz_{idx}" if idx > 0 else "LineupViz"

            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df

                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()

            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_league_lineup_viz_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_league_lineup_viz(
    minutes_min: int = 5,
    group_quantity: int = 5,
    last_n_games: int = 0,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    per_mode_detailed: str = PerModeDetailed.totals,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    league_id_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get league lineup visualization data.

    Args:
        minutes_min: Minimum minutes played (default: 5)
        group_quantity: Number of players in lineup (default: 5)
        last_n_games: Last N games (default: 0)
        measure_type_detailed_defense: Measure type (default: Base)
        per_mode_detailed: Per mode (default: Totals)
        season: Season in YYYY-YY format (default: current NBA season)
        season_type_all_star: Season type (default: Regular Season)
        league_id_nullable: League ID (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with lineup viz data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_league_lineup_viz_logic(
        minutes_min=minutes_min,
        group_quantity=group_quantity,
        last_n_games=last_n_games,
        measure_type_detailed_defense=measure_type_detailed_defense,
        per_mode_detailed=per_mode_detailed,
        season=season,
        season_type_all_star=season_type_all_star,
        league_id_nullable=league_id_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing LeagueLineupViz endpoint...")

    # Test 1: Basic fetch
    json_response = get_league_lineup_viz()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_league_lineup_viz(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("LeagueLineupViz endpoint test completed.")


===== backend\api_tools\league_opponent_shot_dashboard.py =====
"""
Handles fetching league dashboard data for opponent player/team shots.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from functools import lru_cache
from typing import Optional, Dict, Union, Tuple
import pandas as pd

from nba_api.stats.endpoints import LeagueDashOppPtShot
from nba_api.stats.library.parameters import (
    PerModeSimple,
    SeasonTypeAllStar,
    LeagueIDNullable,
    Month,
    Period,
    ConferenceNullable,
    DivisionNullable,
    GameSegmentNullable,
    LastNGames,
    LocationNullable,
    OutcomeNullable,
    SeasonSegmentNullable,
    ShotClockRangeNullable
)

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import validate_season_format, validate_date_format, validate_team_id
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
OPPONENT_SHOT_CSV_DIR = get_cache_dir("opponent_shot_dashboard")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_opponent_shot_dashboard(
    season: str,
    season_type: str,
    per_mode_simple: str,
    team_id_nullable: Optional[int] = None,
    opponent_team_id_nullable: Optional[int] = None
) -> str:
    """
    Generates a file path for saving opponent shot dashboard DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode_simple: Stat mode (Totals, PerGame, etc.)
        team_id_nullable: Filter by a specific team's ID
        opponent_team_id_nullable: Filter by opponent team ID

    Returns:
        Path to the CSV file
    """
    # Clean season type and per mode for filename
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode_simple.replace(" ", "_").lower()

    # Add team IDs to filename if provided
    team_part = f"_team_{team_id_nullable}" if team_id_nullable else ""
    opponent_part = f"_opp_{opponent_team_id_nullable}" if opponent_team_id_nullable and opponent_team_id_nullable != 0 else ""

    filename = f"opp_shot_{season}_{clean_season_type}_{clean_per_mode}{team_part}{opponent_part}.csv"
    return get_cache_file_path(filename, "opponent_shot_dashboard")

def fetch_league_dash_opponent_pt_shot_logic(
    season: str,
    season_type: str = SeasonTypeAllStar.regular,
    league_id_nullable: Optional[str] = LeagueIDNullable.default,
    per_mode_simple: str = PerModeSimple.totals,
    month_nullable: Optional[int] = 0,
    period_nullable: Optional[int] = 0,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    opponent_team_id_nullable: Optional[int] = 0,
    vs_conference_nullable: Optional[str] = ConferenceNullable.default,
    vs_division_nullable: Optional[str] = DivisionNullable.default,
    team_id_nullable: Optional[int] = None,
    conference_nullable: Optional[str] = ConferenceNullable.default,
    division_nullable: Optional[str] = DivisionNullable.default,
    game_segment_nullable: Optional[str] = GameSegmentNullable.default,
    last_n_games_nullable: Optional[int] = 0,
    location_nullable: Optional[str] = LocationNullable.default,
    outcome_nullable: Optional[str] = OutcomeNullable.default,
    po_round_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = SeasonSegmentNullable.default,
    shot_clock_range_nullable: Optional[str] = ShotClockRangeNullable.default,
    close_def_dist_range_nullable: Optional[str] = None,
    shot_dist_range_nullable: Optional[str] = None,
    dribble_range_nullable: Optional[str] = None,
    general_range_nullable: Optional[str] = None,
    touch_time_range_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches league dashboard data for opponent player/team shots.
    This endpoint provides statistics about shots taken by opponents.
    Provides DataFrame output capabilities.

    Args:
        season: YYYY-YY format (e.g., "2023-24").
        season_type: Type of season (Regular Season, Playoffs, etc.). Defaults to Regular Season.
        league_id_nullable: League ID (e.g., "00" for NBA). Defaults to "00".
        per_mode_simple: Stat mode (Totals, PerGame, etc.). Defaults to "Totals".
        month_nullable: Filter by month (1-12). Defaults to 0 (all months).
        period_nullable: Filter by period (1-4 for quarters, 0 for full game). Defaults to 0.
        date_from_nullable: Start date (YYYY-MM-DD).
        date_to_nullable: End date (YYYY-MM-DD).
        opponent_team_id_nullable: Filter by opponent team ID. Defaults to 0 (all opponents).
        vs_conference_nullable: Filter by opponent conference.
        vs_division_nullable: Filter by opponent division.
        team_id_nullable: Filter by a specific team's ID to see shots against them.
        conference_nullable: Filter by team's conference.
        division_nullable: Filter by team's division.
        game_segment_nullable: Filter by game segment.
        last_n_games_nullable: Filter by last N games. Defaults to 0 (all games).
        location_nullable: Filter by location (Home, Road).
        outcome_nullable: Filter by outcome (W, L).
        po_round_nullable: Playoff round.
        season_segment_nullable: Filter by season segment.
        shot_clock_range_nullable: Filter by shot clock range.
        close_def_dist_range_nullable: Filter by closest defender distance.
        shot_dist_range_nullable: Filter by shot distance.
        dribble_range_nullable: Filter by dribble range.
        general_range_nullable: Filter by general range.
        touch_time_range_nullable: Filter by touch time range.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with opponent shot stats or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_dash_opponent_pt_shot_logic for Season: {season}, PerMode: {per_mode_simple}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if team_id_nullable is not None and not validate_team_id(team_id_nullable):
        error_response = format_response(error=Errors.INVALID_TEAM_ID_VALUE.format(team_id=team_id_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if opponent_team_id_nullable is not None and opponent_team_id_nullable != 0 and not validate_team_id(opponent_team_id_nullable):
        error_response = format_response(error=Errors.INVALID_TEAM_ID_VALUE.format(team_id=opponent_team_id_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        endpoint = LeagueDashOppPtShot(
            season=season,
            season_type_all_star=season_type,
            league_id=league_id_nullable,
            per_mode_simple=per_mode_simple,
            month_nullable=month_nullable,
            period_nullable=period_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            opponent_team_id_nullable=opponent_team_id_nullable,
            vs_conference_nullable=vs_conference_nullable,
            vs_division_nullable=vs_division_nullable,
            team_id_nullable=team_id_nullable,
            conference_nullable=conference_nullable,
            division_nullable=division_nullable,
            game_segment_nullable=game_segment_nullable,
            last_n_games_nullable=last_n_games_nullable,
            location_nullable=location_nullable,
            outcome_nullable=outcome_nullable,
            po_round_nullable=po_round_nullable,
            season_segment_nullable=season_segment_nullable,
            shot_clock_range_nullable=shot_clock_range_nullable,
            close_def_dist_range_nullable=close_def_dist_range_nullable,
            shot_dist_range_nullable=shot_dist_range_nullable,
            dribble_range_nullable=dribble_range_nullable,
            general_range_nullable=general_range_nullable,
            touch_time_range_nullable=touch_time_range_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"LeagueDashOppPtShot API call successful for Season: {season}")

        # This endpoint often has one primary dataset, typically named 'league_dash_opp_pt_shot' or similar.
        # We need to confirm the actual name from nba_api if possible, or inspect a raw response.
        # Assuming 'league_dash_opp_pt_shot' as a placeholder.
        # If the endpoint has multiple result sets, they would be endpoint.result_set_name.get_data_frame()

        # Attempt to get the primary dataframe. The actual name might differ.
        # Common names for such endpoints are often the endpoint name itself in snake_case or a generic name.
        # For LeagueDashOppPtShot, a likely name is 'league_dash_opp_pt_shot' or 'OpponentPlayerShooting'.
        # If unsure, one would typically inspect the endpoint object or its available attributes.
        # For now, let's assume it's 'league_dash_opp_pt_shot', the endpoint class name in snake_case.
        if hasattr(endpoint, 'league_dash_opp_pt_shot'):
            data_df = endpoint.league_dash_opp_pt_shot.get_data_frame()
        else: # Fallback if the assumed name is wrong, try to find the primary dataset
            # This is a simplistic way; more robust would be to check endpoint.get_result_sets()
            # and find the one most likely to be the main data.
            # However, direct inspection of nba_api source or a live call is best.
            # Let's assume it's the first available result set if the specific name isn't found.
            result_sets = endpoint.get_available_data()
            if result_sets:
                 data_df = endpoint.data_sets[0].get_data_frame() # endpoint.data_sets[0] is a common pattern
            else:
                logger.warning(f"No data found for LeagueDashOppPtShot with params: {season}, {per_mode_simple}")
                empty_response = format_response({"parameters": {"season": season, "per_mode_simple": per_mode_simple}, "opponent_shots": []})
                if return_dataframe:
                    return empty_response, dataframes
                return empty_response

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["opponent_shots"] = data_df

            # Save to CSV if not empty
            if not data_df.empty:
                csv_path = _get_csv_path_for_opponent_shot_dashboard(
                    season=season,
                    season_type=season_type,
                    per_mode_simple=per_mode_simple,
                    team_id_nullable=team_id_nullable,
                    opponent_team_id_nullable=opponent_team_id_nullable
                )
                _save_dataframe_to_csv(data_df, csv_path)

        data_list = _process_dataframe(data_df, single_row=False)

        if data_list is None:
            logger.error("DataFrame processing failed for LeagueDashOppPtShot.")
            error_response = format_response(error=Errors.LEAGUE_DASH_OPP_PT_SHOT_API_ERROR.format(error="DataFrame processing returned None unexpectedly."))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        response_data = {
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode_simple": per_mode_simple,
                "league_id_nullable": league_id_nullable,
                "month_nullable": month_nullable,
                "period_nullable": period_nullable,
                "date_from_nullable": date_from_nullable,
                "date_to_nullable": date_to_nullable,
                "opponent_team_id_nullable": opponent_team_id_nullable,
                "team_id_nullable": team_id_nullable
            },
            "opponent_shots": data_list
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_opponent_shot_dashboard(
                season=season,
                season_type=season_type,
                per_mode_simple=per_mode_simple,
                team_id_nullable=team_id_nullable,
                opponent_team_id_nullable=opponent_team_id_nullable
            )
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "opponent_shot_dashboard"
            )

            response_data["dataframe_info"] = {
                "message": "Opponent shot dashboard data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "opponent_shots": {
                        "shape": list(data_df.shape) if not data_df.empty else [],
                        "columns": data_df.columns.tolist() if not data_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"Successfully fetched {len(data_list)} opponent shot entries for Season: {season}")

        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"Error in fetch_league_dash_opponent_pt_shot_logic for Season {season}, Mode {per_mode_simple}: {str(e)}",
            exc_info=True
        )
        error_response = format_response(error=Errors.LEAGUE_DASH_OPP_PT_SHOT_API_ERROR.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\league_player_on_details.py =====
"""
Handles fetching NBA league player on details data using the leagueplayerondetails endpoint.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from typing import Optional, Dict, Union, Tuple
import pandas as pd
from functools import lru_cache

from nba_api.stats.endpoints import leagueplayerondetails
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar,
    PerModeDetailed,
    MeasureTypeDetailedDefense, # Adjusted based on docs, likely MeasureTypeDetailed or similar
    LeagueID,
    # TeamID, # Not explicitly in nba_api.stats.library.parameters but a common type; team_id is passed as int
    GameSegmentNullable,
    LocationNullable,
    MonthNullable, # Not an enum, but a number
    OutcomeNullable,
    SeasonSegmentNullable,
    ConferenceNullable,
    DivisionNullable
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    # find_player_id_or_error, # Not directly needed as this is league/team focused
)
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_ON_DETAILS_CSV_DIR = get_cache_dir("player_on_details")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_on_details(
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str,
    team_id: int
) -> str:
    """
    Generates a file path for saving player on details DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        measure_type: The measure type (e.g., 'Base', 'Advanced')
        per_mode: The per mode (e.g., 'Totals', 'PerGame')
        team_id: The team ID

    Returns:
        Path to the CSV file
    """
    # Clean season type, measure type, and per mode for filename
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_measure_type = measure_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"player_on_details_{season}_{clean_season_type}_{clean_measure_type}_{clean_per_mode}_team_{team_id}.csv"
    return get_cache_file_path(filename, "player_on_details")

# Module-level constants for validation sets
# These need to be carefully defined based on nba_api.stats.library.parameters
# and the leagueplayerondetails.md documentation.
_VALID_LPOD_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_LPOD_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
# leagueplayerondetails.md shows MeasureType pattern: ^(Base)\|(Advanced)\|(Misc)\|(Four Factors)\|(Scoring)\|(Opponent)\|(Usage)\|(Defense)$
# This aligns well with MeasureTypeDetailed or MeasureTypeDetailedDefense
_VALID_LPOD_MEASURE_TYPES = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_VALID_LPOD_PACE_ADJUST = {"Y", "N"}
_VALID_LPOD_PLUS_MINUS = {"Y", "N"}
_VALID_LPOD_RANK = {"Y", "N"}
_VALID_LPOD_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}
_VALID_LPOD_GAME_SEGMENTS = {val for val in GameSegmentNullable.__dict__.values() if isinstance(val, str) and val} | {None}
_VALID_LPOD_LOCATIONS = {val for val in LocationNullable.__dict__.values() if isinstance(val, str) and val} | {None}
_VALID_LPOD_OUTCOMES = {val for val in OutcomeNullable.__dict__.values() if isinstance(val, str) and val} | {None}
_VALID_LPOD_SEASON_SEGMENTS = {val for val in SeasonSegmentNullable.__dict__.values() if isinstance(val, str) and val} | {None}
_VALID_LPOD_VS_CONFERENCES = {val for val in ConferenceNullable.__dict__.values() if isinstance(val, str) and val} | {None}
_VALID_LPOD_VS_DIVISIONS = {val for val in DivisionNullable.__dict__.values() if isinstance(val, str) and val} | {None}


def fetch_league_player_on_details_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    measure_type: str = MeasureTypeDetailedDefense.base,
    per_mode: str = PerModeDetailed.totals,
    team_id: int = 0, # TeamID is required by the endpoint, 0 often means all/league
    last_n_games: int = 0,
    month: int = 0, # 0 for all months
    opponent_team_id: int = 0, # 0 for all opponents
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    period: int = 0, # 0 for all periods
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = LeagueID.nba, # Default to NBA
    game_segment_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA league player on details data using the leagueplayerondetails endpoint.
    Provides DataFrame output capabilities.

    Args:
        season: Season in YYYY-YY format (e.g., "2023-24").
        season_type: Type of season (Regular Season, Playoffs, etc.).
        measure_type: Type of statistical measure (Base, Advanced, etc.).
        per_mode: Statistical mode (Totals, PerGame, etc.).
        team_id: Team ID to filter by.
        last_n_games: Filter by last N games.
        month: Filter by month (0 for all months).
        opponent_team_id: Filter by opponent team ID (0 for all opponents).
        pace_adjust: Whether to pace-adjust stats ("Y" or "N").
        plus_minus: Whether to include plus-minus stats ("Y" or "N").
        rank: Whether to include rank stats ("Y" or "N").
        period: Filter by period (0 for all periods).
        vs_division_nullable: Filter by opponent division.
        vs_conference_nullable: Filter by opponent conference.
        season_segment_nullable: Filter by season segment.
        outcome_nullable: Filter by outcome (W, L).
        location_nullable: Filter by location (Home, Road).
        league_id_nullable: League ID.
        game_segment_nullable: Filter by game segment.
        date_to_nullable: End date (YYYY-MM-DD).
        date_from_nullable: Start date (YYYY-MM-DD).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with league player on details data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_player_on_details_logic for Season: {season}, TeamID: {team_id}, Measure: {measure_type}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    # Parameter Validations
    if season_type not in _VALID_LPOD_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_LPOD_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _VALID_LPOD_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_LPOD_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if measure_type not in _VALID_LPOD_MEASURE_TYPES:
        error_response = format_response(error=Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_LPOD_MEASURE_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if pace_adjust.upper() not in _VALID_LPOD_PACE_ADJUST:
        error_response = format_response(error=Errors.INVALID_PACE_ADJUST.format(value=pace_adjust))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if plus_minus.upper() not in _VALID_LPOD_PLUS_MINUS:
        error_response = format_response(error=Errors.INVALID_PLUS_MINUS.format(value=plus_minus))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if rank.upper() not in _VALID_LPOD_RANK:
        error_response = format_response(error=Errors.INVALID_RANK.format(value=rank))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if league_id_nullable and league_id_nullable not in _VALID_LPOD_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_LPOD_LEAGUE_IDS)[:3])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if team_id == 0: # As per docs, TeamID is required. If 0 is passed, it might imply league-wide, but API might reject.
        logger.warning("TeamID is 0, which might be interpreted as league-wide or could be an error by the API if a specific team is expected.")
        # Consider if an error should be returned if team_id is 0, based on API behavior.
        # For now, proceeding as the example URL uses TeamID=1610612739

    # Nullable enum validations
    if game_segment_nullable and game_segment_nullable not in _VALID_LPOD_GAME_SEGMENTS:
        error_response = format_response(error=Errors.INVALID_GAME_SEGMENT.format(value=game_segment_nullable, options=", ".join([s for s in _VALID_LPOD_GAME_SEGMENTS if s][:3])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if location_nullable and location_nullable not in _VALID_LPOD_LOCATIONS:
        error_response = format_response(error=Errors.INVALID_LOCATION.format(value=location_nullable, options=", ".join([s for s in _VALID_LPOD_LOCATIONS if s][:2])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if outcome_nullable and outcome_nullable not in _VALID_LPOD_OUTCOMES:
        error_response = format_response(error=Errors.INVALID_OUTCOME.format(value=outcome_nullable, options=", ".join([s for s in _VALID_LPOD_OUTCOMES if s][:2])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_segment_nullable and season_segment_nullable not in _VALID_LPOD_SEASON_SEGMENTS:
        error_response = format_response(error=Errors.INVALID_SEASON_SEGMENT.format(value=season_segment_nullable, options=", ".join([s for s in _VALID_LPOD_SEASON_SEGMENTS if s][:2])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if vs_conference_nullable and vs_conference_nullable not in _VALID_LPOD_VS_CONFERENCES:
        error_response = format_response(error=Errors.INVALID_CONFERENCE.format(value=vs_conference_nullable, options=", ".join([s for s in _VALID_LPOD_VS_CONFERENCES if s][:2])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if vs_division_nullable and vs_division_nullable not in _VALID_LPOD_VS_DIVISIONS:
        error_response = format_response(error=Errors.INVALID_DIVISION.format(value=vs_division_nullable, options=", ".join([s for s in _VALID_LPOD_VS_DIVISIONS if s][:3])))
        if return_dataframe:
            return error_response, dataframes
        return error_response


    try:
        logger.debug(f"Fetching leagueplayerondetails for TeamID: {team_id}, Season: {season}, Measure: {measure_type}")

        lpod_endpoint = leagueplayerondetails.LeaguePlayerOnDetails(
            season=season,
            season_type_all_star=season_type, # Mapped from season_type
            measure_type_detailed_defense=measure_type, # Mapped from measure_type
            per_mode_detailed=per_mode, # Mapped from per_mode
            team_id=team_id,
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust.upper(),
            plus_minus=plus_minus.upper(),
            rank=rank.upper(),
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            league_id_nullable=league_id_nullable,
            game_segment_nullable=game_segment_nullable,
            date_to_nullable=date_to_nullable,
            date_from_nullable=date_from_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"leagueplayerondetails API call successful for TeamID: {team_id}, Season: {season}")

        # Main dataset from docs: PlayersOnCourtLeaguePlayerDetails
        details_df = lpod_endpoint.players_on_court_league_player_details.get_data_frame()

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["league_player_on_details"] = details_df

            # Save to CSV if not empty
            if not details_df.empty:
                csv_path = _get_csv_path_for_player_on_details(
                    season=season,
                    season_type=season_type,
                    measure_type=measure_type,
                    per_mode=per_mode,
                    team_id=team_id
                )
                _save_dataframe_to_csv(details_df, csv_path)

        details_list = _process_dataframe(details_df, single_row=False)

        if details_list is None: # Check for processing errors
            logger.error(f"DataFrame processing failed for league player on details (TeamID: {team_id}, Season: {season}).")
            error_msg = Errors.LEAGUE_PLAYER_ON_DETAILS_PROCESSING.format(team_id=team_id, season=season)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if details_df.empty: # Genuinely no data
            logger.warning(f"No league player on details data found for TeamID: {team_id}, Season: {season} with specified filters.")
            # Return empty list for the data key as per pattern
            data_payload = []
        else:
            data_payload = details_list

        result = {
            "parameters": {
                "season": season, "season_type": season_type, "measure_type": measure_type, "per_mode": per_mode,
                "team_id": team_id, "last_n_games": last_n_games, "month": month, "opponent_team_id": opponent_team_id,
                "pace_adjust": pace_adjust, "plus_minus": plus_minus, "rank": rank, "period": period,
                "vs_division_nullable": vs_division_nullable, "vs_conference_nullable": vs_conference_nullable,
                "season_segment_nullable": season_segment_nullable, "outcome_nullable": outcome_nullable,
                "location_nullable": location_nullable, "league_id_nullable": league_id_nullable,
                "game_segment_nullable": game_segment_nullable, "date_to_nullable": date_to_nullable,
                "date_from_nullable": date_from_nullable
            },
            "league_player_on_details": data_payload
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_player_on_details(
                season=season,
                season_type=season_type,
                measure_type=measure_type,
                per_mode=per_mode,
                team_id=team_id
            )
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "player_on_details"
            )

            result["dataframe_info"] = {
                "message": "League player on details data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "league_player_on_details": {
                        "shape": list(details_df.shape) if not details_df.empty else [],
                        "columns": details_df.columns.tolist() if not details_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"Successfully fetched league player on details for TeamID: {team_id}, Season: {season}")

        if return_dataframe:
            return format_response(data=result), dataframes
        return format_response(data=result)

    except ValueError as e:
        logger.warning(f"ValueError in fetch_league_player_on_details_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, dataframes
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_league_player_on_details_logic for TeamID {team_id}, Season {season}: {e}", exc_info=True)
        error_msg = Errors.LEAGUE_PLAYER_ON_DETAILS_UNEXPECTED.format(team_id=team_id, season=season, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\league_standings.py =====
"""
Handles fetching and processing NBA league standings data.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Dict, Any, List, Set, Union, Tuple
import pandas as pd
# import numpy as np # Not strictly needed if _process_dataframe handles numpy types
from functools import lru_cache
from datetime import datetime, date

from nba_api.stats.endpoints import leaguestandingsv3
from nba_api.stats.library.parameters import SeasonTypeAllStar, LeagueID
from .utils import format_response, _process_dataframe # Import _process_dataframe
from ..utils.validation import _validate_season_format
from ..config import settings
from ..core.errors import Errors

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
LEAGUE_STANDINGS_RAW_CACHE_SIZE = 32
LEAGUE_STANDINGS_PROCESSED_CACHE_SIZE = 64
DEFAULT_PLAYOFF_RANK_SORT = 99

_VALID_LEAGUE_STANDINGS_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.preseason}
_VALID_LEAGUE_IDS_FOR_STANDINGS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
LEAGUE_STANDINGS_CSV_DIR = os.path.join(CSV_CACHE_DIR, "league_standings")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(LEAGUE_STANDINGS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_league_standings(
    season: str,
    season_type: str,
    league_id: str
) -> str:
    """
    Generates a file path for saving league standings DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Preseason')
        league_id: The league ID (e.g., '00' for NBA)

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"standings_{season}_{clean_season_type}_{league_id}.csv"
    return os.path.join(LEAGUE_STANDINGS_CSV_DIR, filename)

# Columns to extract and their order for the final response
_STANDINGS_COLUMNS_TO_KEEP = [
    'TeamID', 'TeamCity', 'TeamName', 'TeamSlug',
    'Conference', 'ConferenceRecord', 'PlayoffRank',
    'ClinchIndicator',
    'Division', 'DivisionRecord', 'DivisionRank',
    'WINS', 'LOSSES', 'WinPCT', 'LeagueRank', 'Record',
    'HOME', 'ROAD', 'L10',
    'strCurrentStreak', 'ConferenceGamesBack', 'DivisionGamesBack',
    'PointsPG', 'OppPointsPG', 'DiffPointsPG',
    'ClinchedConferenceTitle', 'ClinchedDivisionTitle',
    'ClinchedPlayoffBirth', 'ClinchedPlayIn'
]

# --- Helper Functions ---
@lru_cache(maxsize=LEAGUE_STANDINGS_RAW_CACHE_SIZE)
def get_cached_standings(season: str, season_type: str, league_id: str, timestamp: str) -> pd.DataFrame:
    """
    Cached wrapper for nba_api's LeagueStandingsV3 endpoint.

    Args:
        season: NBA season in 'YYYY-YY' format.
        season_type: Season type (e.g., 'Regular Season').
        league_id: League ID (e.g., '00' for NBA).
        timestamp: ISO-formatted timestamp for cache invalidation.

    Returns:
        pd.DataFrame: DataFrame with league standings.

    Raises:
        Exception: If the API call fails.
    """
    logger.info(f"Cache miss/expiry for standings - fetching for {season}, {season_type}, {league_id} (ts: {timestamp})")
    try:
        standings_endpoint = leaguestandingsv3.LeagueStandingsV3(
            season=season, season_type=season_type, league_id=league_id,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        return standings_endpoint.standings.get_data_frame()
    except Exception as e:
        logger.error(f"API call failed in get_cached_standings for {season}, {season_type}, {league_id}: {e}", exc_info=True)
        raise

def _enrich_standings_records(standings_records: List[Dict[str, Any]]) -> None:
    """
    Enriches processed standings records with 'GB', 'WinPct', and 'STRK' fields.
    Modifies the list in place.
    """
    for team_data in standings_records:
        gb_value = team_data.get('ConferenceGamesBack')
        if isinstance(gb_value, (int, float)):
            team_data['GB'] = gb_value
        elif isinstance(gb_value, str) and gb_value.strip() == '-':
            team_data['GB'] = 0.0
        elif gb_value is None:
            team_data['GB'] = None # Explicitly set to None
        else:
            try:
                team_data['GB'] = float(gb_value)
            except (ValueError, TypeError):
                logger.warning(f"Could not convert ConferenceGamesBack '{gb_value}' to float for team {team_data.get('TeamID')}. Setting GB to None.")
                team_data['GB'] = None

        try:
            win_pct_val = team_data.get('WinPCT') # Original column name
            team_data['WinPct'] = float(win_pct_val) if win_pct_val is not None else 0.0
        except (ValueError, TypeError):
            logger.warning(f"Could not convert WinPCT '{team_data.get('WinPCT')}' to float for team {team_data.get('TeamID')}. Defaulting to 0.0.")
            team_data['WinPct'] = 0.0

        team_data['STRK'] = team_data.get('strCurrentStreak', '')


# --- Main Logic Function ---
def fetch_league_standings_logic(
    season: Optional[str] = None,
    season_type: str = SeasonTypeAllStar.regular,
    league_id: str = LeagueID.nba,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches and processes league standings for a specific season, type, and league.
    Provides DataFrame output capabilities.

    Args:
        season: NBA season in 'YYYY-YY' format. Defaults to current.
        season_type: Season type. Defaults to 'Regular Season'.
        league_id: League ID. Defaults to NBA.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string of team standings or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    effective_season = season or settings.CURRENT_NBA_SEASON
    logger.info(f"Executing fetch_league_standings_logic for season: {effective_season}, type: {season_type}, league: {league_id}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not _validate_season_format(effective_season): # Assuming league_id is not needed for basic season format validation here
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=effective_season)), dataframes
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=effective_season))

    if season_type not in _VALID_LEAGUE_STANDINGS_SEASON_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_LEAGUE_STANDINGS_SEASON_TYPES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_LEAGUE_STANDINGS_SEASON_TYPES)[:5])))

    if league_id not in _VALID_LEAGUE_IDS_FOR_STANDINGS:
        if return_dataframe:
            return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS_FOR_STANDINGS)[:3]))), dataframes
        return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS_FOR_STANDINGS)[:3])))

    try:
        current_hour_timestamp = datetime.now().replace(minute=0, second=0, microsecond=0).isoformat()
        standings_df = get_cached_standings(effective_season, season_type, league_id, current_hour_timestamp)

        if standings_df.empty:
            logger.warning(f"No standings data found for {effective_season}, {season_type}, {league_id} from API.")

            if return_dataframe:
                return format_response({"standings": [], "league_id_processed": league_id}), dataframes
            return format_response({"standings": [], "league_id_processed": league_id})

        cols_to_extract = [col for col in _STANDINGS_COLUMNS_TO_KEEP if col in standings_df.columns]
        if not cols_to_extract:
            logger.error(f"None of the expected columns for standings processing found for {effective_season}, {season_type}, {league_id}.")

            if return_dataframe:
                return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type)), dataframes
            return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type))

        # Use the utility _process_dataframe on the sliced DataFrame
        sliced_df = standings_df[cols_to_extract]

        # Save DataFrame to CSV if requested
        if return_dataframe:
            dataframes["standings"] = sliced_df

            # Save to CSV if not empty
            if not sliced_df.empty:
                csv_path = _get_csv_path_for_league_standings(
                    effective_season, season_type, league_id
                )
                _save_dataframe_to_csv(sliced_df, csv_path)

        processed_standings = _process_dataframe(sliced_df, single_row=False)

        if processed_standings is None: # _process_dataframe returned None due to an internal error
            logger.error(f"DataFrame processing with _process_dataframe failed for standings ({effective_season}, Type: {season_type}, League: {league_id}).")

            if return_dataframe:
                return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type)), dataframes
            return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type))

        if not processed_standings and not sliced_df.empty: # Should not happen if _process_dataframe is robust
             logger.error(f"_process_dataframe resulted in empty list for non-empty sliced_df for standings ({effective_season}, Type: {season_type}, League: {league_id}).")

             if return_dataframe:
                 return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type)), dataframes
             return format_response(error=Errors.LEAGUE_STANDINGS_PROCESSING.format(season=effective_season, season_type=season_type))

        _enrich_standings_records(processed_standings)

        # Sort by Conference, then by PlayoffRank
        processed_standings.sort(key=lambda x: (x.get("Conference", ""), x.get("PlayoffRank", DEFAULT_PLAYOFF_RANK_SORT)))

        logger.info(f"Successfully fetched and processed standings for {effective_season}, {season_type}, {league_id}.")

        if return_dataframe:
            return format_response({"standings": processed_standings, "league_id_processed": league_id}), dataframes
        return format_response({"standings": processed_standings, "league_id_processed": league_id})

    except Exception as e: # Catches API errors from get_cached_standings or other unexpected errors
        logger.error(f"Unexpected error in fetch_league_standings_logic for {effective_season}, {season_type}, {league_id}: {str(e)}", exc_info=True)
        # Determine if it was an API error from get_cached_standings or a processing error
        # For simplicity, using a general error message here. More specific handling could be added.
        error_msg = Errors.LEAGUE_STANDINGS_UNEXPECTED.format(season=effective_season, season_type=season_type, error=str(e))

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\live_game_tools.py =====
"""
Handles fetching and formatting live NBA scoreboard data.
Uses nba_api.live.nba.endpoints.scoreboard.
Provides both JSON and DataFrame outputs with CSV caching.
"""
from typing import Dict, List, Optional, TypedDict, Any, Union, Tuple
from nba_api.live.nba.endpoints import scoreboard
from datetime import datetime
import logging
import os
import pandas as pd
from functools import lru_cache

from ..config import settings
from ..core.errors import Errors
from .utils import format_response
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
LIVE_SCOREBOARD_CSV_DIR = get_cache_dir("live_scoreboard")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_live_scoreboard(date_str: str) -> str:
    """
    Generates a file path for saving live scoreboard DataFrame as CSV.

    Args:
        date_str: The date string in YYYY-MM-DD format

    Returns:
        Path to the CSV file
    """
    filename = f"live_scoreboard_{date_str}.csv"
    return get_cache_file_path(filename, "live_scoreboard")

# --- Module-Level Constants ---
CACHE_TTL_SECONDS = 10  # Time-to-live for cached raw scoreboard data
SCOREBOARD_RAW_CACHE_SIZE = 2 # Max number of raw scoreboard responses to cache

GAME_STATUS_SCHEDULED = 1
GAME_STATUS_IN_PROGRESS = 2
GAME_STATUS_FINAL = 3
DEFAULT_TRICODE = "N/A"

# --- TypedDicts for Data Structure Documentation ---
class GameLeader(TypedDict): # Kept for potential future use, though not currently populated
    name: str
    stats: str

class TeamInfo(TypedDict):
    id: Optional[int]
    code: str
    name: Optional[str] # Name might not always be present in raw data
    score: int
    record: Optional[str]
    wins: Optional[int]
    losses: Optional[int]

class GameStatusInfo(TypedDict): # Renamed from GameStatus to avoid conflict
    clock: Optional[str]
    period: int
    state_code: int # e.g., 1, 2, 3
    state_text: str # e.g., "Halftime", "Q1 0:00", "Final"

class FormattedGameInfo(TypedDict): # Renamed from GameInfo
    game_id: Optional[str]
    start_time_utc: str
    status: GameStatusInfo
    home_team: TeamInfo
    away_team: TeamInfo

class FormattedScoreboardResponse(TypedDict): # Renamed from ScoreboardResponse
    # meta: Dict[str, Any] # Meta not currently included in formatted response
    date: str
    games: List[FormattedGameInfo]

# --- Caching Function for Raw Data ---
@lru_cache(maxsize=SCOREBOARD_RAW_CACHE_SIZE)
def get_cached_scoreboard_data(
    cache_key: str,
    timestamp_bucket: str # Timestamp bucket for more frequent invalidation
) -> Dict[str, Any]:
    """
    Cached wrapper for fetching raw live scoreboard data.
    Uses a timestamp bucket for time-based cache invalidation.

    Args:
        cache_key: A simple string key for the cache (e.g., "live_scoreboard").
        timestamp_bucket: A string derived from the current time, bucketed by CACHE_TTL_SECONDS.

    Returns:
        Dict[str, Any]: The raw dictionary result from the API call.

    Raises:
        Exception: If the ScoreBoard API call fails.
    """
    logger.info(f"Cache miss or expiry for live scoreboard (key: {cache_key}, ts_bucket: {timestamp_bucket}) - fetching new data.")
    try:
        board = scoreboard.ScoreBoard(timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        return board.get_dict()
    except Exception as e:
        logger.error(f"ScoreBoard API call failed: {e}", exc_info=True)
        raise # Re-raise to be handled by the main logic function

# --- Helper for Formatting a Single Game ---
def _format_live_game_details(raw_game_data: Dict[str, Any]) -> FormattedGameInfo:
    """Formats a single raw game dictionary into the desired structure."""
    home_team_raw = raw_game_data.get("homeTeam", {})
    away_team_raw = raw_game_data.get("awayTeam", {})

    game_status_text_raw = raw_game_data.get("gameStatusText", "Status Unknown")
    game_status_code = raw_game_data.get("gameStatus", 0) # 0: Unknown, 1: Scheduled, 2: In Progress, 3: Final

    game_clock_val: Optional[str] = None
    current_period_val = 0
    status_text_final = game_status_text_raw

    if game_status_code == GAME_STATUS_IN_PROGRESS:
        game_clock_val = raw_game_data.get("gameClock")
        current_period_val = raw_game_data.get("period", 0)
        if game_clock_val: # If clock is present, format it with period
            status_text_final = f"Q{current_period_val} {game_clock_val}"
        elif status_text_final == "Halftime":
            pass # Keep "Halftime" as is
        else: # Default live status if clock is missing but game is in progress
            status_text_final = f"Q{current_period_val} In Progress"
    elif game_status_code == GAME_STATUS_SCHEDULED:
         # For scheduled games, gameStatusText might be the start time (e.g., "7:00 PM ET")
         # We already have gameTimeUTC, so can simplify or use that.
         status_text_final = "Scheduled" # Or use game_status_text_raw if more descriptive
    elif game_status_code == GAME_STATUS_FINAL:
        status_text_final = "Final"

    return {
        "game_id": raw_game_data.get("gameId"),
        "start_time_utc": raw_game_data.get("gameTimeUTC", ""),
        "status": {
            "clock": game_clock_val,
            "period": current_period_val,
            "state_code": game_status_code,
            "state_text": status_text_final
        },
        "home_team": {
            "id": home_team_raw.get("teamId"),
            "code": home_team_raw.get("teamTricode", DEFAULT_TRICODE),
            "name": home_team_raw.get("teamName"),
            "score": home_team_raw.get("score", 0),
            "record": f"{home_team_raw.get('wins', 0)}-{home_team_raw.get('losses', 0)}" if home_team_raw.get('wins') is not None else None,
            "wins": home_team_raw.get("wins"),
            "losses": home_team_raw.get("losses")
        },
        "away_team": {
            "id": away_team_raw.get("teamId"),
            "code": away_team_raw.get("teamTricode", DEFAULT_TRICODE),
            "name": away_team_raw.get("teamName"),
            "score": away_team_raw.get("score", 0),
            "record": f"{away_team_raw.get('wins', 0)}-{away_team_raw.get('losses', 0)}" if away_team_raw.get('wins') is not None else None,
            "wins": away_team_raw.get("wins"),
            "losses": away_team_raw.get("losses")
        }
    }

# --- Main Logic Function ---
def fetch_league_scoreboard_logic(
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches and formats live scoreboard data for current NBA games.
    Provides DataFrame output capabilities.

    Args:
        bypass_cache: If True, ignores cached data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string of current game information or an error message.
                 See FormattedScoreboardResponse TypedDict for structure.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_league_scoreboard_logic (bypass_cache: {bypass_cache}, return_dataframe: {return_dataframe})")

    # Store DataFrames if requested
    dataframes = {}

    cache_key = "live_scoreboard_data" # More descriptive key
    # Timestamp bucket for cache invalidation, based on CACHE_TTL_SECONDS
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS))

    try:
        if bypass_cache:
            logger.info("Bypassing cache for live scoreboard.")
            board = scoreboard.ScoreBoard(timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            raw_data = board.get_dict()
        else:
            raw_data = get_cached_scoreboard_data(cache_key=cache_key, timestamp_bucket=timestamp_bucket)

        scoreboard_outer = raw_data.get('scoreboard', {})
        raw_games_list = scoreboard_outer.get('games', [])
        game_date_str = scoreboard_outer.get('gameDate', datetime.now().strftime("%Y-%m-%d"))

        formatted_games_list: List[FormattedGameInfo] = [_format_live_game_details(game) for game in raw_games_list]

        response_data: FormattedScoreboardResponse = {
            "date": game_date_str,
            "games": formatted_games_list
        }

        # Create DataFrames if requested
        if return_dataframe:
            # Create a DataFrame for the games list
            games_data = []
            for game in formatted_games_list:
                # Flatten the nested structure for DataFrame
                game_row = {
                    "game_id": game.get("game_id"),
                    "start_time_utc": game.get("start_time_utc"),
                    "status_clock": game.get("status", {}).get("clock"),
                    "status_period": game.get("status", {}).get("period"),
                    "status_state_code": game.get("status", {}).get("state_code"),
                    "status_state_text": game.get("status", {}).get("state_text"),
                    "home_team_id": game.get("home_team", {}).get("id"),
                    "home_team_code": game.get("home_team", {}).get("code"),
                    "home_team_name": game.get("home_team", {}).get("name"),
                    "home_team_score": game.get("home_team", {}).get("score"),
                    "home_team_record": game.get("home_team", {}).get("record"),
                    "home_team_wins": game.get("home_team", {}).get("wins"),
                    "home_team_losses": game.get("home_team", {}).get("losses"),
                    "away_team_id": game.get("away_team", {}).get("id"),
                    "away_team_code": game.get("away_team", {}).get("code"),
                    "away_team_name": game.get("away_team", {}).get("name"),
                    "away_team_score": game.get("away_team", {}).get("score"),
                    "away_team_record": game.get("away_team", {}).get("record"),
                    "away_team_wins": game.get("away_team", {}).get("wins"),
                    "away_team_losses": game.get("away_team", {}).get("losses")
                }
                games_data.append(game_row)

            # Create DataFrame from the flattened data
            games_df = pd.DataFrame(games_data)
            dataframes["games"] = games_df

            # Save to CSV
            if not games_df.empty:
                csv_path = _get_csv_path_for_live_scoreboard(game_date_str)
                _save_dataframe_to_csv(games_df, csv_path)

                # Add relative path to response
                relative_path = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "live_scoreboard"
                )

                response_data["dataframe_info"] = {
                    "message": "Live scoreboard data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "games": {
                            "shape": list(games_df.shape),
                            "columns": games_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

        logger.info(f"fetch_league_scoreboard_logic formatted {len(formatted_games_list)} games for date {game_date_str}")

        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        current_date_str = datetime.now().strftime('%Y-%m-%d')
        logger.error(f"Error fetching/formatting scoreboard data for {current_date_str}: {str(e)}", exc_info=True)
        error_msg = Errors.LEAGUE_SCOREBOARD_UNEXPECTED.format(game_date=current_date_str, error=str(e)) if hasattr(Errors, 'LEAGUE_SCOREBOARD_UNEXPECTED') else f"Unexpected error fetching scoreboard: {e}"
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\matchup_tools.py =====
"""
Handles fetching and processing player matchup statistics, including
player-vs-player season matchups and defensive player rollup stats.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from datetime import datetime
import pandas as pd
from typing import Dict, Tuple, Any, Type, Optional, Set, List, Union
from functools import lru_cache

from nba_api.stats.endpoints import LeagueSeasonMatchups, MatchupsRollup
from nba_api.stats.library.parameters import SeasonTypeAllStar
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, find_player_id_or_error, PlayerNotFoundError
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
MATCHUPS_CSV_DIR = get_cache_dir("matchups")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_season_matchups(
    def_player_id: int,
    off_player_id: int,
    season: str,
    season_type: str
) -> str:
    """
    Generates a file path for saving season matchups DataFrame as CSV.

    Args:
        def_player_id: Defensive player ID
        off_player_id: Offensive player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"season_matchups_{def_player_id}_{off_player_id}_{season}_{clean_season_type}.csv"
    return get_cache_file_path(filename, "matchups")

def _get_csv_path_for_matchups_rollup(
    def_player_id: int,
    season: str,
    season_type: str
) -> str:
    """
    Generates a file path for saving matchups rollup DataFrame as CSV.

    Args:
        def_player_id: Defensive player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"matchups_rollup_{def_player_id}_{season}_{clean_season_type}.csv"
    return get_cache_file_path(filename, "matchups")

# --- Module-Level Constants ---
CACHE_TTL_SECONDS_MATCHUPS = 3600 * 6  # Cache matchup data for 6 hours
MATCHUP_DATA_CACHE_SIZE = 128

_MATCHUP_VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.preseason, SeasonTypeAllStar.playoffs}

# --- Helper for Fetching and Caching Raw Endpoint Data ---
@lru_cache(maxsize=MATCHUP_DATA_CACHE_SIZE)
def _get_cached_endpoint_dict(
    cache_key_tuple: Tuple,
    timestamp_bucket: str,
    endpoint_class: Type[Any],
    api_params: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Cached wrapper for NBA API endpoints returning a dictionary.
    Used by matchup tools to cache raw API responses.

    Args:
        cache_key_tuple: A unique tuple for caching, including endpoint name and key API parameters.
        timestamp_bucket: A string derived from the current time, bucketed for cache invalidation.
        endpoint_class: The specific NBA API endpoint class to instantiate.
        api_params: Keyword arguments to be passed directly to the endpoint_class constructor.

    Returns:
        The raw dictionary response from the NBA API endpoint's get_dict() method.

    Raises:
        Exception: If the API call fails, to be handled by the caller.
    """
    # Log parameters without the timestamp_bucket for brevity if it's too verbose
    logger.info(f"Cache miss/expiry for {endpoint_class.__name__} (ts: {timestamp_bucket}) - fetching. Params: {api_params}")
    try:
        endpoint_instance = endpoint_class(**api_params, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        return endpoint_instance.get_dict()
    except Exception as e:
        logger.error(f"{endpoint_class.__name__} API call failed with params {api_params}: {e}", exc_info=True)
        raise

def _extract_dataframe_from_response(
    response_dict: Dict[str, Any],
    expected_dataset_name: str,
    endpoint_instance_if_live: Optional[Any] = None
) -> pd.DataFrame:
    """
    Robustly extracts a DataFrame from an endpoint's response dictionary or live instance.
    Tries direct attribute access from the live instance first, then falls back to parsing 'resultSets' from the dictionary.

    Args:
        response_dict: The raw dictionary from the API (either live or cached).
        expected_dataset_name: The 'name' of the dataset in 'resultSets' (e.g., "SeasonMatchups").
        endpoint_instance_if_live: The live NBA API endpoint instance, if the call was not from cache.

    Returns:
        A pandas DataFrame, potentially empty if data is not found or extraction fails.
    """
    df = pd.DataFrame()
    # Convert common dataset name format (e.g., "Season Matchups") to attribute format (e.g., "season_matchups")
    dataset_attr_name = expected_dataset_name.lower().replace(" ", "_")

    if endpoint_instance_if_live and hasattr(endpoint_instance_if_live, dataset_attr_name):
        dataset_obj = getattr(endpoint_instance_if_live, dataset_attr_name)
        if hasattr(dataset_obj, 'get_data_frame'):
            try:
                df = dataset_obj.get_data_frame()
                logger.debug(f"Successfully accessed '{expected_dataset_name}' dataset directly via attribute '{dataset_attr_name}'.")
                return df
            except Exception as e:
                logger.warning(f"Error accessing dataset '{expected_dataset_name}' directly via attribute '{dataset_attr_name}': {e}. Trying 'resultSets' fallback.")
        else:
            logger.warning(f"Dataset attribute '{dataset_attr_name}' for '{expected_dataset_name}' does not have 'get_data_frame' method.")

    logger.debug(f"Attempting fallback to 'resultSets' for dataset: {expected_dataset_name}")
    if 'resultSets' in response_dict and isinstance(response_dict['resultSets'], list):
        for rs_item in response_dict['resultSets']:
            if rs_item.get('name') == expected_dataset_name:
                if rs_item.get('rowSet') is not None and rs_item.get('headers') is not None:
                    df = pd.DataFrame(rs_item['rowSet'], columns=rs_item['headers'])
                    logger.debug(f"Successfully extracted DataFrame for '{expected_dataset_name}' via 'resultSets' by name.")
                    return df
                else:
                    logger.warning(f"Dataset '{expected_dataset_name}' found by name in 'resultSets' but 'rowSet' or 'headers' are missing.")
                    return pd.DataFrame() # Return empty if malformed

        # Fallback if specific name not found but only one result set exists (common pattern)
        if df.empty and len(response_dict['resultSets']) == 1:
            rs_item = response_dict['resultSets'][0]
            if rs_item.get('rowSet') is not None and rs_item.get('headers') is not None:
                logger.warning(f"Dataset '{expected_dataset_name}' not found by name, using the single available result set (name: {rs_item.get('name')}) as fallback.")
                df = pd.DataFrame(rs_item['rowSet'], columns=rs_item['headers'])
                return df
            else:
                 logger.warning(f"Single available result set for '{expected_dataset_name}' is malformed (missing rowSet/headers).")
        elif df.empty and len(response_dict['resultSets']) > 1: # Only log if still empty and multiple sets existed
            logger.warning(f"Dataset '{expected_dataset_name}' not found by name, and multiple result sets exist. Cannot reliably determine fallback beyond the first if it was also unnamed.")

    if df.empty: # Log if still empty after all attempts
        logger.warning(f"Could not extract DataFrame for '{expected_dataset_name}' via direct access or 'resultSets' fallback.")
    return df

# --- Main Logic Functions ---
@lru_cache(maxsize=MATCHUP_DATA_CACHE_SIZE) # Caches the final processed string response
def fetch_league_season_matchups_logic(
    def_player_identifier: str,
    off_player_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches season matchup statistics between two specific players.
    Provides DataFrame output capabilities.

    Args:
        def_player_identifier: Name or ID of the defensive player.
        off_player_identifier: Name or ID of the offensive player.
        season: NBA season in YYYY-YY format. Defaults to current.
        season_type: Type of season. Defaults to "Regular Season".
        bypass_cache: If True, ignores cached raw data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with matchup data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Fetching season matchups: Def '{def_player_identifier}' vs Off '{off_player_identifier}' for {season}, Type: {season_type}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not def_player_identifier or not off_player_identifier:
        error_response = format_response(error=Errors.MISSING_PLAYER_IDENTIFIER)
        if return_dataframe:
            return error_response, dataframes
        return error_response
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response
    if season_type not in _MATCHUP_VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_MATCHUP_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        def_player_id_resolved, def_player_name_resolved = find_player_id_or_error(def_player_identifier)
        off_player_id_resolved, off_player_name_resolved = find_player_id_or_error(off_player_identifier)
    except (PlayerNotFoundError, ValueError) as e:
        logger.warning(f"Player ID lookup failed for matchups: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    api_params = {
        "def_player_id_nullable": str(def_player_id_resolved),
        "off_player_id_nullable": str(off_player_id_resolved),
        "season": season,
        "season_type_playoffs": season_type
    }
    raw_data_cache_key = ("LeagueSeasonMatchups", str(def_player_id_resolved), str(off_player_id_resolved), season, season_type)
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_MATCHUPS))

    try:
        response_dict: Dict[str, Any]
        live_endpoint_instance = None

        if bypass_cache or season == settings.CURRENT_NBA_SEASON:
            logger.info(f"Fetching fresh LeagueSeasonMatchups data (Bypass: {bypass_cache}, Season: {season})")
            live_endpoint_instance = LeagueSeasonMatchups(**api_params, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            response_dict = live_endpoint_instance.get_dict()
            if not bypass_cache and response_dict: # Update cache if fresh call for current season was successful
                 # Consider clearing specific entry or letting LRU handle it.
                 # For simplicity, we can rely on the timestamp_bucket for invalidation.
                 # If we want to force update the cache with this new data:
                 _get_cached_endpoint_dict(raw_data_cache_key, timestamp_bucket, LeagueSeasonMatchups, api_params)
        else:
            response_dict = _get_cached_endpoint_dict(raw_data_cache_key, timestamp_bucket, LeagueSeasonMatchups, api_params)

        matchups_df = _extract_dataframe_from_response(response_dict, "SeasonMatchups", live_endpoint_instance)

        if matchups_df.empty:
            logger.warning(f"No matchup data found for Def {def_player_name_resolved} vs Off {off_player_name_resolved} in {season}.")
            empty_response = {
                "def_player_id": def_player_id_resolved, "def_player_name": def_player_name_resolved,
                "off_player_id": off_player_id_resolved, "off_player_name": off_player_name_resolved,
                "parameters": {"season": season, "season_type": season_type}, "matchups": []
            }

            if return_dataframe:
                # Even if no matchups data, still return the empty DataFrame
                dataframes["matchups"] = matchups_df
                return format_response(empty_response), dataframes
            return format_response(empty_response)

        matchups_list = _process_dataframe(matchups_df, single_row=False)
        if matchups_list is None:
            logger.error(f"DataFrame processing failed for season matchups Def {def_player_name_resolved} vs Off {off_player_name_resolved} ({season}).")
            error_response = format_response(error=Errors.MATCHUPS_PROCESSING)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        result_payload = {
            "def_player_id": def_player_id_resolved, "def_player_name": def_player_name_resolved,
            "off_player_id": off_player_id_resolved, "off_player_name": off_player_name_resolved,
            "parameters": {"season": season, "season_type": season_type},
            "matchups": matchups_list or []
        }

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["matchups"] = matchups_df

            # Save to CSV if not empty
            if not matchups_df.empty:
                csv_path = _get_csv_path_for_season_matchups(
                    def_player_id=def_player_id_resolved,
                    off_player_id=off_player_id_resolved,
                    season=season,
                    season_type=season_type
                )
                _save_dataframe_to_csv(matchups_df, csv_path)

                # Add relative path to response
                relative_path = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "matchups"
                )

                result_payload["dataframe_info"] = {
                    "message": "Season matchups data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "matchups": {
                            "shape": list(matchups_df.shape),
                            "columns": matchups_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

        logger.info(f"Successfully fetched season matchups for Def {def_player_name_resolved} vs Off {off_player_name_resolved}")

        if return_dataframe:
            return format_response(result_payload), dataframes
        return format_response(result_payload)

    except Exception as e:
        logger.error(f"Error fetching season matchups: {e}", exc_info=True)
        error_response = format_response(error=Errors.MATCHUPS_API.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response

@lru_cache(maxsize=MATCHUP_DATA_CACHE_SIZE)
def fetch_matchups_rollup_logic(
    def_player_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches matchup rollup statistics for a defensive player.
    Provides DataFrame output capabilities.

    Args:
        def_player_identifier: Name or ID of the defensive player.
        season: NBA season in YYYY-YY format. Defaults to current.
        season_type: Type of season. Defaults to "Regular Season".
        bypass_cache: If True, ignores cached raw data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with matchup rollup data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Fetching matchup rollup for Def Player: '{def_player_identifier}' in {season}, Type: {season_type}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not def_player_identifier:
        error_response = format_response(error=Errors.MISSING_PLAYER_IDENTIFIER)
        if return_dataframe:
            return error_response, dataframes
        return error_response
    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, dataframes
        return error_response
    if season_type not in _MATCHUP_VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_MATCHUP_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        def_player_id_resolved, def_player_name_resolved = find_player_id_or_error(def_player_identifier)
    except (PlayerNotFoundError, ValueError) as e:
        logger.warning(f"Player ID lookup failed for matchup rollup: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    api_params = {
        "def_player_id_nullable": str(def_player_id_resolved),
        "season": season,
        "season_type_playoffs": season_type
    }
    raw_data_cache_key = ("MatchupsRollup", str(def_player_id_resolved), season, season_type)
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_MATCHUPS))

    try:
        response_dict: Dict[str, Any]
        live_endpoint_instance = None

        if bypass_cache or season == settings.CURRENT_NBA_SEASON:
            logger.info(f"Fetching fresh MatchupsRollup data (Bypass: {bypass_cache}, Season: {season})")
            live_endpoint_instance = MatchupsRollup(**api_params, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            response_dict = live_endpoint_instance.get_dict()
            if not bypass_cache and response_dict:
                 _get_cached_endpoint_dict.cache_clear()
                 _get_cached_endpoint_dict(raw_data_cache_key, timestamp_bucket, MatchupsRollup, api_params)
        else:
            response_dict = _get_cached_endpoint_dict(raw_data_cache_key, timestamp_bucket, MatchupsRollup, api_params)

        rollup_df = _extract_dataframe_from_response(response_dict, "MatchupsRollup", live_endpoint_instance)

        if rollup_df.empty:
            logger.warning(f"No matchup rollup data found for Def Player {def_player_name_resolved} in {season}.")
            empty_response = {
                "def_player_id": def_player_id_resolved, "def_player_name": def_player_name_resolved,
                "parameters": {"season": season, "season_type": season_type}, "rollup": []
            }

            if return_dataframe:
                # Even if no rollup data, still return the empty DataFrame
                dataframes["rollup"] = rollup_df
                return format_response(empty_response), dataframes
            return format_response(empty_response)

        rollup_list = _process_dataframe(rollup_df, single_row=False)
        if rollup_list is None:
            logger.error(f"DataFrame processing failed for matchup rollup Def {def_player_name_resolved} ({season}).")
            error_response = format_response(error=Errors.MATCHUPS_ROLLUP_PROCESSING)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        result_payload = {
            "def_player_id": def_player_id_resolved, "def_player_name": def_player_name_resolved,
            "parameters": {"season": season, "season_type": season_type},
            "rollup": rollup_list or []
        }

        # Store DataFrame if requested
        if return_dataframe:
            dataframes["rollup"] = rollup_df

            # Save to CSV if not empty
            if not rollup_df.empty:
                csv_path = _get_csv_path_for_matchups_rollup(
                    def_player_id=def_player_id_resolved,
                    season=season,
                    season_type=season_type
                )
                _save_dataframe_to_csv(rollup_df, csv_path)

                # Add relative path to response
                relative_path = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "matchups"
                )

                result_payload["dataframe_info"] = {
                    "message": "Matchups rollup data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "rollup": {
                            "shape": list(rollup_df.shape),
                            "columns": rollup_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

        logger.info(f"Successfully fetched matchup rollup for Def Player {def_player_name_resolved}")

        if return_dataframe:
            return format_response(result_payload), dataframes
        return format_response(result_payload)

    except Exception as e:
        logger.error(f"Error fetching matchups rollup for Def Player {def_player_identifier}: {e}", exc_info=True)
        error_response = format_response(error=Errors.MATCHUPS_ROLLUP_API.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response


===== backend\api_tools\odds_tools.py =====
"""
Handles fetching live betting odds for today's NBA games using NBALiveHTTP.
Includes caching logic for the raw API response.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Any, Dict, Optional, Union, List, Tuple
from functools import lru_cache
from datetime import datetime
import pandas as pd

from nba_api.live.nba.library.http import NBALiveHTTP
from ..config import settings
from ..core.errors import Errors
from .utils import format_response
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
CACHE_TTL_SECONDS_ODDS = 3600  # 1 hour
ODDS_RAW_CACHE_SIZE = 2
ODDS_ENDPOINT_PATH = "odds/odds_todaysGames.json"
ODDS_CSV_DIR = get_cache_dir("odds")

# --- Caching Function for Raw Data ---
@lru_cache(maxsize=ODDS_RAW_CACHE_SIZE)
def get_cached_odds_data(
    cache_key: str, # Static part of the key, e.g., "todays_live_odds"
    timestamp_bucket: str # Timestamp bucket for time-based invalidation
) -> Dict[str, Any]:
    """
    Cached wrapper for fetching live odds data using `NBALiveHTTP`.
    The `timestamp_bucket` ensures periodic cache invalidation.

    Args:
        cache_key: A static string for the cache key.
        timestamp_bucket: A string derived from the current time, bucketed by CACHE_TTL_SECONDS_ODDS.

    Returns:
        The raw dictionary response from the odds endpoint.

    Raises:
        Exception: If the API call fails, to be handled by the caller.
    """
    logger.info(f"Cache miss or expiry for odds data - fetching new data (Key: {cache_key}, Timestamp Bucket: {timestamp_bucket})")
    try:
        http_client = NBALiveHTTP()
        response = http_client.send_api_request(
            endpoint=ODDS_ENDPOINT_PATH,
            parameters={}, # This endpoint typically doesn't require parameters
            proxy=None,
            headers=None,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        return response.get_dict()
    except Exception as e:
        logger.error(f"NBALiveHTTP odds request failed: {e}", exc_info=True)
        raise # Re-raise to be handled by the calling function

def _flatten_odds_data(games_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Flattens the nested odds data structure into a list of records suitable for a DataFrame.

    Args:
        games_data: List of game dictionaries with nested market, book, and outcome data

    Returns:
        List of flattened dictionaries with one record per game-market-book-outcome combination
    """
    flattened_records = []

    for game in games_data:
        game_id = game.get('gameId', '')
        away_team_id = game.get('awayTeamId', '')
        home_team_id = game.get('homeTeamId', '')
        game_time = game.get('gameTime', '')
        game_status = game.get('gameStatus', '')
        game_status_text = game.get('gameStatusText', '')

        # Handle games without markets
        if 'markets' not in game or not game['markets']:
            # Add a basic record with just game info
            flattened_records.append({
                'gameId': game_id,
                'awayTeamId': away_team_id,
                'homeTeamId': home_team_id,
                'gameTime': game_time,
                'gameStatus': game_status,
                'gameStatusText': game_status_text,
                'marketId': '',
                'marketName': '',
                'bookId': '',
                'bookName': '',
                'outcomeType': '',
                'odds': '',
                'openingOdds': '',
                'value': ''
            })
            continue

        for market in game['markets']:
            market_id = market.get('marketId', '')
            market_name = market.get('name', '')

            # Handle markets without books
            if 'books' not in market or not market['books']:
                flattened_records.append({
                    'gameId': game_id,
                    'awayTeamId': away_team_id,
                    'homeTeamId': home_team_id,
                    'gameTime': game_time,
                    'gameStatus': game_status,
                    'gameStatusText': game_status_text,
                    'marketId': market_id,
                    'marketName': market_name,
                    'bookId': '',
                    'bookName': '',
                    'outcomeType': '',
                    'odds': '',
                    'openingOdds': '',
                    'value': ''
                })
                continue

            for book in market['books']:
                book_id = book.get('bookId', '')
                book_name = book.get('name', '')

                # Handle books without outcomes
                if 'outcomes' not in book or not book['outcomes']:
                    flattened_records.append({
                        'gameId': game_id,
                        'awayTeamId': away_team_id,
                        'homeTeamId': home_team_id,
                        'gameTime': game_time,
                        'gameStatus': game_status,
                        'gameStatusText': game_status_text,
                        'marketId': market_id,
                        'marketName': market_name,
                        'bookId': book_id,
                        'bookName': book_name,
                        'outcomeType': '',
                        'odds': '',
                        'openingOdds': '',
                        'value': ''
                    })
                    continue

                for outcome in book['outcomes']:
                    flattened_records.append({
                        'gameId': game_id,
                        'awayTeamId': away_team_id,
                        'homeTeamId': home_team_id,
                        'gameTime': game_time,
                        'gameStatus': game_status,
                        'gameStatusText': game_status_text,
                        'marketId': market_id,
                        'marketName': market_name,
                        'bookId': book_id,
                        'bookName': book_name,
                        'outcomeType': outcome.get('type', ''),
                        'odds': outcome.get('odds', ''),
                        'openingOdds': outcome.get('openingOdds', ''),
                        'value': outcome.get('value', '')
                    })

    return flattened_records

def _convert_to_dataframe(games_data: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    Converts the odds data to a pandas DataFrame with flattened structure.

    Args:
        games_data: List of game dictionaries from the API response

    Returns:
        DataFrame with flattened odds data
    """
    flattened_records = _flatten_odds_data(games_data)

    if not flattened_records:
        # Return empty DataFrame with expected columns
        return pd.DataFrame(columns=[
            'gameId', 'awayTeamId', 'homeTeamId', 'gameTime', 'gameStatus', 'gameStatusText',
            'marketId', 'marketName', 'bookId', 'bookName', 'outcomeType', 'odds', 'openingOdds', 'value'
        ])

    return pd.DataFrame(flattened_records)

def _save_to_csv(df: pd.DataFrame, date_str: str = None) -> str:
    """
    Saves the DataFrame to a CSV file using the standardized path_utils approach.

    Args:
        df: DataFrame to save
        date_str: Optional date string to include in the filename (defaults to today's date)

    Returns:
        Relative path to the saved CSV file
    """
    if date_str is None:
        date_str = datetime.now().strftime('%Y-%m-%d')

    filename = f"odds_data_{date_str}.csv"
    file_path = get_cache_file_path(filename, "odds")

    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved odds data to CSV: {file_path}")

        # Return the relative path for inclusion in the response
        return get_relative_cache_path(filename, "odds")
    except Exception as e:
        logger.error(f"Error saving odds data to CSV: {e}", exc_info=True)
        return ""

# --- Main Logic Function ---
def fetch_odds_data_logic(
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, pd.DataFrame]]:
    """
    Fetches live betting odds for today's NBA games.
    The odds data includes various markets from different bookmakers.

    Args:
        bypass_cache (bool, optional): If True, ignores any cached data and fetches fresh data from the API.
                                       Defaults to False.
        return_dataframe (bool, optional): If True, returns both JSON string and pandas DataFrame.
                                          Defaults to False (returns only JSON string).

    Returns:
        Union[str, Tuple[str, pd.DataFrame]]:
            If return_dataframe=False: JSON string containing a list of today's games with their associated odds data.
            If return_dataframe=True: Tuple containing (JSON string, pandas DataFrame)

            Expected dictionary structure passed to format_response:
            {
                "games": [
                    {
                        "gameId": str,
                        "awayTeamId": int,
                        "homeTeamId": int,
                        "gameTime": str (e.g., "2024-01-15T19:00:00-05:00"),
                        "gameStatus": int, // 1: Scheduled, 2: In Progress, 3: Final
                        "gameStatusText": str,
                        "markets": [ // List of betting markets available for the game
                            {
                                "marketId": str,
                                "name": str, // e.g., "2way - Total", "Point Spread", "Moneyline"
                                "books": [ // List of bookmakers offering odds for this market
                                    {
                                        "bookId": str,
                                        "name": str, // e.g., "DraftKings", "FanDuel"
                                        "outcomes": [ // List of possible outcomes and their odds
                                            {
                                                "type": str, // e.g., "home", "away", "over", "under"
                                                "odds": str, // e.g., "-110", "+150"
                                                "openingOdds": Optional[str],
                                                "value": Optional[str] // e.g., for spread or total lines "7.5", "220.5"
                                            }, ...
                                        ]
                                    }, ...
                                ]
                            }, ...
                        ]
                    }, ...
                ]
            }
            Returns {"games": []} if no odds data is found or an error occurs during fetching/processing.
            Or an {'error': 'Error message'} object if a critical issue occurs.
    """
    logger.info(f"Executing fetch_odds_data_logic with bypass_cache={bypass_cache}, return_dataframe={return_dataframe}")

    cache_key_odds = "todays_live_odds_data"
    cache_invalidation_timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_ODDS))

    try:
        raw_response_dict: Dict[str, Any]
        if bypass_cache:
            logger.info("Bypassing cache, fetching fresh live odds data.")
            http_client_direct = NBALiveHTTP()
            api_response = http_client_direct.send_api_request(
                endpoint=ODDS_ENDPOINT_PATH, parameters={}, timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            raw_response_dict = api_response.get_dict()
        else:
            raw_response_dict = get_cached_odds_data(cache_key=cache_key_odds, timestamp_bucket=cache_invalidation_timestamp_bucket)

        games_data_list = raw_response_dict.get("games", [])

        if not isinstance(games_data_list, list):
            logger.error(f"Fetched odds data 'games' field is not a list: {type(games_data_list)}")
            games_data_list = [] # Default to empty list to prevent further errors

        result_payload = {"games": games_data_list}
        logger.info(f"Successfully fetched or retrieved cached odds data. Number of games: {len(games_data_list)}")

        if return_dataframe:
            df = _convert_to_dataframe(games_data_list)

            # Get today's date for the CSV filename
            today_date_str = datetime.now().strftime('%Y-%m-%d')

            # Save to CSV and get the relative path
            csv_relative_path = _save_to_csv(df, today_date_str)

            # Add DataFrame metadata to the response
            if csv_relative_path:
                result_payload["dataframe_info"] = {
                    "message": "Odds data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "odds": {
                            "shape": list(df.shape),
                            "columns": df.columns.tolist(),
                            "csv_path": csv_relative_path
                        }
                    }
                }

            json_response = format_response(result_payload)
            return json_response, df

        json_response = format_response(result_payload)

        return json_response

    except Exception as e:
        logger.error(f"Error fetching or processing odds data: {e}", exc_info=True)
        error_msg = Errors.ODDS_API_UNEXPECTED.format(error=str(e))
        json_response = format_response(error=error_msg)

        if return_dataframe:
            # Return empty DataFrame with expected columns in case of error
            df = pd.DataFrame(columns=[
                'gameId', 'awayTeamId', 'homeTeamId', 'gameTime', 'gameStatus', 'gameStatusText',
                'marketId', 'marketName', 'bookId', 'bookName', 'outcomeType', 'odds', 'openingOdds', 'value'
            ])
            return json_response, df

        return json_response


===== backend\api_tools\player_aggregate_stats.py =====
"""
Handles aggregating various player statistics from different sources,
including common info, career stats, game logs for a specific season, and awards.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import json
from functools import lru_cache
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd
from nba_api.stats.library.parameters import SeasonTypeAllStar
from ..config import settings
from ..core.errors import Errors
from .utils import (
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path
from .player_common_info import fetch_player_info_logic
from .player_career_data import fetch_player_career_stats_logic, fetch_player_awards_logic
from .player_gamelogs import fetch_player_gamelog_logic

logger = logging.getLogger(__name__)

PLAYER_AGGREGATE_STATS_CACHE_SIZE = 128

# --- Cache Directory Setup ---
PLAYER_STATS_CSV_DIR = get_cache_dir("player_stats")

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_stats(player_name: str, season: str, season_type: str, data_type: str) -> str:
    """
    Generates a file path for saving a player stats DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season (YYYY-YY format)
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        data_type: The type of data (e.g., 'info', 'career', 'gamelog', 'awards')

    Returns:
        Path to the CSV file
    """
    # Clean player name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()

    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{data_type}.csv"
    return get_cache_file_path(filename, "player_stats")

def fetch_player_stats_logic(
    player_name: str,
    season: Optional[str] = None,
    season_type: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Aggregates various player statistics including common info, career stats,
    game logs for a specified season, and awards history.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: The season for which to fetch game logs (YYYY-YY format).
               Defaults to the current NBA season if None.
        season_type: The type of season for game logs (e.g., "Regular Season").
                    Defaults to "Regular Season".
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing the aggregated player statistics, or an error message.
                 Successful response structure includes keys like:
                 "player_name", "player_id", "season_requested_for_gamelog",
                 "season_type_requested_for_gamelog", "info", "headline_stats",
                 "available_seasons", "career_stats", "season_gamelog", "awards".
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    effective_season = season if season is not None else settings.CURRENT_NBA_SEASON
    logger.info(f"Executing fetch_player_stats_logic for: '{player_name}', Season for Gamelog: {effective_season}, Type: {season_type}, return_dataframe={return_dataframe}")

    if not _validate_season_format(effective_season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=effective_season)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
    if season_type not in VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(VALID_SEASON_TYPES)[:5]))
        logger.warning(error_msg)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)

        results_json = {}
        dataframes = {}

        # Define the logic functions to call
        logic_functions_config = {
            "info_and_headlines": (fetch_player_info_logic, [player_actual_name]),
            "career": (fetch_player_career_stats_logic, [player_actual_name]),
            "gamelog_for_season": (fetch_player_gamelog_logic, [player_actual_name, effective_season, season_type]),
            "awards_history": (fetch_player_awards_logic, [player_actual_name])
        }

        # Call each logic function and process the results
        for key, (func, args) in logic_functions_config.items():
            result_str = func(*args)
            try:
                data = json.loads(result_str)
                if "error" in data:
                    logger.error(f"Error from {key} logic for {player_actual_name}: {data['error']}")
                    error_response = result_str
                    if return_dataframe:
                        return error_response, {}
                    return error_response

                results_json[key] = data

                # Convert JSON data to DataFrames
                if return_dataframe:
                    if key == "info_and_headlines":
                        # Convert player info to DataFrame
                        if "player_info" in data:
                            info_df = pd.DataFrame([data["player_info"]])
                            dataframes["player_info"] = info_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "info")
                            _save_dataframe_to_csv(info_df, csv_path)

                        # Convert headline stats to DataFrame
                        if "headline_stats" in data:
                            headline_df = pd.DataFrame([data["headline_stats"]])
                            dataframes["headline_stats"] = headline_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "headline_stats")
                            _save_dataframe_to_csv(headline_df, csv_path)

                        # Convert available seasons to DataFrame
                        if "available_seasons" in data:
                            seasons_df = pd.DataFrame(data["available_seasons"])
                            dataframes["available_seasons"] = seasons_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "available_seasons")
                            _save_dataframe_to_csv(seasons_df, csv_path)

                    elif key == "career":
                        # Convert season totals regular season to DataFrame
                        if "season_totals_regular_season" in data:
                            reg_season_df = pd.DataFrame(data["season_totals_regular_season"])
                            dataframes["season_totals_regular_season"] = reg_season_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "season_totals_regular")
                            _save_dataframe_to_csv(reg_season_df, csv_path)

                        # Convert career totals regular season to DataFrame
                        if "career_totals_regular_season" in data:
                            career_reg_df = pd.DataFrame([data["career_totals_regular_season"]])
                            dataframes["career_totals_regular_season"] = career_reg_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "career_totals_regular")
                            _save_dataframe_to_csv(career_reg_df, csv_path)

                        # Convert season totals post season to DataFrame
                        if "season_totals_post_season" in data:
                            post_season_df = pd.DataFrame(data["season_totals_post_season"])
                            dataframes["season_totals_post_season"] = post_season_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "season_totals_post")
                            _save_dataframe_to_csv(post_season_df, csv_path)

                        # Convert career totals post season to DataFrame
                        if "career_totals_post_season" in data:
                            career_post_df = pd.DataFrame([data["career_totals_post_season"]])
                            dataframes["career_totals_post_season"] = career_post_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "career_totals_post")
                            _save_dataframe_to_csv(career_post_df, csv_path)

                    elif key == "gamelog_for_season":
                        # Convert gamelog to DataFrame
                        if "gamelog" in data:
                            gamelog_df = pd.DataFrame(data["gamelog"])
                            dataframes["gamelog"] = gamelog_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "gamelog")
                            _save_dataframe_to_csv(gamelog_df, csv_path)

                    elif key == "awards_history":
                        # Convert awards to DataFrame
                        if "awards" in data:
                            awards_df = pd.DataFrame(data["awards"])
                            dataframes["awards"] = awards_df

                            # Save to CSV
                            csv_path = _get_csv_path_for_player_stats(player_actual_name, effective_season, season_type, "awards")
                            _save_dataframe_to_csv(awards_df, csv_path)

            except json.JSONDecodeError as parse_error:
                logger.error(f"Failed to parse JSON from {key} logic for {player_actual_name}: {parse_error}. Response: {result_str}")
                error_response = format_response(error=f"Failed to process internal {key} results for {player_actual_name}.")
                if return_dataframe:
                    return error_response, {}
                return error_response

        # Extract data from the results
        info_data = results_json.get("info_and_headlines", {})
        career_data = results_json.get("career", {})
        gamelog_data = results_json.get("gamelog_for_season", {})
        awards_data = results_json.get("awards_history", {})

        # Create the result dictionary
        result_dict = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "season_requested_for_gamelog": effective_season,
            "season_type_requested_for_gamelog": season_type,
            "info": info_data.get("player_info", {}),
            "headline_stats": info_data.get("headline_stats", {}),
            "available_seasons": info_data.get("available_seasons", []),  # Include available seasons
            "career_stats": {
                "season_totals_regular_season": career_data.get("season_totals_regular_season", []),
                "career_totals_regular_season": career_data.get("career_totals_regular_season", {}),
                "season_totals_post_season": career_data.get("season_totals_post_season", []),
                "career_totals_post_season": career_data.get("career_totals_post_season", {})
            },
            "season_gamelog": gamelog_data.get("gamelog", []),
            "awards": awards_data.get("awards", [])
        }

        # Add DataFrame metadata to the response if DataFrames are being returned
        if return_dataframe and dataframes:
            dataframe_info = {
                "message": "Player aggregate stats data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            for df_key, df in dataframes.items():
                if not df.empty:
                    # Get the relative path for the CSV file
                    csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_{effective_season}_{season_type.replace(' ', '_').lower()}_{df_key}.csv"
                    relative_path = get_relative_cache_path(csv_filename, "player_stats")

                    dataframe_info["dataframes"][df_key] = {
                        "shape": list(df.shape),
                        "columns": df.columns.tolist(),
                        "csv_path": relative_path
                    }

            if dataframe_info["dataframes"]:
                result_dict["dataframe_info"] = dataframe_info

        logger.info(f"fetch_player_stats_logic completed for '{player_actual_name}'")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            return format_response(result_dict), dataframes

        return format_response(result_dict)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_stats_logic for '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_STATS_UNEXPECTED.format(identifier=player_name, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_career_by_college.py =====
"""
Handles fetching and processing player career by college data
from the PlayerCareerByCollege endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerCareerByCollege endpoint provides comprehensive player career data by college (1 DataFrame):
- Player Info: PLAYER_ID, PLAYER_NAME, COLLEGE (3 columns)
- Games & Minutes: GP, MIN (2 columns)
- Shooting: FGM, FGA, FG_PCT, FG3M, FG3A, FG3_PCT, FTM, FTA, FT_PCT (9 columns)
- Rebounding: OREB, DREB, REB (3 columns)
- Other Stats: AST, STL, BLK, TOV, PF, PTS (6 columns)
- Rich college data: 80-123 players per college with detailed career statistics (23 columns total)
- Perfect for college connections analysis, player development tracking, and recruiting insights
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playercareerbycollege
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_CAREER_BY_COLLEGE_CACHE_SIZE = 256

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PER_MODES: Set[str] = {PerModeSimple.totals, PerModeSimple.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# --- Cache Directory Setup ---
PLAYER_CAREER_BY_COLLEGE_CSV_DIR = get_cache_dir("player_career_by_college")

# Ensure cache directories exist
os.makedirs(PLAYER_CAREER_BY_COLLEGE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_career_by_college(
    college: str,
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    data_set_name: str = "PlayerCareerByCollege"
) -> str:
    """
    Generates a file path for saving player career by college DataFrame.
    
    Args:
        college: College name (required)
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_career_by_college",
        f"college{college.replace(' ', '_')}",
        f"league{league_id}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"per{per_mode_simple.replace(' ', '_')}",
        f"season{season_nullable if season_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "player_career_by_college")

# --- Parameter Validation ---
def _validate_player_career_by_college_params(
    college: str,
    league_id: str,
    per_mode_simple: str,
    season_type_all_star: str,
    season_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_player_career_by_college_logic."""
    if not college or not college.strip():
        return f"Invalid college: {college}. College name is required"
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if per_mode_simple not in VALID_PER_MODES:
        return f"Invalid per_mode_simple: {per_mode_simple}. Valid options: {', '.join(VALID_PER_MODES)}"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if season_nullable and not _validate_season_format(season_nullable):
        return f"Invalid season_nullable format: {season_nullable}. Expected format: YYYY-YY"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_CAREER_BY_COLLEGE_CACHE_SIZE)
def fetch_player_career_by_college_logic(
    college: str,
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player career by college data using the PlayerCareerByCollege endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        college: College name (required)
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player career by college data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_career_by_college_logic for College: {college}, League: {league_id}, "
        f"Type: {season_type_all_star}, Per: {per_mode_simple}, Season: {season_nullable}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_player_career_by_college_params(
        college, league_id, per_mode_simple, season_type_all_star, season_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player career by college: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_player_career_by_college(
        college, league_id, per_mode_simple, season_type_all_star, season_nullable, "PlayerCareerByCollege"
    )
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading player career by college from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "college": college,
                        "league_id": league_id,
                        "per_mode_simple": per_mode_simple,
                        "season_type_all_star": season_type_all_star,
                        "season_nullable": season_nullable
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["PlayerCareerByCollege"] = df
                result_dict["data_sets"]["PlayerCareerByCollege"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "college": college,
            "league_id": league_id,
            "per_mode_simple": per_mode_simple,
            "season_type_all_star": season_type_all_star
        }
        
        # Add season_nullable only if it's not empty
        if season_nullable:
            api_params["season_nullable"] = season_nullable
        
        logger.debug(f"Calling PlayerCareerByCollege with parameters: {api_params}")
        career_by_college_endpoint = playercareerbycollege.PlayerCareerByCollege(**api_params)
        
        # Get data frames
        list_of_dataframes = career_by_college_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "college": college,
                "league_id": league_id,
                "per_mode_simple": per_mode_simple,
                "season_type_all_star": season_type_all_star,
                "season_nullable": season_nullable
            },
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"PlayerCareerByCollege_{idx}" if idx > 0 else "PlayerCareerByCollege"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_career_by_college_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_career_by_college(
    college: str,
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player career by college data.
    
    Args:
        college: College name (required)
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player career by college data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_career_by_college_logic(
        college=college,
        league_id=league_id,
        per_mode_simple=per_mode_simple,
        season_type_all_star=season_type_all_star,
        season_nullable=season_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing PlayerCareerByCollege endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_player_career_by_college(college="Duke")
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_player_career_by_college(college="Duke", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("PlayerCareerByCollege endpoint test completed.")


===== backend\api_tools\player_career_by_college_rollup.py =====
"""
Handles fetching and processing player career by college rollup data
from the PlayerCareerByCollegeRollup endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerCareerByCollegeRollup endpoint provides comprehensive college rollup data (4 DataFrames):
- Tournament Info: REGION, SEED, COLLEGE, PLAYERS (4 columns)
- Games & Minutes: GP, MIN (2 columns)
- Shooting: FGM, FGA, FG_PCT, FG3M, FG3A, FG3_PCT, FTM, FTA, FT_PCT (9 columns)
- Rebounding: OREB, DREB, REB (3 columns)
- Other Stats: AST, STL, BLK, TOV, PF, PTS (6 columns)
- Rich college data: 64 colleges (16 per region) with aggregated career statistics (24 columns total)
- Perfect for college program comparison, tournament-style rankings, and aggregate analysis
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playercareerbycollegerollup
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_CAREER_BY_COLLEGE_ROLLUP_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA
VALID_PER_MODES: Set[str] = {PerModeSimple.totals, PerModeSimple.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star}

# --- Cache Directory Setup ---
PLAYER_CAREER_BY_COLLEGE_ROLLUP_CSV_DIR = get_cache_dir("player_career_by_college_rollup")

# Ensure cache directories exist
os.makedirs(PLAYER_CAREER_BY_COLLEGE_ROLLUP_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_career_by_college_rollup(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    data_set_name: str = "PlayerCareerByCollegeRollup"
) -> str:
    """
    Generates a file path for saving player career by college rollup DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_career_by_college_rollup",
        f"league{league_id}",
        f"type{season_type_all_star.replace(' ', '_')}",
        f"per{per_mode_simple.replace(' ', '_')}",
        f"season{season_nullable if season_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "player_career_by_college_rollup")

# --- Parameter Validation ---
def _validate_player_career_by_college_rollup_params(
    league_id: str,
    per_mode_simple: str,
    season_type_all_star: str,
    season_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_player_career_by_college_rollup_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if per_mode_simple not in VALID_PER_MODES:
        return f"Invalid per_mode_simple: {per_mode_simple}. Valid options: {', '.join(VALID_PER_MODES)}"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if season_nullable and not _validate_season_format(season_nullable):
        return f"Invalid season_nullable format: {season_nullable}. Expected format: YYYY-YY"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_CAREER_BY_COLLEGE_ROLLUP_CACHE_SIZE)
def fetch_player_career_by_college_rollup_logic(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player career by college rollup data using the PlayerCareerByCollegeRollup endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player career by college rollup data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_career_by_college_rollup_logic for League: {league_id}, "
        f"Type: {season_type_all_star}, Per: {per_mode_simple}, Season: {season_nullable}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_player_career_by_college_rollup_params(
        league_id, per_mode_simple, season_type_all_star, season_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player career by college rollup: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["EastRegion", "SouthRegion", "MidwestRegion", "WestRegion"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_player_career_by_college_rollup(
                league_id, per_mode_simple, season_type_all_star, season_nullable, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading player career by college rollup from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id,
                    "per_mode_simple": per_mode_simple,
                    "season_type_all_star": season_type_all_star,
                    "season_nullable": season_nullable
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "per_mode_simple": per_mode_simple,
            "season_type_all_star": season_type_all_star
        }
        
        # Add season_nullable only if it's not empty
        if season_nullable:
            api_params["season_nullable"] = season_nullable
        
        logger.debug(f"Calling PlayerCareerByCollegeRollup with parameters: {api_params}")
        career_rollup_endpoint = playercareerbycollegerollup.PlayerCareerByCollegeRollup(**api_params)
        
        # Get data frames
        list_of_dataframes = career_rollup_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "league_id": league_id,
                "per_mode_simple": per_mode_simple,
                "season_type_all_star": season_type_all_star,
                "season_nullable": season_nullable
            },
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["EastRegion", "SouthRegion", "MidwestRegion", "WestRegion"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"PlayerCareerByCollegeRollup_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_player_career_by_college_rollup(
                        league_id, per_mode_simple, season_type_all_star, season_nullable, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_career_by_college_rollup_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_career_by_college_rollup(
    league_id: str = "00",
    per_mode_simple: str = PerModeSimple.totals,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    season_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player career by college rollup data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        per_mode_simple: Per mode (default: Totals)
        season_type_all_star: Season type (default: Regular Season)
        season_nullable: Season (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player career by college rollup data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_career_by_college_rollup_logic(
        league_id=league_id,
        per_mode_simple=per_mode_simple,
        season_type_all_star=season_type_all_star,
        season_nullable=season_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing PlayerCareerByCollegeRollup endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_player_career_by_college_rollup()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_player_career_by_college_rollup(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("PlayerCareerByCollegeRollup endpoint test completed.")


===== backend\api_tools\player_career_data.py =====
"""
Handles fetching and processing player career statistics and awards information.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
from functools import lru_cache
from typing import Dict, Any, Union, Tuple, Optional, List
import pandas as pd
import json

from nba_api.stats.endpoints import playercareerstats, playerawards
from nba_api.stats.library.parameters import PerModeDetailed, PerMode36
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# Cache sizes
PLAYER_CAREER_STATS_CACHE_SIZE = 256
PLAYER_AWARDS_CACHE_SIZE = 256

# Module-level constant for valid PerMode values for career stats
_VALID_PER_MODES_CAREER = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_PER_MODES_CAREER.update({getattr(PerMode36, attr) for attr in dir(PerMode36) if not attr.startswith('_') and isinstance(getattr(PerMode36, attr), str)})

# --- Cache Directory Setup ---
PLAYER_CAREER_CSV_DIR = get_cache_dir("player_career")
PLAYER_AWARDS_CSV_DIR = get_cache_dir("player_awards")

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_career_stats(player_name: str, per_mode: str, data_type: str) -> str:
    """
    Generates a file path for saving a player career stats DataFrame as CSV.

    Args:
        player_name: The player's name
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data (e.g., 'season_regular', 'career_regular', 'season_post', 'career_post')

    Returns:
        Path to the CSV file
    """
    # Clean player name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()

    # Clean per mode for filename
    clean_per_mode = per_mode.lower()

    filename = f"{clean_player_name}_{clean_per_mode}_{data_type}.csv"
    return get_cache_file_path(filename, "player_career")

def _get_csv_path_for_awards(player_name: str) -> str:
    """
    Generates a file path for saving a player awards DataFrame as CSV.

    Args:
        player_name: The player's name

    Returns:
        Path to the CSV file
    """
    # Clean player name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()

    filename = f"{clean_player_name}_awards.csv"
    return get_cache_file_path(filename, "player_awards")

def fetch_player_career_stats_logic(
    player_name: str,
    per_mode: str = PerModeDetailed.per_game,
    league_id_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player career statistics including regular season and postseason totals.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        per_mode: The statistical mode (e.g., "PerGame", "Totals", "Per36").
                 Defaults to "PerGame".
        league_id_nullable: The league ID to filter results (optional).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing player career statistics, or an error message.
                 Successful response structure:
                 {
                     "player_name": "Player Name",
                     "player_id": 12345,
                     "per_mode_requested": "PerModeValue",
                     "data_retrieved_mode": "PerModeValue",
                     "league_id": "LeagueID",
                     "season_totals_regular_season": [ { ... stats ... } ],
                     "career_totals_regular_season": { ... stats ... },
                     "season_totals_post_season": [ { ... stats ... } ],
                     "career_totals_post_season": { ... stats ... }
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_career_stats_logic for: '{player_name}', Requested PerMode: {per_mode}, league_id: {league_id_nullable}, return_dataframe={return_dataframe}")

    if per_mode not in _VALID_PER_MODES_CAREER:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES_CAREER)[:5]))
        logger.warning(error_msg)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playercareerstats for ID: {player_id} (PerMode '{per_mode}' requested)")

        try:
            career_endpoint = playercareerstats.PlayerCareerStats(
                player_id=player_id,
                per_mode36=per_mode,
                league_id_nullable=league_id_nullable,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            logger.debug(f"playercareerstats API call successful for ID: {player_id}")
        except Exception as api_error:
            logger.error(f"nba_api playercareerstats failed for ID {player_id}: {api_error}", exc_info=True)
            error_msg = Errors.PLAYER_CAREER_STATS_API.format(identifier=player_actual_name, error=str(api_error))
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Get DataFrames from the API response
        season_totals_rs_df = career_endpoint.season_totals_regular_season.get_data_frame()
        career_totals_rs_df = career_endpoint.career_totals_regular_season.get_data_frame()
        season_totals_ps_df = career_endpoint.season_totals_post_season.get_data_frame()
        career_totals_ps_df = career_endpoint.career_totals_post_season.get_data_frame()

        # Define common columns for season totals
        season_totals_cols = [
            'SEASON_ID', 'TEAM_ID', 'TEAM_ABBREVIATION', 'PLAYER_AGE', 'GP', 'GS',
            'MIN', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM',
            'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS'
        ]

        # Filter columns for regular season
        available_season_cols_rs = [col for col in season_totals_cols if col in season_totals_rs_df.columns]
        filtered_season_rs_df = season_totals_rs_df.loc[:, available_season_cols_rs] if not season_totals_rs_df.empty and available_season_cols_rs else pd.DataFrame()

        # Filter columns for post season
        available_season_cols_ps = [col for col in season_totals_cols if col in season_totals_ps_df.columns]
        filtered_season_ps_df = season_totals_ps_df.loc[:, available_season_cols_ps] if not season_totals_ps_df.empty and available_season_cols_ps else pd.DataFrame()

        # Save DataFrames to CSV if returning DataFrames
        if return_dataframe:
            # Save regular season data
            if not filtered_season_rs_df.empty:
                csv_path = _get_csv_path_for_career_stats(player_actual_name, per_mode, "season_regular")
                _save_dataframe_to_csv(filtered_season_rs_df, csv_path)

            if not career_totals_rs_df.empty:
                csv_path = _get_csv_path_for_career_stats(player_actual_name, per_mode, "career_regular")
                _save_dataframe_to_csv(career_totals_rs_df, csv_path)

            # Save post season data
            if not filtered_season_ps_df.empty:
                csv_path = _get_csv_path_for_career_stats(player_actual_name, per_mode, "season_post")
                _save_dataframe_to_csv(filtered_season_ps_df, csv_path)

            if not career_totals_ps_df.empty:
                csv_path = _get_csv_path_for_career_stats(player_actual_name, per_mode, "career_post")
                _save_dataframe_to_csv(career_totals_ps_df, csv_path)

        # Process DataFrames for JSON response
        season_totals_regular_season = _process_dataframe(filtered_season_rs_df, single_row=False)
        career_totals_regular_season = _process_dataframe(career_totals_rs_df, single_row=True)
        season_totals_post_season = _process_dataframe(filtered_season_ps_df, single_row=False)
        career_totals_post_season = _process_dataframe(career_totals_ps_df, single_row=True)

        if season_totals_regular_season is None or career_totals_regular_season is None:
            logger.error(f"DataFrame processing failed for regular season career stats of {player_actual_name}.")
            error_msg = Errors.PLAYER_CAREER_STATS_PROCESSING.format(identifier=player_actual_name)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Create response data
        response_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "per_mode_requested": per_mode,
            "data_retrieved_mode": per_mode,
            "league_id": league_id_nullable,
            "season_totals_regular_season": season_totals_regular_season or [],
            "career_totals_regular_season": career_totals_regular_season or {},
            "season_totals_post_season": season_totals_post_season or [],
            "career_totals_post_season": career_totals_post_season or {}
        }

        # Add DataFrame metadata to the response if DataFrames are being returned
        if return_dataframe:
            dataframe_info = {
                "message": "Player career stats data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add metadata for regular season data
            if not filtered_season_rs_df.empty:
                csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_{per_mode.lower()}_season_regular.csv"
                relative_path = get_relative_cache_path(csv_filename, "player_career")

                dataframe_info["dataframes"]["season_totals_regular_season"] = {
                    "shape": list(filtered_season_rs_df.shape),
                    "columns": filtered_season_rs_df.columns.tolist(),
                    "csv_path": relative_path
                }

            if not career_totals_rs_df.empty:
                csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_{per_mode.lower()}_career_regular.csv"
                relative_path = get_relative_cache_path(csv_filename, "player_career")

                dataframe_info["dataframes"]["career_totals_regular_season"] = {
                    "shape": list(career_totals_rs_df.shape),
                    "columns": career_totals_rs_df.columns.tolist(),
                    "csv_path": relative_path
                }

            # Add metadata for post season data
            if not filtered_season_ps_df.empty:
                csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_{per_mode.lower()}_season_post.csv"
                relative_path = get_relative_cache_path(csv_filename, "player_career")

                dataframe_info["dataframes"]["season_totals_post_season"] = {
                    "shape": list(filtered_season_ps_df.shape),
                    "columns": filtered_season_ps_df.columns.tolist(),
                    "csv_path": relative_path
                }

            if not career_totals_ps_df.empty:
                csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_{per_mode.lower()}_career_post.csv"
                relative_path = get_relative_cache_path(csv_filename, "player_career")

                dataframe_info["dataframes"]["career_totals_post_season"] = {
                    "shape": list(career_totals_ps_df.shape),
                    "columns": career_totals_ps_df.columns.tolist(),
                    "csv_path": relative_path
                }

            if dataframe_info["dataframes"]:
                response_data["dataframe_info"] = dataframe_info

        logger.info(f"fetch_player_career_stats_logic completed for '{player_actual_name}'")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "season_totals_regular_season": filtered_season_rs_df,
                "career_totals_regular_season": career_totals_rs_df,
                "season_totals_post_season": filtered_season_ps_df,
                "career_totals_post_season": career_totals_ps_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_career_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_career_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_career_stats_logic for '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_CAREER_STATS_UNEXPECTED.format(identifier=player_name, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

def fetch_player_awards_logic(
    player_name: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches a list of awards received by the player.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing a list of player awards, or an error message.
                 Successful response structure:
                 {
                     "player_name": "Player Name",
                     "player_id": 12345,
                     "awards": [ { ... award details ... }, ... ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_awards_logic for: '{player_name}', return_dataframe={return_dataframe}")

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playerawards for ID: {player_id}")

        try:
            awards_endpoint = playerawards.PlayerAwards(player_id=player_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            logger.debug(f"playerawards API call successful for ID: {player_id}")
        except Exception as api_error:
            logger.error(f"nba_api playerawards failed for ID {player_id}: {api_error}", exc_info=True)
            error_msg = Errors.PLAYER_AWARDS_API.format(identifier=player_actual_name, error=str(api_error))
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Get DataFrame from the API response
        awards_df = awards_endpoint.player_awards.get_data_frame()

        # Save DataFrame to CSV if returning DataFrame
        if return_dataframe and not awards_df.empty:
            csv_path = _get_csv_path_for_awards(player_actual_name)
            _save_dataframe_to_csv(awards_df, csv_path)

        # Process DataFrame for JSON response
        awards_list = _process_dataframe(awards_df, single_row=False)

        if awards_list is None:
            logger.error(f"DataFrame processing failed for awards of {player_actual_name}.")
            error_msg = Errors.PLAYER_AWARDS_PROCESSING.format(identifier=player_actual_name)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Create response data
        response_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "awards": awards_list
        }

        # Add DataFrame metadata to the response if DataFrames are being returned
        if return_dataframe and not awards_df.empty:
            csv_filename = f"{player_actual_name.replace(' ', '_').replace('.', '').lower()}_awards.csv"
            relative_path = get_relative_cache_path(csv_filename, "player_awards")

            response_data["dataframe_info"] = {
                "message": "Player awards data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "awards": {
                        "shape": list(awards_df.shape),
                        "columns": awards_df.columns.tolist(),
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"fetch_player_awards_logic completed for '{player_actual_name}'")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "awards": awards_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_awards_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_awards_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_awards_logic for '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_AWARDS_UNEXPECTED.format(identifier=player_name, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_clutch.py =====
"""
Handles fetching and processing player clutch performance statistics
from the PlayerDashboardByClutch endpoint.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple, List
from functools import lru_cache

from nba_api.stats.endpoints import playerdashboardbyclutch
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailed
)
import pandas as pd

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_CLUTCH_CACHE_SIZE = 64

_VALID_CLUTCH_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.preseason}
_VALID_CLUTCH_PER_MODES: Set[str] = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_CLUTCH_MEASURE_TYPES: Set[str] = {getattr(MeasureTypeDetailed, attr) for attr in dir(MeasureTypeDetailed) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailed, attr), str)}
_VALID_Y_N_CLUTCH: Set[str] = {"Y", "N", ""} # Used for plus_minus, pace_adjust, rank

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_CLUTCH_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_clutch")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_CLUTCH_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_clutch(
    player_name: str,
    season: str,
    season_type: str,
    dashboard_name: str
) -> str:
    """
    Generates a file path for saving player clutch DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        dashboard_name: The name of the clutch dashboard

    Returns:
        Path to the CSV file
    """
    # Clean player name and dashboard name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_dashboard = dashboard_name.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{clean_dashboard}.csv"
    return os.path.join(PLAYER_CLUTCH_CSV_DIR, filename)

# --- Helper for Parameter Validation ---
def _validate_clutch_params(
    player_name: str, season: str, season_type: str, measure_type: str, per_mode: str,
    plus_minus: str, pace_adjust: str, rank: str, league_id: Optional[str],
    period: int, last_n_games: int, month: int, opponent_team_id: int,
    date_from_nullable: Optional[str], date_to_nullable: Optional[str]
) -> Optional[str]:
    """Validates parameters for fetch_player_clutch_stats_logic."""
    if not player_name or not player_name.strip():
        return Errors.PLAYER_NAME_EMPTY
    if not season or not _validate_season_format(season):
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if date_from_nullable and not validate_date_format(date_from_nullable):
        return Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
    if date_to_nullable and not validate_date_format(date_to_nullable):
        return Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
    if season_type not in _VALID_CLUTCH_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_CLUTCH_SEASON_TYPES)[:5]))
    if per_mode not in _VALID_CLUTCH_PER_MODES:
        return Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_CLUTCH_PER_MODES)[:5]))
    if measure_type not in _VALID_CLUTCH_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_CLUTCH_MEASURE_TYPES)[:5]))
    if plus_minus not in _VALID_Y_N_CLUTCH:
        return Errors.INVALID_PLUS_MINUS.format(value=plus_minus)
    if pace_adjust not in _VALID_Y_N_CLUTCH:
        return Errors.INVALID_PACE_ADJUST.format(value=pace_adjust)
    if rank not in _VALID_Y_N_CLUTCH:
        return Errors.INVALID_RANK.format(value=rank)
    if league_id is not None and not isinstance(league_id, str): # Basic check, can be expanded
        return Errors.INVALID_LEAGUE_ID_FORMAT.format(league_id=league_id)
    if not isinstance(period, int):
        return "Invalid type for period, must be int."
    if not isinstance(last_n_games, int):
        return "Invalid type for last_n_games, must be int."
    if not isinstance(month, int):
        return "Invalid type for month, must be int."
    if not isinstance(opponent_team_id, int):
        return "Invalid type for opponent_team_id, must be int."
    # Add more specific validation for other params if needed, e.g., enums for location, outcome etc.
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_CLUTCH_CACHE_SIZE)
def fetch_player_clutch_stats_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    measure_type: str = MeasureTypeDetailed.base,
    per_mode: str = PerModeDetailed.totals,
    league_id: Optional[str] = "00", # Corresponds to league_id_nullable, NBA default
    plus_minus: str = "N", # Y or N
    pace_adjust: str = "N", # Y or N
    rank: str = "N", # Y or N
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    shot_clock_range_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    vs_division_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player clutch performance statistics using the PlayerDashboardByClutch endpoint.
    The "clutch" aspect is inherent to the endpoint; parameters serve to filter these stats.
    Various clutch scenarios are returned as different data sets within the response.

    Provides DataFrame output capabilities and CSV caching for each data set.

    Args:
        player_name: Name or ID of the player.
        season: Season in YYYY-YY format. Defaults to current NBA season.
        season_type: Type of season (e.g., 'Regular Season', 'Playoffs'). Defaults to 'Regular Season'.
                     Corresponds to 'season_type_playoffs' API parameter.
        measure_type: Type of stats (e.g., 'Base', 'Advanced'). Defaults to 'Base'.
                      Corresponds to 'measure_type_detailed' API parameter.
        per_mode: Statistical mode (e.g., 'Totals', 'PerGame'). Defaults to 'Totals'.
                  Corresponds to 'per_mode_detailed' API parameter.
        league_id: League ID. Defaults to "00" (NBA). Corresponds to 'league_id_nullable'.
        plus_minus: Flag for plus-minus stats ("Y" or "N"). Defaults to "N".
        pace_adjust: Flag for pace adjustment ("Y" or "N"). Defaults to "N".
        rank: Flag for ranking ("Y" or "N"). Defaults to "N".
        last_n_games: Filter by last N games. Defaults to 0 (all games).
        month: Filter by month (1-12). Defaults to 0 (all months).
        opponent_team_id: Filter by opponent team ID. Defaults to 0 (all opponents).
        period: Filter by period (e.g., 1, 2, 3, 4 for quarters, 0 for all). Defaults to 0.
        shot_clock_range_nullable: Filter by shot clock range (e.g., '24-22', '4-0 Very Late').
        game_segment_nullable: Filter by game segment (e.g., 'First Half', 'Overtime').
        location_nullable: Filter by location ('Home' or 'Road').
        outcome_nullable: Filter by game outcome ('W' or 'L').
        vs_conference_nullable: Filter by opponent conference (e.g., 'East', 'West').
        vs_division_nullable: Filter by opponent division (e.g., 'Atlantic', 'Pacific').
        season_segment_nullable: Filter by season segment (e.g., 'Post All-Star').
        date_from_nullable: Start date filter (YYYY-MM-DD).
        date_to_nullable: End date filter (YYYY-MM-DD).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with clutch stats dashboards or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames, where keys are
                                               dashboard names (e.g., 'OverallPlayerDashboard').
    """
    logger.info(
        f"Executing fetch_player_clutch_stats_logic for: '{player_name}', Season: {season}, "
        f"Measure: {measure_type}, LeagueID: {league_id}, return_dataframe={return_dataframe}"
    )

    validation_error = _validate_clutch_params(
        player_name, season, season_type, measure_type, per_mode,
        plus_minus, pace_adjust, rank, league_id, period, last_n_games, month, opponent_team_id,
        date_from_nullable, date_to_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for clutch stats: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id_val, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playerdashboardbyclutch for ID: {player_id_val}, Name: {player_actual_name} Season: {season}")

        # Prepare parameters to be passed to the API
        api_params = {
            "player_id": player_id_val,
            "season": season,
            "season_type_playoffs": season_type,
            "measure_type_detailed": measure_type,
            "per_mode_detailed": per_mode,
            "league_id_nullable": league_id,
            "plus_minus": plus_minus,
            "pace_adjust": pace_adjust,
            "rank": rank,
            "last_n_games": last_n_games,
            "month": month,
            "opponent_team_id": opponent_team_id,
            "period": period,
            "shot_clock_range_nullable": shot_clock_range_nullable,
            "game_segment_nullable": game_segment_nullable,
            "location_nullable": location_nullable,
            "outcome_nullable": outcome_nullable,
            "vs_conference_nullable": vs_conference_nullable,
            "vs_division_nullable": vs_division_nullable,
            "season_segment_nullable": season_segment_nullable,
            "date_from_nullable": date_from_nullable,
            "date_to_nullable": date_to_nullable,
        }
        
        # Filter out None values for nullable parameters, as the API expects them to be absent or have a valid value
        # The nba_api library handles empty strings for some nullable parameters, but None might cause issues.
        # Explicitly passing default values like 0 for ints if not provided, or relying on API defaults.
        filtered_api_params = {k: v for k, v in api_params.items() if v is not None or k in \
            ['last_n_games', 'month', 'opponent_team_id', 'period']} # Keep these even if 0


        logger.debug(f"Calling PlayerDashboardByClutch with parameters: {filtered_api_params}")
        clutch_endpoint = playerdashboardbyclutch.PlayerDashboardByClutch(**filtered_api_params)

        normalized_dict = clutch_endpoint.get_normalized_dict()
        # get_data_frames() returns a list of DataFrames in the order of the data sets
        list_of_dataframes = clutch_endpoint.get_data_frames()
        # get_normalized_dict().keys() gives the names in the same order usually
        data_set_names = list(normalized_dict.keys())

        result_dict: Dict[str, Any] = {
            "player_name": player_actual_name,
            "player_id": player_id_val,
            "parameters_used": filtered_api_params, 
            "data_sets": {}
        }
        
        dataframes_for_return: Dict[str, pd.DataFrame] = {}

        if len(data_set_names) != len(list_of_dataframes):
            logger.error(
                f"Mismatch between number of data set names ({len(data_set_names)}) "
                f"and number of dataframes ({len(list_of_dataframes)}) for {player_actual_name}."
            )
            # Fallback or error handling here if necessary, for now, we log and try to proceed
            # This might indicate an issue with the endpoint or library version for this specific call

        # Iterate using zip, assuming names and dataframes correspond by order
        for set_name, current_df in zip(data_set_names, list_of_dataframes):
            if not current_df.empty:
                try:
                    # Explicitly call with single_row=False to ensure a list of records is returned
                    processed_df_output = _process_dataframe(current_df.copy(), single_row=False)
                    
                    if processed_df_output is not None:
                        result_dict["data_sets"][set_name] = processed_df_output
                    else:
                        # Handle cases where _process_dataframe might return None due to an internal error
                        logger.error(f"_process_dataframe returned None for data set '{set_name}'. Assigning empty list.")
                        result_dict["data_sets"][set_name] = [] 

                    if return_dataframe: # This stores the original, unprocessed DataFrame
                        dataframes_for_return[set_name] = current_df.copy() # Store a copy of the original df
                        # Save the processed_df_output (list of dicts) to CSV if needed, or save current_df
                        # For consistency with to_dict(orient='records'), we should save the processed data if that was the intent
                        # However, the CSV saving logic was using processed_df which was a DataFrame before.
                        # Let's save the current_df to CSV as it's the raw data for that table.
                        csv_path = _get_csv_path_for_player_clutch(
                            player_actual_name, season, season_type, set_name
                        )
                        _save_dataframe_to_csv(current_df.copy(), csv_path) # Save the unprocessed df to CSV
                except Exception as e:
                    logger.error(f"Error processing data set '{set_name}' for player clutch stats: {e}", exc_info=True)
                    result_dict["data_sets"][set_name] = {"error": f"Failed to process data set: {e}"}
            else:
                # If the DataFrame is empty, use the corresponding list from normalized_dict (which might also be empty or just headers)
                result_dict["data_sets"][set_name] = normalized_dict.get(set_name, [])
                logger.info(f"Data set '{set_name}' was empty for player {player_actual_name}, season {season}.")

        final_json_response = json.dumps(result_dict, indent=4)
        if return_dataframe:
            return final_json_response, dataframes_for_return
        return final_json_response

    except PlayerNotFoundError as e:
        logger.warning(f"Player not found for clutch stats: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_clutch_stats_logic: {e}", exc_info=True)
        error_response = format_response(error=f"{Errors.UNEXPECTED_ERROR} Error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # --- Test Cases ---
    SAMPLE_PLAYER_NAME = "LeBron James"
    SAMPLE_SEASON = "2022-23" # settings.CURRENT_NBA_SEASON
    SAMPLE_SEASON_TYPE = SeasonTypeAllStar.regular

    def run_clutch_test(test_name, params, expected_player_name, expect_data=True):
        print(f"\n--- Running Test: {test_name} ---")
        json_response, dataframes = fetch_player_clutch_stats_logic(
            **params,
            return_dataframe=True
        )
        data = json.loads(json_response)

        if "error" in data:
            print(f"Error received: {data['error']}")
            assert not expect_data, f"Expected data for {test_name}, but got error."
            return

        assert "player_name" in data, "player_name missing from response"
        assert data["player_name"] == expected_player_name, f"Expected player {expected_player_name}, got {data['player_name']}"
        assert "data_sets" in data, "'data_sets' missing from response"
        
        if expect_data:
            assert data["data_sets"], f"Expected data_sets to be non-empty for {test_name}"
            print(f"Parameters used: {data.get('parameters_used')}")
            print(f"Available data sets: {list(data['data_sets'].keys())}")

            if dataframes:
                for name, df in dataframes.items():
                    print(f"DataFrame '{name}' shape: {df.shape}")
                    assert not df.empty, f"DataFrame {name} should not be empty."
            else:
                print("No DataFrames returned, but JSON might have data.")
        else:
             assert not data.get("data_sets") or not any(data["data_sets"].values()), f"Expected empty data_sets for {test_name}"


        print(f"--- Test '{test_name}' Completed ---")
        return data, dataframes

    # Test 1: Basic fetch for LeBron James
    run_clutch_test("Basic LeBron Clutch", {"player_name": SAMPLE_PLAYER_NAME, "season": SAMPLE_SEASON}, SAMPLE_PLAYER_NAME)

    # Test 2: Specific season type (Playoffs)
    run_clutch_test("LeBron Playoffs Clutch", {"player_name": SAMPLE_PLAYER_NAME, "season": "2019-20", "season_type": SeasonTypeAllStar.playoffs}, SAMPLE_PLAYER_NAME)

    # Test 3: Different PerMode and MeasureType
    run_clutch_test(
        "LeBron PerGame Advanced Clutch",
        {
            "player_name": SAMPLE_PLAYER_NAME, "season": SAMPLE_SEASON,
            "per_mode": PerModeDetailed.per_game, "measure_type": MeasureTypeDetailed.advanced
        },
        SAMPLE_PLAYER_NAME
    )
    
    # Test 4: With Optional Parameters (e.g., Location, Outcome)
    run_clutch_test(
        "LeBron Clutch Home Wins",
        {
            "player_name": SAMPLE_PLAYER_NAME, "season": SAMPLE_SEASON,
            "location_nullable": "Home", "outcome_nullable": "W"
        },
        SAMPLE_PLAYER_NAME
    )

    # Test 5: With Date Range (expect fewer games or specific focus)
    # Note: Date range might lead to empty sets if no clutch games fall within.
    # Adjust dates for a known period of clutch activity if possible.
    run_clutch_test(
        "LeBron Clutch Specific Dates",
        {
            "player_name": SAMPLE_PLAYER_NAME, "season": "2022-23", # Ensure season matches date range
            "date_from_nullable": "2023-03-01", "date_to_nullable": "2023-03-15"
        },
        SAMPLE_PLAYER_NAME,
        expect_data=True # Might be false if no games in range
    )
    
    # Test 6: Different League ID (WNBA - requires a WNBA player)
    # This will likely fail for LeBron but tests LeagueID pass-through
    try:
        run_clutch_test(
            "WNBA Player Clutch (Example - Diana Taurasi)",
            {"player_name": "Diana Taurasi", "season": "2023", "league_id": "10"}, # WNBA League ID is 10, season format might differ
            "Diana Taurasi"
        )
    except PlayerNotFoundError:
        print("WNBA Player test skipped as Diana Taurasi not found by default find_player_id (expected).")
    except Exception as e:
        print(f"Error in WNBA test: {e}")


    # Test 7: Non-existent player
    try:
        run_clutch_test(
            "Non-existent Player Clutch",
            {"player_name": "Non Existent Player XYZ", "season": SAMPLE_SEASON},
            "",
            expect_data=False
        )
    except PlayerNotFoundError as e:
        print(f"Successfully caught PlayerNotFoundError for non-existent player: {e}")
    
    print("\nAll standalone tests for player_clutch completed.")

===== backend\api_tools\player_common_info.py =====
"""
Handles fetching common player information, including biographical data,
headline statistics, available seasons, and headshot URLs.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from functools import lru_cache
from typing import Dict, Any, Union, Tuple, Optional
import pandas as pd
import json

from nba_api.stats.endpoints import commonplayerinfo
from ..config import settings
from ..core.errors import Errors
from .utils import _process_dataframe, format_response, find_player_id_or_error, PlayerNotFoundError

logger = logging.getLogger(__name__)

PLAYER_INFO_CACHE_SIZE = 256

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_INFO_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_info")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_INFO_CSV_DIR, exist_ok=True)

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_info(player_name: str, data_type: str) -> str:
    """
    Generates a file path for saving player info DataFrame as CSV.

    Args:
        player_name: The player's name
        data_type: The type of data (e.g., 'info', 'headline_stats', 'available_seasons')

    Returns:
        Path to the CSV file
    """
    # Clean player name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()

    filename = f"{clean_player_name}_{data_type}.csv"
    return os.path.join(PLAYER_INFO_CSV_DIR, filename)

def fetch_player_info_logic(
    player_name: str,
    league_id_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches common player information, headline stats, and available seasons for a given player.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        league_id_nullable: The league ID to filter results (optional).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing player information, headline stats, and available seasons,
                 or an error message if the player is not found or an issue occurs.
                 Successful response structure:
                 {
                     "player_info": { ... common player info ... },
                     "headline_stats": { ... headline stats ... },
                     "available_seasons": [ ... list of available seasons ... ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_info_logic for: '{player_name}', league_id={league_id_nullable}, return_dataframe={return_dataframe}")

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching commonplayerinfo for player ID: {player_id} ({player_actual_name})")

        try:
            info_endpoint = commonplayerinfo.CommonPlayerInfo(
                player_id=player_id,
                league_id_nullable=league_id_nullable,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            logger.debug(f"commonplayerinfo API call successful for ID: {player_id}")
        except Exception as api_error:
            logger.error(f"nba_api commonplayerinfo failed for ID {player_id}: {api_error}", exc_info=True)
            error_msg = Errors.PLAYER_INFO_API.format(identifier=player_actual_name, error=str(api_error))
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Get DataFrames from the API response
        player_info_df = info_endpoint.common_player_info.get_data_frame()
        headline_stats_df = info_endpoint.player_headline_stats.get_data_frame()
        available_seasons_df = info_endpoint.available_seasons.get_data_frame()

        # Save DataFrames to CSV if returning DataFrames
        if return_dataframe:
            if not player_info_df.empty:
                csv_path = _get_csv_path_for_player_info(player_actual_name, "info")
                _save_dataframe_to_csv(player_info_df, csv_path)

            if not headline_stats_df.empty:
                csv_path = _get_csv_path_for_player_info(player_actual_name, "headline_stats")
                _save_dataframe_to_csv(headline_stats_df, csv_path)

            if not available_seasons_df.empty:
                csv_path = _get_csv_path_for_player_info(player_actual_name, "available_seasons")
                _save_dataframe_to_csv(available_seasons_df, csv_path)

        # Process DataFrames for JSON response
        player_info_dict = _process_dataframe(player_info_df, single_row=True)
        headline_stats_dict = _process_dataframe(headline_stats_df, single_row=True)
        available_seasons_list = _process_dataframe(available_seasons_df, single_row=False)

        # Check if any essential data processing failed
        if player_info_dict is None or headline_stats_dict is None or available_seasons_list is None:
            logger.error(f"DataFrame processing failed for {player_actual_name} (player_info: {player_info_dict is None}, headline_stats: {headline_stats_dict is None}, available_seasons: {available_seasons_list is None}).")
            error_msg = Errors.PLAYER_INFO_PROCESSING.format(identifier=player_actual_name)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Create response data
        response_data = {
            "player_info": player_info_dict or {}, # Ensure empty dict if None
            "headline_stats": headline_stats_dict or {}, # Ensure empty dict if None
            "available_seasons": available_seasons_list or [], # Ensure empty list if None
            "parameters": {
                "league_id": league_id_nullable
            }
        }

        logger.info(f"fetch_player_info_logic completed for '{player_actual_name}'")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "player_info": player_info_df,
                "headline_stats": headline_stats_df,
                "available_seasons": available_seasons_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_info_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_info_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_info_logic for '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_INFO_UNEXPECTED.format(identifier=player_name, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

def get_player_headshot_url(player_id: int) -> str:
    """
    Constructs the URL for a player's headshot image.

    Args:
        player_id (int): The unique ID of the player.

    Returns:
        str: The URL string for the player's headshot.

    Raises:
        ValueError: If the player_id is not a positive integer.
    """
    if not isinstance(player_id, int) or player_id <= 0:
        logger.error(f"Invalid player_id for headshot URL: {player_id}")
        raise ValueError(f"Invalid player ID provided for headshot: {player_id}")
    return f"{settings.HEADSHOT_BASE_URL}/{player_id}.png"

===== backend\api_tools\player_compare.py =====
"""
Handles fetching and processing player comparison data
from the PlayerCompare endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerCompare endpoint provides comprehensive player comparison data (2 DataFrames):
- Comparison Info: GROUP_SET, DESCRIPTION (2 columns)
- Minutes: MIN (1 column)
- Shooting: FGM, FGA, FG_PCT, FG3M, FG3A, FG3_PCT, FTM, FTA, FT_PCT (9 columns)
- Rebounding: OREB, DREB, REB (3 columns)
- Other Stats: AST, TOV, STL, BLK, BLKA, PF, PFD, PTS, PLUS_MINUS (9 columns)
- Rich comparison data: Head-to-head player statistics with detailed breakdowns (24 columns total)
- Perfect for head-to-head comparisons, performance analysis, and historical comparisons
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple, List
from functools import lru_cache

from nba_api.stats.endpoints import playercompare
from nba_api.stats.library.parameters import SeasonTypePlayoffs, PerModeDetailed, MeasureTypeDetailedDefense
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]

    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

def _validate_player_id_list(player_id_list):
    """Validate player ID list format."""
    if not player_id_list:
        return False

    if isinstance(player_id_list, list):
        # Check if all items are strings or integers
        for player_id in player_id_list:
            if not isinstance(player_id, (str, int)):
                return False
            # If string, check if it's a valid number
            if isinstance(player_id, str):
                try:
                    int(player_id)
                except ValueError:
                    return False
        return True

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_COMPARE_CACHE_SIZE = 256

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_PER_MODES: Set[str] = {PerModeDetailed.totals, PerModeDetailed.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypePlayoffs.regular, SeasonTypePlayoffs.playoffs}
VALID_MEASURE_TYPES: Set[str] = {MeasureTypeDetailedDefense.base, MeasureTypeDetailedDefense.advanced, MeasureTypeDetailedDefense.misc, MeasureTypeDetailedDefense.scoring, MeasureTypeDetailedDefense.usage}
VALID_PACE_ADJUST: Set[str] = {"Y", "N"}
VALID_PLUS_MINUS: Set[str] = {"Y", "N"}
VALID_RANK: Set[str] = {"Y", "N"}

# --- Cache Directory Setup ---
PLAYER_COMPARE_CSV_DIR = get_cache_dir("player_compare")

# Ensure cache directories exist
os.makedirs(PLAYER_COMPARE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_compare(
    vs_player_id_list: List[str],
    player_id_list: List[str],
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    per_mode_detailed: str = PerModeDetailed.totals,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    league_id_nullable: str = "",
    last_n_games: int = 0,
    data_set_name: str = "PlayerCompare"
) -> str:
    """
    Generates a file path for saving player compare DataFrame.

    Args:
        vs_player_id_list: List of player IDs to compare against
        player_id_list: List of player IDs to compare
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        per_mode_detailed: Per mode (default: Totals)
        measure_type_detailed_defense: Measure type (default: Base)
        league_id_nullable: League ID (default: "")
        last_n_games: Last N games (default: 0)
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    vs_players_str = "_".join(vs_player_id_list)
    players_str = "_".join(player_id_list)

    filename_parts = [
        f"player_compare",
        f"vs{vs_players_str}",
        f"players{players_str}",
        f"season{season.replace('-', '_')}",
        f"type{season_type_playoffs.replace(' ', '_')}",
        f"per{per_mode_detailed.replace(' ', '_')}",
        f"measure{measure_type_detailed_defense.replace(' ', '_')}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        f"games{last_n_games}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "player_compare")

# --- Parameter Validation ---
def _validate_player_compare_params(
    vs_player_id_list: List[str],
    player_id_list: List[str],
    season: str,
    season_type_playoffs: str,
    per_mode_detailed: str,
    measure_type_detailed_defense: str,
    league_id_nullable: str,
    last_n_games: int,
    pace_adjust: str,
    plus_minus: str,
    rank: str
) -> Optional[str]:
    """Validates parameters for fetch_player_compare_logic."""
    if not _validate_player_id_list(vs_player_id_list):
        return f"Invalid vs_player_id_list: {vs_player_id_list}. Must be a list of valid player IDs"
    if not _validate_player_id_list(player_id_list):
        return f"Invalid player_id_list: {player_id_list}. Must be a list of valid player IDs"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_playoffs not in VALID_SEASON_TYPES:
        return f"Invalid season_type_playoffs: {season_type_playoffs}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if per_mode_detailed not in VALID_PER_MODES:
        return f"Invalid per_mode_detailed: {per_mode_detailed}. Valid options: {', '.join(VALID_PER_MODES)}"
    if measure_type_detailed_defense not in VALID_MEASURE_TYPES:
        return f"Invalid measure_type_detailed_defense: {measure_type_detailed_defense}. Valid options: {', '.join(VALID_MEASURE_TYPES)}"
    if league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not isinstance(last_n_games, int) or last_n_games < 0:
        return f"Invalid last_n_games: {last_n_games}. Must be a non-negative integer"
    if pace_adjust not in VALID_PACE_ADJUST:
        return f"Invalid pace_adjust: {pace_adjust}. Valid options: {', '.join(VALID_PACE_ADJUST)}"
    if plus_minus not in VALID_PLUS_MINUS:
        return f"Invalid plus_minus: {plus_minus}. Valid options: {', '.join(VALID_PLUS_MINUS)}"
    if rank not in VALID_RANK:
        return f"Invalid rank: {rank}. Valid options: {', '.join(VALID_RANK)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_COMPARE_CACHE_SIZE)
def fetch_player_compare_logic(
    vs_player_id_list: Tuple[str, ...],
    player_id_list: Tuple[str, ...],
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    per_mode_detailed: str = PerModeDetailed.totals,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    league_id_nullable: str = "",
    last_n_games: int = 0,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player comparison data using the PlayerCompare endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        vs_player_id_list: Tuple of player IDs to compare against
        player_id_list: Tuple of player IDs to compare
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        per_mode_detailed: Per mode (default: Totals)
        measure_type_detailed_defense: Measure type (default: Base)
        league_id_nullable: League ID (default: "")
        last_n_games: Last N games (default: 0)
        pace_adjust: Pace adjust (default: "N")
        plus_minus: Plus minus (default: "N")
        rank: Rank (default: "N")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player comparison data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    # Convert tuples back to lists for processing
    vs_player_id_list = list(vs_player_id_list)
    player_id_list = list(player_id_list)

    logger.info(
        f"Executing fetch_player_compare_logic for VS Players: {vs_player_id_list}, Players: {player_id_list}, "
        f"Season: {season}, Type: {season_type_playoffs}, Per: {per_mode_detailed}, "
        f"Measure: {measure_type_detailed_defense}, League: {league_id_nullable}, "
        f"Games: {last_n_games}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_player_compare_params(
        vs_player_id_list, player_id_list, season, season_type_playoffs, per_mode_detailed,
        measure_type_detailed_defense, league_id_nullable, last_n_games, pace_adjust, plus_minus, rank
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player compare: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV files
    dataframes = {}

    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["OverallComparison", "IndividualComparison"]
        all_cached = True

        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_player_compare(
                vs_player_id_list, player_id_list, season, season_type_playoffs, per_mode_detailed,
                measure_type_detailed_defense, league_id_nullable, last_n_games, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading player compare from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break

        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "vs_player_id_list": vs_player_id_list,
                    "player_id_list": player_id_list,
                    "season": season,
                    "season_type_playoffs": season_type_playoffs,
                    "per_mode_detailed": per_mode_detailed,
                    "measure_type_detailed_defense": measure_type_detailed_defense,
                    "league_id_nullable": league_id_nullable,
                    "last_n_games": last_n_games,
                    "pace_adjust": pace_adjust,
                    "plus_minus": plus_minus,
                    "rank": rank
                },
                "data_sets": {}
            }

            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)

            return format_response(result_dict), dataframes

    try:
        # Prepare API parameters
        api_params = {
            "vs_player_id_list": vs_player_id_list,
            "player_id_list": player_id_list,
            "season": season,
            "season_type_playoffs": season_type_playoffs,
            "per_mode_detailed": per_mode_detailed,
            "measure_type_detailed_defense": measure_type_detailed_defense,
            "last_n_games": last_n_games,
            "pace_adjust": pace_adjust,
            "plus_minus": plus_minus,
            "rank": rank
        }

        # Add league_id_nullable (can be empty string)
        api_params["league_id_nullable"] = league_id_nullable

        logger.debug(f"Calling PlayerCompare with parameters: {api_params}")
        player_compare_endpoint = playercompare.PlayerCompare(**api_params)

        # Get data frames
        list_of_dataframes = player_compare_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "vs_player_id_list": vs_player_id_list,
                "player_id_list": player_id_list,
                "season": season,
                "season_type_playoffs": season_type_playoffs,
                "per_mode_detailed": per_mode_detailed,
                "measure_type_detailed_defense": measure_type_detailed_defense,
                "league_id_nullable": league_id_nullable,
                "last_n_games": last_n_games,
                "pace_adjust": pace_adjust,
                "plus_minus": plus_minus,
                "rank": rank
            },
            "data_sets": {}
        }

        # Process each data frame
        data_set_names = ["OverallComparison", "IndividualComparison"]

        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"PlayerCompare_{idx}"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_player_compare(
                        vs_player_id_list, player_id_list, season, season_type_playoffs, per_mode_detailed,
                        measure_type_detailed_defense, league_id_nullable, last_n_games, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_compare_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_compare(
    vs_player_id_list: List[str],
    player_id_list: List[str],
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    per_mode_detailed: str = PerModeDetailed.totals,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    league_id_nullable: str = "",
    last_n_games: int = 0,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player comparison data.

    Args:
        vs_player_id_list: List of player IDs to compare against
        player_id_list: List of player IDs to compare
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        per_mode_detailed: Per mode (default: Totals)
        measure_type_detailed_defense: Measure type (default: Base)
        league_id_nullable: League ID (default: "")
        last_n_games: Last N games (default: 0)
        pace_adjust: Pace adjust (default: "N")
        plus_minus: Plus minus (default: "N")
        rank: Rank (default: "N")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player comparison data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_compare_logic(
        vs_player_id_list=tuple(vs_player_id_list),
        player_id_list=tuple(player_id_list),
        season=season,
        season_type_playoffs=season_type_playoffs,
        per_mode_detailed=per_mode_detailed,
        measure_type_detailed_defense=measure_type_detailed_defense,
        league_id_nullable=league_id_nullable,
        last_n_games=last_n_games,
        pace_adjust=pace_adjust,
        plus_minus=plus_minus,
        rank=rank,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing PlayerCompare endpoint...")

    # Test 1: Basic fetch
    json_response = get_player_compare(
        vs_player_id_list=["2544"],  # LeBron James
        player_id_list=["201939"]    # Stephen Curry
    )
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_player_compare(
        vs_player_id_list=["2544"],  # LeBron James
        player_id_list=["201939"],   # Stephen Curry
        return_dataframe=True
    )
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("PlayerCompare endpoint test completed.")


===== backend\api_tools\player_comparison.py =====
"""
Player comparison module for NBA player analysis.
Provides both JSON and DataFrame outputs with CSV caching.
"""

import logging
import os
import json
import base64
from io import BytesIO
from typing import Dict, List, Any, Optional, Union, Tuple
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.figure import Figure
from nba_api.stats.endpoints import shotchartdetail
from ..config import settings
from .utils import find_player_id_or_error, format_response
from .advanced_shot_charts import draw_court
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_COMPARISON_CSV_DIR = get_cache_dir("player_comparison")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_shots(
    player_id: str,
    season: str,
    season_type: str
) -> str:
    """
    Generates a file path for saving player shot data DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    return get_cache_file_path(
        f"player_{player_id}_shots_{season}_{clean_season_type}.csv",
        "player_comparison"
    )

def _get_csv_path_for_comparison(
    player_ids: List[str],
    season: str,
    season_type: str
) -> str:
    """
    Generates a file path for saving player comparison DataFrame as CSV.

    Args:
        player_ids: List of player IDs
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    # Convert player IDs to strings and join for filename
    player_ids_str = "_".join([str(player_id) for player_id in player_ids])

    return get_cache_file_path(
        f"comparison_{player_ids_str}_{season}_{clean_season_type}.csv",
        "player_comparison"
    )

def compare_player_shots(
    player_names: List[str],
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    output_format: str = "base64",
    chart_type: str = "scatter",
    context_measure: str = "FGA",
    return_dataframe: bool = False
) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
    """
    Compare shot charts for multiple players.

    Args:
        player_names: List of player names to compare
        season: NBA season in format YYYY-YY
        season_type: Type of season (Regular Season, Playoffs, etc.)
        output_format: Output format (base64, file)
        chart_type: Type of chart to create (scatter, heatmap, zones)
        context_measure: Context measure for shot chart (FGA, FGM, FG_PCT, etc.)
        return_dataframe: Whether to return DataFrames along with the visualization data

    Returns:
        If return_dataframe=False:
            Dict containing the visualization data and metadata
        If return_dataframe=True:
            Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]: A tuple containing the visualization data
                                                          and a dictionary of DataFrames
    """
    # Store DataFrames if requested
    dataframes = {}

    if len(player_names) < 2:
        error_response = {
            "error": "At least two players are required for comparison"
        }
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if len(player_names) > 4:
        error_response = {
            "error": "Maximum of 4 players can be compared at once"
        }
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        # Get player data
        player_data = []
        player_ids = []

        for player_name in player_names:
            # Get player ID
            player_id, player_actual_name = find_player_id_or_error(player_name)
            player_ids.append(player_id)

            # Fetch shot chart data
            shotchart_endpoint = shotchartdetail.ShotChartDetail(
                player_id=player_id,
                team_id=0,
                season_nullable=season,
                season_type_all_star=season_type,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS,
                context_measure_simple=context_measure
            )

            shots_df = shotchart_endpoint.shot_chart_detail.get_data_frame()
            league_avg_df = shotchart_endpoint.league_averages.get_data_frame()

            # Store DataFrames if requested
            if return_dataframe:
                dataframes[f"shots_{player_id}"] = shots_df
                dataframes[f"league_avg_{player_id}"] = league_avg_df

                # Save to CSV if not empty
                if not shots_df.empty:
                    csv_path = _get_csv_path_for_player_shots(
                        player_id=player_id,
                        season=season,
                        season_type=season_type
                    )
                    _save_dataframe_to_csv(shots_df, csv_path)

            if shots_df.empty:
                error_response = {
                    "error": f"No shot data found for {player_actual_name} in {season} {season_type}"
                }
                if return_dataframe:
                    return error_response, dataframes
                return error_response

            # Process shot data
            shot_locations = []
            for _, row in shots_df.iterrows():
                shot_locations.append({
                    "x": float(row['LOC_X']),
                    "y": float(row['LOC_Y']),
                    "made": bool(row['SHOT_MADE_FLAG']),
                    "shot_type": row['ACTION_TYPE'],
                    "zone": row['SHOT_ZONE_BASIC'],
                    "distance": float(row['SHOT_DISTANCE']),
                    "game_date": row['GAME_DATE'],
                    "period": int(row['PERIOD'])
                })

            # Calculate overall stats
            total_shots = len(shot_locations)
            made_shots = sum(1 for shot in shot_locations if shot["made"])
            field_goal_percentage = round((made_shots / total_shots) * 100, 1) if total_shots > 0 else 0

            # Calculate zone breakdown
            zone_breakdown = {}
            for zone in set(shot["zone"] for shot in shot_locations):
                zone_shots = [shot for shot in shot_locations if shot["zone"] == zone]
                zone_attempts = len(zone_shots)
                zone_made = sum(1 for shot in zone_shots if shot["made"])
                zone_percentage = round((zone_made / zone_attempts) * 100, 1) if zone_attempts > 0 else 0

                # Get league average for this zone
                league_zone_data = league_avg_df[league_avg_df['SHOT_ZONE_BASIC'] == zone]
                league_percentage = float(league_zone_data['FG_PCT'].iloc[0]) * 100 if not league_zone_data.empty else 0

                zone_breakdown[zone] = {
                    "attempts": zone_attempts,
                    "made": zone_made,
                    "percentage": zone_percentage,
                    "league_percentage": league_percentage,
                    "relative_percentage": round(zone_percentage - league_percentage, 1)
                }

            player_data.append({
                "player_name": player_actual_name,
                "player_id": player_id,
                "shot_locations": shot_locations,
                "overall_stats": {
                    "total_shots": total_shots,
                    "made_shots": made_shots,
                    "field_goal_percentage": field_goal_percentage
                },
                "zone_breakdown": zone_breakdown
            })

        # Create a combined DataFrame for all players if requested
        if return_dataframe and len(player_data) > 0:
            # Create a DataFrame with zone breakdown for all players
            zone_data = []
            for player in player_data:
                for zone, stats in player["zone_breakdown"].items():
                    zone_data.append({
                        "player_id": player["player_id"],
                        "player_name": player["player_name"],
                        "zone": zone,
                        "attempts": stats["attempts"],
                        "made": stats["made"],
                        "percentage": stats["percentage"],
                        "league_percentage": stats["league_percentage"],
                        "relative_percentage": stats["relative_percentage"]
                    })

            if zone_data:
                zone_df = pd.DataFrame(zone_data)
                dataframes["zone_breakdown"] = zone_df

                # Save to CSV
                csv_path = _get_csv_path_for_comparison(
                    player_ids=player_ids,
                    season=season,
                    season_type=season_type
                )
                _save_dataframe_to_csv(zone_df, csv_path)

        # Create visualization based on chart type
        result = None
        if chart_type == "scatter":
            result = create_comparison_scatter(player_data, season, season_type, output_format)
        elif chart_type == "heatmap":
            result = create_comparison_heatmap(player_data, season, season_type, output_format)
        elif chart_type == "zones":
            result = create_comparison_zones(player_data, season, season_type, output_format)
        else:
            error_response = {
                "error": f"Invalid chart type: {chart_type}. Valid types are: scatter, heatmap, zones"
            }
            if return_dataframe:
                return error_response, dataframes
            return error_response

        # Add DataFrame info to the result if requested
        if return_dataframe:
            # Add CSV paths to the result
            csv_paths = {}
            for player_id in player_ids:
                csv_path = _get_csv_path_for_player_shots(
                    player_id=player_id,
                    season=season,
                    season_type=season_type
                )
                csv_paths[f"shots_{player_id}"] = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "player_comparison"
                )

            # Add comparison CSV path
            comparison_csv_path = _get_csv_path_for_comparison(
                player_ids=player_ids,
                season=season,
                season_type=season_type
            )
            csv_paths["zone_breakdown"] = get_relative_cache_path(
                os.path.basename(comparison_csv_path),
                "player_comparison"
            )

            result["dataframe_info"] = {
                "message": "Player comparison data has been converted to DataFrames and saved as CSV files",
                "dataframes": {
                    name: {
                        "shape": list(df.shape) if not df.empty else [],
                        "columns": df.columns.tolist() if not df.empty else [],
                        "csv_path": csv_paths.get(name, "")
                    } for name, df in dataframes.items() if not df.empty
                }
            }

            return result, dataframes

        return result

    except Exception as e:
        logger.error(f"Error comparing player shots: {str(e)}", exc_info=True)
        error_response = {
            "error": f"Failed to compare player shots: {str(e)}"
        }
        if return_dataframe:
            return error_response, dataframes
        return error_response

def create_comparison_scatter(
    player_data: List[Dict[str, Any]],
    season: str,
    season_type: str,
    output_format: str = "base64"
) -> Dict[str, Any]:
    """Create a scatter plot comparison of player shots."""
    # Create figure with subplots
    fig, axes = plt.subplots(1, len(player_data), figsize=(6 * len(player_data), 10), facecolor='white')

    # If only one subplot, make it iterable
    if len(player_data) == 1:
        axes = [axes]

    # Colors for each player
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

    # Draw each player's shots
    for i, (ax, player, color) in enumerate(zip(axes, player_data, colors)):
        # Draw court
        draw_court(ax)

        # Extract shot locations
        shots = player["shot_locations"]

        # Plot shots
        made_x = [shot["x"] for shot in shots if shot["made"]]
        made_y = [shot["y"] for shot in shots if shot["made"]]
        missed_x = [shot["x"] for shot in shots if not shot["made"]]
        missed_y = [shot["y"] for shot in shots if not shot["made"]]

        ax.scatter(made_x, made_y, c=color, alpha=0.7, s=30, marker='o', edgecolor='white', linewidth=0.5, zorder=3)
        ax.scatter(missed_x, missed_y, c=color, alpha=0.4, s=20, marker='x', linewidth=1, zorder=2)

        # Add title
        stats = player["overall_stats"]
        title = f"{player['player_name']}\n"
        title += f"FG: {stats['made_shots']}/{stats['total_shots']} ({stats['field_goal_percentage']}%)"
        ax.set_title(title, pad=20, size=12, weight='bold')

    # Add overall title
    fig.suptitle(f"Shot Chart Comparison - {season} {season_type}", fontsize=16, weight='bold', y=0.98)

    # Adjust layout
    plt.tight_layout(rect=[0, 0, 1, 0.96])

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": "comparison_scatter"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        player_names = "_vs_".join([player["player_name"].replace(" ", "_") for player in player_data])
        filename = f"comparison_scatter_{player_names}_{season}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "comparison_scatter"
        }

def create_comparison_heatmap(
    player_data: List[Dict[str, Any]],
    season: str,
    season_type: str,
    output_format: str = "base64"
) -> Dict[str, Any]:
    """Create a heatmap comparison of player shots."""
    # Create figure with subplots
    fig, axes = plt.subplots(1, len(player_data), figsize=(6 * len(player_data), 10), facecolor='white')

    # If only one subplot, make it iterable
    if len(player_data) == 1:
        axes = [axes]

    # Draw each player's heatmap
    for i, (ax, player) in enumerate(zip(axes, player_data)):
        # Draw court
        draw_court(ax)

        # Extract shot locations
        shots = player["shot_locations"]

        # Create heatmap
        x = [shot["x"] for shot in shots]
        y = [shot["y"] for shot in shots]

        if len(x) > 10:
            # Create hexbin
            hexbin = ax.hexbin(
                x, y,
                gridsize=25,
                cmap='viridis',
                alpha=0.7,
                mincnt=1,
                zorder=3
            )
            plt.colorbar(hexbin, ax=ax, label='Shot Frequency')
        else:
            # Fall back to scatter plot if not enough points
            made_x = [shot["x"] for shot in shots if shot["made"]]
            made_y = [shot["y"] for shot in shots if shot["made"]]
            missed_x = [shot["x"] for shot in shots if not shot["made"]]
            missed_y = [shot["y"] for shot in shots if not shot["made"]]

            ax.scatter(made_x, made_y, c='#2ECC71', alpha=0.7, s=30, marker='o', edgecolor='white', linewidth=0.5, zorder=3)
            ax.scatter(missed_x, missed_y, c='#E74C3C', alpha=0.4, s=20, marker='x', linewidth=1, zorder=2)

        # Add title
        stats = player["overall_stats"]
        title = f"{player['player_name']}\n"
        title += f"FG: {stats['made_shots']}/{stats['total_shots']} ({stats['field_goal_percentage']}%)"
        ax.set_title(title, pad=20, size=12, weight='bold')

    # Add overall title
    fig.suptitle(f"Shot Frequency Comparison - {season} {season_type}", fontsize=16, weight='bold', y=0.98)

    # Adjust layout
    plt.tight_layout(rect=[0, 0, 1, 0.96])

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": "comparison_heatmap"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        player_names = "_vs_".join([player["player_name"].replace(" ", "_") for player in player_data])
        filename = f"comparison_heatmap_{player_names}_{season}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "comparison_heatmap"
        }

def create_comparison_zones(
    player_data: List[Dict[str, Any]],
    season: str,
    season_type: str,
    output_format: str = "base64"
) -> Dict[str, Any]:
    """Create a zone efficiency comparison of player shots."""
    # Define zones to compare
    zones = [
        "Above the Break 3",
        "Left Corner 3",
        "Right Corner 3",
        "Mid-Range",
        "In The Paint (Non-RA)",
        "Restricted Area"
    ]

    # Create figure
    fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')

    # Colors for each player
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

    # Set up bar positions
    bar_width = 0.2
    index = np.arange(len(zones))

    # Plot bars for each player
    for i, (player, color) in enumerate(zip(player_data, colors)):
        # Get zone percentages
        zone_breakdown = player["zone_breakdown"]
        percentages = []

        for zone in zones:
            if zone in zone_breakdown:
                percentages.append(zone_breakdown[zone]["percentage"])
            else:
                percentages.append(0)

        # Plot bars
        bars = ax.bar(index + i * bar_width, percentages, bar_width,
                     alpha=0.8, color=color, label=player["player_name"])

        # Add percentage labels
        for bar, percentage in zip(bars, percentages):
            if percentage > 0:
                ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
                       f"{percentage:.1f}%", ha='center', va='bottom', fontsize=8)

    # Add league average line
    league_percentages = []
    for zone in zones:
        # Get league average from first player (should be the same for all)
        if zone in player_data[0]["zone_breakdown"]:
            league_percentages.append(player_data[0]["zone_breakdown"][zone]["league_percentage"])
        else:
            league_percentages.append(0)

    ax.plot(index + bar_width * (len(player_data) - 1) / 2, league_percentages, 'k--', label='League Average')

    # Add labels and title
    ax.set_xlabel('Shot Zone', fontsize=12)
    ax.set_ylabel('Field Goal Percentage (%)', fontsize=12)
    ax.set_title(f'Zone Efficiency Comparison - {season} {season_type}', fontsize=16, weight='bold')
    ax.set_xticks(index + bar_width * (len(player_data) - 1) / 2)
    ax.set_xticklabels(zones, rotation=45, ha='right')
    ax.set_ylim(0, 100)
    ax.grid(True, alpha=0.3, axis='y')
    ax.legend()

    # Adjust layout
    plt.tight_layout()

    # Return the visualization based on output format
    if output_format == 'base64':
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=300, bbox_inches='tight')
        plt.close(fig)
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        return {
            "image_data": f"data:image/png;base64,{image_base64}",
            "chart_type": "comparison_zones"
        }
    else:
        # Save to file
        output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "output")
        os.makedirs(output_dir, exist_ok=True)
        player_names = "_vs_".join([player["player_name"].replace(" ", "_") for player in player_data])
        filename = f"comparison_zones_{player_names}_{season}.png"
        filepath = os.path.join(output_dir, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close(fig)
        return {
            "file_path": filepath,
            "chart_type": "comparison_zones"
        }


===== backend\api_tools\player_dashboard_game.py =====
"""
Handles fetching player dashboard statistics by game splits.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerDashboardByGameSplits endpoint, which provides
detailed player statistics broken down by various game-related splits:
- Overall stats
- By half (first half, second half)
- By period (quarters, overtime)
- By score margin
- By actual margin
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playerdashboardbygamesplits
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailed
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_DASHBOARD_GAME_CACHE_SIZE = 128
PLAYER_DASHBOARD_GAME_CSV_DIR = get_cache_dir("player_dashboard_game")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailed.base,
    "Advanced": MeasureTypeDetailed.advanced,
    "Misc": MeasureTypeDetailed.misc,
    "Four Factors": MeasureTypeDetailed.four_factors,
    "Scoring": MeasureTypeDetailed.scoring,
    "Opponent": MeasureTypeDetailed.opponent,
    "Usage": MeasureTypeDetailed.usage
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_dashboard(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving a dashboard DataFrame as CSV.

    Args:
        player_id: The player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        dashboard_type: The dashboard type (e.g., overall_player_dashboard)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename
    filename = f"{player_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{dashboard_type}.csv"

    return get_cache_file_path(filename, "player_dashboard_game")

# --- Parameter Validation Functions ---
def _validate_dashboard_params(
    player_name: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str
) -> Optional[str]:
    """
    Validates parameters for the player dashboard function.

    Args:
        player_name: Player name or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not player_name:
        return Errors.PLAYER_NAME_EMPTY

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_DASHBOARD_GAME_CACHE_SIZE)
def fetch_player_dashboard_game_splits_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player dashboard statistics by game splits using the PlayerDashboardByGameSplits endpoint.

    This endpoint provides detailed player statistics broken down by various game-related splits:
    - Overall stats
    - By half (first half, second half)
    - By period (quarters, overtime)
    - By score margin
    - By actual margin

    Args:
        player_name (str): Name or ID of the player
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with dashboard data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_dashboard_game_splits_logic for: '{player_name}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_dashboard_params(
        player_name, season, season_type, measure_type, per_mode
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Get player ID
    try:
        player_id_val, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "player_id": player_id_val,
        "season": season,
        "season_type_playoffs": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "league_id_nullable": "00",
        "last_n_games": 0,
        "month": 0,
        "opponent_team_id": 0,
        "period": 0,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling PlayerDashboardByGameSplits with parameters: {filtered_api_params}")
        dashboard_endpoint = playerdashboardbygamesplits.PlayerDashboardByGameSplits(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = dashboard_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = dashboard_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "OverallPlayerDashboard",
            "ByHalfPlayerDashboard",
            "ByPeriodPlayerDashboard",
            "ByScoreMarginPlayerDashboard",
            "ByActualMarginPlayerDashboard"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "player_name": player_actual_name,
            "player_id": player_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_dashboard(
                            player_id_val, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched game splits dashboard for {player_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_player_dashboard_game_splits_logic for {player_name}: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_DASHBOARD_GAME_API.format(
            player_name=player_name, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\player_dashboard_general.py =====
"""
Handles fetching player dashboard statistics by general splits.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerDashboardByGeneralSplits endpoint, which provides
detailed player statistics broken down by various splits:
- Overall stats
- Location (home/away)
- Wins/losses
- Month
- Pre/post All-Star break
- Starting position
- Days rest
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playerdashboardbygeneralsplits
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailed
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_DASHBOARD_GENERAL_CACHE_SIZE = 128
PLAYER_DASHBOARD_GENERAL_CSV_DIR = get_cache_dir("player_dashboard_general")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailed.base,
    "Advanced": MeasureTypeDetailed.advanced,
    "Misc": MeasureTypeDetailed.misc,
    "Four Factors": MeasureTypeDetailed.four_factors,
    "Scoring": MeasureTypeDetailed.scoring,
    "Opponent": MeasureTypeDetailed.opponent,
    "Usage": MeasureTypeDetailed.usage
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_dashboard(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving a dashboard DataFrame as CSV.

    Args:
        player_id: The player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        dashboard_type: The dashboard type (e.g., overall_player_dashboard)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()
    
    # Create filename
    filename = f"{player_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{dashboard_type}.csv"
    
    return get_cache_file_path(filename, "player_dashboard_general")

# --- Parameter Validation Functions ---
def _validate_dashboard_params(
    player_name: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str
) -> Optional[str]:
    """
    Validates parameters for the player dashboard function.

    Args:
        player_name: Player name or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not player_name:
        return Errors.PLAYER_NAME_EMPTY
    
    if not season:
        return Errors.SEASON_EMPTY
    
    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type, 
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )
    
    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type, 
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )
    
    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode, 
            options=", ".join(list(VALID_PER_MODES.keys()))
        )
    
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_DASHBOARD_GENERAL_CACHE_SIZE)
def fetch_player_dashboard_general_splits_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player dashboard statistics by general splits using the PlayerDashboardByGeneralSplits endpoint.
    
    This endpoint provides detailed player statistics broken down by various splits:
    - Overall stats
    - Location (home/away)
    - Wins/losses
    - Month
    - Pre/post All-Star break
    - Starting position
    - Days rest
    
    Args:
        player_name (str): Name or ID of the player
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.
    
    Returns:
        If return_dataframe=False:
            str: JSON string with dashboard data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_dashboard_general_splits_logic for: '{player_name}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}"
    )
    
    dataframes: Dict[str, pd.DataFrame] = {}
    
    # Validate parameters
    validation_error = _validate_dashboard_params(
        player_name, season, season_type, measure_type, per_mode
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)
    
    # Get player ID
    try:
        player_id_val, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    
    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "player_id": player_id_val,
        "season": season,
        "season_type_playoffs": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "league_id_nullable": "00",
        "last_n_games": 0,
        "month": 0,
        "opponent_team_id": 0,
        "period": 0,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }
    
    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}
    
    try:
        logger.debug(f"Calling PlayerDashboardByGeneralSplits with parameters: {filtered_api_params}")
        dashboard_endpoint = playerdashboardbygeneralsplits.PlayerDashboardByGeneralSplits(**api_params)
        
        # Get normalized dictionary for data set names
        normalized_dict = dashboard_endpoint.get_normalized_dict()
        
        # Get data frames
        list_of_dataframes = dashboard_endpoint.get_data_frames()
        
        # Expected data set names based on documentation
        expected_data_set_names = [
            "OverallPlayerDashboard",
            "LocationPlayerDashboard",
            "WinsLossesPlayerDashboard",
            "MonthPlayerDashboard",
            "PrePostAllStarPlayerDashboard",
            "DaysRestPlayerDashboard",
            "StartingPositionPlayerDashboard"
        ]
        
        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "player_name": player_actual_name,
            "player_id": player_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }
        
        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_dashboard(
                            player_id_val, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        # Return response
        logger.info(f"Successfully fetched general splits dashboard for {player_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)
        
    except Exception as e:
        logger.error(
            f"API error in fetch_player_dashboard_general_splits_logic for {player_name}: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_DASHBOARD_GENERAL_API.format(
            player_name=player_name, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\player_dashboard_lastn.py =====
"""
Handles fetching player dashboard statistics by last N games.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerDashboardByLastNGames endpoint, which provides
detailed player statistics broken down by various last N games splits:
- Overall stats
- Last 5 games
- Last 10 games
- Last 15 games
- Last 20 games
- Game number
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playerdashboardbylastngames
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailed
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_DASHBOARD_LASTN_CACHE_SIZE = 128
PLAYER_DASHBOARD_LASTN_CSV_DIR = get_cache_dir("player_dashboard_lastn")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailed.base,
    "Advanced": MeasureTypeDetailed.advanced,
    "Misc": MeasureTypeDetailed.misc,
    "Four Factors": MeasureTypeDetailed.four_factors,
    "Scoring": MeasureTypeDetailed.scoring,
    "Opponent": MeasureTypeDetailed.opponent,
    "Usage": MeasureTypeDetailed.usage
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_dashboard(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving a dashboard DataFrame as CSV.

    Args:
        player_id: The player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        dashboard_type: The dashboard type (e.g., overall_player_dashboard)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename
    filename = f"{player_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{dashboard_type}.csv"

    return get_cache_file_path(filename, "player_dashboard_lastn")

# --- Parameter Validation Functions ---
def _validate_dashboard_params(
    player_name: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str
) -> Optional[str]:
    """
    Validates parameters for the player dashboard function.

    Args:
        player_name: Player name or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not player_name:
        return Errors.PLAYER_NAME_EMPTY

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_DASHBOARD_LASTN_CACHE_SIZE)
def fetch_player_dashboard_lastn_games_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player dashboard statistics by last N games using the PlayerDashboardByLastNGames endpoint.

    This endpoint provides detailed player statistics broken down by various last N games splits:
    - Overall stats
    - Last 5 games
    - Last 10 games
    - Last 15 games
    - Last 20 games
    - Game number

    Args:
        player_name (str): Name or ID of the player
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with dashboard data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_dashboard_lastn_games_logic for: '{player_name}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_dashboard_params(
        player_name, season, season_type, measure_type, per_mode
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Get player ID
    try:
        player_id_val, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "player_id": player_id_val,
        "season": season,
        "season_type_playoffs": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "league_id_nullable": "00",
        "last_n_games": 0,
        "month": 0,
        "opponent_team_id": 0,
        "period": 0,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling PlayerDashboardByLastNGames with parameters: {filtered_api_params}")
        dashboard_endpoint = playerdashboardbylastngames.PlayerDashboardByLastNGames(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = dashboard_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = dashboard_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "OverallPlayerDashboard",
            "Last5PlayerDashboard",
            "Last10PlayerDashboard",
            "Last15PlayerDashboard",
            "Last20PlayerDashboard",
            "GameNumberPlayerDashboard"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "player_name": player_actual_name,
            "player_id": player_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_dashboard(
                            player_id_val, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched last N games dashboard for {player_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_player_dashboard_lastn_games_logic for {player_name}: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_DASHBOARD_LASTN_API.format(
            player_name=player_name, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\player_dashboard_shooting.py =====
"""
Handles fetching player dashboard statistics by shooting splits.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerDashboardByShootingSplits endpoint, which provides
detailed player shooting statistics broken down by various splits:
- Overall shooting stats
- Shot type (jump shots, layups, etc.)
- Shot area (restricted area, paint, mid-range, etc.)
- Shot distance (0-5 ft, 5-8 ft, etc.)
- Assisted/unassisted shots
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playerdashboardbyshootingsplits
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailed
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_DASHBOARD_SHOOTING_CACHE_SIZE = 128
PLAYER_DASHBOARD_SHOOTING_CSV_DIR = get_cache_dir("player_dashboard_shooting")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailed.base,
    "Advanced": MeasureTypeDetailed.advanced,
    "Misc": MeasureTypeDetailed.misc,
    "Four Factors": MeasureTypeDetailed.four_factors,
    "Scoring": MeasureTypeDetailed.scoring,
    "Opponent": MeasureTypeDetailed.opponent,
    "Usage": MeasureTypeDetailed.usage
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_dashboard(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving a dashboard DataFrame as CSV.

    Args:
        player_id: The player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        dashboard_type: The dashboard type (e.g., overall_player_dashboard)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename
    filename = f"{player_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{dashboard_type}.csv"

    return get_cache_file_path(filename, "player_dashboard_shooting")

# --- Parameter Validation Functions ---
def _validate_dashboard_params(
    player_name: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str
) -> Optional[str]:
    """
    Validates parameters for the player dashboard function.

    Args:
        player_name: Player name or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not player_name:
        return Errors.PLAYER_NAME_EMPTY

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_DASHBOARD_SHOOTING_CACHE_SIZE)
def fetch_player_dashboard_shooting_splits_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player dashboard statistics by shooting splits using the PlayerDashboardByShootingSplits endpoint.

    This endpoint provides detailed player shooting statistics broken down by various splits:
    - Overall shooting stats
    - Shot type (jump shots, layups, etc.)
    - Shot area (restricted area, paint, mid-range, etc.)
    - Shot distance (0-5 ft, 5-8 ft, etc.)
    - Assisted/unassisted shots

    Args:
        player_name (str): Name or ID of the player
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with dashboard data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_dashboard_shooting_splits_logic for: '{player_name}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_dashboard_params(
        player_name, season, season_type, measure_type, per_mode
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Get player ID
    try:
        player_id_val, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "player_id": player_id_val,
        "season": season,
        "season_type_playoffs": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "league_id_nullable": "00",
        "last_n_games": 0,
        "month": 0,
        "opponent_team_id": 0,
        "period": 0,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling PlayerDashboardByShootingSplits with parameters: {filtered_api_params}")
        dashboard_endpoint = playerdashboardbyshootingsplits.PlayerDashboardByShootingSplits(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = dashboard_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = dashboard_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "OverallPlayerDashboard",
            "Shot5FTPlayerDashboard",
            "Shot8FTPlayerDashboard",
            "ShotAreaPlayerDashboard",
            "ShotTypePlayerDashboard",
            "ShotTypeSummaryPlayerDashboard",
            "AssitedShotPlayerDashboard",
            "AssistedBy"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "player_name": player_actual_name,
            "player_id": player_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_dashboard(
                            player_id_val, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched shooting splits dashboard for {player_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_player_dashboard_shooting_splits_logic for {player_name}: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_DASHBOARD_SHOOTING_API.format(
            player_name=player_name, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\player_dashboard_stats.py =====
"""
Handles fetching various player dashboard statistics including profile,
defensive stats, and hustle stats.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import os
import logging
import json
from typing import Optional, List, Dict, Any, Set, Union, Tuple
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import (
    playerprofilev2,
    playerdashptshotdefend,
    leaguehustlestatsplayer,
    commonplayerinfo
)
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeDetailed, PerMode36, LeagueID, PerModeSimple, PerModeTime
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_PROFILE_CACHE_SIZE = 256
PLAYER_DEFENSE_CACHE_SIZE = 128
PLAYER_HUSTLE_CACHE_SIZE = 64
MAX_LEAGUE_WIDE_HUSTLE_RESULTS = 200
NBA_API_DEFAULT_TEAM_ID_ALL = 0 # Standard value for 'all teams' or no specific team filter

# --- Cache Directory Setup ---
PLAYER_DASHBOARD_CSV_DIR = get_cache_dir("player_dashboard")
PLAYER_PROFILE_CSV_DIR = get_cache_dir("player_dashboard/profile")
PLAYER_DEFENSE_CSV_DIR = get_cache_dir("player_dashboard/defense")
PLAYER_HUSTLE_CSV_DIR = get_cache_dir("player_dashboard/hustle")

# Validation sets for parameters
_VALID_PER_MODES_PROFILE: Set[str] = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_PER_MODES_PROFILE.update({getattr(PerMode36, attr) for attr in dir(PerMode36) if not attr.startswith('_') and isinstance(getattr(PerMode36, attr), str)})

_VALID_PLAYER_DASHBOARD_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

_VALID_DEFENSE_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

_VALID_HUSTLE_PER_MODES: Set[str] = {getattr(PerModeTime, attr) for attr in dir(PerModeTime) if not attr.startswith('_') and isinstance(getattr(PerModeTime, attr), str)}

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_profile(player_id: str, per_mode: str) -> str:
    """
    Generates a file path for saving player profile DataFrame as CSV.

    Args:
        player_id: The player's ID
        per_mode: The per mode (e.g., 'PerGame', 'Totals')

    Returns:
        Path to the CSV file
    """
    # Clean per mode for filename
    clean_per_mode = per_mode.replace(" ", "_").lower()

    return get_cache_file_path(f"player_{player_id}_profile_{clean_per_mode}.csv", "player_dashboard/profile")

def _get_csv_path_for_player_defense(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    opponent_team_id: int
) -> str:
    """
    Generates a file path for saving player defense DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        opponent_team_id: The opponent team ID

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    # Clean per mode for filename
    clean_per_mode = per_mode.replace(" ", "_").lower()

    return get_cache_file_path(
        f"player_{player_id}_defense_{season}_{clean_season_type}_{clean_per_mode}_{opponent_team_id}.csv",
        "player_dashboard/defense"
    )

def _get_csv_path_for_player_hustle(
    season: str,
    season_type: str,
    per_mode: str,
    player_id: Optional[str] = None,
    team_id: Optional[int] = None,
    league_id: str = LeagueID.nba
) -> str:
    """
    Generates a file path for saving player hustle DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        player_id: The player's ID (optional)
        team_id: The team ID (optional)
        league_id: The league ID (e.g., '00' for NBA)

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    # Clean per mode for filename
    clean_per_mode = per_mode.replace(" ", "_").lower()

    # Create filename based on filters
    if player_id:
        filename = f"player_{player_id}_hustle_{season}_{clean_season_type}_{clean_per_mode}.csv"
    elif team_id:
        filename = f"team_{team_id}_hustle_{season}_{clean_season_type}_{clean_per_mode}.csv"
    else:
        filename = f"league_{league_id}_hustle_{season}_{clean_season_type}_{clean_per_mode}.csv"

    return get_cache_file_path(filename, "player_dashboard/hustle")

# --- Helper for Parameter Validation ---
def _validate_common_dashboard_params(
    season: Optional[str],
    season_type: str,
    valid_season_types: Set[str],
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> Optional[str]:
    """Validates common parameters for dashboard stats functions."""
    if season and not _validate_season_format(season): # season can be None for some hustle stats calls
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if date_from and not validate_date_format(date_from):
        return Errors.INVALID_DATE_FORMAT.format(date=date_from)
    if date_to and not validate_date_format(date_to):
        return Errors.INVALID_DATE_FORMAT.format(date=date_to)
    if season_type not in valid_season_types:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(valid_season_types)[:5]))
    return None

# --- Logic Functions ---
def fetch_player_profile_logic(
    player_name: str,
    per_mode: Optional[str] = PerModeDetailed.per_game,
    league_id: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches comprehensive player profile information including career stats, season stats,
    highs, and next game details.
    Provides DataFrame output capabilities.

    Args:
        player_name (str): The name or ID of the player.
        per_mode (str, optional): Statistical mode for career/season stats (e.g., "PerGame").
                                  Defaults to "PerGame".
        league_id (str, optional): League ID to filter data. Defaults to None (NBA).
                                  Example: "00" for NBA.
        return_dataframe (bool, optional): Whether to return DataFrames along with the JSON response.
                                          Defaults to False.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing the player's profile data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    # Store DataFrames if requested
    dataframes = {}

    effective_per_mode = per_mode
    if effective_per_mode is None or effective_per_mode == "None":
        effective_per_mode = PerModeDetailed.per_game
        logger.info(f"per_mode was None or 'None', defaulting to {effective_per_mode} for {player_name}")

    logger.info(f"Executing fetch_player_profile_logic for: '{player_name}', Effective PerMode: {effective_per_mode}, return_dataframe={return_dataframe}")

    if effective_per_mode not in _VALID_PER_MODES_PROFILE:
        error_msg = Errors.INVALID_PER_MODE.format(value=effective_per_mode, options=", ".join(list(_VALID_PER_MODES_PROFILE)[:5]))
        logger.warning(error_msg)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playerprofilev2 for ID: {player_id}, PerMode: {effective_per_mode}, LeagueID: {league_id}")
        try:
            profile_endpoint = playerprofilev2.PlayerProfileV2(
                player_id=player_id,
                per_mode36=effective_per_mode,
                league_id_nullable=league_id,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            logger.debug(f"playerprofilev2 API call object created for ID: {player_id}")
        except Exception as api_error:
            logger.error(f"nba_api playerprofilev2 instantiation or initial request failed for ID {player_id}: {api_error}", exc_info=True)
            # Attempt to log raw response if JSONDecodeError
            raw_response_text = None
            if isinstance(api_error, json.JSONDecodeError) and hasattr(profile_endpoint, 'nba_response') and profile_endpoint.nba_response and hasattr(profile_endpoint.nba_response, '_response'):
                raw_response_text = profile_endpoint.nba_response._response
                logger.error(f"Raw NBA API response that caused JSONDecodeError (first 500 chars): {raw_response_text[:500] if raw_response_text else 'N/A'}")
            error_msg = Errors.PLAYER_PROFILE_API.format(identifier=player_actual_name, error=str(api_error))
            if return_dataframe:
                return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        def get_df_safe(dataset_name: str) -> pd.DataFrame:
            """Safely retrieves a DataFrame from the endpoint, returning an empty DataFrame on error."""
            if hasattr(profile_endpoint, dataset_name):
                dataset = getattr(profile_endpoint, dataset_name)
                if hasattr(dataset, 'get_data_frame'):
                    return dataset.get_data_frame()
            logger.warning(f"Dataset '{dataset_name}' not found or not a valid DataSet in PlayerProfileV2 for {player_actual_name} (ID: {player_id}).")
            return pd.DataFrame()

        career_totals_rs_df = get_df_safe('career_totals_regular_season')
        season_totals_rs_df = get_df_safe('season_totals_regular_season')
        career_totals_ps_df = get_df_safe('career_totals_post_season')
        season_totals_ps_df = get_df_safe('season_totals_post_season')
        season_highs_df = get_df_safe('season_highs')
        career_highs_df = get_df_safe('career_highs')
        next_game_df = get_df_safe('next_game')

        # Save DataFrames if requested
        if return_dataframe:
            dataframes["career_totals_regular_season"] = career_totals_rs_df
            dataframes["season_totals_regular_season"] = season_totals_rs_df
            dataframes["career_totals_post_season"] = career_totals_ps_df
            dataframes["season_totals_post_season"] = season_totals_ps_df
            dataframes["season_highs"] = season_highs_df
            dataframes["career_highs"] = career_highs_df
            dataframes["next_game"] = next_game_df

            # Save to CSV if not empty
            if not career_totals_rs_df.empty:
                csv_path = _get_csv_path_for_player_profile(player_id, effective_per_mode)
                _save_dataframe_to_csv(career_totals_rs_df, csv_path)

            if not season_totals_rs_df.empty:
                csv_path = get_cache_file_path(f"player_{player_id}_season_totals_rs_{effective_per_mode.replace(' ', '_').lower()}.csv", "player_dashboard/profile")
                _save_dataframe_to_csv(season_totals_rs_df, csv_path)

            if not career_totals_ps_df.empty:
                csv_path = get_cache_file_path(f"player_{player_id}_career_totals_ps_{effective_per_mode.replace(' ', '_').lower()}.csv", "player_dashboard/profile")
                _save_dataframe_to_csv(career_totals_ps_df, csv_path)

            if not season_totals_ps_df.empty:
                csv_path = get_cache_file_path(f"player_{player_id}_season_totals_ps_{effective_per_mode.replace(' ', '_').lower()}.csv", "player_dashboard/profile")
                _save_dataframe_to_csv(season_totals_ps_df, csv_path)

        season_highs_dict = _process_dataframe(season_highs_df, single_row=True)
        career_highs_dict = _process_dataframe(career_highs_df, single_row=True)
        next_game_dict = _process_dataframe(next_game_df, single_row=True)

        career_totals_rs = _process_dataframe(career_totals_rs_df, single_row=True)
        season_totals_rs_list = _process_dataframe(season_totals_rs_df, single_row=False)
        career_totals_ps = _process_dataframe(career_totals_ps_df, single_row=True)
        season_totals_ps_list = _process_dataframe(season_totals_ps_df, single_row=False)

        if career_totals_rs is None or season_totals_rs_list is None:
            logger.error(f"Essential profile data (regular season totals) processing failed for {player_actual_name}.")
            error_msg = Errors.PLAYER_PROFILE_PROCESSING.format(identifier=player_actual_name)
            if return_dataframe:
                return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        # Fetch player info from commonplayerinfo endpoint
        try:
            logger.debug(f"Fetching commonplayerinfo for player ID: {player_id} ({player_actual_name})")
            info_endpoint = commonplayerinfo.CommonPlayerInfo(player_id=player_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            player_info_df = info_endpoint.common_player_info.get_data_frame()
            player_info_dict = _process_dataframe(player_info_df, single_row=True)

            if player_info_dict is None:
                logger.warning(f"Failed to process player info for {player_actual_name}. Using empty dict.")
                player_info_dict = {}
        except Exception as info_error:
            logger.error(f"Error fetching commonplayerinfo for {player_actual_name}: {info_error}", exc_info=True)
            player_info_dict = {}

        logger.info(f"fetch_player_profile_logic completed for '{player_actual_name}'")
        response_data = {
            "player_name": player_actual_name, "player_id": player_id,
            "player_info": player_info_dict,
            "parameters": {
                "per_mode": effective_per_mode,
                "league_id": league_id
            },
            "career_highs": career_highs_dict or {},
            "season_highs": season_highs_dict or {},
            "next_game": next_game_dict or {},
            "career_totals": {
                "regular_season": career_totals_rs or {},
                "post_season": career_totals_ps or {}
            },
            "season_totals": {
                "regular_season": season_totals_rs_list or [],
                "post_season": season_totals_ps_list or []
            }
        }

        if return_dataframe:
            # Add DataFrame info to the response
            response_data["dataframe_info"] = {
                "message": "Player profile data has been converted to DataFrames and saved as CSV files",
                "dataframes": {
                    "player_info": {
                        "shape": list(player_info_df.shape) if 'player_info_df' in locals() and not player_info_df.empty else [],
                        "columns": player_info_df.columns.tolist() if 'player_info_df' in locals() and not player_info_df.empty else [],
                        "csv_path": get_relative_cache_path(f"player_{player_id}_info.csv", "player_dashboard/profile")
                    },
                    "career_totals_regular_season": {
                        "shape": list(career_totals_rs_df.shape) if not career_totals_rs_df.empty else [],
                        "columns": career_totals_rs_df.columns.tolist() if not career_totals_rs_df.empty else [],
                        "csv_path": get_relative_cache_path(f"player_{player_id}_profile_{effective_per_mode.replace(' ', '_').lower()}.csv", "player_dashboard/profile")
                    },
                    "season_totals_regular_season": {
                        "shape": list(season_totals_rs_df.shape) if not season_totals_rs_df.empty else [],
                        "columns": season_totals_rs_df.columns.tolist() if not season_totals_rs_df.empty else [],
                        "csv_path": get_relative_cache_path(f"player_{player_id}_season_totals_rs_{effective_per_mode.replace(' ', '_').lower()}.csv", "player_dashboard/profile")
                    }
                }
            }

            # Add player_info DataFrame to the dataframes dictionary if available
            if 'player_info_df' in locals() and not player_info_df.empty:
                dataframes["player_info"] = player_info_df

            # Save player_info DataFrame to CSV if available
            if 'player_info_df' in locals() and not player_info_df.empty:
                csv_path = get_cache_file_path(f"player_{player_id}_info.csv", "player_dashboard/profile")
                _save_dataframe_to_csv(player_info_df, csv_path)

            return format_response(response_data), dataframes

        return format_response(response_data)
    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_profile_logic: {e}")
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_profile_logic: {e}")
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_profile_logic for '{player_name}': {e}", exc_info=True)
        error_msg = Errors.PLAYER_PROFILE_UNEXPECTED.format(identifier=player_name, error=str(e))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

@lru_cache(maxsize=PLAYER_DEFENSE_CACHE_SIZE)
def fetch_player_defense_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.per_game, # Note: API uses PerModeSimple for this endpoint
    opponent_team_id: int = NBA_API_DEFAULT_TEAM_ID_ALL,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    last_n_games: int = 0,
    league_id: str = LeagueID.nba,
    month: int = 0,
    period: int = 0,
    team_id: int = NBA_API_DEFAULT_TEAM_ID_ALL,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None,
    season_segment: Optional[str] = None,
    outcome: Optional[str] = None,
    location: Optional[str] = None,
    game_segment: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player defensive stats against various opponents or overall.
    Provides DataFrame output capabilities.

    Args:
        player_name (str): The name or ID of the player.
        season (str, optional): The season in YYYY-YY format. Defaults to current NBA season.
        season_type (str, optional): Type of season (e.g., "Regular Season"). Defaults to "Regular Season".
        per_mode (str, optional): Statistical mode (e.g., "PerGame"). Defaults to "PerGame".
                                 Note: Endpoint uses PerModeSimple.
        opponent_team_id (int, optional): Opponent's team ID. Defaults to 0 (all opponents).
        date_from (Optional[str], optional): Start date (YYYY-MM-DD). Defaults to None.
        date_to (Optional[str], optional): End date (YYYY-MM-DD). Defaults to None.
        last_n_games (int, optional): Number of most recent games to include. Defaults to 0 (all games).
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        month (int, optional): Month number (1-12). Defaults to 0 (all months).
        period (int, optional): Period number (1-4, 0 for all). Defaults to 0 (all periods).
        team_id (int, optional): Team ID of the player. Defaults to 0 (all teams).
        vs_conference (Optional[str], optional): Conference filter (e.g., "East", "West"). Defaults to None.
        vs_division (Optional[str], optional): Division filter. Defaults to None.
        season_segment (Optional[str], optional): Season segment filter (e.g., "Post All-Star"). Defaults to None.
        outcome (Optional[str], optional): Game outcome filter (e.g., "W", "L"). Defaults to None.
        location (Optional[str], optional): Game location filter (e.g., "Home", "Road"). Defaults to None.
        game_segment (Optional[str], optional): Game segment filter (e.g., "First Half"). Defaults to None.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with defensive stats or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    dataframes: Dict[str, pd.DataFrame] = {}
    logger.info(
        f"Executing fetch_player_defense_logic for: '{player_name}', Season: {season}, "
        f"SeasonType: {season_type}, PerMode: {per_mode}, OpponentTeamID: {opponent_team_id}, "
        f"DateFrom: {date_from}, DateTo: {date_to}, LastNGames: {last_n_games}, "
        f"LeagueID: {league_id}, Month: {month}, Period: {period}, TeamID: {team_id}, "
        f"VsConference: {vs_conference}, VsDivision: {vs_division}, SeasonSegment: {season_segment}, "
        f"Outcome: {outcome}, Location: {location}, GameSegment: {game_segment}, "
        f"DataFrame: {return_dataframe}"
    )

    # Validate parameters
    param_error = _validate_common_dashboard_params(season, season_type, _VALID_PLAYER_DASHBOARD_SEASON_TYPES, date_from, date_to)
    if param_error:
        logger.warning(param_error)
        if return_dataframe: return format_response(error=param_error), dataframes
        return format_response(error=param_error)

    if per_mode not in _VALID_DEFENSE_PER_MODES: # Ensure using the correct validation set
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_DEFENSE_PER_MODES)[:5]))
        logger.warning(error_msg)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        logger.warning(f"Player not found: {e}")
        if return_dataframe: return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        logger.debug(f"Fetching playerdashptshotdefend for ID: {player_id}, Season: {season}, PerMode: {per_mode}")
        defense_endpoint = playerdashptshotdefend.PlayerDashPtShotDefend(
            player_id=player_id,
            team_id=team_id,  # Player's team ID
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            opponent_team_id=opponent_team_id,
            date_from_nullable=date_from,
            date_to_nullable=date_to,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            period=period,
            vs_conference_nullable=vs_conference,
            vs_division_nullable=vs_division,
            season_segment_nullable=season_segment,
            outcome_nullable=outcome,
            location_nullable=location,
            game_segment_nullable=game_segment,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        def_roll_up_df = defense_endpoint.defending_shots.get_data_frame() # This is the primary dataset

        if return_dataframe:
            dataframes["defending_shots"] = def_roll_up_df
            if not def_roll_up_df.empty:
                csv_path = _get_csv_path_for_player_defense(
                    player_id, season, season_type, per_mode, opponent_team_id
                )
                _save_dataframe_to_csv(def_roll_up_df, csv_path)

        processed_def_roll_up = _process_dataframe(def_roll_up_df, single_row=False) if not def_roll_up_df.empty else []

        if processed_def_roll_up is None and not def_roll_up_df.empty: # Processing error
            error_msg = Errors.PLAYER_DEFENSE_PROCESSING.format(identifier=player_actual_name, season=season)
            logger.error(error_msg)
            if return_dataframe: return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        # Construct the response
        response_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "opponent_team_id": opponent_team_id,
                "date_from": date_from,
                "date_to": date_to,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "period": period,
                "team_id": team_id,
                "vs_conference": vs_conference,
                "vs_division": vs_division,
                "season_segment": season_segment,
                "outcome": outcome,
                "location": location,
                "game_segment": game_segment
            },
            "defending_shots": processed_def_roll_up
        }
        logger.info(f"Successfully fetched player defense stats for {player_actual_name}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(f"API error in fetch_player_defense_logic for {player_actual_name}: {e}", exc_info=True)
        error_msg = Errors.PLAYER_DEFENSE_API.format(identifier=player_actual_name, error=str(e))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

@lru_cache(maxsize=PLAYER_HUSTLE_CACHE_SIZE)
def fetch_player_hustle_stats_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeTime.per_game, # Note: API uses PerModeTime for this endpoint
    player_name: Optional[str] = None,
    team_id: Optional[int] = None, # Team ID to filter by. Use 0 for all teams if player_name is None.
    league_id: str = LeagueID.nba,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    college: Optional[str] = None,
    conference: Optional[str] = None,
    country: Optional[str] = None,
    division: Optional[str] = None,
    draft_pick: Optional[str] = None,
    draft_year: Optional[str] = None,
    height: Optional[str] = None,
    location: Optional[str] = None,
    month: Optional[int] = None,
    opponent_team_id: Optional[int] = None,
    outcome: Optional[str] = None,
    po_round: Optional[str] = None,
    player_experience: Optional[str] = None,
    player_position: Optional[str] = None,
    season_segment: Optional[str] = None,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None,
    weight: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player or team hustle statistics (e.g., screen assists, deflections).
    Can fetch for a specific player, a specific team, or league-wide.
    Provides DataFrame output capabilities.

    Args:
        season (str, optional): The season in YYYY-YY format. Defaults to current NBA season.
        season_type (str, optional): Type of season. Defaults to "Regular Season".
        per_mode (str, optional): Statistical mode. Defaults to "PerGame". Endpoint uses PerModeTime.
        player_name (Optional[str], optional): Name or ID of the player. Defaults to None (league-wide).
        team_id (Optional[int], optional): Team ID. Defaults to None. If player_name is None and team_id is None,
                                          fetches league-wide stats. Use 0 for all teams when player_name is None.
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        date_from (Optional[str], optional): Start date (YYYY-MM-DD). Defaults to None.
        date_to (Optional[str], optional): End date (YYYY-MM-DD). Defaults to None.
        college (Optional[str], optional): College filter. Defaults to None.
        conference (Optional[str], optional): Conference filter. Defaults to None.
        country (Optional[str], optional): Country filter. Defaults to None.
        division (Optional[str], optional): Division filter. Defaults to None.
        draft_pick (Optional[str], optional): Draft pick filter. Defaults to None.
        draft_year (Optional[str], optional): Draft year filter. Defaults to None.
        height (Optional[str], optional): Height filter. Defaults to None.
        location (Optional[str], optional): Game location filter (e.g., "Home", "Road"). Defaults to None.
        month (Optional[int], optional): Month number (1-12). Defaults to None.
        opponent_team_id (Optional[int], optional): Opponent team ID filter. Defaults to None.
        outcome (Optional[str], optional): Game outcome filter (e.g., "W", "L"). Defaults to None.
        po_round (Optional[str], optional): Playoff round filter. Defaults to None.
        player_experience (Optional[str], optional): Player experience filter. Defaults to None.
        player_position (Optional[str], optional): Player position filter. Defaults to None.
        season_segment (Optional[str], optional): Season segment filter (e.g., "Post All-Star"). Defaults to None.
        vs_conference (Optional[str], optional): Conference filter (e.g., "East", "West"). Defaults to None.
        vs_division (Optional[str], optional): Division filter. Defaults to None.
        weight (Optional[str], optional): Weight filter. Defaults to None.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with hustle stats or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    dataframes: Dict[str, pd.DataFrame] = {}
    player_id_resolved: Optional[str] = None
    player_actual_name_resolved: Optional[str] = None
    effective_team_id: int = team_id if team_id is not None else NBA_API_DEFAULT_TEAM_ID_ALL

    log_identifier = "league-wide"
    if player_name:
        log_identifier = f"player '{player_name}'"
    elif team_id is not None: # team_id could be 0 for all teams, or a specific team
        log_identifier = f"team ID '{team_id}'"


    logger.info(
        f"Executing fetch_player_hustle_stats_logic for {log_identifier}, Season: {season}, "
        f"SeasonType: {season_type}, PerMode: {per_mode}, LeagueID: {league_id}, "
        f"DateFrom: {date_from}, DateTo: {date_to}, College: {college}, Conference: {conference}, "
        f"Country: {country}, Division: {division}, DraftPick: {draft_pick}, DraftYear: {draft_year}, "
        f"Height: {height}, Location: {location}, Month: {month}, OpponentTeamID: {opponent_team_id}, "
        f"Outcome: {outcome}, PORound: {po_round}, PlayerExperience: {player_experience}, "
        f"PlayerPosition: {player_position}, SeasonSegment: {season_segment}, "
        f"VsConference: {vs_conference}, VsDivision: {vs_division}, Weight: {weight}, "
        f"DataFrame: {return_dataframe}"
    )

    # Validate parameters
    param_error = _validate_common_dashboard_params(season, season_type, _VALID_PLAYER_DASHBOARD_SEASON_TYPES, date_from, date_to)
    if param_error:
        logger.warning(param_error)
        if return_dataframe: return format_response(error=param_error), dataframes
        return format_response(error=param_error)

    if per_mode not in _VALID_HUSTLE_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_HUSTLE_PER_MODES)[:5]))
        logger.warning(error_msg)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Resolve player_id if player_name is provided
    if player_name:
        try:
            player_id_resolved, player_actual_name_resolved = find_player_id_or_error(player_name)
            # If a player is specified, team_id is often ignored by the hustle endpoint or means "player's team stats"
            # For leaguehustlestatsplayer, PlayerID takes precedence. TeamID is used if PlayerID is not set.
            effective_team_id = NBA_API_DEFAULT_TEAM_ID_ALL # Usually, for player-specific hustle, team context is not a direct filter on this endpoint.
        except PlayerNotFoundError as e:
            logger.warning(f"Player not found for hustle stats: {e}")
            if return_dataframe: return format_response(error=str(e)), dataframes
            return format_response(error=str(e))

    # NBA_API Specifics:
    # - If PlayerID is set, TeamID is for context but data is for the player.
    # - If PlayerID is NOT set, TeamID is used to filter for a team's hustle stats.
    # - If PlayerID and TeamID are NOT set (or team_id=0), it's league-wide.

    api_player_id_param = player_id_resolved if player_id_resolved else "0" # API expects "0" if no specific player
    # If player_id_resolved is None (no player_name given), then effective_team_id (which is team_id or 0) is used.
    # If player_id_resolved is set, the API typically uses this and team_id becomes contextual or ignored.
    # For leaguehustlestatsplayer, if player_id_resolved is set, team_id param is less critical.
    # If player_id_resolved is NOT set, team_id (or 0 for league-wide) is the filter.
    api_team_id_param = effective_team_id

    if player_id_resolved: # Specific player
        api_team_id_param = NBA_API_DEFAULT_TEAM_ID_ALL # For specific player, API often wants team_id=0
    # else: api_team_id_param remains effective_team_id (user's team_id or 0 for league-wide)


    try:
        logger.debug(
            f"Fetching leaguehustlestatsplayer - PlayerID: {api_player_id_param}, TeamID: {api_team_id_param}, "
            f"Season: {season}, PerMode: {per_mode}, LeagueID: {league_id}"
        )
        hustle_endpoint = leaguehustlestatsplayer.LeagueHustleStatsPlayer(
            per_mode_time=per_mode,
            season_type_all_star=season_type,
            season=season,
            league_id_nullable=league_id,
            date_from_nullable=date_from,
            date_to_nullable=date_to,
            college_nullable=college,
            conference_nullable=conference,
            country_nullable=country,
            division_simple_nullable=division,
            draft_pick_nullable=draft_pick,
            draft_year_nullable=draft_year,
            height_nullable=height,
            location_nullable=location,
            month_nullable=month,
            opponent_team_id_nullable=opponent_team_id,
            outcome_nullable=outcome,
            po_round_nullable=po_round,
            player_experience_nullable=player_experience,
            player_position_nullable=player_position,
            season_segment_nullable=season_segment,
            vs_conference_nullable=vs_conference,
            vs_division_nullable=vs_division,
            weight_nullable=weight,
            team_id_nullable=api_team_id_param,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"LeagueHustleStatsPlayer API call successful for {log_identifier}")

        hustle_stats_df = hustle_endpoint.hustle_stats_player.get_data_frame()

        # If a specific player_id was intended, filter the league-wide results
        if player_id_resolved and not hustle_stats_df.empty:
            hustle_stats_df = hustle_stats_df[hustle_stats_df['PLAYER_ID'] == player_id_resolved].reset_index(drop=True)
            if hustle_stats_df.empty:
                logger.warning(f"Player ID {player_id_resolved} not found in LeagueHustleStatsPlayer results for {log_identifier}")

        if return_dataframe:
            dataframes["hustle_stats"] = hustle_stats_df
            if not hustle_stats_df.empty:
                csv_path = _get_csv_path_for_player_hustle(
                    season, season_type, per_mode, player_id_resolved, team_id if not player_id_resolved else None, league_id
                )
                _save_dataframe_to_csv(hustle_stats_df, csv_path)

        # Limit results if league-wide and too many rows
        if not player_id_resolved and (team_id is None or team_id == NBA_API_DEFAULT_TEAM_ID_ALL) and len(hustle_stats_df) > MAX_LEAGUE_WIDE_HUSTLE_RESULTS:
            logger.warning(f"League-wide hustle stats query returned {len(hustle_stats_df)} results. Capping at {MAX_LEAGUE_WIDE_HUSTLE_RESULTS} for response.")
            hustle_stats_df_limited = hustle_stats_df.head(MAX_LEAGUE_WIDE_HUSTLE_RESULTS)
            processed_hustle_stats = _process_dataframe(hustle_stats_df_limited, single_row=False)
            additional_info = f" (Results capped at {MAX_LEAGUE_WIDE_HUSTLE_RESULTS})"
        else:
            processed_hustle_stats = _process_dataframe(hustle_stats_df, single_row=False) if not hustle_stats_df.empty else []
            additional_info = ""


        if processed_hustle_stats is None and not hustle_stats_df.empty : # Processing error
            error_msg = Errors.PLAYER_HUSTLE_PROCESSING.format(identifier=log_identifier, season=season)
            logger.error(error_msg)
            if return_dataframe: return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        # Construct the response
        response_data = {
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "player_name_filter": player_actual_name_resolved,
                "player_id_filter": player_id_resolved,
                "team_id_filter": team_id, # Original team_id from request
                "league_id": league_id,
                "date_from": date_from,
                "date_to": date_to,
                "college": college,
                "conference": conference,
                "country": country,
                "division": division,
                "draft_pick": draft_pick,
                "draft_year": draft_year,
                "height": height,
                "location": location,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "outcome": outcome,
                "po_round": po_round,
                "player_experience": player_experience,
                "player_position": player_position,
                "season_segment": season_segment,
                "vs_conference": vs_conference,
                "vs_division": vs_division,
                "weight": weight,
                "info": f"Hustle stats for {log_identifier}{additional_info}"
            },
            "hustle_stats": processed_hustle_stats
        }
        logger.info(f"Successfully fetched hustle stats for {log_identifier}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(f"API error in fetch_player_hustle_stats_logic for {log_identifier}: {e}", exc_info=True)
        error_msg = Errors.PLAYER_HUSTLE_API.format(identifier=log_identifier, error=str(e))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\player_dashboard_team_performance.py =====
"""
Handles fetching player dashboard statistics by team performance.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playerdashboardbyteamperformance
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailed
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_TEAM_PERFORMANCE_CSV_DIR = get_cache_dir("player_team_performance")

# --- Parameter Validation Sets ---
_VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_MEASURE_TYPES = {getattr(MeasureTypeDetailed, attr) for attr in dir(MeasureTypeDetailed) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailed, attr), str)}
_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_performance(
    player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving player team performance DataFrame as CSV.

    Args:
        player_id: The player's ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        measure_type: The measure type (e.g., 'Base', 'Advanced')
        dashboard_type: The dashboard type (e.g., 'overall', 'points_scored')

    Returns:
        Path to the CSV file
    """
    # Clean parameters for filename
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()
    clean_measure_type = measure_type.replace(" ", "_").lower()

    return get_cache_file_path(
        f"player_{player_id}_team_performance_{dashboard_type}_{season}_{clean_season_type}_{clean_per_mode}_{clean_measure_type}.csv",
        "player_team_performance"
    )

def fetch_player_dashboard_by_team_performance_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.totals,
    measure_type: str = MeasureTypeDetailed.base,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    period: int = 0,
    plus_minus: str = "N",
    rank: str = "N",
    vs_division: Optional[str] = None,
    vs_conference: Optional[str] = None,
    shot_clock_range: Optional[str] = None,
    season_segment: Optional[str] = None,
    po_round: Optional[int] = None,
    outcome: Optional[str] = None,
    location: Optional[str] = None,
    league_id: Optional[str] = None,
    game_segment: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player dashboard statistics by team performance.

    Args:
        player_name: Name or ID of the player
        season: Season in YYYY-YY format
        season_type: Type of season (e.g., 'Regular Season', 'Playoffs')
        per_mode: Statistical mode (e.g., 'PerGame', 'Totals')
        measure_type: Measure type (e.g., 'Base', 'Advanced')
        last_n_games: Number of most recent games to include
        month: Filter by month (0 for all)
        opponent_team_id: Filter by opponent team ID
        pace_adjust: Whether to adjust for pace ('Y' or 'N')
        period: Filter by period (0 for all)
        plus_minus: Whether to include plus/minus ('Y' or 'N')
        rank: Whether to include statistical ranks ('Y' or 'N')
        vs_division: Filter by division
        vs_conference: Filter by conference
        shot_clock_range: Filter by shot clock range
        season_segment: Filter by season segment
        po_round: Filter by playoff round
        outcome: Filter by game outcome ('W' or 'L')
        location: Filter by game location ('Home' or 'Road')
        league_id: League ID
        game_segment: Filter by game segment
        date_from: Start date filter (YYYY-MM-DD)
        date_to: End date filter (YYYY-MM-DD)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: A JSON string containing the player's team performance data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    # Store DataFrames if requested
    dataframes = {}

    logger.info(f"Executing fetch_player_dashboard_by_team_performance_logic for '{player_name}', season {season}, type {season_type}, return_dataframe={return_dataframe}")

    # Validate season
    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate dates
    if date_from and not validate_date_format(date_from):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_from)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_to and not validate_date_format(date_to):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_to)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate season type
    if season_type not in _VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate per_mode
    if per_mode not in _VALID_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate measure_type
    if measure_type not in _VALID_MEASURE_TYPES:
        error_msg = Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_MEASURE_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate league_id
    if league_id and league_id not in _VALID_LEAGUE_IDS:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_LEAGUE_IDS)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError: {e}")
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))
    except Exception as e:
        logger.error(f"Unexpected error finding player: {e}", exc_info=True)
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        endpoint = playerdashboardbyteamperformance.PlayerDashboardByTeamPerformance(
            player_id=player_id,
            season=season,
            season_type_playoffs=season_type,
            per_mode_detailed=per_mode,
            measure_type_detailed=measure_type,
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            vs_division_nullable=vs_division,
            vs_conference_nullable=vs_conference,
            shot_clock_range_nullable=shot_clock_range,
            season_segment_nullable=season_segment,
            po_round_nullable=po_round,
            outcome_nullable=outcome,
            location_nullable=location,
            league_id_nullable=league_id,
            game_segment_nullable=game_segment,
            date_from_nullable=date_from,
            date_to_nullable=date_to,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        datasets = endpoint.get_data_frames()
        dataset_names = [
            "overall_player_dashboard",
            "points_scored_player_dashboard",
            "ponts_against_player_dashboard",
            "score_differential_player_dashboard"
        ]

        # Store DataFrames and save to CSV if requested
        if return_dataframe:
            for idx, name in enumerate(dataset_names):
                if idx < len(datasets):
                    df = datasets[idx]
                    dataframes[name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_team_performance(
                            player_id=player_id,
                            season=season,
                            season_type=season_type,
                            per_mode=per_mode,
                            measure_type=measure_type,
                            dashboard_type=name
                        )
                        _save_dataframe_to_csv(df, csv_path)

        # Process DataFrames for JSON response
        result = {}
        for idx, name in enumerate(dataset_names):
            df = datasets[idx] if idx < len(datasets) else pd.DataFrame()
            result[name] = _process_dataframe(df, single_row=False)

        # Create response data
        response_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "measure_type": measure_type,
                "last_n_games": last_n_games,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "pace_adjust": pace_adjust,
                "period": period,
                "plus_minus": plus_minus,
                "rank": rank,
                "vs_division": vs_division,
                "vs_conference": vs_conference,
                "shot_clock_range": shot_clock_range,
                "season_segment": season_segment,
                "po_round": po_round,
                "outcome": outcome,
                "location": location,
                "league_id": league_id,
                "game_segment": game_segment,
                "date_from": date_from,
                "date_to": date_to
            },
            "team_performance_dashboards": result
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_paths = {}
            for name in dataset_names:
                if name in dataframes and not dataframes[name].empty:
                    csv_path = _get_csv_path_for_team_performance(
                        player_id=player_id,
                        season=season,
                        season_type=season_type,
                        per_mode=per_mode,
                        measure_type=measure_type,
                        dashboard_type=name
                    )
                    csv_paths[name] = get_relative_cache_path(os.path.basename(csv_path), "player_team_performance")

            response_data["dataframe_info"] = {
                "message": "Player team performance data has been converted to DataFrames and saved as CSV files",
                "dataframes": {
                    name: {
                        "shape": list(dataframes[name].shape) if name in dataframes and not dataframes[name].empty else [],
                        "columns": dataframes[name].columns.tolist() if name in dataframes and not dataframes[name].empty else [],
                        "csv_path": csv_paths.get(name, "")
                    } for name in dataset_names if name in dataframes
                }
            }

            return format_response(response_data), dataframes

        return format_response(response_data)

    except Exception as api_error:
        logger.error(f"nba_api playerdashboardbyteamperformance failed: {api_error}", exc_info=True)
        error_msg = Errors.PLAYER_DASHBOARD_TEAM_PERFORMANCE_API.format(identifier=player_actual_name, error=str(api_error)) if hasattr(Errors, "PLAYER_DASHBOARD_TEAM_PERFORMANCE_API") else str(api_error)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\player_estimated_metrics.py =====
"""
Handles fetching player estimated metrics (E_OFF_RATING, E_DEF_RATING, E_NET_RATING, etc.)
for a given season and season type.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import os
import logging
from functools import lru_cache
from typing import Union, Tuple, Dict, Optional

import pandas as pd

from nba_api.stats.endpoints import playerestimatedmetrics
from nba_api.stats.library.parameters import LeagueID, SeasonTypeAllStar
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import settings
from core.errors import Errors
from api_tools.utils import _process_dataframe, format_response
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_ESTIMATED_METRICS_CSV_DIR = get_cache_dir("player_estimated_metrics")

# --- Module-level constants for validation sets ---
_VALID_PEM_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}
_VALID_PEM_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_estimated_metrics(
    season: str,
    season_type: str,
    league_id: str
) -> str:
    """
    Generates a file path for saving player estimated metrics DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        league_id: The league ID (e.g., '00' for NBA)

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    return get_cache_file_path(
        f"player_estimated_metrics_{season}_{clean_season_type}_{league_id}.csv",
        "player_estimated_metrics"
    )

def fetch_player_estimated_metrics_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    league_id: str = LeagueID.nba,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, pd.DataFrame]]:
    """
    Fetches player estimated metrics (E_OFF_RATING, E_DEF_RATING, E_NET_RATING, etc.)
    for a given season and season type using nba_api's PlayerEstimatedMetrics endpoint.

    Args:
        season: NBA season in 'YYYY-YY' format (e.g., '2023-24').
        season_type: Season type (e.g., 'Regular Season', 'Playoffs').
        league_id: League ID (default: NBA '00').
        return_dataframe: Whether to return DataFrame along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON-formatted string with estimated metrics or an error message.
        If return_dataframe=True:
            Tuple[str, pd.DataFrame]: A tuple containing the JSON response string
                                     and the DataFrame with estimated metrics.
    """
    logger.info(f"Executing fetch_player_estimated_metrics_logic for Season: {season}, Season Type: {season_type}, League ID: {league_id}, return_dataframe={return_dataframe}")

    # Create an empty DataFrame for error cases
    empty_df = pd.DataFrame()

    if not _validate_season_format(season):
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season)), empty_df
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))

    if league_id not in _VALID_PEM_LEAGUE_IDS:
        if return_dataframe:
            return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_PEM_LEAGUE_IDS)[:3]))), empty_df
        return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_VALID_PEM_LEAGUE_IDS)[:3])))

    if season_type not in _VALID_PEM_SEASON_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_PEM_SEASON_TYPES)[:5]))), empty_df
        return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_PEM_SEASON_TYPES)[:5])))

    try:
        pem_endpoint = playerestimatedmetrics.PlayerEstimatedMetrics(
            league_id=league_id,
            season=season,
            season_type=season_type,  # Corrected parameter name
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        metrics_df = pem_endpoint.player_estimated_metrics.get_data_frame()

        # Save DataFrame to CSV if requested and not empty
        if return_dataframe and not metrics_df.empty:
            csv_path = _get_csv_path_for_estimated_metrics(
                season=season,
                season_type=season_type,
                league_id=league_id
            )
            _save_dataframe_to_csv(metrics_df, csv_path)

        metrics_list = _process_dataframe(metrics_df, single_row=False)

        if metrics_list is None:
            logger.error(f"DataFrame processing failed for player estimated metrics (Season: {season}, Type: {season_type}).")
            error_msg = Errors.PROCESSING_ERROR.format(error="player estimated metrics data processing failed")  # Generic processing error
            if return_dataframe:
                return format_response(error=error_msg), empty_df
            return format_response(error=error_msg)

        if metrics_df.empty:
            logger.warning(f"No player estimated metrics data found for Season: {season}, Type: {season_type}.")
            data_payload = []
        else:
            data_payload = metrics_list

        result = {
            "parameters": {
                "season": season,
                "season_type": season_type,
                "league_id": league_id
            },
            "player_estimated_metrics": data_payload
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            if not metrics_df.empty:
                csv_path = _get_csv_path_for_estimated_metrics(
                    season=season,
                    season_type=season_type,
                    league_id=league_id
                )
                relative_path = get_relative_cache_path(
                    os.path.basename(csv_path),
                    "player_estimated_metrics"
                )

                result["dataframe_info"] = {
                    "message": "Player estimated metrics data has been converted to DataFrame and saved as CSV file",
                    "shape": list(metrics_df.shape),
                    "columns": metrics_df.columns.tolist(),
                    "csv_path": relative_path
                }

            logger.info(f"Successfully fetched player estimated metrics for Season: {season}, Type: {season_type} with DataFrame")
            return format_response(data=result), metrics_df

        logger.info(f"Successfully fetched player estimated metrics for Season: {season}, Type: {season_type}")
        return format_response(data=result)

    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_estimated_metrics_logic: {e}")
        if return_dataframe:
            return format_response(error=str(e)), empty_df
        return format_response(error=str(e))
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_estimated_metrics_logic for Season {season}, Type {season_type}: {e}", exc_info=True)
        # Using a more generic error message as specific ones for this endpoint don't exist yet
        error_msg = Errors.API_ERROR.format(error=f"fetching player estimated metrics: {str(e)}")
        if return_dataframe:
            return format_response(error=error_msg), empty_df
        return format_response(error=error_msg)

===== backend\api_tools\player_fantasy_profile.py =====
"""
Handles fetching and processing player fantasy profile data
from the PlayerFantasyProfile endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerFantasyProfile endpoint provides comprehensive fantasy profile data (5 DataFrames):
- Group Info: GROUP_SET, GROUP_VALUE, SEASON_YEAR (where applicable) (2-3 columns)
- Team Performance: GP, W, L, W_PCT (4 columns)
- Minutes: MIN (1 column)
- Shooting: FGM, FGA, FG_PCT, FG3M, FG3A, FG3_PCT, FTM, FTA, FT_PCT (9 columns)
- Rebounding: OREB, DREB, REB (3 columns)
- Other Stats: AST, TOV, STL, BLK, BLKA, PF, PFD, PTS, PLUS_MINUS (9 columns)
- Fantasy Metrics: DD2, TD3, FAN_DUEL_PTS, NBA_FANTASY_PTS (4 columns)
- Rich fantasy data: Complete fantasy basketball statistics with multiple breakdowns (32-33 columns total)
- Perfect for fantasy basketball analysis, player evaluation, and performance tracking
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playerfantasyprofile
from nba_api.stats.library.parameters import SeasonTypePlayoffs, MeasureTypeBase, PerMode36
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]

    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

def _validate_player_id(player_id):
    """Validate player ID format."""
    if not player_id:
        return False

    # Check if it's a string or integer
    if isinstance(player_id, (str, int)):
        # If string, check if it's a valid number
        if isinstance(player_id, str):
            try:
                int(player_id)
                return True
            except ValueError:
                return False
        return True

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_FANTASY_PROFILE_CACHE_SIZE = 256

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_MEASURE_TYPES: Set[str] = {MeasureTypeBase.base}
VALID_PER_MODES: Set[str] = {PerMode36.totals, PerMode36.per_game}
VALID_SEASON_TYPES: Set[str] = {SeasonTypePlayoffs.regular, SeasonTypePlayoffs.playoffs}
VALID_YES_NO: Set[str] = {"Y", "N"}

# --- Cache Directory Setup ---
PLAYER_FANTASY_PROFILE_CSV_DIR = get_cache_dir("player_fantasy_profile")

# Ensure cache directories exist
os.makedirs(PLAYER_FANTASY_PROFILE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_fantasy_profile(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    measure_type_base: str = MeasureTypeBase.base,
    per_mode36: str = PerMode36.totals,
    league_id_nullable: str = "",
    pace_adjust_no: str = "N",
    plus_minus_no: str = "N",
    rank_no: str = "N",
    data_set_name: str = "PlayerFantasyProfile"
) -> str:
    """
    Generates a file path for saving player fantasy profile DataFrame.

    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        measure_type_base: Measure type (default: Base)
        per_mode36: Per mode (default: Totals)
        league_id_nullable: League ID (default: "")
        pace_adjust_no: Pace adjust (default: "N")
        plus_minus_no: Plus minus (default: "N")
        rank_no: Rank (default: "N")
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_fantasy_profile",
        f"player{player_id}",
        f"season{season.replace('-', '_')}",
        f"type{season_type_playoffs.replace(' ', '_')}",
        f"measure{measure_type_base.replace(' ', '_')}",
        f"per{per_mode36.replace(' ', '_')}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        f"pace{pace_adjust_no}",
        f"plus{plus_minus_no}",
        f"rank{rank_no}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "player_fantasy_profile")

# --- Parameter Validation ---
def _validate_player_fantasy_profile_params(
    player_id: str,
    season: str,
    season_type_playoffs: str,
    measure_type_base: str,
    per_mode36: str,
    league_id_nullable: str,
    pace_adjust_no: str,
    plus_minus_no: str,
    rank_no: str
) -> Optional[str]:
    """Validates parameters for fetch_player_fantasy_profile_logic."""
    if not _validate_player_id(player_id):
        return f"Invalid player_id: {player_id}. Must be a valid player ID"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_playoffs not in VALID_SEASON_TYPES:
        return f"Invalid season_type_playoffs: {season_type_playoffs}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if measure_type_base not in VALID_MEASURE_TYPES:
        return f"Invalid measure_type_base: {measure_type_base}. Valid options: {', '.join(VALID_MEASURE_TYPES)}"
    if per_mode36 not in VALID_PER_MODES:
        return f"Invalid per_mode36: {per_mode36}. Valid options: {', '.join(VALID_PER_MODES)}"
    if league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if pace_adjust_no not in VALID_YES_NO:
        return f"Invalid pace_adjust_no: {pace_adjust_no}. Valid options: {', '.join(VALID_YES_NO)}"
    if plus_minus_no not in VALID_YES_NO:
        return f"Invalid plus_minus_no: {plus_minus_no}. Valid options: {', '.join(VALID_YES_NO)}"
    if rank_no not in VALID_YES_NO:
        return f"Invalid rank_no: {rank_no}. Valid options: {', '.join(VALID_YES_NO)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_FANTASY_PROFILE_CACHE_SIZE)
def fetch_player_fantasy_profile_logic(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    measure_type_base: str = MeasureTypeBase.base,
    per_mode36: str = PerMode36.totals,
    league_id_nullable: str = "",
    pace_adjust_no: str = "N",
    plus_minus_no: str = "N",
    rank_no: str = "N",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player fantasy profile data using the PlayerFantasyProfile endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        measure_type_base: Measure type (default: Base)
        per_mode36: Per mode (default: Totals)
        league_id_nullable: League ID (default: "")
        pace_adjust_no: Pace adjust (default: "N")
        plus_minus_no: Plus minus (default: "N")
        rank_no: Rank (default: "N")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player fantasy profile data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_fantasy_profile_logic for Player: {player_id}, Season: {season}, "
        f"Type: {season_type_playoffs}, Measure: {measure_type_base}, Per: {per_mode36}, "
        f"League: {league_id_nullable}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_player_fantasy_profile_params(
        player_id, season, season_type_playoffs, measure_type_base, per_mode36,
        league_id_nullable, pace_adjust_no, plus_minus_no, rank_no
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player fantasy profile: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV files
    dataframes = {}

    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["Overall", "Location", "LastNGames", "DaysRest", "VsOpponent"]
        all_cached = True

        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_player_fantasy_profile(
                player_id, season, season_type_playoffs, measure_type_base, per_mode36,
                league_id_nullable, pace_adjust_no, plus_minus_no, rank_no, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading player fantasy profile from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break

        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "player_id": player_id,
                    "season": season,
                    "season_type_playoffs": season_type_playoffs,
                    "measure_type_base": measure_type_base,
                    "per_mode36": per_mode36,
                    "league_id_nullable": league_id_nullable,
                    "pace_adjust_no": pace_adjust_no,
                    "plus_minus_no": plus_minus_no,
                    "rank_no": rank_no
                },
                "data_sets": {}
            }

            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)

            return format_response(result_dict), dataframes

    try:
        # Prepare API parameters
        api_params = {
            "player_id": player_id,
            "season": season,
            "season_type_playoffs": season_type_playoffs,
            "measure_type_base": measure_type_base,
            "per_mode36": per_mode36,
            "pace_adjust_no": pace_adjust_no,
            "plus_minus_no": plus_minus_no,
            "rank_no": rank_no
        }

        # Add league_id_nullable (can be empty string)
        api_params["league_id_nullable"] = league_id_nullable

        logger.debug(f"Calling PlayerFantasyProfile with parameters: {api_params}")
        fantasy_profile_endpoint = playerfantasyprofile.PlayerFantasyProfile(**api_params)

        # Get data frames
        list_of_dataframes = fantasy_profile_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "player_id": player_id,
                "season": season,
                "season_type_playoffs": season_type_playoffs,
                "measure_type_base": measure_type_base,
                "per_mode36": per_mode36,
                "league_id_nullable": league_id_nullable,
                "pace_adjust_no": pace_adjust_no,
                "plus_minus_no": plus_minus_no,
                "rank_no": rank_no
            },
            "data_sets": {}
        }

        # Process each data frame
        data_set_names = ["Overall", "Location", "LastNGames", "DaysRest", "VsOpponent"]

        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"PlayerFantasyProfile_{idx}"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_player_fantasy_profile(
                        player_id, season, season_type_playoffs, measure_type_base, per_mode36,
                        league_id_nullable, pace_adjust_no, plus_minus_no, rank_no, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_fantasy_profile_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_fantasy_profile(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_playoffs: str = SeasonTypePlayoffs.regular,
    measure_type_base: str = MeasureTypeBase.base,
    per_mode36: str = PerMode36.totals,
    league_id_nullable: str = "",
    pace_adjust_no: str = "N",
    plus_minus_no: str = "N",
    rank_no: str = "N",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player fantasy profile data.

    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        season_type_playoffs: Season type (default: Regular Season)
        measure_type_base: Measure type (default: Base)
        per_mode36: Per mode (default: Totals)
        league_id_nullable: League ID (default: "")
        pace_adjust_no: Pace adjust (default: "N")
        plus_minus_no: Plus minus (default: "N")
        rank_no: Rank (default: "N")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player fantasy profile data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_fantasy_profile_logic(
        player_id=player_id,
        season=season,
        season_type_playoffs=season_type_playoffs,
        measure_type_base=measure_type_base,
        per_mode36=per_mode36,
        league_id_nullable=league_id_nullable,
        pace_adjust_no=pace_adjust_no,
        plus_minus_no=plus_minus_no,
        rank_no=rank_no,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing PlayerFantasyProfile endpoint...")

    # Test 1: Basic fetch
    json_response = get_player_fantasy_profile(player_id="2544")  # LeBron James
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_player_fantasy_profile(player_id="2544", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("PlayerFantasyProfile endpoint test completed.")


===== backend\api_tools\player_fantasy_profile_bar_graph.py =====
"""
Handles fetching and processing player fantasy profile bar graph data
from the PlayerFantasyProfileBarGraph endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerFantasyProfileBarGraph endpoint provides fantasy bar graph data (2 DataFrames):
- Player Info: PLAYER_ID, PLAYER_NAME, TEAM_ID, TEAM_ABBREVIATION (4 columns)
- Fantasy Metrics: FAN_DUEL_PTS, NBA_FANTASY_PTS (2 columns)
- Core Stats: PTS, REB, AST (3 columns)
- Shooting: FG3M, FT_PCT, FG_PCT (3 columns)
- Defense: STL, BLK (2 columns)
- Turnovers: TOV (1 column)
- Rich fantasy data: Per-game fantasy statistics optimized for bar graph visualization (15 columns total)
- Perfect for fantasy bar graph visualization, player comparison charts, and performance dashboards
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playerfantasyprofilebargraph
from nba_api.stats.library.parameters import SeasonTypeAllStar
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

def _validate_player_id(player_id):
    """Validate player ID format."""
    if not player_id:
        return False
    
    # Check if it's a string or integer
    if isinstance(player_id, (str, int)):
        # If string, check if it's a valid number
        if isinstance(player_id, str):
            try:
                int(player_id)
                return True
            except ValueError:
                return False
        return True
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_FANTASY_PROFILE_BAR_GRAPH_CACHE_SIZE = 256

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star, ""}

# --- Cache Directory Setup ---
PLAYER_FANTASY_PROFILE_BAR_GRAPH_CSV_DIR = get_cache_dir("player_fantasy_profile_bar_graph")

# Ensure cache directories exist
os.makedirs(PLAYER_FANTASY_PROFILE_BAR_GRAPH_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_fantasy_profile_bar_graph(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    league_id_nullable: str = "",
    season_type_all_star_nullable: str = "",
    data_set_name: str = "PlayerFantasyProfileBarGraph"
) -> str:
    """
    Generates a file path for saving player fantasy profile bar graph DataFrame.
    
    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        league_id_nullable: League ID (default: "")
        season_type_all_star_nullable: Season type (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_fantasy_profile_bar_graph",
        f"player{player_id}",
        f"season{season.replace('-', '_')}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        f"type{season_type_all_star_nullable.replace(' ', '_') if season_type_all_star_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "player_fantasy_profile_bar_graph")

# --- Parameter Validation ---
def _validate_player_fantasy_profile_bar_graph_params(
    player_id: str,
    season: str,
    league_id_nullable: str,
    season_type_all_star_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_player_fantasy_profile_bar_graph_logic."""
    if not _validate_player_id(player_id):
        return f"Invalid player_id: {player_id}. Must be a valid player ID"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if season_type_all_star_nullable not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star_nullable: {season_type_all_star_nullable}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_FANTASY_PROFILE_BAR_GRAPH_CACHE_SIZE)
def fetch_player_fantasy_profile_bar_graph_logic(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    league_id_nullable: str = "",
    season_type_all_star_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player fantasy profile bar graph data using the PlayerFantasyProfileBarGraph endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        league_id_nullable: League ID (default: "")
        season_type_all_star_nullable: Season type (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player fantasy profile bar graph data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_fantasy_profile_bar_graph_logic for Player: {player_id}, Season: {season}, "
        f"League: {league_id_nullable}, Type: {season_type_all_star_nullable}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_player_fantasy_profile_bar_graph_params(
        player_id, season, league_id_nullable, season_type_all_star_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player fantasy profile bar graph: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["SeasonStats", "RecentStats"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_player_fantasy_profile_bar_graph(
                player_id, season, league_id_nullable, season_type_all_star_nullable, data_set_name
            )
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading player fantasy profile bar graph from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "player_id": player_id,
                    "season": season,
                    "league_id_nullable": league_id_nullable,
                    "season_type_all_star_nullable": season_type_all_star_nullable
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "player_id": player_id,
            "season": season
        }
        
        # Add optional parameters if they're not empty
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable
        if season_type_all_star_nullable:
            api_params["season_type_all_star_nullable"] = season_type_all_star_nullable
        
        logger.debug(f"Calling PlayerFantasyProfileBarGraph with parameters: {api_params}")
        fantasy_bar_graph_endpoint = playerfantasyprofilebargraph.PlayerFantasyProfileBarGraph(**api_params)
        
        # Get data frames
        list_of_dataframes = fantasy_bar_graph_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "player_id": player_id,
                "season": season,
                "league_id_nullable": league_id_nullable,
                "season_type_all_star_nullable": season_type_all_star_nullable
            },
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["SeasonStats", "RecentStats"]
        
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use predefined names for the data sets
                data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"PlayerFantasyProfileBarGraph_{idx}"
                
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df
                    
                    # Save DataFrame to CSV
                    csv_path = _get_csv_path_for_player_fantasy_profile_bar_graph(
                        player_id, season, league_id_nullable, season_type_all_star_nullable, data_set_name
                    )
                    _save_dataframe_to_csv(df, csv_path)
                
                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_fantasy_profile_bar_graph_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_fantasy_profile_bar_graph(
    player_id: str,
    season: str = CURRENT_NBA_SEASON,
    league_id_nullable: str = "",
    season_type_all_star_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player fantasy profile bar graph data.
    
    Args:
        player_id: Player ID (required)
        season: Season (default: current season)
        league_id_nullable: League ID (default: "")
        season_type_all_star_nullable: Season type (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player fantasy profile bar graph data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_fantasy_profile_bar_graph_logic(
        player_id=player_id,
        season=season,
        league_id_nullable=league_id_nullable,
        season_type_all_star_nullable=season_type_all_star_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing PlayerFantasyProfileBarGraph endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_player_fantasy_profile_bar_graph(player_id="2544")  # LeBron James
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_player_fantasy_profile_bar_graph(player_id="2544", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("PlayerFantasyProfileBarGraph endpoint test completed.")


===== backend\api_tools\player_gamelogs.py =====
"""
Handles fetching and processing player game logs for a specific season and season type.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from functools import lru_cache
from typing import Dict, Any, Union, Tuple, Optional, List
import pandas as pd
import json

from nba_api.stats.endpoints import playergamelog
from nba_api.stats.library.parameters import SeasonTypeAllStar
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format

logger = logging.getLogger(__name__)

PLAYER_GAMELOG_CACHE_SIZE = 256

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_GAMELOG_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_gamelogs")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_GAMELOG_CSV_DIR, exist_ok=True)

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_gamelog(player_name: str, season: str, season_type: str) -> str:
    """
    Generates a file path for saving player gamelog DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean player name for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()

    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_gamelog.csv"
    return os.path.join(PLAYER_GAMELOG_CSV_DIR, filename)

def fetch_player_gamelog_logic(
    player_name: str,
    season: str,
    season_type: str = SeasonTypeAllStar.regular,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player game logs for a specified player, season, and season type.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: The NBA season in YYYY-YY format (e.g., "2023-24").
        season_type: The type of season (e.g., "Regular Season", "Playoffs").
                    Defaults to "Regular Season".
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing a list of game logs, or an error message if an issue occurs.
                 Successful response structure:
                 {
                     "player_name": "Player Name",
                     "player_id": 12345,
                     "season": "YYYY-YY",
                     "season_type": "Season Type",
                     "gamelog": [ { ... game log data ... }, ... ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_gamelog_logic for: '{player_name}', Season: {season}, Type: {season_type}, return_dataframe={return_dataframe}")

    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
    if season_type not in VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(VALID_SEASON_TYPES)[:5])) # Show some options
        logger.warning(error_msg)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching playergamelog for ID: {player_id}, Season: {season}, Type: {season_type}")

        try:
            gamelog_endpoint = playergamelog.PlayerGameLog(
                player_id=player_id,
                season=season,
                season_type_all_star=season_type,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            logger.debug(f"playergamelog API call successful for ID: {player_id}, Season: {season}")
        except Exception as api_error:
            logger.error(f"nba_api playergamelog failed for ID {player_id}, Season {season}: {api_error}", exc_info=True)
            error_msg = Errors.PLAYER_GAMELOG_API.format(identifier=player_actual_name, season=season, error=str(api_error))
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Get DataFrame from the API response
        gamelog_df = gamelog_endpoint.get_data_frames()[0]

        if gamelog_df.empty:
            logger.warning(f"No gamelog data found for {player_actual_name} ({season}, {season_type}).")
            response_data = {
                "player_name": player_actual_name,
                "player_id": player_id,
                "season": season,
                "season_type": season_type,
                "gamelog": []
            }

            if return_dataframe:
                # Return empty DataFrame
                empty_df = pd.DataFrame()
                return format_response(response_data), {"gamelog": empty_df}

            return format_response(response_data)

        # Define columns to include
        gamelog_cols = [
            'GAME_ID', 'GAME_DATE', 'MATCHUP', 'WL', 'MIN', 'FGM', 'FGA', 'FG_PCT',
            'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB',
            'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PLUS_MINUS',
            'VIDEO_AVAILABLE'
        ]

        # Filter columns
        available_gamelog_cols = [col for col in gamelog_cols if col in gamelog_df.columns]
        filtered_gamelog_df = gamelog_df.loc[:, available_gamelog_cols] if available_gamelog_cols else pd.DataFrame()

        # Save DataFrame to CSV if returning DataFrame
        if return_dataframe and not filtered_gamelog_df.empty:
            csv_path = _get_csv_path_for_player_gamelog(player_actual_name, season, season_type)
            _save_dataframe_to_csv(filtered_gamelog_df, csv_path)

        # Process DataFrame for JSON response
        gamelog_list = _process_dataframe(filtered_gamelog_df, single_row=False)

        if gamelog_list is None:
            logger.error(f"DataFrame processing failed for gamelog of {player_actual_name} ({season}).")
            error_msg = Errors.PLAYER_GAMELOG_PROCESSING.format(identifier=player_actual_name, season=season)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        # Create response data
        response_data = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "season": season,
            "season_type": season_type,
            "gamelog": gamelog_list
        }

        logger.info(f"fetch_player_gamelog_logic completed for '{player_actual_name}', Season: {season}")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "gamelog": filtered_gamelog_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_gamelog_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_gamelog_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_gamelog_logic for '{player_name}', Season {season}: {e}", exc_info=True)
        error_msg = Errors.PLAYER_GAMELOG_UNEXPECTED.format(identifier=player_name, season=season, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_game_logs.py =====
"""
Handles fetching player game logs.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerGameLogs endpoint, which provides
detailed game-by-game statistics for players:
- Basic and advanced statistics for each game
- Game information (date, matchup, outcome)
- Statistical rankings
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playergamelogs
from nba_api.stats.library.parameters import (
    SeasonType, PerModeSimple, MeasureTypePlayerGameLogs
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_GAME_LOGS_CACHE_SIZE = 128
PLAYER_GAME_LOGS_CSV_DIR = get_cache_dir("player_game_logs")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonType.regular,
    "Pre Season": SeasonType.preseason
    # "All" season type removed because it causes 'resultSet' errors
}

VALID_PER_MODES = {
    "Totals": PerModeSimple.totals,
    "PerGame": PerModeSimple.per_game
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypePlayerGameLogs.base,
    "Advanced": MeasureTypePlayerGameLogs.advanced
}

VALID_GAME_SEGMENTS = {
    "First Half": "First Half",
    "Second Half": "Second Half",
    "Overtime": "Overtime"
}

VALID_LOCATIONS = {
    "Home": "Home",
    "Road": "Road"
}

VALID_OUTCOMES = {
    "W": "W",
    "L": "L"
}

VALID_SEASON_SEGMENTS = {
    "Post All-Star": "Post All-Star",
    "Pre All-Star": "Pre All-Star"
}

VALID_SHOT_CLOCK_RANGES = {
    "24-22": "24-22",
    "22-18 Very Early": "22-18 Very Early",
    "18-15 Early": "18-15 Early",
    "15-7 Average": "15-7 Average",
    "7-4 Late": "7-4 Late",
    "4-0 Very Late": "4-0 Very Late"
}

VALID_CONFERENCES = {
    "East": "East",
    "West": "West"
}

VALID_DIVISIONS = {
    "Atlantic": "Atlantic",
    "Central": "Central",
    "Southeast": "Southeast",
    "Northwest": "Northwest",
    "Pacific": "Pacific",
    "Southwest": "Southwest"
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_game_logs(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    player_id: Optional[str] = None,
    team_id: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a player game logs DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        player_id: Optional player ID filter
        team_id: Optional team ID filter
        date_from: Optional start date filter (MM/DD/YYYY)
        date_to: Optional end date filter (MM/DD/YYYY)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_") if season else "all_seasons"
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"player_game_logs_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}"
    ]

    if player_id:
        filename_parts.append(f"player_{player_id}")

    if team_id:
        filename_parts.append(f"team_{team_id}")

    if date_from:
        date_from_clean = date_from.replace("/", "_")
        filename_parts.append(f"from_{date_from_clean}")

    if date_to:
        date_to_clean = date_to.replace("/", "_")
        filename_parts.append(f"to_{date_to_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "player_game_logs")

# --- Parameter Validation Functions ---
def _validate_player_game_logs_params(
    season_type: str,
    per_mode: str,
    measure_type: str,
    game_segment: Optional[str] = None,
    location: Optional[str] = None,
    outcome: Optional[str] = None,
    season_segment: Optional[str] = None,
    shot_clock_range: Optional[str] = None,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the player game logs function.

    Args:
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        game_segment: Optional game segment filter
        location: Optional location filter
        outcome: Optional outcome filter
        season_segment: Optional season segment filter
        shot_clock_range: Optional shot clock range filter
        vs_conference: Optional conference filter
        vs_division: Optional division filter

    Returns:
        Error message if validation fails, None otherwise
    """
    if season_type and season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode and per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type and measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if game_segment and game_segment not in VALID_GAME_SEGMENTS:
        return f"Invalid game_segment: '{game_segment}'. Valid options: {', '.join(list(VALID_GAME_SEGMENTS.keys()))}"

    if location and location not in VALID_LOCATIONS:
        return f"Invalid location: '{location}'. Valid options: {', '.join(list(VALID_LOCATIONS.keys()))}"

    if outcome and outcome not in VALID_OUTCOMES:
        return f"Invalid outcome: '{outcome}'. Valid options: {', '.join(list(VALID_OUTCOMES.keys()))}"

    if season_segment and season_segment not in VALID_SEASON_SEGMENTS:
        return f"Invalid season_segment: '{season_segment}'. Valid options: {', '.join(list(VALID_SEASON_SEGMENTS.keys()))}"

    if shot_clock_range and shot_clock_range not in VALID_SHOT_CLOCK_RANGES:
        return f"Invalid shot_clock_range: '{shot_clock_range}'. Valid options: {', '.join(list(VALID_SHOT_CLOCK_RANGES.keys()))}"

    if vs_conference and vs_conference not in VALID_CONFERENCES:
        return f"Invalid vs_conference: '{vs_conference}'. Valid options: {', '.join(list(VALID_CONFERENCES.keys()))}"

    if vs_division and vs_division not in VALID_DIVISIONS:
        return f"Invalid vs_division: '{vs_division}'. Valid options: {', '.join(list(VALID_DIVISIONS.keys()))}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_GAME_LOGS_CACHE_SIZE)
def fetch_player_game_logs_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    player_id: str = "",
    team_id: str = "",
    date_from: str = "",
    date_to: str = "",
    game_segment: str = "",
    last_n_games: int = 0,
    league_id: str = "00",  # NBA
    location: str = "",
    month: int = 0,
    opponent_team_id: str = "",
    outcome: str = "",
    po_round: str = "",
    period: int = 0,
    season_segment: str = "",
    shot_clock_range: str = "",
    vs_conference: str = "",
    vs_division: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player game logs using the PlayerGameLogs endpoint.

    This endpoint provides detailed game-by-game statistics for players:
    - Basic and advanced statistics for each game
    - Game information (date, matchup, outcome)
    - Statistical rankings

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        player_id (str, optional): Player ID filter. Defaults to "".
        team_id (str, optional): Team ID filter. Defaults to "".
        date_from (str, optional): Start date filter (MM/DD/YYYY). Defaults to "".
        date_to (str, optional): End date filter (MM/DD/YYYY). Defaults to "".
        game_segment (str, optional): Game segment filter. Defaults to "".
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        location (str, optional): Location filter (Home/Road). Defaults to "".
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (str, optional): Opponent team ID filter. Defaults to "".
        outcome (str, optional): Outcome filter (W/L). Defaults to "".
        po_round (str, optional): Playoff round filter. Defaults to "".
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        season_segment (str, optional): Season segment filter. Defaults to "".
        shot_clock_range (str, optional): Shot clock range filter. Defaults to "".
        vs_conference (str, optional): Conference filter. Defaults to "".
        vs_division (str, optional): Division filter. Defaults to "".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player game logs data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_game_logs_logic for: "
        f"Season: {season}, Type: {season_type}, Player ID: {player_id}, Team ID: {team_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_player_game_logs_params(
        season_type, per_mode, measure_type, game_segment, location, outcome,
        season_segment, shot_clock_range, vs_conference, vs_division
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "season_nullable": season,
        "season_type_nullable": VALID_SEASON_TYPES.get(season_type, ""),
        "per_mode_simple_nullable": VALID_PER_MODES.get(per_mode, ""),
        "measure_type_player_game_logs_nullable": VALID_MEASURE_TYPES.get(measure_type, ""),
        "player_id_nullable": player_id,
        "team_id_nullable": team_id,
        "date_from_nullable": date_from,
        "date_to_nullable": date_to,
        # Removed problematic parameters that cause 'resultSet' errors
        # "game_segment_nullable": game_segment,
        # "last_n_games_nullable": str(last_n_games) if last_n_games > 0 else "",
        "league_id_nullable": league_id,
        "location_nullable": location,
        # "month_nullable": str(month) if month > 0 else "",
        # "opp_team_id_nullable": opponent_team_id,
        "outcome_nullable": outcome,
        # "po_round_nullable": po_round,
        # "period_nullable": str(period) if period > 0 else "",
        "season_segment_nullable": season_segment,
        # "shot_clock_range_nullable": shot_clock_range,
        "vs_conference_nullable": vs_conference,
        "vs_division_nullable": vs_division,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out empty values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v and k != "timeout"}

    try:
        logger.debug(f"Calling PlayerGameLogs with parameters: {filtered_api_params}")
        game_logs_endpoint = playergamelogs.PlayerGameLogs(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = game_logs_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = game_logs_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "PlayerGameLogs"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_player_game_logs(
                            season, season_type, per_mode, measure_type, player_id, team_id, date_from, date_to
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched player game logs for Season: {season}, Player ID: {player_id}, Team ID: {team_id}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_player_game_logs_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_GAME_LOGS_API.format(
            player_id=player_id or "N/A", team_id=team_id or "N/A", season=season or "N/A", error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\player_game_streak_finder.py =====
"""
Handles fetching and processing player game streak finder data
from the PlayerGameStreakFinder endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerGameStreakFinder endpoint provides game streak data (1 DataFrame):
- Player Info: PLAYER_NAME_LAST_FIRST, PLAYER_ID (2 columns)
- Streak Info: GAMESTREAK, STARTDATE, ENDDATE, ACTIVESTREAK (4 columns)
- Career Info: NUMSEASONS, LASTSEASON, FIRSTSEASON (3 columns)
- Rich streak data: Player game streaks with detailed information (9 columns total)
- Perfect for streak analysis, durability tracking, and historical performance evaluation
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playergamestreakfinder
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return True  # Empty season is allowed
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

def _validate_player_id(player_id):
    """Validate player ID format."""
    if not player_id:
        return True  # Empty player ID is allowed
    
    # Check if it's a string or integer
    if isinstance(player_id, (str, int)):
        # If string, check if it's a valid number
        if isinstance(player_id, str):
            try:
                int(player_id)
                return True
            except ValueError:
                return False
        return True
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_GAME_STREAK_FINDER_CACHE_SIZE = 256

# Valid parameter sets (focusing on most useful parameters)
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_SEASON_TYPES: Set[str] = {"Regular Season", "Playoffs", ""}
VALID_ACTIVE_STREAKS: Set[str] = {"Y", "N", ""}
VALID_LOCATIONS: Set[str] = {"Home", "Road", ""}
VALID_OUTCOMES: Set[str] = {"W", "L", ""}

# --- Cache Directory Setup ---
PLAYER_GAME_STREAK_FINDER_CSV_DIR = get_cache_dir("player_game_streak_finder")

# Ensure cache directories exist
os.makedirs(PLAYER_GAME_STREAK_FINDER_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_game_streak_finder(
    player_id_nullable: str = "",
    season_nullable: str = "",
    season_type_nullable: str = "",
    league_id_nullable: str = "",
    active_streaks_only_nullable: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    gt_pts_nullable: str = "",
    data_set_name: str = "PlayerGameStreakFinder"
) -> str:
    """
    Generates a file path for saving player game streak finder DataFrame.
    
    Args:
        player_id_nullable: Player ID (default: "")
        season_nullable: Season (default: "")
        season_type_nullable: Season type (default: "")
        league_id_nullable: League ID (default: "")
        active_streaks_only_nullable: Active streaks only (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        gt_pts_nullable: Greater than points (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_game_streak_finder",
        f"player{player_id_nullable if player_id_nullable else 'all'}",
        f"season{season_nullable.replace('-', '_') if season_nullable else 'all'}",
        f"type{season_type_nullable.replace(' ', '_') if season_type_nullable else 'all'}",
        f"league{league_id_nullable if league_id_nullable else 'all'}",
        f"active{active_streaks_only_nullable if active_streaks_only_nullable else 'all'}",
        f"location{location_nullable if location_nullable else 'all'}",
        f"outcome{outcome_nullable if outcome_nullable else 'all'}",
        f"pts{gt_pts_nullable if gt_pts_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "player_game_streak_finder")

# --- Parameter Validation ---
def _validate_player_game_streak_finder_params(
    player_id_nullable: str,
    season_nullable: str,
    season_type_nullable: str,
    league_id_nullable: str,
    active_streaks_only_nullable: str,
    location_nullable: str,
    outcome_nullable: str,
    gt_pts_nullable: str
) -> Optional[str]:
    """Validates parameters for fetch_player_game_streak_finder_logic."""
    if not _validate_player_id(player_id_nullable):
        return f"Invalid player_id_nullable: {player_id_nullable}. Must be a valid player ID or empty"
    if not _validate_season_format(season_nullable):
        return f"Invalid season_nullable format: {season_nullable}. Expected format: YYYY-YY or empty"
    if season_type_nullable not in VALID_SEASON_TYPES:
        return f"Invalid season_type_nullable: {season_type_nullable}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if league_id_nullable not in VALID_LEAGUE_IDS:
        return f"Invalid league_id_nullable: {league_id_nullable}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if active_streaks_only_nullable not in VALID_ACTIVE_STREAKS:
        return f"Invalid active_streaks_only_nullable: {active_streaks_only_nullable}. Valid options: {', '.join(VALID_ACTIVE_STREAKS)}"
    if location_nullable not in VALID_LOCATIONS:
        return f"Invalid location_nullable: {location_nullable}. Valid options: {', '.join(VALID_LOCATIONS)}"
    if outcome_nullable not in VALID_OUTCOMES:
        return f"Invalid outcome_nullable: {outcome_nullable}. Valid options: {', '.join(VALID_OUTCOMES)}"
    if gt_pts_nullable and gt_pts_nullable != "":
        try:
            int(gt_pts_nullable)
        except ValueError:
            return f"Invalid gt_pts_nullable: {gt_pts_nullable}. Must be a number or empty"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_GAME_STREAK_FINDER_CACHE_SIZE)
def fetch_player_game_streak_finder_logic(
    player_id_nullable: str = "",
    season_nullable: str = "",
    season_type_nullable: str = "",
    league_id_nullable: str = "",
    active_streaks_only_nullable: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    gt_pts_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player game streak finder data using the PlayerGameStreakFinder endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        player_id_nullable: Player ID (default: "")
        season_nullable: Season (default: "")
        season_type_nullable: Season type (default: "")
        league_id_nullable: League ID (default: "")
        active_streaks_only_nullable: Active streaks only (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        gt_pts_nullable: Greater than points (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player game streak finder data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_game_streak_finder_logic for Player: {player_id_nullable}, Season: {season_nullable}, "
        f"Type: {season_type_nullable}, League: {league_id_nullable}, Active: {active_streaks_only_nullable}, "
        f"Location: {location_nullable}, Outcome: {outcome_nullable}, Points: {gt_pts_nullable}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_player_game_streak_finder_params(
        player_id_nullable, season_nullable, season_type_nullable, league_id_nullable,
        active_streaks_only_nullable, location_nullable, outcome_nullable, gt_pts_nullable
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for player game streak finder: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_player_game_streak_finder(
        player_id_nullable, season_nullable, season_type_nullable, league_id_nullable,
        active_streaks_only_nullable, location_nullable, outcome_nullable, gt_pts_nullable, "PlayerGameStreakFinder"
    )
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading player game streak finder from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "player_id_nullable": player_id_nullable,
                        "season_nullable": season_nullable,
                        "season_type_nullable": season_type_nullable,
                        "league_id_nullable": league_id_nullable,
                        "active_streaks_only_nullable": active_streaks_only_nullable,
                        "location_nullable": location_nullable,
                        "outcome_nullable": outcome_nullable,
                        "gt_pts_nullable": gt_pts_nullable
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["PlayerGameStreakFinder"] = df
                result_dict["data_sets"]["PlayerGameStreakFinder"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters (only include non-empty parameters)
        api_params = {}
        
        if player_id_nullable:
            api_params["player_id_nullable"] = player_id_nullable
        if season_nullable:
            api_params["season_nullable"] = season_nullable
        if season_type_nullable:
            api_params["season_type_nullable"] = season_type_nullable
        if league_id_nullable:
            api_params["league_id_nullable"] = league_id_nullable
        if active_streaks_only_nullable:
            api_params["active_streaks_only_nullable"] = active_streaks_only_nullable
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if gt_pts_nullable:
            api_params["gt_pts_nullable"] = gt_pts_nullable
        
        logger.debug(f"Calling PlayerGameStreakFinder with parameters: {api_params}")
        streak_finder_endpoint = playergamestreakfinder.PlayerGameStreakFinder(**api_params)
        
        # Get data frames
        list_of_dataframes = streak_finder_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "player_id_nullable": player_id_nullable,
                "season_nullable": season_nullable,
                "season_type_nullable": season_type_nullable,
                "league_id_nullable": league_id_nullable,
                "active_streaks_only_nullable": active_streaks_only_nullable,
                "location_nullable": location_nullable,
                "outcome_nullable": outcome_nullable,
                "gt_pts_nullable": gt_pts_nullable
            },
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"PlayerGameStreakFinder_{idx}" if idx > 0 else "PlayerGameStreakFinder"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_game_streak_finder_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_game_streak_finder(
    player_id_nullable: str = "",
    season_nullable: str = "",
    season_type_nullable: str = "",
    league_id_nullable: str = "",
    active_streaks_only_nullable: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    gt_pts_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player game streak finder data.
    
    Args:
        player_id_nullable: Player ID (default: "")
        season_nullable: Season (default: "")
        season_type_nullable: Season type (default: "")
        league_id_nullable: League ID (default: "")
        active_streaks_only_nullable: Active streaks only (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        gt_pts_nullable: Greater than points (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with player game streak finder data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_game_streak_finder_logic(
        player_id_nullable=player_id_nullable,
        season_nullable=season_nullable,
        season_type_nullable=season_type_nullable,
        league_id_nullable=league_id_nullable,
        active_streaks_only_nullable=active_streaks_only_nullable,
        location_nullable=location_nullable,
        outcome_nullable=outcome_nullable,
        gt_pts_nullable=gt_pts_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing PlayerGameStreakFinder endpoint...")
    
    # Test 1: Basic fetch (all players)
    json_response = get_player_game_streak_finder()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: Specific player with DataFrame output
    json_response, dataframes = get_player_game_streak_finder(player_id_nullable="2544", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("PlayerGameStreakFinder endpoint test completed.")


===== backend\api_tools\player_index.py =====
"""
Handles fetching and processing player index data
from the PlayerIndex endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayerIndex endpoint provides a comprehensive directory of all players (575+ players)
with detailed information including:
- Player details: ID, names, slug
- Team info: team name, city, abbreviation
- Physical stats: height, weight, position, jersey number
- Background: college, country, draft information
- Career stats: points, rebounds, assists averages
- Career span: from/to years
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playerindex
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return False

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2023-24"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_INDEX_CACHE_SIZE = 32

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", "20"}  # NBA, WNBA, G-League
VALID_POSITIONS: Set[str] = {"F", "C", "G", "F-C", "F-G", "G-F", "C-F"}
VALID_ACTIVE_FLAGS: Set[str] = {"Y", "N"}
VALID_ALLSTAR_FLAGS: Set[str] = {"Y", "N"}
VALID_HISTORICAL_FLAGS: Set[str] = {"Y", "N"}

# --- Cache Directory Setup ---
PLAYER_INDEX_CSV_DIR = get_cache_dir("player_index")

# Ensure cache directories exist
os.makedirs(PLAYER_INDEX_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_index(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    active: Optional[str] = None,
    allstar: Optional[str] = None,
    historical: Optional[str] = None,
    team_id: Optional[str] = None,
    position: Optional[str] = None
) -> str:
    """
    Generates a file path for saving player index DataFrame.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format
        active: Active flag filter (optional)
        allstar: All-star flag filter (optional)
        historical: Historical flag filter (optional)
        team_id: Team ID filter (optional)
        position: Position filter (optional)

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"player_index_league{league_id}",
        f"season{season}"
    ]

    # Add optional filters to filename
    if active:
        filename_parts.append(f"active{active}")
    if allstar:
        filename_parts.append(f"allstar{allstar}")
    if historical:
        filename_parts.append(f"hist{historical}")
    if team_id:
        filename_parts.append(f"team{team_id}")
    if position:
        filename_parts.append(f"pos{position}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "player_index")

# --- Parameter Validation ---
def _validate_player_index_params(
    league_id: str,
    season: str,
    active: Optional[str] = None,
    allstar: Optional[str] = None,
    historical: Optional[str] = None,
    position: Optional[str] = None
) -> Optional[str]:
    """Validates parameters for fetch_player_index_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not season or not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if active and active not in VALID_ACTIVE_FLAGS:
        return f"Invalid active flag: {active}. Valid options: {', '.join(VALID_ACTIVE_FLAGS)}"
    if allstar and allstar not in VALID_ALLSTAR_FLAGS:
        return f"Invalid allstar flag: {allstar}. Valid options: {', '.join(VALID_ALLSTAR_FLAGS)}"
    if historical and historical not in VALID_HISTORICAL_FLAGS:
        return f"Invalid historical flag: {historical}. Valid options: {', '.join(VALID_HISTORICAL_FLAGS)}"
    if position and position not in VALID_POSITIONS:
        return f"Invalid position: {position}. Valid options: {', '.join(VALID_POSITIONS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_INDEX_CACHE_SIZE)
def fetch_player_index_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    active: Optional[str] = None,
    allstar: Optional[str] = None,
    historical: Optional[str] = None,
    team_id: Optional[str] = None,
    position: Optional[str] = None,
    college: Optional[str] = None,
    country: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player index data using the PlayerIndex endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        active: Active players only flag (optional)
        allstar: All-star players only flag (optional)
        historical: Historical players flag (optional)
        team_id: Team ID filter (optional)
        position: Position filter (optional)
        college: College filter (optional)
        country: Country filter (optional)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player index data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_player_index_logic for League: {league_id}, Season: {season}, "
        f"Active: {active}, AllStar: {allstar}, Historical: {historical}, Team: {team_id}, "
        f"Position: {position}, College: {college}, Country: {country}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_player_index_params(league_id, season, active, allstar, historical, position)
    if validation_error:
        logger.warning(f"Parameter validation failed for player index: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_player_index(league_id, season, active, allstar, historical, team_id, position)
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading player index from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season,
                        "active_nullable": active,
                        "allstar_nullable": allstar,
                        "historical_nullable": historical,
                        "team_id_nullable": team_id,
                        "player_position_abbreviation_nullable": position,
                        "college_nullable": college,
                        "country_nullable": country
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["PlayerIndex"] = df
                result_dict["data_sets"]["PlayerIndex"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters (only include non-None values)
        api_params = {
            "league_id": league_id,
            "season": season
        }

        # Add optional parameters if provided
        if active:
            api_params["active_nullable"] = active
        if allstar:
            api_params["allstar_nullable"] = allstar
        if historical:
            api_params["historical_nullable"] = historical
        if team_id:
            api_params["team_id_nullable"] = team_id
        if position:
            api_params["player_position_abbreviation_nullable"] = position
        if college:
            api_params["college_nullable"] = college
        if country:
            api_params["country_nullable"] = country

        logger.debug(f"Calling PlayerIndex with parameters: {api_params}")
        player_index_endpoint = playerindex.PlayerIndex(**api_params)

        # Get data frames
        list_of_dataframes = player_index_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": api_params,
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if not df.empty:
                # Use a generic name for the data set
                data_set_name = f"PlayerIndex_{idx}" if idx > 0 else "PlayerIndex"

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Use the first (main) DataFrame for CSV storage
                    if idx == 0:
                        combined_df = df.copy()

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV
        if return_dataframe and not combined_df.empty:
            _save_dataframe_to_csv(combined_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_player_index_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_player_index(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    active: Optional[str] = None,
    allstar: Optional[str] = None,
    historical: Optional[str] = None,
    team_id: Optional[str] = None,
    position: Optional[str] = None,
    college: Optional[str] = None,
    country: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get player index data.

    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season in YYYY-YY format (default: current NBA season)
        active: Active players only flag (optional)
        allstar: All-star players only flag (optional)
        historical: Historical players flag (optional)
        team_id: Team ID filter (optional)
        position: Position filter (optional)
        college: College filter (optional)
        country: Country filter (optional)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with player index data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_player_index_logic(
        league_id=league_id,
        season=season,
        active=active,
        allstar=allstar,
        historical=historical,
        team_id=team_id,
        position=position,
        college=college,
        country=country,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing PlayerIndex endpoint...")

    # Test 1: Basic fetch
    json_response = get_player_index()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_player_index(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("PlayerIndex endpoint test completed.")


===== backend\api_tools\player_listings.py =====
"""
Handles fetching NBA player listings data using the CommonAllPlayers endpoint.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from functools import lru_cache
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd

from nba_api.stats.endpoints import CommonAllPlayers
from nba_api.stats.library.parameters import LeagueID

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import _validate_season_format, _validate_league_id
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYER_LISTINGS_CSV_DIR = get_cache_dir("player_listings")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_listings(season: str, league_id: str, is_only_current_season: int) -> str:
    """
    Generates a file path for saving a player listings DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        league_id: The league ID
        is_only_current_season: Flag for current season only (1) or all players (0)

    Returns:
        Path to the CSV file
    """
    filename = f"players_{season}_{league_id}_{is_only_current_season}.csv"
    return get_cache_file_path(filename, "player_listings")

def fetch_common_all_players_logic(
    season: str,
    league_id: str = LeagueID.nba,
    is_only_current_season: int = 1, # 1 for current season only, 0 for all players historically linked to that season
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches a list of all players for a given league and season, or all players historically
    if is_only_current_season is set to 0.

    Provides DataFrame output capabilities.

    Args:
        season: The NBA season identifier in YYYY-YY format (e.g., "2023-24").
        league_id: The league ID. Defaults to "00" (NBA).
        is_only_current_season: Flag to filter for only the current season's active players (1)
                               or all players historically associated with that season context (0).
                               Defaults to 1.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string containing a list of players or an error message.
                 Expected structure:
                 {
                     "parameters": {"season": str, "league_id": str, "is_only_current_season": int},
                     "players": [
                         {
                             "PERSON_ID": int, "DISPLAY_LAST_COMMA_FIRST": str, "DISPLAY_FIRST_LAST": str,
                             "ROSTERSTATUS": int (0 or 1), "FROM_YEAR": int, "TO_YEAR": int,
                             "PLAYERCODE": str, "TEAM_ID": int, "TEAM_CITY": str, "TEAM_NAME": str,
                             "TEAM_ABBREVIATION": str, "TEAM_CODE": str, "GAMES_PLAYED_FLAG": str ("Y" or "N"),
                             "OTHERLEAGUE_EXPERIENCE_CH": str
                         }, ...
                     ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_common_all_players_logic for Season: {season}, LeagueID: {league_id}, "
        f"IsOnlyCurrentSeason: {is_only_current_season}, return_dataframe={return_dataframe}"
    )

    if not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    if not _validate_league_id(league_id):
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=["00", "10", "20"]))
        if return_dataframe:
            return error_response, {}
        return error_response

    if is_only_current_season not in [0, 1]:
        error_response = format_response(error=Errors.INVALID_PARAMETER_FORMAT.format(
            param_name="is_only_current_season",
            param_value=is_only_current_season,
            expected_format="0 or 1"
        ))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        # Fetch data from the API
        common_all_players_endpoint = CommonAllPlayers(
            is_only_current_season=is_only_current_season,
            league_id=league_id,
            season=season,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"CommonAllPlayers API call successful for Season: {season}")

        # Get DataFrame
        players_df = common_all_players_endpoint.common_all_players.get_data_frame()

        # Save to CSV if returning DataFrame
        if return_dataframe:
            csv_path = _get_csv_path_for_player_listings(season, league_id, is_only_current_season)
            _save_dataframe_to_csv(players_df, csv_path)

        # Process the DataFrame for JSON response
        players_list = _process_dataframe(players_df, single_row=False)

        if players_list is None: # Should not happen if single_row=False, but defensive check
            logger.error(f"DataFrame processing failed for CommonAllPlayers (Season: {season}).")
            error_response = format_response(error=Errors.COMMON_ALL_PLAYERS_API_ERROR.format(error="DataFrame processing returned None unexpectedly."))
            if return_dataframe:
                return error_response, {}
            return error_response

        # Create the result dictionary
        response_data = {
            "parameters": {
                "season": season,
                "league_id": league_id,
                "is_only_current_season": is_only_current_season
            },
            "players": players_list
        }

        # Add DataFrame info to the response if requested
        if return_dataframe:
            csv_path = _get_csv_path_for_player_listings(season, league_id, is_only_current_season)
            relative_path = get_relative_cache_path(
                os.path.basename(csv_path),
                "player_listings"
            )

            response_data["dataframe_info"] = {
                "message": "Player listings data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "players": {
                        "shape": list(players_df.shape) if not players_df.empty else [],
                        "columns": players_df.columns.tolist() if not players_df.empty else [],
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"Successfully fetched {len(players_list)} players for Season: {season}, LeagueID: {league_id}, IsOnlyCurrentSeason: {is_only_current_season}")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "players": players_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"Error in fetch_common_all_players_logic for Season {season}, LeagueID {league_id}: {str(e)}",
            exc_info=True
        )
        # Use the specific error constant
        error_response = format_response(error=Errors.COMMON_ALL_PLAYERS_API_ERROR.format(error=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_passing.py =====
"""
Handles fetching and processing player passing statistics, including passes made and received.
Requires an initial lookup for the player's current team_id via commonplayerinfo.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
from typing import Set, Dict, Union, Tuple, Optional

import pandas as pd
from nba_api.stats.endpoints import commonplayerinfo, playerdashptpass
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_PASSING_CACHE_SIZE = 256

_VALID_PASSING_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PASSING_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_PASSING_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_passing")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_PASSING_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_passing(
    player_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving player passing DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data ('passes_made' or 'passes_received')

    Returns:
        Path to the CSV file
    """
    # Clean player name and data type for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{clean_per_mode}_{data_type}.csv"
    return os.path.join(PLAYER_PASSING_CSV_DIR, filename)

def fetch_player_passing_stats_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    opponent_team_id: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player passing statistics (passes made and received) for a given season.
    This function first determines the player's team_id for the given season via commonplayerinfo,
    then uses that team_id to fetch passing stats from playerdashptpass.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode (PerModeSimple). Defaults to PerGame.
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        opponent_team_id: Filter by opponent team ID (0 for all teams).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        date_to_nullable: End date filter in format YYYY-MM-DD.
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing player passing stats or an error message.
                 Successful response structure:
                 {
                     "player_name": "Player Name",
                     "player_id": 12345,
                     "parameters": {"season": "YYYY-YY", "season_type": "Season Type", "per_mode": "PerModeValue"},
                     "passes_made": [ { ... stats ... } ],
                     "passes_received": [ { ... stats ... } ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_passing_stats_logic for player: {player_name}, Season: {season}, PerMode: {per_mode}, " +
              f"LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, OpponentTeamID: {opponent_team_id}, " +
              f"VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, SeasonSegment: {season_segment_nullable}, " +
              f"Outcome: {outcome_nullable}, Location: {location_nullable}, DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, " +
              f"return_dataframe={return_dataframe}")

    if not player_name or not player_name.strip():
        error_response = format_response(error=Errors.PLAYER_NAME_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    if season_type not in _VALID_PASSING_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_PASSING_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, {}
        return error_response

    if per_mode not in _VALID_PASSING_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PASSING_PER_MODES)[:3]))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching commonplayerinfo for team ID lookup (Player: {player_actual_name}, ID: {player_id})")

        # Get player info to find team ID
        player_info_endpoint = commonplayerinfo.CommonPlayerInfo(player_id=player_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        info_dict = _process_dataframe(player_info_endpoint.common_player_info.get_data_frame(), single_row=True)

        if info_dict is None or "TEAM_ID" not in info_dict:
            logger.error(f"Could not retrieve team ID for player {player_actual_name} (ID: {player_id})")
            error_response = format_response(error=Errors.TEAM_ID_NOT_FOUND.format(player_name=player_actual_name))
            if return_dataframe:
                return error_response, {}
            return error_response

        team_id = info_dict["TEAM_ID"]
        logger.debug(f"Found Team ID {team_id} for player {player_actual_name}")

        # Call the API
        logger.debug(f"Fetching playerdashptpass for Player ID: {player_id}, Team ID: {team_id}, Season: {season}")
        pass_stats_endpoint = playerdashptpass.PlayerDashPtPass(
            player_id=player_id,
            team_id=team_id,
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            opponent_team_id=opponent_team_id,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            date_to_nullable=date_to_nullable,
            date_from_nullable=date_from_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"playerdashptpass API call successful for {player_actual_name}")

        # Get DataFrames from the API response
        passes_made_df = pass_stats_endpoint.passes_made.get_data_frame()
        passes_received_df = pass_stats_endpoint.passes_received.get_data_frame()

        # Process DataFrames for JSON response
        passes_made_list = _process_dataframe(passes_made_df, single_row=False)
        passes_received_list = _process_dataframe(passes_received_df, single_row=False)

        # Store DataFrames if requested
        dataframes = {}

        if return_dataframe:
            dataframes["passes_made"] = passes_made_df
            dataframes["passes_received"] = passes_received_df

            # Save DataFrames to CSV if not empty
            if not passes_made_df.empty:
                csv_path = _get_csv_path_for_player_passing(
                    player_actual_name, season, season_type, per_mode, "passes_made"
                )
                _save_dataframe_to_csv(passes_made_df, csv_path)

            if not passes_received_df.empty:
                csv_path = _get_csv_path_for_player_passing(
                    player_actual_name, season, season_type, per_mode, "passes_received"
                )
                _save_dataframe_to_csv(passes_received_df, csv_path)

        if passes_made_list is None or passes_received_list is None:
            if passes_made_df.empty and passes_received_df.empty:
                logger.warning(f"No passing stats found for player {player_actual_name} with given filters.")

                response_data = {
                    "player_name": player_actual_name,
                    "player_id": player_id,
                    "parameters": {
                        "season": season,
                        "season_type": season_type,
                        "per_mode": per_mode,
                        "last_n_games": last_n_games,
                        "league_id": league_id,
                        "month": month,
                        "opponent_team_id": opponent_team_id,
                        "vs_division": vs_division_nullable,
                        "vs_conference": vs_conference_nullable,
                        "season_segment": season_segment_nullable,
                        "outcome": outcome_nullable,
                        "location": location_nullable,
                        "date_from": date_from_nullable,
                        "date_to": date_to_nullable
                    },
                    "passes_made": [],
                    "passes_received": []
                }

                if return_dataframe:
                    return format_response(response_data), dataframes
                return format_response(response_data)
            else:
                logger.error(f"DataFrame processing failed for passing stats of {player_actual_name} (Season: {season}).")
                error_msg = Errors.PLAYER_PASSING_PROCESSING.format(identifier=player_actual_name, season=season)
                error_response = format_response(error=error_msg)
                if return_dataframe:
                    return error_response, dataframes
                return error_response

        # Create response data
        result = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "passes_made": passes_made_list or [],
            "passes_received": passes_received_list or []
        }

        logger.info(f"fetch_player_passing_stats_logic completed for {player_actual_name}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_passing_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_passing_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.error(f"Error fetching passing stats for {player_name} (Season: {season}): {str(e)}", exc_info=True)
        error_msg = Errors.PLAYER_PASSING_UNEXPECTED.format(identifier=player_name, season=season, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_rebounding.py =====
"""
Handles fetching and processing player rebounding statistics,
categorized by overall, shot type, contest level, shot distance, and rebound distance.
Requires an initial lookup for the player's current team_id via commonplayerinfo.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
from typing import Set, Dict, Union, Tuple, Optional

import pandas as pd
from nba_api.stats.endpoints import commonplayerinfo, playerdashptreb
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_REBOUNDING_CACHE_SIZE = 256

_VALID_REB_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_REB_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_REBOUNDING_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_rebounding")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_REBOUNDING_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_rebounding(
    player_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving player rebounding DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data ('overall', 'shot_type', 'contest', 'shot_distance', 'rebound_distance')

    Returns:
        Path to the CSV file
    """
    # Clean player name and data type for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{clean_per_mode}_{data_type}.csv"
    return os.path.join(PLAYER_REBOUNDING_CSV_DIR, filename)

def fetch_player_rebounding_stats_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player rebounding statistics, broken down by various categories.
    This function first determines the player's team_id for the given season,
    then uses that to fetch detailed rebounding stats.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode (PerModeSimple). Defaults to PerGame.
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        opponent_team_id: Filter by opponent team ID (0 for all teams).
        period: Period number (0 for all periods).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half").
        date_to_nullable: End date filter in format YYYY-MM-DD.
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing player rebounding stats or an error message.
                 Successful response structure:
                 {
                     "player_name": "Player Name",
                     "player_id": 12345,
                     "parameters": {"season": "YYYY-YY", ...},
                     "overall": { ... overall rebounding stats ... },
                     "by_shot_type": [ { ... stats ... } ],
                     "by_contest": [ { ... stats ... } ],
                     "by_shot_distance": [ { ... stats ... } ],
                     "by_rebound_distance": [ { ... stats ... } ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_rebounding_stats_logic for player: {player_name}, Season: {season}, PerMode: {per_mode}, " +
              f"LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, OpponentTeamID: {opponent_team_id}, Period: {period}, " +
              f"VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, SeasonSegment: {season_segment_nullable}, " +
              f"Outcome: {outcome_nullable}, Location: {location_nullable}, GameSegment: {game_segment_nullable}, " +
              f"DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, return_dataframe={return_dataframe}")

    if not player_name or not player_name.strip():
        error_response = format_response(error=Errors.PLAYER_NAME_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    if season_type not in _VALID_REB_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_REB_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, {}
        return error_response

    if per_mode not in _VALID_REB_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_REB_PER_MODES)[:3]))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching commonplayerinfo for team ID lookup (Player: {player_actual_name}, ID: {player_id})")

        # Get player info to find team ID
        player_info_endpoint = commonplayerinfo.CommonPlayerInfo(player_id=player_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        info_dict = _process_dataframe(player_info_endpoint.common_player_info.get_data_frame(), single_row=True)

        if info_dict is None or "TEAM_ID" not in info_dict:
            logger.error(f"Could not retrieve team ID for player {player_actual_name} (ID: {player_id})")
            error_response = format_response(error=Errors.TEAM_ID_NOT_FOUND.format(player_name=player_actual_name))
            if return_dataframe:
                return error_response, {}
            return error_response

        team_id = info_dict["TEAM_ID"]
        logger.debug(f"Found Team ID {team_id} for player {player_actual_name}")

        # Call the API
        logger.debug(f"Fetching playerdashptreb for Player ID: {player_id}, Team ID: {team_id}, Season: {season}")
        reb_stats_endpoint = playerdashptreb.PlayerDashPtReb(
            player_id=player_id,
            team_id=team_id,
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            opponent_team_id=opponent_team_id,
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            game_segment_nullable=game_segment_nullable,
            date_to_nullable=date_to_nullable,
            date_from_nullable=date_from_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"playerdashptreb API call successful for {player_actual_name}")

        # Get DataFrames from the API response
        overall_df = reb_stats_endpoint.overall_rebounding.get_data_frame()
        shot_type_df = reb_stats_endpoint.shot_type_rebounding.get_data_frame()
        contested_df = reb_stats_endpoint.num_contested_rebounding.get_data_frame()
        distances_df = reb_stats_endpoint.shot_distance_rebounding.get_data_frame()
        reb_dist_df = reb_stats_endpoint.reb_distance_rebounding.get_data_frame()

        # Store DataFrames if requested
        dataframes = {}

        if return_dataframe:
            dataframes["overall"] = overall_df
            dataframes["by_shot_type"] = shot_type_df
            dataframes["by_contest"] = contested_df
            dataframes["by_shot_distance"] = distances_df
            dataframes["by_rebound_distance"] = reb_dist_df

            # Save DataFrames to CSV if not empty
            if not overall_df.empty:
                csv_path = _get_csv_path_for_player_rebounding(
                    player_actual_name, season, season_type, per_mode, "overall"
                )
                _save_dataframe_to_csv(overall_df, csv_path)

            if not shot_type_df.empty:
                csv_path = _get_csv_path_for_player_rebounding(
                    player_actual_name, season, season_type, per_mode, "shot_type"
                )
                _save_dataframe_to_csv(shot_type_df, csv_path)

            if not contested_df.empty:
                csv_path = _get_csv_path_for_player_rebounding(
                    player_actual_name, season, season_type, per_mode, "contest"
                )
                _save_dataframe_to_csv(contested_df, csv_path)

            if not distances_df.empty:
                csv_path = _get_csv_path_for_player_rebounding(
                    player_actual_name, season, season_type, per_mode, "shot_distance"
                )
                _save_dataframe_to_csv(distances_df, csv_path)

            if not reb_dist_df.empty:
                csv_path = _get_csv_path_for_player_rebounding(
                    player_actual_name, season, season_type, per_mode, "rebound_distance"
                )
                _save_dataframe_to_csv(reb_dist_df, csv_path)

        # Process DataFrames for JSON response
        overall_data = _process_dataframe(overall_df, single_row=True)
        shot_type_data = _process_dataframe(shot_type_df, single_row=False)
        contested_data = _process_dataframe(contested_df, single_row=False)
        distances_data = _process_dataframe(distances_df, single_row=False)
        reb_dist_data = _process_dataframe(reb_dist_df, single_row=False)

        if overall_data is None or \
           shot_type_data is None or \
           contested_data is None or \
           distances_data is None or \
           reb_dist_data is None:
            logger.error(f"DataFrame processing failed for rebounding stats of {player_actual_name} (Season: {season}). At least one DF processing returned None.")
            error_msg = Errors.PLAYER_REBOUNDING_PROCESSING.format(identifier=player_actual_name, season=season) # Ensure season is included
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if overall_df.empty and \
           shot_type_df.empty and \
           contested_df.empty and \
           distances_df.empty and \
           reb_dist_df.empty:
            logger.warning(f"No rebounding stats found for player {player_actual_name} with given filters (all original DFs were empty).")

            response_data = {
                "player_name": player_actual_name,
                "player_id": player_id,
                "parameters": {
                    "season": season,
                    "season_type": season_type,
                    "per_mode": per_mode,
                    "last_n_games": last_n_games,
                    "league_id": league_id,
                    "month": month,
                    "opponent_team_id": opponent_team_id,
                    "period": period,
                    "vs_division": vs_division_nullable,
                    "vs_conference": vs_conference_nullable,
                    "season_segment": season_segment_nullable,
                    "outcome": outcome_nullable,
                    "location": location_nullable,
                    "game_segment": game_segment_nullable,
                    "date_from": date_from_nullable,
                    "date_to": date_to_nullable
                },
                "overall": {},
                "by_shot_type": [],
                "by_contest": [],
                "by_shot_distance": [],
                "by_rebound_distance": []
            }

            if return_dataframe:
                return format_response(response_data), dataframes
            return format_response(response_data)

        # Create response data
        result = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "overall": overall_data or {},
            "by_shot_type": shot_type_data or [],
            "by_contest": contested_data or [],
            "by_shot_distance": distances_data or [],
            "by_rebound_distance": reb_dist_data or []
        }

        logger.info(f"fetch_player_rebounding_stats_logic completed for {player_actual_name}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_rebounding_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_rebounding_stats_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.error(f"Error fetching rebounding stats for {player_name} (Season: {season}): {str(e)}", exc_info=True)
        error_msg = Errors.PLAYER_REBOUNDING_UNEXPECTED.format(identifier=player_name, season=season, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_shooting_tracking.py =====
"""
Handles fetching and processing player shooting tracking statistics,
categorized by general, shot clock, dribbles, touch time, and defender distance.
Requires an initial lookup for the player's current team_id via commonplayerinfo.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Set, Dict, Union, Tuple
from functools import lru_cache

import pandas as pd
from nba_api.stats.endpoints import commonplayerinfo, playerdashptshots
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_SHOOTING_TRACKING_CACHE_SIZE = 128
NBA_API_DEFAULT_OPPONENT_TEAM_ID = 0 # Standard value for no specific opponent filter

_VALID_SHOOTING_TRACKING_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_SHOOTING_TRACKING_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_SHOOTING_TRACKING_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_shooting_tracking")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_SHOOTING_TRACKING_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_shooting_tracking(
    player_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving player shooting tracking DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data ('general', 'shot_clock', 'dribble', 'touch_time',
                                     'defender_distance', 'defender_distance_10ft_plus')

    Returns:
        Path to the CSV file
    """
    # Clean player name and data type for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{clean_per_mode}_{data_type}.csv"
    return os.path.join(PLAYER_SHOOTING_TRACKING_CSV_DIR, filename)

def fetch_player_shots_tracking_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.totals,
    opponent_team_id: int = NBA_API_DEFAULT_OPPONENT_TEAM_ID,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player shooting tracking statistics, broken down by various categories.
    This function first determines the player's team_id for the given season,
    then uses that to fetch detailed shooting stats.

    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode (PerModeSimple). Defaults to Totals.
        opponent_team_id: Filter by opponent team ID. Defaults to 0 (all).
        date_from: Start date filter (YYYY-MM-DD).
        date_to: End date filter (YYYY-MM-DD).
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        period: Period number (0 for all periods).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half").
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing player shooting tracking stats or an error message.
                 Successful response structure includes keys like: "general_shooting",
                 "by_shot_clock", "by_dribble_count", "by_touch_time",
                 "by_defender_distance", "by_defender_distance_10ft_plus".
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_shots_tracking_logic for player name: {player_name}, Season: {season}, PerMode: {per_mode}, " +
              f"OpponentTeamID: {opponent_team_id}, DateFrom: {date_from}, DateTo: {date_to}, LastNGames: {last_n_games}, " +
              f"LeagueID: {league_id}, Month: {month}, Period: {period}, VsDivision: {vs_division_nullable}, " +
              f"VsConference: {vs_conference_nullable}, SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, " +
              f"Location: {location_nullable}, GameSegment: {game_segment_nullable}, return_dataframe={return_dataframe}")

    if not player_name or not player_name.strip():
        error_response = format_response(error=Errors.PLAYER_NAME_EMPTY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    if date_from and not validate_date_format(date_from):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from))
        if return_dataframe:
            return error_response, {}
        return error_response

    if date_to and not validate_date_format(date_to):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to))
        if return_dataframe:
            return error_response, {}
        return error_response

    if season_type not in _VALID_SHOOTING_TRACKING_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SHOOTING_TRACKING_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, {}
        return error_response

    if per_mode not in _VALID_SHOOTING_TRACKING_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_SHOOTING_TRACKING_PER_MODES)[:3])))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id_int, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching commonplayerinfo for team ID lookup (Player: {player_actual_name}, ID: {player_id_int})")

        # Get player info to find team ID
        player_info_endpoint = commonplayerinfo.CommonPlayerInfo(player_id=player_id_int, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        info_dict = _process_dataframe(player_info_endpoint.common_player_info.get_data_frame(), single_row=True)

        if info_dict is None or "TEAM_ID" not in info_dict:
            logger.error(f"Could not retrieve team ID for player {player_actual_name} (ID: {player_id_int})")
            error_response = format_response(error=Errors.TEAM_ID_NOT_FOUND.format(player_name=player_actual_name))
            if return_dataframe:
                return error_response, {}
            return error_response

        team_id = info_dict["TEAM_ID"]
        logger.debug(f"Found Team ID {team_id} for player {player_actual_name}")

        # Call the API
        logger.debug(f"Fetching playerdashptshots for Player ID: {player_id_int}, Team ID: {team_id}, Season: {season}")
        shooting_stats_endpoint = playerdashptshots.PlayerDashPtShots(
            player_id=player_id_int,
            team_id=team_id,
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            opponent_team_id=opponent_team_id,
            date_from_nullable=date_from,
            date_to_nullable=date_to,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            game_segment_nullable=game_segment_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"playerdashptshots API call successful for {player_actual_name}")

        # Get DataFrames from the API response
        general_df = shooting_stats_endpoint.general_shooting.get_data_frame()
        shot_clock_df = shooting_stats_endpoint.shot_clock_shooting.get_data_frame()
        dribbles_df = shooting_stats_endpoint.dribble_shooting.get_data_frame()
        touch_time_df = shooting_stats_endpoint.touch_time_shooting.get_data_frame()
        defender_dist_df = shooting_stats_endpoint.closest_defender_shooting.get_data_frame()
        defender_dist_10ft_df = shooting_stats_endpoint.closest_defender10ft_plus_shooting.get_data_frame()

        # Store DataFrames if requested
        dataframes = {}

        if return_dataframe:
            dataframes["general_shooting"] = general_df
            dataframes["by_shot_clock"] = shot_clock_df
            dataframes["by_dribble_count"] = dribbles_df
            dataframes["by_touch_time"] = touch_time_df
            dataframes["by_defender_distance"] = defender_dist_df
            dataframes["by_defender_distance_10ft_plus"] = defender_dist_10ft_df

            # Save DataFrames to CSV if not empty
            if not general_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "general"
                )
                _save_dataframe_to_csv(general_df, csv_path)

            if not shot_clock_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "shot_clock"
                )
                _save_dataframe_to_csv(shot_clock_df, csv_path)

            if not dribbles_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "dribble"
                )
                _save_dataframe_to_csv(dribbles_df, csv_path)

            if not touch_time_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "touch_time"
                )
                _save_dataframe_to_csv(touch_time_df, csv_path)

            if not defender_dist_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "defender_distance"
                )
                _save_dataframe_to_csv(defender_dist_df, csv_path)

            if not defender_dist_10ft_df.empty:
                csv_path = _get_csv_path_for_player_shooting_tracking(
                    player_actual_name, season, season_type, per_mode, "defender_distance_10ft_plus"
                )
                _save_dataframe_to_csv(defender_dist_10ft_df, csv_path)

        # Process DataFrames for JSON response
        general_list = _process_dataframe(general_df, single_row=False)
        shot_clock_list = _process_dataframe(shot_clock_df, single_row=False)
        dribbles_list = _process_dataframe(dribbles_df, single_row=False)
        touch_time_list = _process_dataframe(touch_time_df, single_row=False)
        defender_dist_list = _process_dataframe(defender_dist_df, single_row=False)
        defender_dist_10ft_list = _process_dataframe(defender_dist_10ft_df, single_row=False)

        # Check for processing errors first (any individually processed dataframe is None)
        if general_list is None or \
           shot_clock_list is None or \
           dribbles_list is None or \
           touch_time_list is None or \
           defender_dist_list is None or \
           defender_dist_10ft_list is None:
            logger.error(f"DataFrame processing failed for shooting stats of {player_actual_name} (ID: {player_id_int}, Season: {season}). At least one DF processing returned None.")
            error_msg = Errors.PLAYER_SHOTS_TRACKING_PROCESSING.format(identifier=player_actual_name, season=season)
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if not (general_list or shot_clock_list or dribbles_list or touch_time_list or defender_dist_list or defender_dist_10ft_list):
            original_dfs = [general_df, shot_clock_df, dribbles_df, touch_time_df, defender_dist_df, defender_dist_10ft_df]
            if all(df.empty for df in original_dfs):
                logger.warning(f"No shooting stats data found for player {player_actual_name} (ID: {player_id_int}) with given filters (all original DFs were empty).")

                response_data = {
                    "player_id": player_id_int,
                    "player_name": player_actual_name,
                    "team_id": team_id,
                    "parameters": {
                        "season": season,
                        "season_type": season_type,
                        "per_mode": per_mode,
                        "opponent_team_id": opponent_team_id,
                        "date_from": date_from,
                        "date_to": date_to,
                        "last_n_games": last_n_games,
                        "league_id": league_id,
                        "month": month,
                        "period": period,
                        "vs_division": vs_division_nullable,
                        "vs_conference": vs_conference_nullable,
                        "season_segment": season_segment_nullable,
                        "outcome": outcome_nullable,
                        "location": location_nullable,
                        "game_segment": game_segment_nullable
                    },
                    "general_shooting": [],
                    "by_shot_clock": [],
                    "by_dribble_count": [],
                    "by_touch_time": [],
                    "by_defender_distance": [],
                    "by_defender_distance_10ft_plus": []
                }

                if return_dataframe:
                    return format_response(response_data), dataframes
                return format_response(response_data)

        # Create response data
        result = {
            "player_id": player_id_int,
            "player_name": player_actual_name,
            "team_id": team_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "opponent_team_id": opponent_team_id,
                "date_from": date_from,
                "date_to": date_to,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable
            },
            "general_shooting": general_list or [],
            "by_shot_clock": shot_clock_list or [],
            "by_dribble_count": dribbles_list or [],
            "by_touch_time": touch_time_list or [],
            "by_defender_distance": defender_dist_list or [],
            "by_defender_distance_10ft_plus": defender_dist_10ft_list or []
        }

        logger.info(f"fetch_player_shots_tracking_logic completed for {player_actual_name}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_shots_tracking_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e:
        logger.warning(f"ValueError in fetch_player_shots_tracking_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        player_id_log = player_id_int if 'player_id_int' in locals() else 'unknown'
        logger.error(f"Error fetching shots tracking stats for player {player_name} (resolved ID: {player_id_log}, Season: {season}): {str(e)}", exc_info=True)
        error_msg = Errors.PLAYER_SHOTS_TRACKING_UNEXPECTED.format(identifier=player_name, season=season, error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_shot_charts.py =====
"""
Handles fetching player shot chart data, processing it, and generating visualizations.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from functools import lru_cache
from typing import List, Dict, Tuple, Optional, Any, Union

import pandas as pd
from nba_api.stats.endpoints import shotchartdetail
from nba_api.stats.library.parameters import SeasonTypeAllStar
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_player_id_or_error,
    PlayerNotFoundError
)
from ..utils.validation import _validate_season_format
from .visualization import create_shotchart

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_SHOTCHART_CACHE_SIZE = 128
NBA_API_DEFAULT_TEAM_ID = 0  # For player-specific calls where team is inferred
SHOTCHART_CONTEXT_MEASURE = 'FGA' # Field Goal Attempts, common context for shot charts
DEFAULT_SHOT_ZONE = "Unknown"
SHOT_PERCENTAGE_PRECISION = 1

_VALID_SHOTCHART_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}

# --- Cache Directory Setup ---
CSV_CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache")
PLAYER_SHOTCHART_CSV_DIR = os.path.join(CSV_CACHE_DIR, "player_shotchart")

# Ensure cache directories exist
os.makedirs(CSV_CACHE_DIR, exist_ok=True)
os.makedirs(PLAYER_SHOTCHART_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_shotchart(
    player_name: str,
    season: str,
    season_type: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving player shot chart DataFrame as CSV.

    Args:
        player_name: The player's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        data_type: The type of data ('shots' or 'league_averages')

    Returns:
        Path to the CSV file
    """
    # Clean player name and data type for filename
    clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"{clean_player_name}_{season}_{clean_season_type}_{data_type}.csv"
    return os.path.join(PLAYER_SHOTCHART_CSV_DIR, filename)

# --- Helper Functions ---
def _calculate_zone_summary(shots_data_list: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """Calculates shot attempts, makes, and percentages by shot zone."""
    zone_summary: Dict[str, Dict[str, Any]] = {}
    for shot in shots_data_list:
        zone = shot.get("SHOT_ZONE_BASIC", DEFAULT_SHOT_ZONE)
        if zone not in zone_summary:
            zone_summary[zone] = {"attempts": 0, "made": 0, "percentage": 0.0}
        zone_summary[zone]["attempts"] += 1
        if shot.get("SHOT_MADE_FLAG") == 1:
            zone_summary[zone]["made"] += 1

    for zone_stats in zone_summary.values():
        if zone_stats["attempts"] > 0:
            zone_stats["percentage"] = round(zone_stats["made"] / zone_stats["attempts"] * 100, SHOT_PERCENTAGE_PRECISION)
        else:
            zone_stats["percentage"] = 0.0
    return zone_summary

def _calculate_overall_shot_stats(shots_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Calculates overall shot statistics (total shots, made, FG%)."""
    total_shots = len(shots_data_list)
    made_shots = sum(1 for shot in shots_data_list if shot.get("SHOT_MADE_FLAG") == 1)
    field_goal_percentage = 0.0
    if total_shots > 0:
        field_goal_percentage = round(made_shots / total_shots * 100, SHOT_PERCENTAGE_PRECISION)

    return {
        "total_shots": total_shots,
        "made_shots": made_shots,
        "field_goal_percentage": field_goal_percentage
    }

def _generate_shot_visualization(
    shot_summary_for_viz: Dict[str, Any],
    player_actual_name: str,
    output_base_dir: str
) -> Tuple[Optional[str], Optional[str]]:
    """Generates the shot chart visualization image and returns its path or an error."""
    visualization_path, visualization_error = None, None
    try:
        # Ensure the output directory exists
        # The 'output' subdir is relative to the 'backend' directory.
        # __file__ is in backend/api_tools/player_shot_charts.py
        # os.path.dirname(__file__) -> backend/api_tools
        # os.path.dirname(os.path.dirname(__file__)) -> backend
        # So, output_dir should be backend/output
        output_dir = os.path.join(output_base_dir, "output", "shot_charts") # More specific subdir
        os.makedirs(output_dir, exist_ok=True)

        visualization_path = create_shotchart(shot_summary_for_viz, output_dir)
        logger.info(f"Shot chart visualization created for {player_actual_name} at: {visualization_path}")
    except Exception as viz_error:
        logger.error(f"Failed to create shot chart visualization for {player_actual_name}: {viz_error}", exc_info=True)
        visualization_error = str(viz_error)
    return visualization_path, visualization_error

# --- Main Logic Function ---
def fetch_player_shotchart_logic(
    player_name: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    game_id_nullable: Optional[str] = None,
    player_position_nullable: Optional[str] = None,
    rookie_year_nullable: Optional[str] = None,
    context_filter_nullable: Optional[str] = None,
    clutch_time_nullable: Optional[str] = None,
    ahead_behind_nullable: Optional[str] = None,
    point_diff_nullable: Optional[str] = None,
    position_nullable: Optional[str] = None,
    range_type_nullable: Optional[str] = None,
    start_period_nullable: Optional[str] = None,
    start_range_nullable: Optional[str] = None,
    end_period_nullable: Optional[str] = None,
    end_range_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player shot chart data, processes it, and generates a visualization.
    Provides DataFrame output capabilities.

    Args:
        player_name: The name or ID of the player.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        opponent_team_id: Filter by opponent team ID (0 for all teams).
        period: Period number (0 for all periods).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half").
        date_to_nullable: End date filter in format YYYY-MM-DD.
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        game_id_nullable: Filter by game ID.
        player_position_nullable: Filter by player position (e.g., "Guard", "Center", "Forward").
        rookie_year_nullable: Filter by rookie year.
        context_filter_nullable: Context filter.
        clutch_time_nullable: Filter by clutch time (e.g., "Last 5 Minutes", "Last 2 Minutes").
        ahead_behind_nullable: Filter by ahead/behind status (e.g., "Ahead or Behind", "Ahead or Tied").
        point_diff_nullable: Filter by point differential.
        position_nullable: Filter by position.
        range_type_nullable: Filter by range type.
        start_period_nullable: Filter by start period.
        start_range_nullable: Filter by start range.
        end_period_nullable: Filter by end period.
        end_range_nullable: Filter by end range.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with shot chart data, summary, and visualization path/error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_player_shotchart_logic for: '{player_name}', Season: {season}, Type: {season_type}, " +
              f"LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, OpponentTeamID: {opponent_team_id}, " +
              f"Period: {period}, VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, " +
              f"SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, Location: {location_nullable}, " +
              f"GameSegment: {game_segment_nullable}, DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, " +
              f"GameID: {game_id_nullable}, PlayerPosition: {player_position_nullable}, RookieYear: {rookie_year_nullable}, " +
              f"ContextFilter: {context_filter_nullable}, ClutchTime: {clutch_time_nullable}, AheadBehind: {ahead_behind_nullable}, " +
              f"PointDiff: {point_diff_nullable}, Position: {position_nullable}, RangeType: {range_type_nullable}, " +
              f"StartPeriod: {start_period_nullable}, StartRange: {start_range_nullable}, EndPeriod: {end_period_nullable}, " +
              f"EndRange: {end_range_nullable}, return_dataframe={return_dataframe}")

    if not season or not _validate_season_format(season):
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    if season_type not in _VALID_SHOTCHART_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SHOTCHART_SEASON_TYPES)[:5]))
        logger.warning(error_msg)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        player_id, player_actual_name = find_player_id_or_error(player_name)
        logger.debug(f"Fetching shotchartdetail for ID: {player_id}, Season: {season}, Type: {season_type}")

        # Store DataFrames if requested
        dataframes = {}

        try:
            shotchart_endpoint = shotchartdetail.ShotChartDetail(
                player_id=player_id,
                team_id=NBA_API_DEFAULT_TEAM_ID,
                season_nullable=season,
                season_type_all_star=season_type,
                context_measure_simple=SHOTCHART_CONTEXT_MEASURE,
                last_n_games=last_n_games,
                league_id=league_id,
                month=month,
                opponent_team_id=opponent_team_id,
                period=period,
                vs_division_nullable=vs_division_nullable,
                vs_conference_nullable=vs_conference_nullable,
                season_segment_nullable=season_segment_nullable,
                outcome_nullable=outcome_nullable,
                location_nullable=location_nullable,
                game_segment_nullable=game_segment_nullable,
                date_to_nullable=date_to_nullable,
                date_from_nullable=date_from_nullable,
                game_id_nullable=game_id_nullable,
                player_position_nullable=player_position_nullable,
                rookie_year_nullable=rookie_year_nullable,
                context_filter_nullable=context_filter_nullable,
                clutch_time_nullable=clutch_time_nullable,
                ahead_behind_nullable=ahead_behind_nullable,
                point_diff_nullable=point_diff_nullable,
                position_nullable=position_nullable,
                range_type_nullable=range_type_nullable,
                start_period_nullable=start_period_nullable,
                start_range_nullable=start_range_nullable,
                end_period_nullable=end_period_nullable,
                end_range_nullable=end_range_nullable,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            shots_df = shotchart_endpoint.shot_chart_detail.get_data_frame()
            league_avg_df = shotchart_endpoint.league_averages.get_data_frame()
            logger.debug(f"shotchartdetail API call successful for ID: {player_id}")

            if return_dataframe:
                dataframes["shots"] = shots_df
                dataframes["league_averages"] = league_avg_df

                # Save DataFrames to CSV if not empty
                if not shots_df.empty:
                    csv_path = _get_csv_path_for_player_shotchart(
                        player_actual_name, season, season_type, "shots"
                    )
                    _save_dataframe_to_csv(shots_df, csv_path)

                if not league_avg_df.empty:
                    csv_path = _get_csv_path_for_player_shotchart(
                        player_actual_name, season, season_type, "league_averages"
                    )
                    _save_dataframe_to_csv(league_avg_df, csv_path)

        except Exception as api_error:
            logger.error(f"nba_api shotchartdetail failed for ID {player_id}, Season {season}: {api_error}", exc_info=True)
            error_response = format_response(error=Errors.PLAYER_SHOTCHART_API.format(identifier=player_actual_name, season=season, error=str(api_error)))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        shots_data_list = _process_dataframe(shots_df, single_row=False)
        league_averages_list = _process_dataframe(league_avg_df, single_row=False)

        if shots_data_list is None or league_averages_list is None:
            logger.error(f"DataFrame processing failed for shot chart of {player_actual_name} (Season: {season})")
            error_response = format_response(error=Errors.PLAYER_SHOTCHART_PROCESSING.format(identifier=player_actual_name, season=season))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if not shots_data_list:
            logger.warning(f"No shot data found for {player_actual_name} ({season}, {season_type}).")
            response_data = {
                "player_name": player_actual_name,
                "player_id": player_id,
                "season": season,
                "season_type": season_type,
                "parameters": {
                    "last_n_games": last_n_games,
                    "league_id": league_id,
                    "month": month,
                    "opponent_team_id": opponent_team_id,
                    "period": period,
                    "vs_division": vs_division_nullable,
                    "vs_conference": vs_conference_nullable,
                    "season_segment": season_segment_nullable,
                    "outcome": outcome_nullable,
                    "location": location_nullable,
                    "game_segment": game_segment_nullable,
                    "date_from": date_from_nullable,
                    "date_to": date_to_nullable,
                    "game_id": game_id_nullable,
                    "player_position": player_position_nullable,
                    "rookie_year": rookie_year_nullable,
                    "context_filter": context_filter_nullable,
                    "clutch_time": clutch_time_nullable,
                    "ahead_behind": ahead_behind_nullable,
                    "point_diff": point_diff_nullable,
                    "position": position_nullable,
                    "range_type": range_type_nullable,
                    "start_period": start_period_nullable,
                    "start_range": start_range_nullable,
                    "end_period": end_period_nullable,
                    "end_range": end_range_nullable
                },
                "overall_stats": {"total_shots": 0, "made_shots": 0, "field_goal_percentage": 0.0},
                "zone_breakdown": {},
                "shot_data_summary": [],
                "league_averages": league_averages_list or [],
                "visualization_path": None,
                "visualization_error": None,
                "message": "No shot data found for the specified criteria."
            }
            if return_dataframe:
                return format_response(response_data), dataframes
            return format_response(response_data)

        overall_stats = _calculate_overall_shot_stats(shots_data_list)
        zone_summary = _calculate_zone_summary(shots_data_list)

        shot_summary_for_viz = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "season": season,
            "season_type": season_type,
            "parameters": {
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable,
                "game_id": game_id_nullable,
                "player_position": player_position_nullable,
                "rookie_year": rookie_year_nullable,
                "context_filter": context_filter_nullable,
                "clutch_time": clutch_time_nullable,
                "ahead_behind": ahead_behind_nullable,
                "point_diff": point_diff_nullable,
                "position": position_nullable,
                "range_type": range_type_nullable,
                "start_period": start_period_nullable,
                "start_range": start_range_nullable,
                "end_period": end_period_nullable,
                "end_range": end_range_nullable
            },
            "overall_stats": overall_stats,
            "zone_breakdown": zone_summary,
            "shot_locations": [{"x": s.get("LOC_X"), "y": s.get("LOC_Y"), "made": s.get("SHOT_MADE_FLAG") == 1, "zone": s.get("SHOT_ZONE_BASIC")} for s in shots_data_list]
        }

        # Determine base directory for output (backend directory)
        project_backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        visualization_path, visualization_error = _generate_shot_visualization(shot_summary_for_viz, player_actual_name, project_backend_dir)

        response_summary = {
            "player_name": player_actual_name,
            "player_id": player_id,
            "season": season,
            "season_type": season_type,
            "parameters": {
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable,
                "game_id": game_id_nullable,
                "player_position": player_position_nullable,
                "rookie_year": rookie_year_nullable,
                "context_filter": context_filter_nullable,
                "clutch_time": clutch_time_nullable,
                "ahead_behind": ahead_behind_nullable,
                "point_diff": point_diff_nullable,
                "position": position_nullable,
                "range_type": range_type_nullable,
                "start_period": start_period_nullable,
                "start_range": start_range_nullable,
                "end_period": end_period_nullable,
                "end_range": end_range_nullable
            },
            "overall_stats": overall_stats,
            "zone_breakdown": zone_summary,
            "shot_data_summary": shots_data_list,
            "league_averages": league_averages_list or [],
            "visualization_path": visualization_path,
            "visualization_error": visualization_error
        }
        logger.info(f"fetch_player_shotchart_logic completed for '{player_actual_name}'")

        if return_dataframe:
            return format_response(response_summary), dataframes
        return format_response(response_summary)

    except PlayerNotFoundError as e:
        logger.warning(f"PlayerNotFoundError in fetch_player_shotchart_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ValueError as e: # Catches validation errors from find_player_id_or_error if name is empty
        logger.warning(f"ValueError in fetch_player_shotchart_logic: {e}")
        error_response = format_response(error=str(e))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.critical(f"Unexpected error in fetch_player_shotchart_logic for '{player_name}', Season {season}: {e}", exc_info=True)
        error_response = format_response(error=Errors.PLAYER_SHOTCHART_UNEXPECTED.format(identifier=player_name, season=season, error=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\player_vs_player.py =====
"""
Handles fetching player vs player comparison statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the PlayerVsPlayer endpoint, which provides
detailed statistics comparing two players:
- Head-to-head performance
- On/off court statistics
- Shot area and distance breakdowns
- Player biographical information
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import playervsplayer
from nba_api.stats.library.parameters import (
    SeasonTypePlayoffs, PerModeDetailed, MeasureTypeDetailedDefense
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_VS_PLAYER_CACHE_SIZE = 128
PLAYER_VS_PLAYER_CSV_DIR = get_cache_dir("player_vs_player")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypePlayoffs.regular,
    "Playoffs": SeasonTypePlayoffs.playoffs,
    "Pre Season": SeasonTypePlayoffs.preseason
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_vs_player(
    player_id: str,
    vs_player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    data_set_name: str
) -> str:
    """
    Generates a file path for saving a player vs player DataFrame as CSV.

    Args:
        player_id: The ID of the first player
        vs_player_id: The ID of the second player (comparison player)
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        data_set_name: The name of the data set (e.g., OnOffCourt, Overall)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()
    data_set_clean = data_set_name.replace(" ", "_").lower()

    # Create filename
    filename = f"player_{player_id}_vs_{vs_player_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{data_set_clean}.csv"

    return get_cache_file_path(filename, "player_vs_player")

# --- Parameter Validation Functions ---
def _validate_player_vs_player_params(
    player_id: str,
    vs_player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str
) -> Optional[str]:
    """
    Validates parameters for the player vs player stats function.

    Args:
        player_id: ID of the first player
        vs_player_id: ID of the second player (comparison player)
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not player_id:
        return Errors.PLAYER_ID_EMPTY

    if not vs_player_id:
        return "Comparison player ID cannot be empty"

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYER_VS_PLAYER_CACHE_SIZE)
def fetch_player_vs_player_stats_logic(
    player_id: str,
    vs_player_id: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    period: int = 0,
    date_from: str = "",
    date_to: str = "",
    game_segment: str = "",
    location: str = "",
    outcome: str = "",
    season_segment: str = "",
    vs_conference: str = "",
    vs_division: str = "",
    league_id: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches player vs player comparison statistics using the PlayerVsPlayer endpoint.

    This endpoint provides detailed statistics comparing two players:
    - Head-to-head performance
    - On/off court statistics
    - Shot area and distance breakdowns
    - Player biographical information

    Args:
        player_id (str): ID of the first player
        vs_player_id (str): ID of the second player (comparison player)
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        last_n_games (int, optional): Last N games filter. Defaults to 0 (all games).
        month (int, optional): Month filter (0-12). Defaults to 0 (all months).
        opponent_team_id (int, optional): Opponent team ID filter. Defaults to 0 (all teams).
        period (int, optional): Period filter (0-4). Defaults to 0 (all periods).
        date_from (str, optional): Start date filter (MM/DD/YYYY). Defaults to "".
        date_to (str, optional): End date filter (MM/DD/YYYY). Defaults to "".
        game_segment (str, optional): Game segment filter. Defaults to "".
        location (str, optional): Location filter (Home/Road). Defaults to "".
        outcome (str, optional): Outcome filter (W/L). Defaults to "".
        season_segment (str, optional): Season segment filter. Defaults to "".
        vs_conference (str, optional): Conference filter. Defaults to "".
        vs_division (str, optional): Division filter. Defaults to "".
        league_id (str, optional): League ID filter. Defaults to "".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with player vs player stats data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_player_vs_player_stats_logic for: "
        f"Player: {player_id}, Vs Player: {vs_player_id}, Season: {season}, Type: {season_type}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_player_vs_player_params(
        player_id, vs_player_id, season, season_type, per_mode, measure_type
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "player_id": player_id,
        "vs_player_id": vs_player_id,
        "season": season,
        "season_type_playoffs": VALID_SEASON_TYPES[season_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "last_n_games": str(last_n_games),
        "month": str(month),
        "opponent_team_id": opponent_team_id,
        "period": str(period),
        "pace_adjust": "N",
        "plus_minus": "N",
        "rank": "N",
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Add optional filters if provided
    if date_from:
        api_params["date_from_nullable"] = date_from

    if date_to:
        api_params["date_to_nullable"] = date_to

    if game_segment:
        api_params["game_segment_nullable"] = game_segment

    if location:
        api_params["location_nullable"] = location

    if outcome:
        api_params["outcome_nullable"] = outcome

    if season_segment:
        api_params["season_segment_nullable"] = season_segment

    if vs_conference:
        api_params["vs_conference_nullable"] = vs_conference

    if vs_division:
        api_params["vs_division_nullable"] = vs_division

    if league_id:
        api_params["league_id_nullable"] = league_id

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling PlayerVsPlayer with parameters: {filtered_api_params}")
        player_vs_player_endpoint = playervsplayer.PlayerVsPlayer(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = player_vs_player_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = player_vs_player_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "OnOffCourt", "Overall", "PlayerInfo", "ShotAreaOffCourt",
            "ShotAreaOnCourt", "ShotAreaOverall", "ShotDistanceOffCourt",
            "ShotDistanceOnCourt", "ShotDistanceOverall", "VsPlayerInfo"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_player_vs_player(
                            player_id, vs_player_id, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched player vs player stats for {player_id} vs {vs_player_id} in {season} {season_type}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_player_vs_player_stats_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.PLAYER_VS_PLAYER_API.format(
            player_id=player_id, vs_player_id=vs_player_id, season=season, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\playoff_picture.py =====
"""
Handles fetching and processing playoff picture data
from the PlayoffPicture endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The PlayoffPicture endpoint provides comprehensive playoff picture data (6 DataFrames):
- Series Data (East/West): CONFERENCE, HIGH_SEED_RANK, HIGH_SEED_TEAM, LOW_SEED_RANK, LOW_SEED_TEAM, SERIES_W, SERIES_L, REMAINING_G (12 columns)
- Standings Data (East/West): CONFERENCE, RANK, TEAM, WINS, LOSSES, PCT, DIV, CONF, HOME, AWAY, GB, CLINCHED_PLAYOFFS, CLINCHED_CONFERENCE, CLINCHED_DIVISION, ELIMINATED_PLAYOFFS (25 columns)
- Remaining Games (East/West): TEAM, TEAM_ID, REMAINING_G, REMAINING_HOME_G, REMAINING_AWAY_G (5 columns)
- Rich playoff data: Complete playoff standings, series results, and remaining games (5-25 columns total)
- Perfect for playoff analysis, standings tracking, and postseason evaluation
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import playoffpicture
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season_id):
    """Validate season ID format."""
    if not season_id:
        return True  # Empty season is allowed (uses default)
    
    # Check if it's a valid season ID format (e.g., "22024" for 2024-25 season)
    if isinstance(season_id, (str, int)):
        try:
            season_int = int(season_id)
            # Season IDs are typically 5 digits starting with 2
            return 20000 <= season_int <= 99999
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON_ID = "22024"  # 2024-25 season

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYOFF_PICTURE_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
PLAYOFF_PICTURE_CSV_DIR = get_cache_dir("playoff_picture")

# Ensure cache directories exist
os.makedirs(PLAYOFF_PICTURE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_playoff_picture(
    league_id: str = "00",
    season_id: str = CURRENT_NBA_SEASON_ID,
    data_set_name: str = "PlayoffPicture"
) -> str:
    """
    Generates a file path for saving playoff picture DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_id: Season ID (default: current season)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"playoff_picture",
        f"league{league_id}",
        f"season{season_id}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "playoff_picture")

# --- Parameter Validation ---
def _validate_playoff_picture_params(
    league_id: str,
    season_id: str
) -> Optional[str]:
    """Validates parameters for fetch_playoff_picture_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not _validate_season_format(season_id):
        return f"Invalid season_id format: {season_id}. Expected format: 5-digit season ID (e.g., 22024)"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=PLAYOFF_PICTURE_CACHE_SIZE)
def fetch_playoff_picture_logic(
    league_id: str = "00",
    season_id: str = CURRENT_NBA_SEASON_ID,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches playoff picture data using the PlayoffPicture endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_id: Season ID (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with playoff picture data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_playoff_picture_logic for League: {league_id}, Season: {season_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_playoff_picture_params(league_id, season_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for playoff picture: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["EastSeries", "WestSeries", "EastStandings", "WestStandings", "EastRemainingGames", "WestRemainingGames"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_playoff_picture(league_id, season_id, data_set_name)
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 50:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading playoff picture from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id,
                    "season_id": season_id
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season_id": season_id
        }
        
        logger.debug(f"Calling PlayoffPicture with parameters: {api_params}")
        playoff_picture_endpoint = playoffpicture.PlayoffPicture(**api_params)
        
        # Get data frames
        list_of_dataframes = playoff_picture_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "league_id": league_id,
                "season_id": season_id
            },
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["EastSeries", "WestSeries", "EastStandings", "WestStandings", "EastRemainingGames", "WestRemainingGames"]
        
        for idx, df in enumerate(list_of_dataframes):
            # Use predefined names for the data sets
            data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"PlayoffPicture_{idx}"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Save DataFrame to CSV
                csv_path = _get_csv_path_for_playoff_picture(league_id, season_id, data_set_name)
                _save_dataframe_to_csv(df, csv_path)
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_playoff_picture_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_playoff_picture(
    league_id: str = "00",
    season_id: str = CURRENT_NBA_SEASON_ID,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get playoff picture data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season_id: Season ID (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with playoff picture data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_playoff_picture_logic(
        league_id=league_id,
        season_id=season_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing PlayoffPicture endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_playoff_picture()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_playoff_picture(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("PlayoffPicture endpoint test completed.")


===== backend\api_tools\playoff_series.py =====
"""
Handles fetching information about playoff series for a given league and season.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from typing import Optional, Union, Tuple, Dict
import pandas as pd

from nba_api.stats.endpoints import CommonPlayoffSeries
from nba_api.stats.library.parameters import LeagueID

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import _validate_season_format, _validate_league_id
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
PLAYOFF_SERIES_CSV_DIR = get_cache_dir("playoff_series")

# --- Helper Functions ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_playoff_series(season: str, league_id: str, series_id: Optional[str] = None) -> str:
    """
    Generates a file path for saving a playoff series DataFrame as CSV.

    Args:
        season: The season (YYYY-YY format)
        league_id: The league ID
        series_id: Optional series ID to filter for

    Returns:
        Path to the CSV file
    """
    # Clean parameters for filename
    clean_season = season.replace("-", "_")
    clean_league_id = league_id

    if series_id:
        filename = f"playoff_series_{clean_season}_{clean_league_id}_{series_id}.csv"
    else:
        filename = f"playoff_series_{clean_season}_{clean_league_id}.csv"

    return get_cache_file_path(filename, "playoff_series")

def fetch_common_playoff_series_logic(
    season: str,
    league_id: str = LeagueID.nba,
    series_id: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches information about playoff series for a given league and season.
    Can optionally be filtered by a specific SeriesID.

    Provides DataFrame output capabilities.

    Args:
        season: The NBA season identifier in YYYY-YY format (e.g., "2022-23").
        league_id: The league ID. Defaults to "00" (NBA).
        series_id: A specific SeriesID to filter for. Defaults to None.
                  The NBA API docs refer to this as series_id_nullable.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing a list of playoff series game details or an error message.
                 Expected structure:
                 {
                     "parameters": {"season": str, "league_id": str, "series_id": Optional[str]},
                     "playoff_series": [
                         {
                             "GAME_ID": str, "HOME_TEAM_ID": int, "VISITOR_TEAM_ID": int,
                             "SERIES_ID": str, "GAME_NUM": int
                         }, ...
                     ]
                 }
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_common_playoff_series_logic for Season: {season}, LeagueID: {league_id}, "
        f"SeriesID: {series_id}, return_dataframe={return_dataframe}"
    )

    if not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not _validate_league_id(league_id):
        # Specify valid options for clarity in the error message
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id, options=["00", "10", "20"])
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

    # series_id can be None, so no specific validation on its format other than being a string if provided.
    # The API handles empty string or None for series_id_nullable.

    try:
        playoff_series_endpoint = CommonPlayoffSeries(
            league_id=league_id,
            season=season,
            series_id_nullable=series_id, # Parameter name as per nba_api
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"CommonPlayoffSeries API call successful for Season: {season}, SeriesID: {series_id}")

        series_df = playoff_series_endpoint.playoff_series.get_data_frame()

        # Save DataFrame to CSV if returning DataFrames
        if return_dataframe and not series_df.empty:
            csv_path = _get_csv_path_for_playoff_series(season, league_id, series_id)
            _save_dataframe_to_csv(series_df, csv_path)

        series_list = _process_dataframe(series_df, single_row=False)

        if series_list is None: # Should not happen with single_row=False
            logger.error(f"DataFrame processing failed for CommonPlayoffSeries (Season: {season}, SeriesID: {series_id}).")
            error_msg = Errors.COMMON_PLAYOFF_SERIES_API_ERROR.format(error="DataFrame processing returned None unexpectedly.")
            error_response = format_response(error=error_msg)
            if return_dataframe:
                return error_response, {}
            return error_response

        response_data = {
            "parameters": {
                "season": season,
                "league_id": league_id,
                "series_id": series_id
            },
            "playoff_series": series_list
        }

        # Add DataFrame metadata to the response if DataFrames are being returned
        if return_dataframe and not series_df.empty:
            csv_filename = os.path.basename(_get_csv_path_for_playoff_series(season, league_id, series_id))
            relative_path = get_relative_cache_path(csv_filename, "playoff_series")

            response_data["dataframe_info"] = {
                "message": "Playoff series data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "playoff_series": {
                        "shape": list(series_df.shape),
                        "columns": series_df.columns.tolist(),
                        "csv_path": relative_path
                    }
                }
            }

        logger.info(f"Successfully fetched {len(series_list)} series entries for Season: {season}, LeagueID: {league_id}, SeriesID: {series_id}")

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            dataframes = {
                "playoff_series": series_df
            }
            return format_response(response_data), dataframes

        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"Error in fetch_common_playoff_series_logic for Season {season}, LeagueID {league_id}, SeriesID {series_id}: {str(e)}",
            exc_info=True
        )
        error_msg = Errors.COMMON_PLAYOFF_SERIES_API_ERROR.format(error=str(e))
        error_response = format_response(error=error_msg)
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\raptor_metrics.py =====
"""
Implementation of RAPTOR-style advanced metrics for NBA players.
Based on the methodology described by FiveThirtyEight.
Provides both JSON and DataFrame outputs with CSV caching.
"""

import logging
import json
import time
import os
from typing import Dict, List, Optional, Any, Tuple, Union
import pandas as pd
import numpy as np
from nba_api.stats.endpoints import playercareerstats, commonplayerinfo, playerawards
from nba_api.stats.endpoints import leaguedashplayerstats, playerdashboardbyyearoveryear
from nba_api.stats.static import players
from .utils import retry_on_timeout, format_response
from ..config import settings
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
RAPTOR_METRICS_CSV_DIR = get_cache_dir("raptor_metrics")
HISTORICAL_DATA_DIR = get_cache_dir("historical_data")
LEAGUE_STATS_DIR = get_cache_dir("league_stats")

# Award point values for achievements (used in ELO calculation)
AWARD_VALUES = {
    "MVP": 50,              # Most Valuable Player
    "Finals MVP": 40,       # Finals MVP
    "Defensive Player of the Year": 30,  # Defensive Player of the Year
    "All-NBA": 20,          # All-NBA Team
    "All-Defensive": 15,    # All-Defensive Team
    "All-Star": 10,         # All-Star
    "All-Rookie": 5,        # All-Rookie Team
    "Rookie of the Year": 15,  # Rookie of the Year
    "Scoring Champion": 20, # Scoring Champion
    "Blocks Champion": 15,  # Blocks Leader
    "Steals Champion": 15,  # Steals Leader
    "Assists Champion": 15, # Assists Leader
    "Rebounds Champion": 15,  # Rebounds Leader
}

# RAPTOR weights for boxscore stats (based on FiveThirtyEight's methodology)
RAPTOR_WEIGHTS = {
    # Offensive weights
    "offense": {
        "intercept": -3.88704,
        "MPG": 0.026112,
        "PTS/100": 0.662784,
        "TSA/100": -0.51622,
        "AST/100": 0.430454,
        "TOV/100": -0.893465,
        "ORB/100": 0.303023,
        "DRB/100": -0.085637,
        "STL/100": 0.418092,
        "BLK/100": -0.230734,
        "PF/100": -0.108369,
        "OnCourt": 0.018381,
        "On-Off": 0.032054,
    },
    # Defensive weights
    "defense": {
        "intercept": -3.079144,
        "MPG": 0.033637,
        "PTS/100": -0.081412,
        "TSA/100": 0.025422,
        "AST/100": -0.025109,
        "TOV/100": -0.055809,
        "ORB/100": -0.099034,
        "DRB/100": 0.191569,
        "STL/100": 1.150891,
        "BLK/100": 0.611107,
        "PF/100": 0.010649,
        "OnCourt": 0.089391,
        "On-Off": 0.021717,
    }
}

# Position adjustment targets (minute-weighted average for each position)
POSITION_ADJUSTMENTS = {
    "PG": {"offense": 0.3, "defense": -0.3},
    "SG": {"offense": 0.2, "defense": -0.2},
    "SF": {"offense": 0.0, "defense": 0.0},
    "PF": {"offense": -0.2, "defense": 0.2},
    "C": {"offense": -0.5, "defense": 0.5},
}

def get_player_raptor_metrics(
    player_id: int,
    season: str = "2023-24",
    return_dataframe: bool = False
) -> Union[Dict[str, Any], Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Calculate RAPTOR-style metrics for a player.

    Provides DataFrame output capabilities.

    Args:
        player_id: The NBA API player ID
        season: The season to calculate metrics for (e.g., "2023-24")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            Dictionary with RAPTOR metrics
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    try:
        # Get player info (for position)
        player_info = retry_on_timeout(lambda: commonplayerinfo.CommonPlayerInfo(player_id=player_id))
        player_info_df = player_info.get_data_frames()[0]

        if player_info_df.empty:
            logger.warning(f"No player info found for player ID {player_id}")
            error_response = {"error": f"No player info found for player ID {player_id}"}
            if return_dataframe:
                return format_response(error_response), {}
            return error_response

        player_name = player_info_df['DISPLAY_FIRST_LAST'].iloc[0]
        position = player_info_df['POSITION'].iloc[0]

        # Map position to one of PG, SG, SF, PF, C
        position_category = map_position_to_category(position)

        # Get basic stats
        basic_stats = get_player_basic_stats(player_id, season)
        if not basic_stats:
            logger.warning(f"No basic stats found for player ID {player_id} in season {season}")
            error_response = {"error": f"No basic stats found for player ID {player_id} in season {season}"}
            if return_dataframe:
                return format_response(error_response), {}
            return error_response

        # Get advanced stats
        advanced_stats = get_player_advanced_stats(player_id, season)
        if not advanced_stats:
            logger.warning(f"No advanced stats found for player ID {player_id} in season {season}")
            error_response = {"error": f"No advanced stats found for player ID {player_id} in season {season}"}
            if return_dataframe:
                return format_response(error_response), {}
            return error_response

        # Calculate RAPTOR metrics
        raptor_metrics = calculate_raptor_metrics(basic_stats, advanced_stats, position_category)

        # Get historical data for ELO rating
        historical_data = get_historical_player_data(player_id)

        # Calculate ELO rating
        elo_rating = calculate_elo_rating(raptor_metrics, historical_data)

        # Add ELO rating to metrics
        raptor_metrics.update(elo_rating)

        # Add player info
        raptor_metrics["PLAYER_NAME"] = player_name
        raptor_metrics["POSITION"] = position
        raptor_metrics["POSITION_CATEGORY"] = position_category

        # Generate skill grades
        skill_grades = generate_skill_grades(player_id, raptor_metrics, basic_stats)
        raptor_metrics["SKILL_GRADES"] = skill_grades

        # Create a DataFrame from the metrics
        if return_dataframe:
            # Convert the metrics to a DataFrame
            metrics_df = pd.DataFrame([raptor_metrics])

            # Create a separate DataFrame for skill grades
            grades_df = pd.DataFrame([skill_grades])

            # Save DataFrames to CSV
            clean_player_name = player_name.replace(" ", "_").replace(".", "").lower()
            metrics_csv_path = get_cache_file_path(f"{clean_player_name}_{season}_raptor_metrics.csv", "raptor_metrics")
            grades_csv_path = get_cache_file_path(f"{clean_player_name}_{season}_skill_grades.csv", "raptor_metrics")

            metrics_df.to_csv(metrics_csv_path, index=False)
            grades_df.to_csv(grades_csv_path, index=False)

            # Add DataFrame metadata to the response
            metrics_relative_path = get_relative_cache_path(os.path.basename(metrics_csv_path), "raptor_metrics")
            grades_relative_path = get_relative_cache_path(os.path.basename(grades_csv_path), "raptor_metrics")

            raptor_metrics["dataframe_info"] = {
                "message": "RAPTOR metrics data has been converted to DataFrames and saved as CSV files",
                "dataframes": {
                    "raptor_metrics": {
                        "shape": list(metrics_df.shape),
                        "columns": metrics_df.columns.tolist(),
                        "csv_path": metrics_relative_path
                    },
                    "skill_grades": {
                        "shape": list(grades_df.shape),
                        "columns": grades_df.columns.tolist(),
                        "csv_path": grades_relative_path
                    }
                }
            }

            # Return the JSON response and DataFrames
            dataframes = {
                "raptor_metrics": metrics_df,
                "skill_grades": grades_df
            }

            return format_response(raptor_metrics), dataframes

        return raptor_metrics

    except Exception as e:
        logger.error(f"Error calculating RAPTOR metrics for player ID {player_id}: {str(e)}", exc_info=True)
        error_response = {"error": f"Error calculating RAPTOR metrics: {str(e)}"}
        if return_dataframe:
            return format_response(error_response), {}
        return error_response

def map_position_to_category(position: str) -> str:
    """Map NBA position to one of the 5 position categories."""
    position = position.upper()

    if "GUARD" in position or "G" == position:
        if "POINT" in position or "PG" in position:
            return "PG"
        else:
            return "SG"
    elif "FORWARD" in position or "F" == position:
        if "SMALL" in position or "SF" in position:
            return "SF"
        else:
            return "PF"
    elif "CENTER" in position or "C" == position:
        return "C"

    # Handle hyphenated positions
    if "-" in position:
        positions = position.split("-")
        if "G" in positions and "F" in positions:
            return "SG"  # Guard-Forward is typically a shooting guard
        elif "F" in positions and "C" in positions:
            return "PF"  # Forward-Center is typically a power forward

    # Default to SF if we can't determine
    return "SF"

def get_player_basic_stats(player_id: int, season: str) -> Dict[str, float]:
    """
    Get basic stats for a player from the NBA API.

    Args:
        player_id: The NBA API player ID
        season: The season to get stats for (e.g., "2023-24")

    Returns:
        Dictionary with basic stats
    """
    try:
        def fetch_basic_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Base',
                per_mode_detailed='PerGame'
            )

        basic_stats_data = retry_on_timeout(fetch_basic_stats)
        basic_stats_df = basic_stats_data.get_data_frames()[0]

        if basic_stats_df.empty:
            logger.warning(f"No basic stats found for player ID {player_id} in season {season}")
            return {}

        # Extract the player's row
        player_stats = basic_stats_df[basic_stats_df['PLAYER_ID'] == player_id]

        if player_stats.empty:
            logger.warning(f"Player ID {player_id} not found in basic stats data")
            return {}

        # Convert to dictionary
        stats_dict = player_stats.iloc[0].to_dict()

        # Calculate per-100 possession stats (needed for RAPTOR)
        def fetch_per_100_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Base',
                per_mode_detailed='Per100Possessions'
            )

        per_100_data = retry_on_timeout(fetch_per_100_stats)
        per_100_df = per_100_data.get_data_frames()[0]

        if not per_100_df.empty:
            player_per_100 = per_100_df[per_100_df['PLAYER_ID'] == player_id]
            if not player_per_100.empty:
                per_100_dict = player_per_100.iloc[0].to_dict()

                # Add per-100 stats with specific keys
                stats_dict['PTS/100'] = per_100_dict.get('PTS', 0)
                stats_dict['AST/100'] = per_100_dict.get('AST', 0)
                stats_dict['TOV/100'] = per_100_dict.get('TOV', 0)
                stats_dict['ORB/100'] = per_100_dict.get('OREB', 0)
                stats_dict['DRB/100'] = per_100_dict.get('DREB', 0)
                stats_dict['STL/100'] = per_100_dict.get('STL', 0)
                stats_dict['BLK/100'] = per_100_dict.get('BLK', 0)
                stats_dict['PF/100'] = per_100_dict.get('PF', 0)

                # Calculate TSA/100 (Total Shot Attempts per 100)
                fga = per_100_dict.get('FGA', 0)
                fta = per_100_dict.get('FTA', 0)
                stats_dict['TSA/100'] = fga + (0.44 * fta)

        return stats_dict

    except Exception as e:
        logger.error(f"Error fetching basic stats for player ID {player_id}: {str(e)}", exc_info=True)
        return {}

def get_player_advanced_stats(player_id: int, season: str) -> Dict[str, float]:
    """
    Get advanced stats for a player from the NBA API.

    Args:
        player_id: The NBA API player ID
        season: The season to get stats for (e.g., "2023-24")

    Returns:
        Dictionary with advanced stats
    """
    try:
        def fetch_advanced_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=season,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Advanced',
                per_mode_detailed='PerGame'
            )

        advanced_stats_data = retry_on_timeout(fetch_advanced_stats)
        advanced_stats_df = advanced_stats_data.get_data_frames()[0]

        if advanced_stats_df.empty:
            logger.warning(f"No advanced stats found for player ID {player_id} in season {season}")
            return {}

        # Extract the player's row
        player_stats = advanced_stats_df[advanced_stats_df['PLAYER_ID'] == player_id]

        if player_stats.empty:
            logger.warning(f"Player ID {player_id} not found in advanced stats data")
            return {}

        # Convert to dictionary
        stats_dict = player_stats.iloc[0].to_dict()

        # Add on-court and on-off metrics (estimated since we don't have direct access)
        # In a real implementation, these would come from play-by-play data
        stats_dict['OnCourt'] = stats_dict.get('NET_RATING', 0) / 100  # Scale to match RAPTOR
        stats_dict['On-Off'] = stats_dict.get('PIE', 0) * 2  # Estimate based on PIE

        return stats_dict

    except Exception as e:
        logger.error(f"Error fetching advanced stats for player ID {player_id}: {str(e)}", exc_info=True)
        return {}

def calculate_raptor_metrics(basic_stats: Dict[str, float], advanced_stats: Dict[str, float], position: str) -> Dict[str, float]:
    """
    Calculate RAPTOR metrics based on player stats.

    Args:
        basic_stats: Dictionary with basic stats
        advanced_stats: Dictionary with advanced stats
        position: Player position category (PG, SG, SF, PF, C)

    Returns:
        Dictionary with RAPTOR metrics
    """
    metrics = {}

    # Combine stats
    all_stats = {**basic_stats, **advanced_stats}

    # Calculate minutes per game
    mpg = all_stats.get('MIN', 0)

    # Calculate offensive RAPTOR
    offensive_raptor = RAPTOR_WEIGHTS['offense']['intercept']
    offensive_raptor += RAPTOR_WEIGHTS['offense']['MPG'] * mpg

    # Add weighted box score components
    for stat, weight in RAPTOR_WEIGHTS['offense'].items():
        if stat in ['intercept', 'MPG']:
            continue

        if stat in all_stats:
            offensive_raptor += weight * all_stats[stat]

    # Calculate defensive RAPTOR
    defensive_raptor = RAPTOR_WEIGHTS['defense']['intercept']
    defensive_raptor += RAPTOR_WEIGHTS['defense']['MPG'] * mpg

    # Add weighted box score components
    for stat, weight in RAPTOR_WEIGHTS['defense'].items():
        if stat in ['intercept', 'MPG']:
            continue

        if stat in all_stats:
            defensive_raptor += weight * all_stats[stat]

    # Apply position adjustments
    if position in POSITION_ADJUSTMENTS:
        offensive_raptor += POSITION_ADJUSTMENTS[position]['offense']
        defensive_raptor += POSITION_ADJUSTMENTS[position]['defense']

    # Calculate total RAPTOR
    total_raptor = offensive_raptor + defensive_raptor

    # Calculate WAR (Wins Above Replacement)
    minutes_played = all_stats.get('MIN', 0) * all_stats.get('GP', 0)
    team_minutes = 48 * 5 * all_stats.get('GP', 0)  # 48 minutes * 5 players * games

    # Net Rating Contribution
    net_rating_contribution = (minutes_played / (team_minutes / 4.5)) * total_raptor

    # WAR per 82 games
    war_per_82 = (minutes_played / team_minutes) * 27.769 + 2.519 * net_rating_contribution

    # Actual WAR (scaled to games played)
    games_played = all_stats.get('GP', 0)
    war = war_per_82 * (games_played / 82) if games_played > 0 else 0

    # Store metrics
    metrics['RAPTOR_OFFENSE'] = round(offensive_raptor, 1)
    metrics['RAPTOR_DEFENSE'] = round(defensive_raptor, 1)
    metrics['RAPTOR_TOTAL'] = round(total_raptor, 1)
    metrics['WAR'] = round(war, 1)

    # Add traditional advanced metrics
    for key in ['TS_PCT', 'USG_PCT', 'AST_PCT', 'REB_PCT', 'OREB_PCT', 'DREB_PCT', 'PIE']:
        if key in advanced_stats:
            metrics[key] = advanced_stats[key]

    # Add offensive and defensive ratings
    if 'OFF_RATING' in advanced_stats:
        metrics['ORTG'] = advanced_stats['OFF_RATING']
    if 'DEF_RATING' in advanced_stats:
        metrics['DRTG'] = advanced_stats['DEF_RATING']

    return metrics

def calculate_elo_rating(metrics: Dict[str, float], historical_data: Dict[str, Any]) -> Dict[str, float]:
    """
    Calculate ELO rating based on current performance and historical data.

    Args:
        metrics: Dictionary with current season metrics
        historical_data: Dictionary with historical player data

    Returns:
        Dictionary with ELO rating components
    """
    elo_metrics = {}

    # Base ELO starts at 1500 (average player)
    base_elo = 1500

    # Current season performance (from RAPTOR or other metrics)
    current_season_bonus = 0

    # Use RAPTOR if available, otherwise fall back to other metrics
    if 'RAPTOR_TOTAL' in metrics:
        # RAPTOR typically ranges from -5 to +5 for most players
        raptor = metrics['RAPTOR_TOTAL']
        current_season_bonus = raptor * 40  # Scale to ELO points
    elif 'PIE' in metrics:
        # PIE typically ranges from 0 to 0.2 for most players
        pie = metrics['PIE']
        current_season_bonus = pie * 1000
    else:
        # Fallback to offensive and defensive ratings
        ortg = metrics.get('ORTG', 110)
        drtg = metrics.get('DRTG', 110)
        net_rtg = ortg - drtg
        current_season_bonus = net_rtg * 10

    # Historical performance factors
    historical_bonus = 0

    if historical_data:
        # Career longevity bonus (max +100 for 15+ years)
        years_played = min(15, historical_data.get("years_played", 0))
        longevity_bonus = years_played * 6.67  # Up to +100 for 15 years

        # Career achievements bonus
        achievements_bonus = historical_data.get("achievements_value", 0)  # Up to +200

        # Career statistical excellence
        career_stats_bonus = 0

        # Career PER bonus (15 is average, 25+ is excellent)
        career_per = historical_data.get("career_per", 0)
        if career_per > 15:
            career_per_bonus = min(100, (career_per - 15) * 10)  # Up to +100
            career_stats_bonus += career_per_bonus

        # Career WS bonus
        career_ws = historical_data.get("career_ws", 0)
        career_ws_bonus = min(100, career_ws * 1.5)  # Up to +100
        career_stats_bonus += career_ws_bonus

        # Career VORP bonus
        career_vorp = historical_data.get("career_vorp", 0)
        career_vorp_bonus = min(100, career_vorp * 5)  # Up to +100
        career_stats_bonus += career_vorp_bonus

        # Average the statistical bonuses
        career_stats_bonus = career_stats_bonus / 3

        # Playoff performance bonus
        playoff_bonus = 0
        playoff_stats = historical_data.get("playoff_stats", {})

        if playoff_stats:
            playoff_games = playoff_stats.get("games", 0)

            # Playoff experience bonus (up to +50 for 100+ playoff games)
            playoff_exp_bonus = min(50, playoff_games * 0.5)

            # Playoff scoring bonus
            playoff_ppg = playoff_stats.get("points_per_game", 0)
            playoff_scoring_bonus = min(50, playoff_ppg * 2)

            playoff_bonus = playoff_exp_bonus + playoff_scoring_bonus
            playoff_bonus = min(100, playoff_bonus)

        # Combine all historical factors
        historical_bonus = longevity_bonus + achievements_bonus + career_stats_bonus + playoff_bonus

        # Scale historical bonus (max +400)
        historical_bonus = min(400, historical_bonus)

    # Calculate final ELO rating
    # 60% current season, 40% historical performance
    elo_rating = base_elo + (current_season_bonus * 0.6) + (historical_bonus * 0.4)

    # Add components for reference
    elo_metrics["ELO_RATING"] = round(elo_rating, 0)
    elo_metrics["ELO_CURRENT"] = round(base_elo + current_season_bonus, 0)
    elo_metrics["ELO_HISTORICAL"] = round(base_elo + historical_bonus, 0)

    return elo_metrics

def get_historical_player_data(player_id: int) -> Dict[str, Any]:
    """
    Get historical data for a player, including career stats and achievements.
    Uses caching to avoid repeated API calls.

    Args:
        player_id: The NBA API player ID

    Returns:
        Dict with historical player data including:
        - years_played: Number of years in the league
        - achievements_value: Numerical value of career achievements
        - career_per: Career PER (Player Efficiency Rating)
        - career_ws: Career Win Shares
        - career_vorp: Career VORP (Value Over Replacement Player)
        - playoff_stats: Dictionary with playoff performance metrics
    """
    # Check if we have cached data
    cache_file = get_cache_file_path(f"player_{player_id}_history.json", "historical_data")

    # Try to load from cache first
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
                # Check if cache is recent enough (1 week)
                cache_time = os.path.getmtime(cache_file)
                if (time.time() - cache_time) < 604800:  # 7 days in seconds
                    return cached_data
        except Exception as e:
            logger.warning(f"Error reading cache for player {player_id}: {str(e)}")

    try:
        # Fetch career stats
        career_stats = retry_on_timeout(lambda: playercareerstats.PlayerCareerStats(player_id=player_id))
        career_totals_df = career_stats.get_data_frames()[1]  # Career totals

        # Get years played
        season_totals_df = career_stats.get_data_frames()[0]  # Season-by-season
        years_played = len(season_totals_df['SEASON_ID'].unique())

        # Get playoff stats
        playoff_totals_df = None
        try:
            playoff_totals_df = career_stats.get_data_frames()[2]  # Playoff totals
        except:
            pass

        # Initialize historical data
        historical_data = {
            "years_played": years_played,
            "achievements_value": 0,
            "career_per": 0,
            "career_ws": 0,
            "career_vorp": 0,
            "playoff_stats": {}
        }

        # Get advanced career stats if available
        if not career_totals_df.empty:
            # Some players might not have these stats
            # In a real implementation, we would calculate these from raw stats
            historical_data["career_per"] = 15.0  # League average PER
            historical_data["career_ws"] = years_played * 3.0  # Estimate based on years played
            historical_data["career_vorp"] = years_played * 1.0  # Estimate based on years played

        # Get playoff stats if available
        if playoff_totals_df is not None and not playoff_totals_df.empty:
            playoff_games = playoff_totals_df['GP'].iloc[0] if 'GP' in playoff_totals_df.columns else 0
            playoff_points = playoff_totals_df['PTS'].iloc[0] if 'PTS' in playoff_totals_df.columns else 0

            historical_data["playoff_stats"] = {
                "games": int(playoff_games),
                "points": int(playoff_points),
                "points_per_game": round(playoff_points / playoff_games, 1) if playoff_games > 0 else 0
            }

        # Fetch awards
        awards = retry_on_timeout(lambda: playerawards.PlayerAwards(player_id=player_id))
        awards_df = awards.get_data_frames()[0]

        # Calculate achievements value
        achievements_value = 0

        if not awards_df.empty:
            for _, award in awards_df.iterrows():
                award_type = award['DESCRIPTION']

                # Check for exact matches
                if award_type in AWARD_VALUES:
                    achievements_value += AWARD_VALUES[award_type]
                else:
                    # Check for partial matches
                    for award_key, award_value in AWARD_VALUES.items():
                        if award_key in award_type:
                            achievements_value += award_value
                            break

        # Cap achievements value
        achievements_value = min(200, achievements_value)
        historical_data["achievements_value"] = achievements_value

        # Cache the data
        try:
            with open(cache_file, 'w') as f:
                json.dump(historical_data, f)
        except Exception as e:
            logger.warning(f"Error caching historical data for player {player_id}: {str(e)}")

        return historical_data

    except Exception as e:
        logger.error(f"Error fetching historical data for player {player_id}: {str(e)}", exc_info=True)
        return {
            "years_played": 0,
            "achievements_value": 0,
            "career_per": 0,
            "career_ws": 0,
            "career_vorp": 0,
            "playoff_stats": {}
        }

def generate_skill_grades(player_id: int, metrics: Dict[str, float], basic_stats: Dict[str, float]) -> Dict[str, str]:
    """
    Generate skill grades based on player metrics and league-wide percentiles.

    Args:
        player_id: The NBA API player ID
        metrics: Dictionary with player metrics
        basic_stats: Dictionary with basic stats (can be empty if not available)

    Returns:
        Dictionary mapping skills to letter grades (A+, A, A-, B+, B, etc.)
    """
    # Check if basic_stats is None or empty
    if not basic_stats:
        logger.warning(f"No basic stats provided for player {player_id}. Using metrics only for skill grades.")
        basic_stats = {}
    try:
        # Get league-wide stats for percentile calculations
        league_stats = get_league_stats_for_percentiles()

        # Define grade thresholds based on percentiles
        grade_thresholds = {
            'A+': 0.95,  # 95th percentile and above
            'A': 0.90,   # 90th percentile and above
            'A-': 0.85,  # 85th percentile and above
            'B+': 0.80,  # 80th percentile and above
            'B': 0.70,   # 70th percentile and above
            'B-': 0.60,  # 60th percentile and above
            'C+': 0.55,  # 55th percentile and above
            'C': 0.45,   # 45th percentile and above
            'C-': 0.35,  # 35th percentile and above
            'D+': 0.30,  # 30th percentile and above
            'D': 0.20,   # 20th percentile and above
            'D-': 0.10,  # 10th percentile and above
            'F': 0.0     # Below 10th percentile
        }

        # Initialize grades dictionary
        grades = {}

        # Calculate percentile for each skill
        skill_percentiles = {}

        # 1. Perimeter Shooting
        shooting_metrics = []

        # Use RAPTOR offense component if available
        if "RAPTOR_OFFENSE" in metrics:
            raptor_offense = metrics["RAPTOR_OFFENSE"]
            # Convert to percentile (RAPTOR typically ranges from -5 to +5)
            raptor_offense_percentile = min(1.0, max(0.0, (raptor_offense + 5) / 10))
            shooting_metrics.append((raptor_offense_percentile, 0.3))  # 30% weight

        # Use TS% (True Shooting Percentage)
        if "TS_PCT" in metrics and "TS_PCT" in league_stats:
            ts_pct = metrics["TS_PCT"]
            ts_pct_percentile = calculate_percentile(ts_pct, league_stats["TS_PCT"])
            shooting_metrics.append((ts_pct_percentile, 0.3))  # 30% weight

        # Use 3PT% (Three-Point Percentage)
        if "FG3_PCT" in basic_stats and "FG3_PCT" in league_stats:
            fg3_pct = basic_stats["FG3_PCT"]
            fg3_pct_percentile = calculate_percentile(fg3_pct, league_stats["FG3_PCT"])
            shooting_metrics.append((fg3_pct_percentile, 0.4))  # 40% weight

        if shooting_metrics:
            skill_percentiles["perimeter_shooting"] = sum(pct * weight for pct, weight in shooting_metrics) / sum(weight for _, weight in shooting_metrics)
        else:
            skill_percentiles["perimeter_shooting"] = 0.5  # Default to average

        # 2. Interior Scoring
        interior_metrics = []

        # Use FG% (Field Goal Percentage)
        if "FG_PCT" in basic_stats and "FG_PCT" in league_stats:
            fg_pct = basic_stats["FG_PCT"]
            fg_pct_percentile = calculate_percentile(fg_pct, league_stats["FG_PCT"])
            interior_metrics.append((fg_pct_percentile, 0.4))  # 40% weight

        # Use points in the paint (estimated from total points and position)
        if "PTS" in basic_stats and "PTS" in league_stats:
            pts = basic_stats["PTS"]
            pts_percentile = calculate_percentile(pts, league_stats["PTS"])

            # Adjust based on position (centers and power forwards score more in the paint)
            position = metrics.get("POSITION_CATEGORY", "SF")
            position_factor = 1.0
            if position in ["C", "PF"]:
                position_factor = 1.2
            elif position in ["PG", "SG"]:
                position_factor = 0.8

            interior_metrics.append((pts_percentile * position_factor, 0.6))  # 60% weight

        if interior_metrics:
            skill_percentiles["interior_scoring"] = min(1.0, sum(pct * weight for pct, weight in interior_metrics) / sum(weight for _, weight in interior_metrics))
        else:
            skill_percentiles["interior_scoring"] = 0.5  # Default to average

        # 3. Playmaking
        playmaking_metrics = []

        # Use assists
        if "AST" in basic_stats and "AST" in league_stats:
            ast = basic_stats["AST"]
            ast_percentile = calculate_percentile(ast, league_stats["AST"])
            playmaking_metrics.append((ast_percentile, 0.4))  # 40% weight

        # Use assist percentage
        if "AST_PCT" in metrics and "AST_PCT" in league_stats:
            ast_pct = metrics["AST_PCT"]
            ast_pct_percentile = calculate_percentile(ast_pct, league_stats["AST_PCT"])
            playmaking_metrics.append((ast_pct_percentile, 0.4))  # 40% weight

        # Use assist-to-turnover ratio
        if "AST" in basic_stats and "TOV" in basic_stats:
            ast = basic_stats["AST"]
            tov = basic_stats["TOV"]
            ast_to = ast / tov if tov > 0 else ast

            if "AST_TO" in league_stats:
                ast_to_percentile = calculate_percentile(ast_to, league_stats["AST_TO"])
                playmaking_metrics.append((ast_to_percentile, 0.2))  # 20% weight

        if playmaking_metrics:
            skill_percentiles["playmaking"] = sum(pct * weight for pct, weight in playmaking_metrics) / sum(weight for _, weight in playmaking_metrics)
        else:
            skill_percentiles["playmaking"] = 0.5  # Default to average

        # 4. Perimeter Defense
        perimeter_def_metrics = []

        # Use RAPTOR defense component if available
        if "RAPTOR_DEFENSE" in metrics:
            raptor_defense = metrics["RAPTOR_DEFENSE"]
            # Convert to percentile (RAPTOR typically ranges from -5 to +5)
            raptor_defense_percentile = min(1.0, max(0.0, (raptor_defense + 5) / 10))
            perimeter_def_metrics.append((raptor_defense_percentile, 0.4))  # 40% weight

        # Use steals
        if "STL" in basic_stats and "STL" in league_stats:
            stl = basic_stats["STL"]
            stl_percentile = calculate_percentile(stl, league_stats["STL"])
            perimeter_def_metrics.append((stl_percentile, 0.4))  # 40% weight

        # Use defensive rating (lower is better)
        if "DRTG" in metrics and "DRTG" in league_stats:
            drtg = metrics["DRTG"]
            # Invert percentile since lower DRTG is better
            drtg_percentile = 1.0 - calculate_percentile(drtg, league_stats["DRTG"])
            perimeter_def_metrics.append((drtg_percentile, 0.2))  # 20% weight

        if perimeter_def_metrics:
            skill_percentiles["perimeter_defense"] = sum(pct * weight for pct, weight in perimeter_def_metrics) / sum(weight for _, weight in perimeter_def_metrics)
        else:
            skill_percentiles["perimeter_defense"] = 0.5  # Default to average

        # 5. Interior Defense
        interior_def_metrics = []

        # Use RAPTOR defense component if available
        if "RAPTOR_DEFENSE" in metrics:
            raptor_defense = metrics["RAPTOR_DEFENSE"]
            # Convert to percentile (RAPTOR typically ranges from -5 to +5)
            raptor_defense_percentile = min(1.0, max(0.0, (raptor_defense + 5) / 10))
            interior_def_metrics.append((raptor_defense_percentile, 0.3))  # 30% weight

        # Use blocks
        if "BLK" in basic_stats and "BLK" in league_stats:
            blk = basic_stats["BLK"]
            blk_percentile = calculate_percentile(blk, league_stats["BLK"])
            interior_def_metrics.append((blk_percentile, 0.4))  # 40% weight

        # Use defensive rebounding
        if "DREB_PCT" in metrics and "DREB_PCT" in league_stats:
            dreb_pct = metrics["DREB_PCT"]
            dreb_pct_percentile = calculate_percentile(dreb_pct, league_stats["DREB_PCT"])
            interior_def_metrics.append((dreb_pct_percentile, 0.3))  # 30% weight

        if interior_def_metrics:
            skill_percentiles["interior_defense"] = sum(pct * weight for pct, weight in interior_def_metrics) / sum(weight for _, weight in interior_def_metrics)
        else:
            skill_percentiles["interior_defense"] = 0.5  # Default to average

        # 6. Rebounding
        rebounding_metrics = []

        # Use total rebounds
        if "REB" in basic_stats and "REB" in league_stats:
            reb = basic_stats["REB"]
            reb_percentile = calculate_percentile(reb, league_stats["REB"])
            rebounding_metrics.append((reb_percentile, 0.3))  # 30% weight

        # Use rebound percentage
        if "REB_PCT" in metrics and "REB_PCT" in league_stats:
            reb_pct = metrics["REB_PCT"]
            reb_pct_percentile = calculate_percentile(reb_pct, league_stats["REB_PCT"])
            rebounding_metrics.append((reb_pct_percentile, 0.3))  # 30% weight

        # Use offensive rebounding
        if "OREB_PCT" in metrics and "OREB_PCT" in league_stats:
            oreb_pct = metrics["OREB_PCT"]
            oreb_pct_percentile = calculate_percentile(oreb_pct, league_stats["OREB_PCT"])
            rebounding_metrics.append((oreb_pct_percentile, 0.2))  # 20% weight

        # Use defensive rebounding
        if "DREB_PCT" in metrics and "DREB_PCT" in league_stats:
            dreb_pct = metrics["DREB_PCT"]
            dreb_pct_percentile = calculate_percentile(dreb_pct, league_stats["DREB_PCT"])
            rebounding_metrics.append((dreb_pct_percentile, 0.2))  # 20% weight

        if rebounding_metrics:
            skill_percentiles["rebounding"] = sum(pct * weight for pct, weight in rebounding_metrics) / sum(weight for _, weight in rebounding_metrics)
        else:
            skill_percentiles["rebounding"] = 0.5  # Default to average

        # 7. Off-Ball Movement (harder to measure directly)
        offball_metrics = []

        # Use true shooting percentage as a proxy
        if "TS_PCT" in metrics and "TS_PCT" in league_stats:
            ts_pct = metrics["TS_PCT"]
            ts_pct_percentile = calculate_percentile(ts_pct, league_stats["TS_PCT"])
            offball_metrics.append((ts_pct_percentile, 0.5))  # 50% weight

        # Use offensive rating
        if "ORTG" in metrics and "ORTG" in league_stats:
            ortg = metrics["ORTG"]
            ortg_percentile = calculate_percentile(ortg, league_stats["ORTG"])
            offball_metrics.append((ortg_percentile, 0.5))  # 50% weight

        if offball_metrics:
            skill_percentiles["off_ball_movement"] = sum(pct * weight for pct, weight in offball_metrics) / sum(weight for _, weight in offball_metrics)
        else:
            skill_percentiles["off_ball_movement"] = 0.5  # Default to average

        # 8. Hustle (combination of steals, blocks, offensive rebounds)
        hustle_metrics = []

        # Use steals
        if "STL" in basic_stats and "STL" in league_stats:
            stl = basic_stats["STL"]
            stl_percentile = calculate_percentile(stl, league_stats["STL"])
            hustle_metrics.append((stl_percentile, 0.3))  # 30% weight

        # Use blocks
        if "BLK" in basic_stats and "BLK" in league_stats:
            blk = basic_stats["BLK"]
            blk_percentile = calculate_percentile(blk, league_stats["BLK"])
            hustle_metrics.append((blk_percentile, 0.3))  # 30% weight

        # Use offensive rebounding
        if "OREB_PCT" in metrics and "OREB_PCT" in league_stats:
            oreb_pct = metrics["OREB_PCT"]
            oreb_pct_percentile = calculate_percentile(oreb_pct, league_stats["OREB_PCT"])
            hustle_metrics.append((oreb_pct_percentile, 0.4))  # 40% weight

        if hustle_metrics:
            skill_percentiles["hustle"] = sum(pct * weight for pct, weight in hustle_metrics) / sum(weight for _, weight in hustle_metrics)
        else:
            skill_percentiles["hustle"] = 0.5  # Default to average

        # 9. Versatility (based on all-around contributions)
        versatility_metrics = []

        # Use points
        if "PTS" in basic_stats and "PTS" in league_stats:
            pts = basic_stats["PTS"]
            pts_percentile = calculate_percentile(pts, league_stats["PTS"])
            versatility_metrics.append((pts_percentile, 0.2))  # 20% weight

        # Use assists
        if "AST" in basic_stats and "AST" in league_stats:
            ast = basic_stats["AST"]
            ast_percentile = calculate_percentile(ast, league_stats["AST"])
            versatility_metrics.append((ast_percentile, 0.2))  # 20% weight

        # Use rebounds
        if "REB" in basic_stats and "REB" in league_stats:
            reb = basic_stats["REB"]
            reb_percentile = calculate_percentile(reb, league_stats["REB"])
            versatility_metrics.append((reb_percentile, 0.2))  # 20% weight

        # Use steals
        if "STL" in basic_stats and "STL" in league_stats:
            stl = basic_stats["STL"]
            stl_percentile = calculate_percentile(stl, league_stats["STL"])
            versatility_metrics.append((stl_percentile, 0.2))  # 20% weight

        # Use blocks
        if "BLK" in basic_stats and "BLK" in league_stats:
            blk = basic_stats["BLK"]
            blk_percentile = calculate_percentile(blk, league_stats["BLK"])
            versatility_metrics.append((blk_percentile, 0.2))  # 20% weight

        if versatility_metrics:
            # For versatility, we want to reward balanced contributions
            # Calculate the base percentile
            base_percentile = sum(pct * weight for pct, weight in versatility_metrics) / sum(weight for _, weight in versatility_metrics)

            # Calculate the standard deviation of percentiles (lower = more balanced)
            percentiles = [pct for pct, _ in versatility_metrics]
            std_dev = np.std(percentiles) if len(percentiles) > 1 else 0

            # Apply a balance bonus (higher for more balanced players)
            balance_factor = max(0, 1 - std_dev)
            skill_percentiles["versatility"] = base_percentile * (0.7 + 0.3 * balance_factor)
        else:
            skill_percentiles["versatility"] = 0.5  # Default to average

        # Convert percentiles to letter grades
        for skill, percentile in skill_percentiles.items():
            for grade, threshold in sorted(grade_thresholds.items(), key=lambda x: x[1], reverse=True):
                if percentile >= threshold:
                    grades[skill] = grade
                    break

            # Ensure every skill has a grade
            if skill not in grades:
                grades[skill] = 'F'

        return grades

    except Exception as e:
        logger.error(f"Error generating skill grades for player {player_id}: {str(e)}", exc_info=True)
        # Return default grades if there's an error
        return {
            "perimeter_shooting": "C",
            "interior_scoring": "C",
            "playmaking": "C",
            "perimeter_defense": "C",
            "interior_defense": "C",
            "rebounding": "C",
            "off_ball_movement": "C",
            "hustle": "C",
            "versatility": "C"
        }

def calculate_percentile(value: float, distribution: List[float]) -> float:
    """
    Calculate the percentile of a value within a distribution.

    Args:
        value: The value to find the percentile for
        distribution: List of values representing the distribution

    Returns:
        Percentile as a float between 0 and 1
    """
    if not distribution:
        return 0.5  # Default to 50th percentile if no distribution

    # Count how many values in the distribution are less than or equal to the given value
    count = sum(1 for x in distribution if x <= value)

    # Calculate percentile
    percentile = count / len(distribution)

    return percentile

def get_league_stats_for_percentiles() -> Dict[str, List[float]]:
    """
    Get league-wide statistics for calculating percentiles.
    Uses cached data or fetches from the NBA API.

    Returns:
        Dictionary mapping stat names to lists of values across the league
    """
    # Check if we have cached league stats
    cache_file = get_cache_file_path("league_stats.json", "league_stats")

    # Try to load from cache first
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
                # Check if cache is recent enough (1 day)
                cache_time = os.path.getmtime(cache_file)
                if (time.time() - cache_time) < 86400:  # 1 day in seconds
                    return cached_data
        except Exception as e:
            logger.warning(f"Error reading league stats cache: {str(e)}")

    try:
        # Fetch basic stats
        def fetch_basic_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season=settings.CURRENT_NBA_SEASON,
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Base',
                per_mode_detailed='PerGame',
            )

        basic_stats = retry_on_timeout(fetch_basic_stats)
        basic_df = basic_stats.get_data_frames()[0]

        # Fetch advanced stats
        def fetch_advanced_stats():
            return leaguedashplayerstats.LeagueDashPlayerStats(
                season="2023-24",  # Current season
                season_type_all_star='Regular Season',
                measure_type_detailed_defense='Advanced',
                per_mode_detailed='PerGame',
            )

        advanced_stats = retry_on_timeout(fetch_advanced_stats)
        advanced_df = advanced_stats.get_data_frames()[0]

        # Combine the dataframes on PLAYER_ID
        merged_df = pd.merge(basic_df, advanced_df, on='PLAYER_ID', suffixes=('', '_ADV'))

        # Extract the stats we need for percentiles
        league_stats = {}

        # Basic stats
        basic_stat_columns = [
            'PTS', 'AST', 'REB', 'STL', 'BLK', 'FG_PCT', 'FG3_PCT', 'FT_PCT'
        ]

        for col in basic_stat_columns:
            if col in merged_df.columns:
                league_stats[col] = merged_df[col].dropna().tolist()

        # Advanced stats
        advanced_stat_columns = [
            'TS_PCT', 'USG_PCT', 'AST_PCT', 'REB_PCT', 'OREB_PCT', 'DREB_PCT',
            'PACE', 'PIE', 'OFF_RATING', 'DEF_RATING'
        ]

        for col in advanced_stat_columns:
            if col in merged_df.columns:
                league_stats[col] = merged_df[col].dropna().tolist()

        # Calculate additional metrics
        if 'AST' in merged_df.columns and 'TOV' in merged_df.columns:
            # Assist to turnover ratio
            merged_df['AST_TO'] = merged_df.apply(
                lambda row: row['AST'] / row['TOV'] if row['TOV'] > 0 else row['AST'], axis=1
            )
            league_stats['AST_TO'] = merged_df['AST_TO'].dropna().tolist()

        # Rename some columns for consistency
        if 'OFF_RATING' in league_stats:
            league_stats['ORTG'] = league_stats['OFF_RATING']
        if 'DEF_RATING' in league_stats:
            league_stats['DRTG'] = league_stats['DEF_RATING']

        # Cache the league stats
        try:
            with open(cache_file, 'w') as f:
                json.dump(league_stats, f)
        except Exception as e:
            logger.warning(f"Error caching league stats: {str(e)}")

        return league_stats

    except Exception as e:
        logger.error(f"Error fetching league stats: {str(e)}", exc_info=True)
        return {}


===== backend\api_tools\schedule_league_v2_int.py =====
"""
Handles fetching and processing schedule league v2 int data
from the ScheduleLeagueV2Int endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The ScheduleLeagueV2Int endpoint provides comprehensive league schedule data (3 DataFrames):
- Games Data: Complete game schedule with detailed information (1,403 games, 808 columns)
- Weeks Data: Season weeks with start/end dates (36 weeks, 6 columns)
- Broadcasters Data: Broadcaster information by league and season (128 broadcasters, 7 columns)
- Rich schedule data: Complete league schedule with games, weeks, and broadcaster information
- Perfect for schedule analysis, game tracking, and broadcast planning
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import scheduleleaguev2int
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return True  # Empty season is allowed (uses default)
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
SCHEDULE_LEAGUE_V2_INT_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
SCHEDULE_LEAGUE_V2_INT_CSV_DIR = get_cache_dir("schedule_league_v2_int")

# Ensure cache directories exist
os.makedirs(SCHEDULE_LEAGUE_V2_INT_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_schedule_league_v2_int(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    data_set_name: str = "ScheduleLeagueV2Int"
) -> str:
    """
    Generates a file path for saving schedule league v2 int DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"schedule_league_v2_int",
        f"league{league_id}",
        f"season{season.replace('-', '_')}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "schedule_league_v2_int")

# --- Parameter Validation ---
def _validate_schedule_league_v2_int_params(
    league_id: str,
    season: str
) -> Optional[str]:
    """Validates parameters for fetch_schedule_league_v2_int_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=SCHEDULE_LEAGUE_V2_INT_CACHE_SIZE)
def fetch_schedule_league_v2_int_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches schedule league v2 int data using the ScheduleLeagueV2Int endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with schedule league v2 int data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_schedule_league_v2_int_logic for League: {league_id}, Season: {season}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_schedule_league_v2_int_params(league_id, season)
    if validation_error:
        logger.warning(f"Parameter validation failed for schedule league v2 int: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV files
    dataframes = {}
    
    # Try to load from cache first
    if return_dataframe:
        # Check for all possible data sets
        data_set_names = ["Games", "Weeks", "Broadcasters"]
        all_cached = True
        
        for data_set_name in data_set_names:
            csv_path = _get_csv_path_for_schedule_league_v2_int(league_id, season, data_set_name)
            if os.path.exists(csv_path):
                try:
                    # Check if the file is empty or too small
                    file_size = os.path.getsize(csv_path)
                    if file_size < 100:  # If file is too small, it's probably empty or corrupted
                        logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                        all_cached = False
                        break
                    else:
                        logger.info(f"Loading schedule league v2 int from CSV: {csv_path}")
                        # Read CSV with appropriate data types
                        df = pd.read_csv(csv_path, encoding='utf-8')
                        dataframes[data_set_name] = df
                except Exception as e:
                    logger.error(f"Error loading CSV: {e}", exc_info=True)
                    all_cached = False
                    break
            else:
                all_cached = False
                break
        
        # If all data sets are cached, return cached data
        if all_cached and dataframes:
            # Process for JSON response
            result_dict = {
                "parameters": {
                    "league_id": league_id,
                    "season": season
                },
                "data_sets": {}
            }
            
            for data_set_name, df in dataframes.items():
                result_dict["data_sets"][data_set_name] = _process_dataframe(df, single_row=False)
            
            return format_response(result_dict), dataframes
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season
        }
        
        logger.debug(f"Calling ScheduleLeagueV2Int with parameters: {api_params}")
        schedule_endpoint = scheduleleaguev2int.ScheduleLeagueV2Int(**api_params)
        
        # Get data frames
        list_of_dataframes = schedule_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "league_id": league_id,
                "season": season
            },
            "data_sets": {}
        }
        
        # Process each data frame
        data_set_names = ["Games", "Weeks", "Broadcasters"]
        
        for idx, df in enumerate(list_of_dataframes):
            # Use predefined names for the data sets
            data_set_name = data_set_names[idx] if idx < len(data_set_names) else f"ScheduleLeagueV2Int_{idx}"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Save DataFrame to CSV
                csv_path = _get_csv_path_for_schedule_league_v2_int(league_id, season, data_set_name)
                _save_dataframe_to_csv(df, csv_path)
            
            # Process for JSON response (limit to first 100 records for large datasets)
            if len(df) > 100:
                logger.info(f"Large dataset detected ({len(df)} records), limiting JSON response to first 100 records")
                processed_data = _process_dataframe(df.head(100), single_row=False)
                result_dict["data_sets"][data_set_name] = {
                    "total_records": len(df),
                    "displayed_records": 100,
                    "data": processed_data
                }
            else:
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_schedule_league_v2_int_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_schedule_league_v2_int(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get schedule league v2 int data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with schedule league v2 int data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_schedule_league_v2_int_logic(
        league_id=league_id,
        season=season,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing ScheduleLeagueV2Int endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_schedule_league_v2_int()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_schedule_league_v2_int(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()[:10]}...")  # Show first 10 columns
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("ScheduleLeagueV2Int endpoint test completed.")


===== backend\api_tools\scoreboard_tools.py =====
"""
Handles fetching NBA scoreboard data for a specific date.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import json
import os
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import date, datetime
from functools import lru_cache
from requests.exceptions import ReadTimeout, ConnectionError
import pandas as pd

from nba_api.live.nba.endpoints import ScoreBoard as LiveScoreBoard
from nba_api.stats.endpoints import scoreboardv2
from nba_api.stats.library.parameters import LeagueID

from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, retry_on_timeout
from ..utils.validation import validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
SCOREBOARD_CSV_DIR = get_cache_dir("scoreboard")

# Cache TTL constants
CACHE_TTL_SECONDS_LIVE = 60
CACHE_TTL_SECONDS_STATIC = 3600 * 6

@lru_cache(maxsize=2)
def get_cached_live_scoreboard_data(cache_key: str, timestamp: str) -> Dict[str, Any]:
    """
    Cached wrapper for fetching raw live scoreboard data using `nba_api.live.nba.endpoints.ScoreBoard`.
    The `timestamp` (typically current minute) is part of the cache key to ensure frequent updates.

    Args:
        cache_key (str): A static string part of the cache key (e.g., "live_scoreboard").
        timestamp (str): An ISO format timestamp string, typically for the current minute,
                         used to manage cache invalidation for live data.

    Returns:
        Dict[str, Any]: The raw dictionary response from the LiveScoreBoard endpoint.

    Raises:
        Exception: If the API call fails, to be caught by the caller.
    """
    logger.info(f"Cache miss/expiry for live scoreboard - fetching new data (Timestamp: {timestamp}, CacheKey: {cache_key})")
    try:
        board = LiveScoreBoard(timeout=settings.DEFAULT_TIMEOUT_SECONDS) # Changed
        return board.get_dict()
    except Exception as e:
        logger.error(f"Live ScoreBoard API call failed: {e}", exc_info=True)
        raise e

@lru_cache(maxsize=32) # Cache more historical/future dates
def get_cached_static_scoreboard_data(cache_key: Tuple, timestamp: str, **kwargs: Any) -> Dict[str, Any]:
    """
    Cached wrapper for fetching and initially processing static scoreboard data using `nba_api.stats.endpoints.scoreboardv2`.
    The `timestamp` (typically current hour) is part of the cache key for hourly invalidation.
    `kwargs` are passed directly to the `scoreboardv2.ScoreboardV2` endpoint.

    Args:
        cache_key (Tuple): A tuple representing the static parts of the cache key (e.g., ("static_scoreboard", game_date, league_id)).
        timestamp (str): An ISO format timestamp string, typically for the current hour,
                         used to manage cache invalidation for static data.
        **kwargs: Arguments to pass to `scoreboardv2.ScoreboardV2` (e.g., `game_date`, `league_id`, `day_offset`).

    Returns:
        Dict[str, Any]: A dictionary containing processed DataFrames (as lists of dicts) for "game_header" and "line_score".
                        Example: {"game_header": [...], "line_score": [...]}

    Raises:
        Exception: If the API call or initial DataFrame processing fails, to be caught by the caller.
    """
    logger.info(f"Cache miss/expiry for static scoreboard - fetching new data for {kwargs.get('game_date')} (Timestamp: {timestamp}, CacheKey: {cache_key})")
    try:
        scoreboard_endpoint = scoreboardv2.ScoreboardV2(**kwargs, timeout=settings.DEFAULT_TIMEOUT_SECONDS) # Changed
        game_header_list = _process_dataframe(scoreboard_endpoint.game_header.get_data_frame(), single_row=False)
        line_score_list = _process_dataframe(scoreboard_endpoint.line_score.get_data_frame(), single_row=False)

        # Ensure lists are returned even if processing results in None (e.g. due to empty df and processing error)
        return {
            "game_header": game_header_list if game_header_list is not None else [],
            "line_score": line_score_list if line_score_list is not None else []
        }
    except Exception as e:
        logger.error(f"ScoreboardV2 API call or processing failed for {kwargs.get('game_date')}: {e}", exc_info=True)
        raise e

def fetch_scoreboard_data_logic(
    game_date: Optional[str] = None,
    league_id: str = LeagueID.nba,
    day_offset: int = 0,
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches NBA scoreboard data for a specific date.
    Uses the live NBA API endpoint for the current date and the static ScoreboardV2 endpoint
    for past or future dates. Data is cached to minimize API calls.

    Provides DataFrame output capabilities.

    Args:
        game_date: The date for the scoreboard in YYYY-MM-DD format.
                   Defaults to the current local date if None.
        league_id: The league ID. Valid values from `LeagueID` (e.g., "00" for NBA).
                   Defaults to "00" (NBA). This primarily applies to the static ScoreboardV2.
        day_offset: Day offset from `game_date` if `game_date` is also provided.
                    This primarily applies to the static ScoreboardV2. Defaults to 0.
        bypass_cache: If True, ignores cached data and fetches fresh data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string containing formatted scoreboard data.
                 Expected dictionary structure passed to format_response:
                 {
                     "gameDate": str (YYYY-MM-DD), // The target date for the scoreboard
                     "games": [
                         { // Structure for each game
                             "gameId": str,
                             "gameStatus": int, // 1: Scheduled, 2: In Progress, 3: Final
                             "gameStatusText": str, // e.g., "7:00 PM ET", "Q2 5:30", "Final", "Halftime"
                             "period": int, // Current period if game is live
                             "gameClock": Optional[str], // Current game clock if game is live (e.g., "05:30")
                             "homeTeam": {
                                 "teamId": int, "teamTricode": str, "score": int,
                                 "wins": Optional[int], "losses": Optional[int]
                             },
                             "awayTeam": {
                                 "teamId": int, "teamTricode": str, "score": int,
                                 "wins": Optional[int], "losses": Optional[int]
                             },
                             "gameEt": str // Game start time in Eastern Time (UTC for live, EST for static)
                         }, ...
                     ]
                 }
                 Returns {"games": []} if no games are found for the date.
                 Or an {'error': 'Error message'} object if a critical issue occurs.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    effective_date_str = game_date if game_date is not None else date.today().strftime('%Y-%m-%d')
    # Explicitly log the date being used, especially when game_date is None
    if game_date is None:
        logger.info(f"fetch_scoreboard_data_logic: game_date is None, defaulting to effective_date_str (today's date): {effective_date_str}")

    logger.info(f"Executing fetch_scoreboard_data_logic for Date: {effective_date_str}, League: {league_id}, Offset: {day_offset}, BypassCache: {bypass_cache}")

    if not validate_date_format(effective_date_str):
        logger.error(f"Invalid date format provided: {effective_date_str}. Use YYYY-MM-DD.")
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=effective_date_str))
        if return_dataframe:
            return error_response, {}
        return error_response

    VALID_LEAGUE_IDS = {getattr(LeagueID, lid) for lid in dir(LeagueID) if not lid.startswith('_') and isinstance(getattr(LeagueID, lid), str)}
    if league_id not in VALID_LEAGUE_IDS:
        error_response = format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(VALID_LEAGUE_IDS)))
        if return_dataframe:
            return error_response, {}
        return error_response

    is_today_target = (effective_date_str == date.today().strftime('%Y-%m-%d') and day_offset == 0)
    formatted_games_list: List[Dict[str, Any]] = []
    force_static_fetch_for_today = False # Flag to indicate stale live data

    try:
        if is_today_target:
            # Live data for today
            cache_ts_live = datetime.now().replace(second=0, microsecond=0).isoformat() # Minute-level timestamp
            live_cache_key = "live_scoreboard_data"

            raw_live_data: Dict[str, Any]
            if bypass_cache:
                logger.info("Bypassing cache, fetching fresh live scoreboard data.")
                live_board = LiveScoreBoard(timeout=settings.DEFAULT_TIMEOUT_SECONDS) # Changed
                raw_live_data = live_board.get_dict()
            else:
                raw_live_data = get_cached_live_scoreboard_data(cache_key=live_cache_key, timestamp=cache_ts_live)

            # Check if live data is stale
            live_api_game_date = raw_live_data.get('scoreboard', {}).get('gameDate')
            actual_current_date = date.today().strftime('%Y-%m-%d')

            if live_api_game_date != actual_current_date:
                logger.warning(f"Live scoreboard API returned stale data for date '{live_api_game_date}' when expecting '{actual_current_date}'. Forcing static fetch for today.")
                force_static_fetch_for_today = True
            else:
                # Process live data if not stale
                scoreboard_data_live = raw_live_data.get('scoreboard', {})
                raw_games_live = scoreboard_data_live.get('games', [])
                for game_item in raw_games_live:
                    home_team_live = game_item.get("homeTeam", {})
                    away_team_live = game_item.get("awayTeam", {})
                    game_status_code = game_item.get("gameStatus", 0) # 1=Sched, 2=In Prog, 3=Final
                    game_status_str = game_item.get("gameStatusText", "Status Unknown")
                    game_clock_live = game_item.get("gameClock")
                    current_period_live = game_item.get("period", 0)

                    if game_status_code == 2: # In Progress
                        if game_clock_live: game_status_str = f"Q{current_period_live} {game_clock_live}"
                        elif game_status_str != "Halftime": game_status_str = f"Q{current_period_live} In Progress"

                    formatted_games_list.append({
                        "gameId": game_item.get("gameId"), "gameStatus": game_status_code,
                        "gameStatusText": game_status_str, "period": current_period_live,
                        "gameClock": game_clock_live,
                        "homeTeam": {"teamId": home_team_live.get("teamId"), "teamTricode": home_team_live.get("teamTricode", "N/A"), "score": home_team_live.get("score", 0), "wins": home_team_live.get("wins"), "losses": home_team_live.get("losses")},
                        "awayTeam": {"teamId": away_team_live.get("teamId"), "teamTricode": away_team_live.get("teamTricode", "N/A"), "score": away_team_live.get("score", 0), "wins": away_team_live.get("wins"), "losses": away_team_live.get("losses")},
                        "gameEt": game_item.get("gameTimeUTC", "") # Live endpoint provides UTC
                    })
                logger.info(f"Processed {len(formatted_games_list)} games from live scoreboard data for {effective_date_str}.")

        if not is_today_target or force_static_fetch_for_today:
            date_for_static_fetch = actual_current_date if force_static_fetch_for_today else effective_date_str
            if force_static_fetch_for_today:
                 logger.info(f"Using static fetch for actual current date: {date_for_static_fetch} due to stale live feed.")

            cache_ts_static = datetime.now().replace(minute=0, second=0, microsecond=0).isoformat()
            static_cache_key = ("static_scoreboard_data", date_for_static_fetch, league_id, day_offset) # Use date_for_static_fetch
            api_params_static = {"game_date": date_for_static_fetch, "league_id": league_id, "day_offset": day_offset} # Use date_for_static_fetch

            processed_static_data: Dict[str, List[Dict[str, Any]]]
            if bypass_cache or force_static_fetch_for_today:
                logger.info(f"Fetching fresh static scoreboard data for {date_for_static_fetch} (Bypass: {bypass_cache}, StaleLive: {force_static_fetch_for_today}).")

                def fetch_static_scoreboard():
                    # Use a slightly longer timeout for this specific potentially problematic call
                    endpoint = scoreboardv2.ScoreboardV2(**api_params_static, timeout=settings.DEFAULT_TIMEOUT_SECONDS + 15) # Increased timeout to 45s
                    return endpoint

                try:
                    static_endpoint_instance = retry_on_timeout(fetch_static_scoreboard)
                    game_header_list = _process_dataframe(static_endpoint_instance.game_header.get_data_frame(), single_row=False)
                    line_score_list = _process_dataframe(static_endpoint_instance.line_score.get_data_frame(), single_row=False)
                    processed_static_data = {"game_header": game_header_list or [], "line_score": line_score_list or []}
                except (ReadTimeout, ConnectionError) as e: # Catch errors from retry_on_timeout if all retries fail
                    logger.error(f"Failed to fetch static scoreboard for {date_for_static_fetch} after retries: {e}", exc_info=True)
                    error_response = format_response(error=Errors.NBA_API_TIMEOUT.format(endpoint_name=f"ScoreboardV2 for {date_for_static_fetch}", details=str(e)))
                    if return_dataframe:
                        return error_response, {}
                    return error_response
                except Exception as e: # Catch other unexpected errors
                    logger.error(f"Unexpected error fetching/processing static scoreboard for {date_for_static_fetch}: {e}", exc_info=True)
                    error_response = format_response(error=Errors.NBA_API_GENERAL_ERROR.format(endpoint_name=f"ScoreboardV2 for {date_for_static_fetch}", details=str(e)))
                    if return_dataframe:
                        return error_response, {}
                    return error_response

            else:
                # This is the cached path
                processed_static_data = get_cached_static_scoreboard_data(cache_key=static_cache_key, timestamp=cache_ts_static, **api_params_static)

            game_headers_map = {
                gh['GAME_ID']: gh
                for gh in processed_static_data.get("game_header", [])
                if gh and 'GAME_ID' in gh and gh['GAME_ID'] is not None
            }
            line_scores_by_game_map: Dict[str, Dict[str, Any]] = {}

            for ls_item in processed_static_data.get("line_score", []):
                game_id_static = ls_item.get('GAME_ID')
                team_id_static = ls_item.get('TEAM_ID')
                header_static = game_headers_map.get(game_id_static)
                if not game_id_static or not team_id_static or not header_static: continue
                if game_id_static not in line_scores_by_game_map: line_scores_by_game_map[game_id_static] = {}

                team_key_static = 'homeTeam' if team_id_static == header_static.get('HOME_TEAM_ID') else 'awayTeam'
                wins_static, losses_static = None, None
                wl_record_static = ls_item.get('TEAM_WINS_LOSSES')
                if wl_record_static and '-' in wl_record_static:
                    try: wins_static, losses_static = map(int, wl_record_static.split('-'))
                    except (ValueError, IndexError): pass

                line_scores_by_game_map[game_id_static][team_key_static] = {
                    "teamId": team_id_static, "teamTricode": ls_item.get('TEAM_ABBREVIATION'),
                    "score": ls_item.get('PTS'), "wins": wins_static, "losses": losses_static
                }

            for game_id_hdr, header_item in game_headers_map.items():
                game_line_scores_static = line_scores_by_game_map.get(game_id_hdr, {})
                home_team_data_static = game_line_scores_static.get('homeTeam')
                away_team_data_static = game_line_scores_static.get('awayTeam')
                if home_team_data_static and away_team_data_static:
                    formatted_games_list.append({
                        "gameId": game_id_hdr, "gameStatus": header_item.get('GAME_STATUS_ID'),
                        "gameStatusText": header_item.get('GAME_STATUS_TEXT'),
                        "period": header_item.get('LIVE_PERIOD'), "gameClock": header_item.get('LIVE_PC_TIME'),
                        "homeTeam": home_team_data_static, "awayTeam": away_team_data_static,
                        "gameEt": header_item.get('GAME_DATE_EST') # ScoreboardV2 provides EST
                    })
                else:
                    logger.warning(f"Missing home/away team line score data for game {game_id_hdr} in ScoreboardV2 response for {effective_date_str}.")
            logger.info(f"Processed {len(formatted_games_list)} games from static scoreboard data for {effective_date_str}.")

        final_result = {"gameDate": effective_date_str, "games": formatted_games_list}

        # If DataFrame output is requested, create and save DataFrames
        if return_dataframe:
            # Create a DataFrame for the games
            games_df = pd.DataFrame(formatted_games_list)

            # Create separate DataFrames for home and away teams
            home_teams_data = []
            away_teams_data = []

            for game in formatted_games_list:
                game_id = game.get("gameId")

                # Extract home team data
                home_team = game.get("homeTeam", {})
                if home_team:
                    home_team_data = {
                        "gameId": game_id,
                        "teamId": home_team.get("teamId"),
                        "teamTricode": home_team.get("teamTricode"),
                        "score": home_team.get("score"),
                        "wins": home_team.get("wins"),
                        "losses": home_team.get("losses"),
                        "isHome": True
                    }
                    home_teams_data.append(home_team_data)

                # Extract away team data
                away_team = game.get("awayTeam", {})
                if away_team:
                    away_team_data = {
                        "gameId": game_id,
                        "teamId": away_team.get("teamId"),
                        "teamTricode": away_team.get("teamTricode"),
                        "score": away_team.get("score"),
                        "wins": away_team.get("wins"),
                        "losses": away_team.get("losses"),
                        "isHome": False
                    }
                    away_teams_data.append(away_team_data)

            # Combine home and away teams into a single DataFrame
            teams_df = pd.DataFrame(home_teams_data + away_teams_data)

            # Save DataFrames to CSV
            clean_date = effective_date_str.replace("-", "_")
            games_csv_path = get_cache_file_path(f"scoreboard_games_{clean_date}.csv", "scoreboard")
            teams_csv_path = get_cache_file_path(f"scoreboard_teams_{clean_date}.csv", "scoreboard")

            if not games_df.empty:
                games_df.to_csv(games_csv_path, index=False)

            if not teams_df.empty:
                teams_df.to_csv(teams_csv_path, index=False)

            # Add DataFrame metadata to the response
            games_relative_path = get_relative_cache_path(os.path.basename(games_csv_path), "scoreboard")
            teams_relative_path = get_relative_cache_path(os.path.basename(teams_csv_path), "scoreboard")

            final_result["dataframe_info"] = {
                "message": "Scoreboard data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            if not games_df.empty:
                final_result["dataframe_info"]["dataframes"]["games"] = {
                    "shape": list(games_df.shape),
                    "columns": games_df.columns.tolist(),
                    "csv_path": games_relative_path
                }

            if not teams_df.empty:
                final_result["dataframe_info"]["dataframes"]["teams"] = {
                    "shape": list(teams_df.shape),
                    "columns": teams_df.columns.tolist(),
                    "csv_path": teams_relative_path
                }

            # Return the JSON response and DataFrames
            dataframes = {
                "games": games_df,
                "teams": teams_df
            }

            return format_response(final_result), dataframes

        # Return just the JSON response if DataFrames are not requested
        return format_response(final_result)

    except ReadTimeout as e:
        logger.error(f"Global ReadTimeout in fetch_scoreboard_data_logic for {effective_date_str}: {e}", exc_info=True)
        error_response = format_response(error=Errors.NBA_API_TIMEOUT.format(endpoint_name=f"Scoreboard for {effective_date_str}", details=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response
    except ConnectionError as e:
        logger.error(f"Global ConnectionError in fetch_scoreboard_data_logic for {effective_date_str}: {e}", exc_info=True)
        error_response = format_response(error=Errors.NBA_API_CONNECTION_ERROR.format(endpoint_name=f"Scoreboard for {effective_date_str}", details=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response
    except Exception as e:
        logger.error(f"Unexpected error fetching/formatting scoreboard data for {effective_date_str}: {e}", exc_info=True)
        # Ensure a generic error is returned to the client
        error_response = format_response(error=Errors.UNEXPECTED_ERROR_SCOREBOARD.format(date=effective_date_str, error_details=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response

===== backend\api_tools\search.py =====
"""
Provides search functionalities for players, teams, and games within the NBA data.
Utilizes cached static lists for players and teams, and LeagueGameFinder for games.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import List, Dict, Optional, Tuple, Union
from functools import lru_cache
from nba_api.stats.static import players, teams
from nba_api.stats.endpoints import leaguegamefinder
from nba_api.stats.library.parameters import SeasonTypeAllStar, LeagueID
import pandas as pd

from ..config import settings
from ..core.constants import (
    DEFAULT_PLAYER_SEARCH_LIMIT,
    MIN_PLAYER_SEARCH_LENGTH,
    MAX_SEARCH_RESULTS # Used as default limit for all searches
)
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error, # Used in game search
    TeamNotFoundError
)
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
PLAYER_NAME_FRAGMENT_CACHE_SIZE = 512
PLAYER_SEARCH_CACHE_SIZE = 256
TEAM_SEARCH_CACHE_SIZE = 128
GAME_SEARCH_CACHE_SIZE = 128
GAME_SEARCH_DELIMITERS = [" vs ", " at ", " vs. "]

# --- Cache Directory Setup ---
SEARCH_CSV_DIR = get_cache_dir("search")
PLAYER_SEARCH_CSV_DIR = get_cache_dir("search/players")
TEAM_SEARCH_CSV_DIR = get_cache_dir("search/teams")
GAME_SEARCH_CSV_DIR = get_cache_dir("search/games")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_player_search(query: str) -> str:
    """
    Generates a file path for saving player search results as CSV.

    Args:
        query: The search query

    Returns:
        Path to the CSV file
    """
    # Clean query for filename
    clean_query = query.replace(" ", "_").replace(".", "").lower()

    filename = f"player_search_{clean_query}.csv"
    return get_cache_file_path(filename, "search/players")

def _get_csv_path_for_team_search(query: str) -> str:
    """
    Generates a file path for saving team search results as CSV.

    Args:
        query: The search query

    Returns:
        Path to the CSV file
    """
    # Clean query for filename
    clean_query = query.replace(" ", "_").replace(".", "").lower()

    filename = f"team_search_{clean_query}.csv"
    return get_cache_file_path(filename, "search/teams")

def _get_csv_path_for_game_search(query: str, season: str, season_type: str) -> str:
    """
    Generates a file path for saving game search results as CSV.

    Args:
        query: The search query
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean query and season type for filename
    clean_query = query.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"game_search_{clean_query}_{season}_{clean_season_type}.csv"
    return get_cache_file_path(filename, "search/games")

# --- Global Caches for Static Data ---
_player_list_cache: Optional[List[Dict]] = None
_team_list_cache: Optional[List[Dict]] = None

# --- Helper Functions for Static Data Caching ---
def _get_cached_player_list() -> List[Dict]:
    """Gets the full player list, caching it in memory after the first call."""
    global _player_list_cache
    if _player_list_cache is None:
        logger.info("Fetching and caching full player list from nba_api.stats.static...")
        try:
            _player_list_cache = players.get_players()
            if not _player_list_cache: # Should not happen with nba_api, but good check
                 logger.warning("players.get_players() returned an empty or invalid list.")
                 _player_list_cache = [] # Ensure it's an empty list on failure
            else:
                 logger.info(f"Successfully cached {len(_player_list_cache)} players.")
        except Exception as e:
            logger.error(f"Failed to fetch and cache player list: {e}", exc_info=True)
            _player_list_cache = [] # Ensure it's an empty list on failure
    return _player_list_cache

def _get_cached_team_list() -> List[Dict]:
    """Gets the full team list, caching it in memory after the first call."""
    global _team_list_cache
    if _team_list_cache is None:
        logger.info("Fetching and caching full team list from nba_api.stats.static...")
        try:
            _team_list_cache = teams.get_teams()
            if not _team_list_cache: # Should not happen
                 logger.warning("teams.get_teams() returned an empty or invalid list.")
                 _team_list_cache = []
            else:
                 logger.info(f"Successfully cached {len(_team_list_cache)} teams.")
        except Exception as e:
            logger.error(f"Failed to fetch and cache team list: {e}", exc_info=True)
            _team_list_cache = []
    return _team_list_cache

# --- Player Search Logic ---
@lru_cache(maxsize=PLAYER_NAME_FRAGMENT_CACHE_SIZE)
def find_players_by_name_fragment(name_fragment: str, limit: int = DEFAULT_PLAYER_SEARCH_LIMIT) -> List[Dict]:
    """
    Finds players whose full name contains the given fragment (case-insensitive).
    Uses the in-memory cached full player list.
    """
    if not name_fragment or len(name_fragment) < MIN_PLAYER_SEARCH_LENGTH:
        return []
    all_players = _get_cached_player_list()
    if not all_players: return []

    name_fragment_lower = name_fragment.lower()
    matching_players = []
    try:
        for player in all_players:
            if name_fragment_lower in player['full_name'].lower():
                matching_players.append({
                    'id': player['id'],
                    'full_name': player['full_name'],
                    'is_active': player.get('is_active', False) # is_active might not always be present
                })
                if len(matching_players) >= limit: break
    except Exception as e:
        logger.error(f"Error filtering player list for fragment '{name_fragment}': {e}", exc_info=True)
        return [] # Return empty on error during filtering
    logger.debug(f"Found {len(matching_players)} players for fragment '{name_fragment}' (limit {limit}).")
    return matching_players

def search_players_logic(
    query: str,
    limit: int = MAX_SEARCH_RESULTS,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public API to search for players by name fragment.

    Provides DataFrame output capabilities.

    Args:
        query: The search query string.
        limit: Maximum number of results to return.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing search results or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing search_players_logic with query: '{query}', limit: {limit}, return_dataframe={return_dataframe}")

    if not query:
        error_response = format_response(error=Errors.EMPTY_SEARCH_QUERY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if len(query) < MIN_PLAYER_SEARCH_LENGTH:
        error_response = format_response(error=Errors.SEARCH_QUERY_TOO_SHORT.format(min_length=MIN_PLAYER_SEARCH_LENGTH))
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        # Get matching players
        matching_players = find_players_by_name_fragment(query, limit)

        # Create response data
        response_data = {"players": matching_players}

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            # Convert list of dictionaries to DataFrame
            if matching_players:
                players_df = pd.DataFrame(matching_players)

                # Save DataFrame to CSV
                csv_path = _get_csv_path_for_player_search(query)
                _save_dataframe_to_csv(players_df, csv_path)

                # Add DataFrame metadata to the response
                csv_filename = os.path.basename(csv_path)
                relative_path = get_relative_cache_path(csv_filename, "search/players")

                response_data["dataframe_info"] = {
                    "message": "Player search results have been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "players": {
                            "shape": list(players_df.shape),
                            "columns": players_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

                dataframes = {"players": players_df}
                return format_response(response_data), dataframes
            else:
                # Return empty DataFrame
                empty_df = pd.DataFrame()
                return format_response(response_data), {"players": empty_df}

        return format_response(response_data)

    except Exception as e: # Should be rare as find_players_by_name_fragment handles its errors
        logger.error(f"Unexpected error in search_players_logic: {str(e)}", exc_info=True)
        error_response = format_response(error=Errors.PLAYER_SEARCH_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Team Search Logic ---
def search_teams_logic(
    query: str,
    limit: int = MAX_SEARCH_RESULTS,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public API to search for teams by name, city, nickname, or abbreviation.

    Provides DataFrame output capabilities.

    Args:
        query: The search query string.
        limit: Maximum number of results to return.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing search results or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing search_teams_logic with query: '{query}', limit: {limit}, return_dataframe={return_dataframe}")

    if not query:
        error_response = format_response(error=Errors.EMPTY_SEARCH_QUERY)
        if return_dataframe:
            return error_response, {}
        return error_response

    try:
        # Get all teams
        all_teams = _get_cached_team_list()
        if not all_teams:
            logger.warning("Team list cache is empty, cannot search teams.")
            response_data = {"teams": []}
            if return_dataframe:
                empty_df = pd.DataFrame()
                return format_response(response_data), {"teams": empty_df}
            return format_response(response_data)

        # Filter teams
        query_lower = query.lower()
        filtered_teams = [
            team for team in all_teams
            if query_lower in team.get('full_name', '').lower() or
               query_lower in team.get('city', '').lower() or
               query_lower in team.get('nickname', '').lower() or
               query_lower in team.get('abbreviation', '').lower()
        ][:limit] # Apply limit after filtering

        # Create response data
        response_data = {"teams": filtered_teams}

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            # Convert list of dictionaries to DataFrame
            if filtered_teams:
                teams_df = pd.DataFrame(filtered_teams)

                # Save DataFrame to CSV
                csv_path = _get_csv_path_for_team_search(query)
                _save_dataframe_to_csv(teams_df, csv_path)

                # Add DataFrame metadata to the response
                csv_filename = os.path.basename(csv_path)
                relative_path = get_relative_cache_path(csv_filename, "search/teams")

                response_data["dataframe_info"] = {
                    "message": "Team search results have been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "teams": {
                            "shape": list(teams_df.shape),
                            "columns": teams_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

                dataframes = {"teams": teams_df}
                return format_response(response_data), dataframes
            else:
                # Return empty DataFrame
                empty_df = pd.DataFrame()
                return format_response(response_data), {"teams": empty_df}

        return format_response(response_data)

    except Exception as e:
        logger.error(f"Error in search_teams_logic: {str(e)}", exc_info=True)
        error_response = format_response(error=Errors.TEAM_SEARCH_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Game Search Logic ---
def _parse_game_search_query(query: str) -> Tuple[Optional[int], Optional[str], Optional[int], Optional[str]]:
    """Parses a game search query to identify one or two teams."""
    team1_id: Optional[int] = None
    team1_name: Optional[str] = None
    team2_id: Optional[int] = None
    team2_name: Optional[str] = None

    query_parts: List[str] = []
    for delim in GAME_SEARCH_DELIMITERS:
        if delim in query.lower():
            parts = query.lower().split(delim)
            if len(parts) == 2:
                query_parts = [parts[0].strip(), parts[1].strip()]
                break

    if len(query_parts) == 2:
        try: team1_id, team1_name = find_team_id_or_error(query_parts[0])
        except TeamNotFoundError: logger.warning(f"Game search: First team '{query_parts[0]}' not found.")
        try: team2_id, team2_name = find_team_id_or_error(query_parts[1])
        except TeamNotFoundError: logger.warning(f"Game search: Second team '{query_parts[1]}' not found.")
        logger.info(f"Parsed game query into two teams: '{team1_name or query_parts[0]}' (ID: {team1_id}) and '{team2_name or query_parts[1]}' (ID: {team2_id})")
    else:
        try: team1_id, team1_name = find_team_id_or_error(query)
        except TeamNotFoundError: logger.warning(f"Game search: Single team query '{query}' did not match any team.")
        logger.info(f"Parsed game query for single team: '{team1_name}' (ID: {team1_id})")

    return team1_id, team1_name, team2_id, team2_name

def _filter_for_head_to_head(games_df: pd.DataFrame, team1_id: Optional[int], team2_id: Optional[int]) -> pd.DataFrame:
    """Filters DataFrame for games explicitly between team1 and team2 if both IDs are valid."""
    if games_df.empty or not (team1_id and team2_id):
        return games_df

    team1_info = teams.find_team_name_by_id(team1_id) # nba_api.stats.static.teams
    team2_info = teams.find_team_name_by_id(team2_id)

    if team1_info and team2_info:
        team1_abbr = team1_info['abbreviation']
        team2_abbr = team2_info['abbreviation']
        # Ensure MATCHUP column exists and filter
        if 'MATCHUP' in games_df.columns:
            return games_df[
                games_df['MATCHUP'].str.contains(team1_abbr, case=False, na=False) &
                games_df['MATCHUP'].str.contains(team2_abbr, case=False, na=False)
            ]
        else:
            logger.warning("'MATCHUP' column not found in game finder results for head-to-head filtering.")
    else:
        logger.warning("Could not retrieve team abbreviations for head-to-head game filtering.")
    return games_df # Return original if filtering can't be applied

def search_games_logic(
    query: str,
    season: str, # Made season non-optional as LeagueGameFinder requires it or player/team ID
    season_type: str = SeasonTypeAllStar.regular,
    limit: int = MAX_SEARCH_RESULTS,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Searches for games based on a query (e.g., "TeamA vs TeamB", "Lakers").

    Provides DataFrame output capabilities.

    Args:
        query: The search query string.
        season: The NBA season in YYYY-YY format.
        season_type: The type of season (e.g., "Regular Season", "Playoffs").
        limit: Maximum number of results to return.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing search results or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing search_games_logic with query: '{query}', season: {season}, type: {season_type}, limit: {limit}, return_dataframe={return_dataframe}")

    if not query:
        error_response = format_response(error=Errors.EMPTY_SEARCH_QUERY)
        if return_dataframe:
            return error_response, {}
        return error_response

    if not _validate_season_format(season): # Season is now mandatory
        error_response = format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
        if return_dataframe:
            return error_response, {}
        return error_response

    valid_season_types = [getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)]
    if season_type not in valid_season_types:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(valid_season_types)))
        if return_dataframe:
            return error_response, {}
        return error_response

    team1_id, _, team2_id, _ = _parse_game_search_query(query)

    # If no team could be identified from the query, it's unlikely LeagueGameFinder will succeed well.
    if not team1_id and not team2_id:
        logger.warning(f"No valid teams identified from game search query: '{query}'. Returning empty.")
        response_data = {"games": []}
        if return_dataframe:
            empty_df = pd.DataFrame()
            return format_response(response_data), {"games": empty_df}
        return format_response(response_data)

    try:
        # Call the API
        game_finder = leaguegamefinder.LeagueGameFinder(
            team_id_nullable=team1_id,
            vs_team_id_nullable=team2_id if team1_id else None,
            season_nullable=season,
            season_type_nullable=season_type,
            league_id_nullable=LeagueID.nba, # Default to NBA
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug("LeagueGameFinder API call successful.")

        # Get DataFrame from the API response
        games_df = game_finder.league_game_finder_results.get_data_frame()

        if games_df.empty:
            response_data = {"games": []}
            if return_dataframe:
                empty_df = pd.DataFrame()
                return format_response(response_data), {"games": empty_df}
            return format_response(response_data)

        # If both team IDs were found from a "vs" query, perform stricter filtering
        if team1_id and team2_id and any(d in query.lower() for d in GAME_SEARCH_DELIMITERS):
            filtered_games_df = _filter_for_head_to_head(games_df, team1_id, team2_id)
        else:
            filtered_games_df = games_df

        # Process DataFrame for JSON response
        games_list = _process_dataframe(filtered_games_df, single_row=False)
        if games_list is None:
            logger.error("Failed to process game finder results after potential filtering.")
            error_response = format_response(error=Errors.PROCESSING_ERROR.format(error="game search results"))
            if return_dataframe:
                return error_response, {}
            return error_response

        if games_list: # Sort and limit only if list is not empty
            games_list.sort(key=lambda x: x.get("GAME_DATE", ""), reverse=True)
            games_list = games_list[:limit]

            # Also limit the DataFrame if we're returning it
            if return_dataframe and not filtered_games_df.empty:
                # Sort DataFrame by GAME_DATE in descending order
                if "GAME_DATE" in filtered_games_df.columns:
                    filtered_games_df = filtered_games_df.sort_values(by="GAME_DATE", ascending=False)

                # Limit to the same number of rows as the JSON response
                filtered_games_df = filtered_games_df.head(limit)

        # Create response data
        response_data = {"games": games_list}

        # Return the appropriate result based on return_dataframe
        if return_dataframe:
            # Save DataFrame to CSV if not empty
            if not filtered_games_df.empty:
                csv_path = _get_csv_path_for_game_search(query, season, season_type)
                _save_dataframe_to_csv(filtered_games_df, csv_path)

                # Add DataFrame metadata to the response
                csv_filename = os.path.basename(csv_path)
                relative_path = get_relative_cache_path(csv_filename, "search/games")

                response_data["dataframe_info"] = {
                    "message": "Game search results have been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "games": {
                            "shape": list(filtered_games_df.shape),
                            "columns": filtered_games_df.columns.tolist(),
                            "csv_path": relative_path
                        }
                    }
                }

                dataframes = {"games": filtered_games_df}
                return format_response(response_data), dataframes
            else:
                # Return empty DataFrame
                empty_df = pd.DataFrame()
                return format_response(response_data), {"games": empty_df}

        return format_response(response_data)

    except Exception as e:
        logger.error(f"Error in search_games_logic API call/processing: {str(e)}", exc_info=True)
        error_response = format_response(error=Errors.GAME_SEARCH_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, {}
        return error_response


===== backend\api_tools\shot_charts.py =====
"""
Shot chart module for NBA player shot analysis.
Provides both JSON and DataFrame outputs with CSV caching.
"""

import logging
import json
import os
from typing import Dict, List, Optional, Any, Union, Tuple
import pandas as pd
import numpy as np
from nba_api.stats.endpoints import shotchartdetail
from nba_api.stats.static import players, teams
from .utils import retry_on_timeout, format_response, get_player_id_from_name
from ..config import settings
from ..core.errors import Errors
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
SHOT_CHARTS_CSV_DIR = get_cache_dir("shot_charts")

def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_shot_chart(player_id: int, season: str, season_type: str) -> str:
    """
    Generates a file path for saving shot chart data as CSV.

    Args:
        player_id: The player ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    # Handle None season
    season_str = season if season else "current"

    filename = f"player_{player_id}_shots_{season_str}_{clean_season_type}.csv"
    return get_cache_file_path(filename, "shot_charts")

def fetch_player_shot_chart(
    player_name: str,
    season: Optional[str] = None,
    season_type: str = "Regular Season",
    context_measure: str = "FGA",
    last_n_games: int = 0,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches shot chart data for a specified player.
    Provides DataFrame output capabilities.

    Args:
        player_name (str): The full name of the player to analyze.
        season (str, optional): The NBA season in format YYYY-YY (e.g., "2023-24").
            If None, uses the current season.
        season_type (str): The type of season. Options: "Regular Season", "Playoffs", "Pre Season", "All Star".
        context_measure (str): The statistical measure. Options: "FGA", "FGM", "FG_PCT", etc.
        last_n_games (int): Number of most recent games to include (0 for all games).
        return_dataframe (bool, optional): Whether to return DataFrames along with the JSON response.
                                          Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string containing shot chart data and zone analysis.
                 Expected structure:
                 {
                     "player_name": str,
                     "player_id": int,
                     "team_name": str,
                     "team_id": int,
                     "season": str,
                     "season_type": str,
                     "shots": [
                         {
                             "x": float,  // X coordinate on court
                             "y": float,  // Y coordinate on court
                             "made": bool,  // Whether shot was made
                             "value": int,  // 2 or 3 points
                             "shot_type": str,  // Shot type description
                             "shot_zone": str,  // Shot zone description
                             "distance": float,  // Distance in feet
                             "game_date": str,  // Date of game
                             "period": int,  // Quarter/period
                         },
                         // ... other shots
                     ],
                     "zones": [
                         {
                             "zone": str,  // Zone name
                             "attempts": int,  // Field goal attempts
                             "made": int,  // Field goals made
                             "percentage": float,  // FG percentage
                             "leaguePercentage": float,  // League average FG percentage
                             "relativePercentage": float,  // Difference from league average
                         },
                         // ... other zones
                     ]
                 }
                 Or an {'error': 'Error message'} object if an issue occurs.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    try:
        # Initialize dataframes dictionary if returning DataFrames
        dataframes = {}

        # Get player ID
        player_id_result = get_player_id_from_name(player_name)
        if isinstance(player_id_result, dict) and 'error' in player_id_result:
            error_response = format_response(error=player_id_result['error'])
            if return_dataframe:
                return error_response, dataframes
            return error_response

        player_id = player_id_result

        # Get team ID (use 0 if we can't determine it)
        team_id = 0
        team_name = ""

        # Find the player's current team
        all_players = players.get_players()
        for p in all_players:
            if p['id'] == player_id:
                player_name = p['full_name']  # Use the official name from the API
                break

        # Use the NBA API to get shot chart data
        def fetch_shot_chart():
            return shotchartdetail.ShotChartDetail(
                player_id=player_id,
                team_id=team_id,
                season_nullable=season,
                season_type_all_star=season_type,
                context_measure_simple=context_measure,
                last_n_games=last_n_games,
                league_id='00'
            )

        shot_chart_data = retry_on_timeout(fetch_shot_chart)

        # Process shot data
        shots_df = shot_chart_data.get_data_frames()[0]
        league_avg_df = shot_chart_data.get_data_frames()[1]

        if shots_df.empty:
            error_response = format_response(error=f"No shot data found for {player_name}")
            if return_dataframe:
                return error_response, dataframes
            return error_response

        # Get team info from the first shot
        if team_id == 0 and not shots_df.empty:
            team_id = shots_df.iloc[0]['TEAM_ID']
            team_name = shots_df.iloc[0]['TEAM_NAME']

        # Transform shot data to our format
        shots = []
        for _, row in shots_df.iterrows():
            shot = {
                'x': float(row['LOC_X']),
                'y': float(row['LOC_Y']),
                'made': bool(row['SHOT_MADE_FLAG']),
                'value': 3 if row['SHOT_TYPE'] == '3PT Field Goal' else 2,
                'shot_type': row['ACTION_TYPE'],
                'shot_zone': f"{row['SHOT_ZONE_BASIC']} - {row['SHOT_ZONE_AREA']}",
                'distance': float(row['SHOT_DISTANCE']),
                'game_date': row['GAME_DATE'],
                'period': int(row['PERIOD']),
            }
            shots.append(shot)

        # Process zone data
        zones = []
        for zone_basic in league_avg_df['SHOT_ZONE_BASIC'].unique():
            zone_data = league_avg_df[league_avg_df['SHOT_ZONE_BASIC'] == zone_basic]

            # Player's data for this zone
            player_zone_data = shots_df[shots_df['SHOT_ZONE_BASIC'] == zone_basic]
            player_attempts = len(player_zone_data)
            player_made = player_zone_data['SHOT_MADE_FLAG'].sum()
            player_pct = player_made / player_attempts if player_attempts > 0 else 0

            # League average for this zone
            league_attempts = zone_data['FGA'].sum()
            league_made = zone_data['FGM'].sum()
            league_pct = league_made / league_attempts if league_attempts > 0 else 0

            # Calculate relative percentage
            relative_pct = player_pct - league_pct

            zone = {
                'zone': zone_basic,
                'attempts': int(player_attempts),
                'made': int(player_made),
                'percentage': float(player_pct),
                'leaguePercentage': float(league_pct),
                'relativePercentage': float(relative_pct)
            }
            zones.append(zone)

        result = {
            'player_name': player_name,
            'player_id': player_id,
            'team_name': team_name,
            'team_id': team_id,
            'season': season,
            'season_type': season_type,
            'shots': shots,
            'zones': zones
        }

        # If DataFrame output is requested, save DataFrames and return them
        if return_dataframe:
            # Add shots DataFrame
            shots_df_processed = pd.DataFrame(shots)
            dataframes["shots"] = shots_df_processed

            # Add zones DataFrame
            zones_df = pd.DataFrame(zones)
            dataframes["zones"] = zones_df

            # Add raw shot data
            dataframes["raw_shots"] = shots_df

            # Add league averages
            dataframes["league_averages"] = league_avg_df

            # Save DataFrames to CSV
            if not shots_df_processed.empty:
                shots_csv_path = _get_csv_path_for_shot_chart(player_id, season, season_type)
                _save_dataframe_to_csv(shots_df_processed, shots_csv_path)

                # Add DataFrame metadata to the response
                result["dataframe_info"] = {
                    "message": "Shot chart data has been converted to DataFrames and saved as CSV files",
                    "dataframes": {
                        "shots": {
                            "shape": list(shots_df_processed.shape),
                            "columns": shots_df_processed.columns.tolist(),
                            "csv_path": get_relative_cache_path(os.path.basename(shots_csv_path), "shot_charts")
                        },
                        "zones": {
                            "shape": list(zones_df.shape),
                            "columns": zones_df.columns.tolist()
                        },
                        "raw_shots": {
                            "shape": list(shots_df.shape),
                            "columns": shots_df.columns.tolist()
                        },
                        "league_averages": {
                            "shape": list(league_avg_df.shape),
                            "columns": league_avg_df.columns.tolist()
                        }
                    }
                }

            return format_response(result), dataframes

        # Return just the JSON response if DataFrames are not requested
        return format_response(result)

    except Exception as e:
        logger.error(f"Error in fetch_player_shot_chart for {player_name}: {str(e)}", exc_info=True)
        error_response = format_response(error=f"Failed to fetch shot chart for {player_name}: {str(e)}")
        if return_dataframe:
            return error_response, {}
        return error_response


===== backend\api_tools\shot_chart_league_wide.py =====
"""
Handles fetching and processing shot chart league wide data
from the ShotChartLeagueWide endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The ShotChartLeagueWide endpoint provides league-wide shot chart data (1 DataFrame):
- Zone Info: GRID_TYPE, SHOT_ZONE_BASIC, SHOT_ZONE_AREA, SHOT_ZONE_RANGE (4 columns)
- Shooting Stats: FGA, FGM, FG_PCT (3 columns)
- Rich shot data: League averages by shot zones with detailed shooting statistics (7 columns total)
- Perfect for shot chart analysis, zone-based shooting evaluation, and league-wide shooting trends
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import shotchartleaguewide
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return True  # Empty season is allowed (uses default)
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
SHOT_CHART_LEAGUE_WIDE_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10"}  # NBA, WNBA

# --- Cache Directory Setup ---
SHOT_CHART_LEAGUE_WIDE_CSV_DIR = get_cache_dir("shot_chart_league_wide")

# Ensure cache directories exist
os.makedirs(SHOT_CHART_LEAGUE_WIDE_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_shot_chart_league_wide(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    data_set_name: str = "ShotChartLeagueWide"
) -> str:
    """
    Generates a file path for saving shot chart league wide DataFrame.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"shot_chart_league_wide",
        f"league{league_id}",
        f"season{season.replace('-', '_')}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "shot_chart_league_wide")

# --- Parameter Validation ---
def _validate_shot_chart_league_wide_params(
    league_id: str,
    season: str
) -> Optional[str]:
    """Validates parameters for fetch_shot_chart_league_wide_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=SHOT_CHART_LEAGUE_WIDE_CACHE_SIZE)
def fetch_shot_chart_league_wide_logic(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches shot chart league wide data using the ShotChartLeagueWide endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with shot chart league wide data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_shot_chart_league_wide_logic for League: {league_id}, Season: {season}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_shot_chart_league_wide_params(league_id, season)
    if validation_error:
        logger.warning(f"Parameter validation failed for shot chart league wide: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_shot_chart_league_wide(league_id, season, "ShotChartLeagueWide")
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading shot chart league wide from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["ShotChartLeagueWide"] = df
                result_dict["data_sets"]["ShotChartLeagueWide"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters
        api_params = {
            "league_id": league_id,
            "season": season
        }
        
        logger.debug(f"Calling ShotChartLeagueWide with parameters: {api_params}")
        shot_chart_endpoint = shotchartleaguewide.ShotChartLeagueWide(**api_params)
        
        # Get data frames
        list_of_dataframes = shot_chart_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "league_id": league_id,
                "season": season
            },
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"ShotChartLeagueWide_{idx}" if idx > 0 else "ShotChartLeagueWide"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_shot_chart_league_wide_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_shot_chart_league_wide(
    league_id: str = "00",
    season: str = CURRENT_NBA_SEASON,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get shot chart league wide data.
    
    Args:
        league_id: League ID (default: "00" for NBA)
        season: Season (default: current season)
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with shot chart league wide data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_shot_chart_league_wide_logic(
        league_id=league_id,
        season=season,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing ShotChartLeagueWide endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_shot_chart_league_wide()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_shot_chart_league_wide(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("ShotChartLeagueWide endpoint test completed.")


===== backend\api_tools\synergy_tools.py =====
"""
Handles fetching and processing Synergy Sports play type statistics.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from typing import Optional, Dict, Any, Tuple, Type, List, Set, Union
from datetime import datetime
import pandas as pd

from nba_api.stats.endpoints.synergyplaytypes import SynergyPlayTypes
from nba_api.stats.library.parameters import (
    LeagueID,
    PerModeSimple,
    PlayerOrTeamAbbreviation,
    SeasonTypeAllStar
)
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
SYNERGY_DATA_CACHE_SIZE = 128
CACHE_TTL_SECONDS_SYNERGY = 3600 * 4  # 4 hours

# --- Cache Directory Setup ---
SYNERGY_CSV_DIR = get_cache_dir("synergy")

def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_synergy(
    play_type: str,
    type_grouping: Optional[str],
    player_or_team: str,
    season: str,
    season_type: str
) -> str:
    """
    Generates a file path for saving synergy data as CSV.

    Args:
        play_type: The play type (e.g., 'Isolation', 'PostUp')
        type_grouping: The type grouping ('offensive' or 'defensive')
        player_or_team: Whether the data is for players or teams ('P' or 'T')
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')

    Returns:
        Path to the CSV file
    """
    # Clean values for filename
    clean_play_type = play_type.replace(" ", "_").lower()
    clean_type_grouping = type_grouping.replace(" ", "_").lower() if type_grouping else "all"
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"synergy_{clean_play_type}_{clean_type_grouping}_{player_or_team}_{season}_{clean_season_type}.csv"
    return get_cache_file_path(filename, "synergy")

VALID_PLAY_TYPES: Set[str] = {
    "Cut", "Handoff", "Isolation", "Misc", "OffScreen", "PostUp",
    "PRBallHandler", "PRRollman", "OffRebound", "SpotUp", "Transition"
}
VALID_TYPE_GROUPINGS: Set[str] = {"offensive", "defensive"}

# Validation sets for API parameters, constructed once
_SYNERGY_VALID_PLAYER_TEAM_ABBR: Set[str] = {getattr(PlayerOrTeamAbbreviation, attr) for attr in dir(PlayerOrTeamAbbreviation) if not attr.startswith('_') and isinstance(getattr(PlayerOrTeamAbbreviation, attr), str)}
_SYNERGY_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}
_SYNERGY_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}
_SYNERGY_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}


# --- Helper Functions ---
def get_cached_synergy_data(
    timestamp_bucket: str,
    endpoint_class: Type[SynergyPlayTypes],
    api_kwargs: Dict[str, Any]
) -> Dict[str, Any]:
    """Wrapper for the SynergyPlayTypes NBA API endpoint."""
    logger.info(f"Fetching Synergy data (ts: {timestamp_bucket}). Params: {api_kwargs}")
    try:
        synergy_stats_endpoint = endpoint_class(**api_kwargs, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
        return synergy_stats_endpoint.get_dict()
    except Exception as e:
        logger.error(f"{endpoint_class.__name__} API call failed: {e}", exc_info=True)
        raise

def _validate_synergy_params(
    season: str, play_type_nullable: Optional[str], type_grouping_nullable: Optional[str],
    player_or_team: str, league_id: str, per_mode: str, season_type: str
) -> Optional[str]:
    """Validates parameters for fetch_synergy_play_types_logic."""
    if not _validate_season_format(season):
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if play_type_nullable is None: # This is a required field for the API to return meaningful data
        return Errors.SYNERGY_PLAY_TYPE_REQUIRED.format(options=", ".join(VALID_PLAY_TYPES))
    if play_type_nullable not in VALID_PLAY_TYPES:
        return Errors.INVALID_PLAY_TYPE.format(play_type=play_type_nullable, options=", ".join(VALID_PLAY_TYPES))
    if type_grouping_nullable is not None and type_grouping_nullable not in VALID_TYPE_GROUPINGS:
        return Errors.INVALID_TYPE_GROUPING.format(type_grouping=type_grouping_nullable, options=", ".join(VALID_TYPE_GROUPINGS))
    if player_or_team not in _SYNERGY_VALID_PLAYER_TEAM_ABBR:
        return Errors.INVALID_PLAYER_OR_TEAM_ABBREVIATION.format(value=player_or_team, valid_values=", ".join(_SYNERGY_VALID_PLAYER_TEAM_ABBR))
    if league_id not in _SYNERGY_VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_SYNERGY_VALID_LEAGUE_IDS)[:5])) # Show some options
    if per_mode not in _SYNERGY_VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_SYNERGY_VALID_PER_MODES)[:5]))
    if season_type not in _SYNERGY_VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_SYNERGY_VALID_SEASON_TYPES)[:5]))
    return None

def _extract_synergy_dataframe(response_dict: Dict[str, Any], api_params_logging: Dict[str, Any]) -> pd.DataFrame:
    """Extracts the SynergyPlayType DataFrame from the raw API response."""
    result_set_data = None
    if 'resultSets' in response_dict and isinstance(response_dict['resultSets'], list):
        for rs_item in response_dict['resultSets']:
            if isinstance(rs_item, dict) and rs_item.get('name') == 'SynergyPlayType':
                result_set_data = rs_item
                break
        # Fallback: if not found by name and only one result set exists, assume it's the one.
        if not result_set_data and len(response_dict['resultSets']) == 1 and isinstance(response_dict['resultSets'][0], dict):
            logger.warning(f"SynergyPlayType dataset not found by name, using first available result set for params: {api_params_logging}")
            result_set_data = response_dict['resultSets'][0]

    if not result_set_data or 'headers' not in result_set_data or 'rowSet' not in result_set_data:
        logger.warning(f"Could not find expected 'SynergyPlayType' data structure in API response for params: {api_params_logging}. Response (first 500 chars): {str(response_dict)[:500]}")
        return pd.DataFrame() # Return empty DataFrame

    return pd.DataFrame(result_set_data['rowSet'], columns=result_set_data['headers'])

def _apply_synergy_id_filters(
    data_list: List[Dict[str, Any]],
    player_or_team: str,
    player_id: Optional[int],
    team_id: Optional[int]
) -> List[Dict[str, Any]]:
    """Applies post-fetch filtering by player_id or team_id if provided."""
    if not data_list:
        return []

    filtered_list = data_list
    if player_id is not None and player_or_team == PlayerOrTeamAbbreviation.player:
        filtered_list = [entry for entry in data_list if entry.get("PLAYER_ID") == player_id]
        if not filtered_list: logger.warning(f"No Synergy data after filtering for player_id: {player_id}")
    elif team_id is not None and player_or_team == PlayerOrTeamAbbreviation.team:
        filtered_list = [entry for entry in data_list if entry.get("TEAM_ID") == team_id]
        if not filtered_list: logger.warning(f"No Synergy data after filtering for team_id: {team_id}")
    return filtered_list

# --- Main Logic Function ---
def fetch_synergy_play_types_logic(
    league_id: str = LeagueID.nba,
    per_mode: str = PerModeSimple.per_game,
    player_or_team: str = PlayerOrTeamAbbreviation.team,
    season_type: str = SeasonTypeAllStar.regular,
    season: str = settings.CURRENT_NBA_SEASON,
    play_type_nullable: Optional[str] = None, # Made explicit that it can be None, but validation enforces it
    type_grouping_nullable: Optional[str] = None,
    player_id_nullable: Optional[int] = None, # For post-fetch filtering
    team_id_nullable: Optional[int] = None,   # For post-fetch filtering
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches Synergy Sports play type statistics.
    Provides DataFrame output capabilities.

    A specific `play_type_nullable` is REQUIRED for meaningful data.
    `type_grouping_nullable` ("offensive" or "defensive") is also highly recommended.
    Optional `player_id_nullable` or `team_id_nullable` can be used for post-fetch filtering.

    Args:
        league_id: The league ID. Valid values from `LeagueID` (e.g., "00" for NBA).
        per_mode: Statistical mode. Valid values from `PerModeSimple` (e.g., "PerGame").
        player_or_team: Whether to fetch player or team data. Valid values from `PlayerOrTeamAbbreviation`.
        season_type: The season type. Valid values from `SeasonTypeAllStar`.
        season: The season in YYYY-YY format.
        play_type_nullable: The play type to fetch. REQUIRED for meaningful data.
        type_grouping_nullable: The type grouping ("offensive" or "defensive").
        player_id_nullable: Player ID for post-fetch filtering.
        team_id_nullable: Team ID for post-fetch filtering.
        bypass_cache: If True, ignores cached data and fetches fresh data.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string containing synergy play type statistics.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_synergy_play_types_logic for S: {season}, P/T: {player_or_team}, PlayType: {play_type_nullable}, Grouping: {type_grouping_nullable}, PlayerID: {player_id_nullable}, TeamID: {team_id_nullable}, DataFrame: {return_dataframe}")

    # Initialize dataframes dictionary if returning DataFrames
    dataframes = {}

    validation_error = _validate_synergy_params(
        season, play_type_nullable, type_grouping_nullable, player_or_team,
        league_id, per_mode, season_type
    )
    if validation_error:
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, dataframes
        return error_response

    # API parameters do not include player_id_nullable or team_id_nullable directly for SynergyPlayTypes
    # These are used for post-filtering if player_or_team is general ('P' or 'T')
    api_params_for_call = {
        "league_id": league_id, "per_mode_simple": per_mode,
        "player_or_team_abbreviation": player_or_team,
        "season_type_all_star": season_type, "season": season,
        "play_type_nullable": play_type_nullable, "type_grouping_nullable": type_grouping_nullable
    }
    # Create a timestamp bucket for logging
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_SYNERGY))

    try:
        response_dict: Dict[str, Any]
        if bypass_cache:
            logger.info(f"Bypassing cache, fetching fresh Synergy data with params: {api_params_for_call}")
            synergy_endpoint = SynergyPlayTypes(**api_params_for_call, timeout=settings.DEFAULT_TIMEOUT_SECONDS)
            response_dict = synergy_endpoint.get_dict()
        else:
            response_dict = get_cached_synergy_data(
                timestamp_bucket=timestamp_bucket,
                endpoint_class=SynergyPlayTypes, api_kwargs=api_params_for_call
            )
    except KeyError as ke: # Handles cases where API response might be malformed
        logger.warning(f"KeyError ('{str(ke)}') fetching Synergy data, params: {api_params_for_call}. Returning empty with message.")
        response_data = {
            "parameters": {**api_params_for_call, "player_id_nullable": player_id_nullable, "team_id_nullable": team_id_nullable},
            "synergy_stats": [],
            "message": f"Could not retrieve Synergy data due to API response format issue (KeyError: {str(ke)})."
        }
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)
    except Exception as e:
        logger.error(f"Error fetching Synergy play types (pre-df): {e}", exc_info=True)
        error_response = format_response(error=Errors.SYNERGY_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        synergy_df = _extract_synergy_dataframe(response_dict, api_params_for_call)
        processed_data = _process_dataframe(synergy_df, single_row=False)

        if processed_data is None:
            # _process_dataframe returns None on its internal error, or if df was empty and it decided so.
            # If synergy_df was empty, _extract_synergy_dataframe already logged it.
            if not synergy_df.empty: # Log only if df was not empty but processing failed
                 logger.error(f"DataFrame processing failed for Synergy stats with params: {api_params_for_call}")

            if not synergy_df.empty:
                error_response = format_response(error=Errors.SYNERGY_PROCESSING)
                if return_dataframe:
                    return error_response, dataframes
                return error_response
            else:
                response_data = {
                    "parameters": {**api_params_for_call, "player_id_nullable": player_id_nullable, "team_id_nullable": team_id_nullable},
                    "synergy_stats": [],
                    "message": "No Synergy play type data rows returned by API or processing failed."
                }
                if return_dataframe:
                    return format_response(response_data), dataframes
                return format_response(response_data)

        # Apply post-fetch filtering
        final_data = _apply_synergy_id_filters(processed_data, player_or_team, player_id_nullable, team_id_nullable)

        if not final_data and (player_id_nullable or team_id_nullable):
            # Log if filtering resulted in empty but original data was present
            logger.warning(f"Synergy data became empty after ID filtering. PlayerID: {player_id_nullable}, TeamID: {team_id_nullable}")
            # Return empty stats but include parameters for context
            response_data = {
                "parameters": {**api_params_for_call, "player_id_nullable": player_id_nullable, "team_id_nullable": team_id_nullable},
                "synergy_stats": []
            }
            if return_dataframe:
                return format_response(response_data), dataframes
            return format_response(response_data)

        logger.info(f"Successfully fetched Synergy stats. Found {len(final_data)} entries after all filters.")

        response_data = {
            "parameters": {**api_params_for_call, "player_id_nullable": player_id_nullable, "team_id_nullable": team_id_nullable},
            "synergy_stats": final_data
        }

        # If DataFrame output is requested, save DataFrames and return them
        if return_dataframe:
            # Add synergy stats DataFrame
            synergy_stats_df = pd.DataFrame(final_data) if final_data else pd.DataFrame()
            dataframes["synergy_stats"] = synergy_stats_df

            # Add raw synergy data
            dataframes["raw_synergy_data"] = synergy_df

            # Save DataFrame to CSV if not empty
            if not synergy_stats_df.empty and play_type_nullable:
                csv_path = _get_csv_path_for_synergy(
                    play_type_nullable,
                    type_grouping_nullable,
                    player_or_team,
                    season,
                    season_type
                )
                _save_dataframe_to_csv(synergy_stats_df, csv_path)

                # Add DataFrame metadata to the response
                csv_filename = os.path.basename(csv_path)
                relative_path = get_relative_cache_path(csv_filename, "synergy")

                response_data["dataframe_info"] = {
                    "message": "Synergy play type data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "synergy_stats": {
                            "shape": list(synergy_stats_df.shape),
                            "columns": synergy_stats_df.columns.tolist(),
                            "csv_path": relative_path
                        },
                        "raw_synergy_data": {
                            "shape": list(synergy_df.shape),
                            "columns": synergy_df.columns.tolist()
                        }
                    }
                }

            return format_response(response_data), dataframes

        # Return just the JSON response if DataFrames are not requested
        return format_response(response_data)

    except Exception as e:
        logger.error(f"Error processing Synergy result sets or applying filters: {e}", exc_info=True)
        error_response = format_response(error=Errors.SYNERGY_PROCESSING)
        if return_dataframe:
            return error_response, dataframes
        return error_response


===== backend\api_tools\teamplayeronoffsummary.py =====
import logging
import json
import os # Added for path operations
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd
from nba_api.stats.endpoints import teamplayeronoffsummary
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense
)
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, find_team_id_or_error
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path # Added imports

logger = logging.getLogger(__name__)

# Define cache directory
TEAM_PLAYER_ON_OFF_CSV_DIR = get_cache_dir("team_player_on_off_summary")

_VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_MEASURE_TYPES = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Ensure the directory exists
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_player_on_off_summary(
    team_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str # To distinguish between overall, on_court, off_court
) -> str:
    """
    Generates a file path for saving team player on/off summary DataFrame as CSV.

    Args:
        team_id: The team's ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        measure_type: The measure type (e.g., 'Base', 'Advanced')
        dashboard_type: The type of dashboard (e.g., 'overall', 'on_court')

    Returns:
        Path to the CSV file
    """
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()
    clean_measure_type = measure_type.replace(" ", "_").lower()
    clean_dashboard_type = dashboard_type.replace(" ", "_").lower()

    filename = f"team_{team_id}_onoffsummary_{clean_dashboard_type}_{season}_{clean_season_type}_{clean_per_mode}_{clean_measure_type}.csv"
    return get_cache_file_path(filename, "team_player_on_off_summary")

def fetch_teamplayeronoffsummary_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type_all_star: str = SeasonTypeAllStar.regular,
    per_mode_detailed: str = PerModeDetailed.totals,
    measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    period: int = 0,
    plus_minus: str = "N",
    rank: str = "N",
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team player on/off summary statistics from the NBA API.
    Args:
        team_identifier: Name, abbreviation, or ID of the team
        season: NBA season in YYYY-YY format
        season_type_all_star: Type of season (e.g., 'Regular Season', 'Playoffs')
        per_mode_detailed: Statistical mode (e.g., 'PerGame', 'Totals')
        measure_type_detailed_defense: Measure type (e.g., 'Base', 'Advanced')
        last_n_games: Number of most recent games to include
        month: Filter by month (0 for all)
        opponent_team_id: Filter by opponent team ID
        pace_adjust: Whether to adjust for pace ('Y' or 'N')
        period: Filter by period (0 for all)
        plus_minus: Whether to include plus/minus ('Y' or 'N')
        rank: Whether to include statistical ranks ('Y' or 'N')
        vs_division_nullable: Filter by division
        vs_conference_nullable: Filter by conference
        season_segment_nullable: Filter by season segment
        outcome_nullable: Filter by game outcome ('W' or 'L')
        location_nullable: Filter by game location ('Home' or 'Road')
        league_id_nullable: League ID
        game_segment_nullable: Filter by game segment
        date_from_nullable: Start date filter (YYYY-MM-DD)
        date_to_nullable: End date filter (YYYY-MM-DD)
        return_dataframe: Whether to return DataFrames along with the JSON response
    Returns:
        If return_dataframe=False:
            str: A JSON string containing the team player on/off summary data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string and a dictionary of DataFrames.
    """
    dataframes = {}
    logger.info(f"Executing fetch_teamplayeronoffsummary_logic for '{team_identifier}', season {season}, type {season_type_all_star}, return_dataframe={return_dataframe}")

    # Validate season
    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate dates
    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate season type
    if season_type_all_star not in _VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type_all_star, options=", ".join(list(_VALID_SEASON_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate per_mode
    if per_mode_detailed not in _VALID_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode_detailed, options=", ".join(list(_VALID_PER_MODES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate measure_type
    if measure_type_detailed_defense not in _VALID_MEASURE_TYPES:
        error_msg = Errors.INVALID_MEASURE_TYPE.format(value=measure_type_detailed_defense, options=", ".join(list(_VALID_MEASURE_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate league_id
    if league_id_nullable and league_id_nullable not in _VALID_LEAGUE_IDS:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_LEAGUE_IDS)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
    except Exception as e:
        logger.error(f"Error finding team: {e}", exc_info=True)
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        endpoint = teamplayeronoffsummary.TeamPlayerOnOffSummary(
            team_id=team_id,
            season=season,
            season_type_all_star=season_type_all_star,
            per_mode_detailed=per_mode_detailed,
            measure_type_detailed_defense=measure_type_detailed_defense,
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            league_id_nullable=league_id_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        # DataFrames from the API response
        overall_df = endpoint.overall_team_player_on_off_summary.get_data_frame()
        off_court_df = endpoint.players_off_court_team_player_on_off_summary.get_data_frame()
        on_court_df = endpoint.players_on_court_team_player_on_off_summary.get_data_frame()

        if return_dataframe:
            dataframes["overall"] = overall_df
            dataframes["off_court"] = off_court_df
            dataframes["on_court"] = on_court_df

            # Save DataFrames to CSV
            if not overall_df.empty:
                csv_path_overall = _get_csv_path_for_team_player_on_off_summary(
                    str(team_id), season, season_type_all_star, per_mode_detailed, measure_type_detailed_defense, "overall"
                )
                _save_dataframe_to_csv(overall_df, csv_path_overall)

            if not off_court_df.empty:
                csv_path_off_court = _get_csv_path_for_team_player_on_off_summary(
                    str(team_id), season, season_type_all_star, per_mode_detailed, measure_type_detailed_defense, "off_court"
                )
                _save_dataframe_to_csv(off_court_df, csv_path_off_court)

            if not on_court_df.empty:
                csv_path_on_court = _get_csv_path_for_team_player_on_off_summary(
                    str(team_id), season, season_type_all_star, per_mode_detailed, measure_type_detailed_defense, "on_court"
                )
                _save_dataframe_to_csv(on_court_df, csv_path_on_court)

        # Process DataFrames for JSON response
        overall_list = _process_dataframe(overall_df, single_row=False)
        off_court_list = _process_dataframe(off_court_df, single_row=False)
        on_court_list = _process_dataframe(on_court_df, single_row=False)

        response_data = {
            "team_name": team_actual_name,
            "team_id": team_id,
            "parameters": {
                "season": season,
                "season_type": season_type_all_star,
                "per_mode": per_mode_detailed,
                "measure_type": measure_type_detailed_defense,
                "last_n_games": last_n_games,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "pace_adjust": pace_adjust,
                "period": period,
                "plus_minus": plus_minus,
                "rank": rank,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "league_id": league_id_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "overall_team_player_on_off_summary": overall_list,
            "players_off_court_team_player_on_off_summary": off_court_list,
            "players_on_court_team_player_on_off_summary": on_court_list
        }

        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as api_error:
        logger.error(f"nba_api teamplayeronoffsummary failed: {api_error}", exc_info=True)
        error_label = team_actual_name if 'team_actual_name' in locals() else team_identifier
        error_msg = Errors.TEAM_PLAYER_ON_OFF_SUMMARY_API.format(identifier=error_label, error=str(api_error)) if hasattr(Errors, "TEAM_PLAYER_ON_OFF_SUMMARY_API") else Errors.API_ERROR.format(error=str(api_error))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\teamvsplayer.py =====
import logging
import json
import os # Added for path operations
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd
from nba_api.stats.endpoints import teamvsplayer
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense
)
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, find_team_id_or_error, find_player_id_or_error
from ..utils.validation import _validate_season_format, validate_date_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path # Added imports

logger = logging.getLogger(__name__)

# Define cache directory
TEAM_VS_PLAYER_CSV_DIR = get_cache_dir("team_vs_player")

_VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_MEASURE_TYPES = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_vs_player(
    team_id: str,
    vs_player_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving team vs player DataFrame as CSV.

    Args:
        team_id: The team's ID
        vs_player_id: The opposing player's ID
        season: The season in YYYY-YY format
        season_type: The season type
        per_mode: The per mode
        measure_type: The measure type
        dashboard_type: The type of dashboard (e.g., 'overall', 'on_off_court')

    Returns:
        Path to the CSV file
    """
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()
    clean_measure_type = measure_type.replace(" ", "_").lower()
    clean_dashboard_type = dashboard_type.replace(" ", "_").lower()

    filename = f"team_{team_id}_vs_player_{vs_player_id}_{clean_dashboard_type}_{season}_{clean_season_type}_{clean_per_mode}_{clean_measure_type}.csv"
    return get_cache_file_path(filename, "team_vs_player")

def fetch_teamvsplayer_logic(
    team_identifier: str,
    vs_player_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.totals,
    measure_type: str = MeasureTypeDetailedDefense.base,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    period: int = 0,
    plus_minus: str = "N",
    rank: str = "N",
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    player_identifier: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team vs player statistics from the NBA API.
    Args:
        team_identifier: Name, abbreviation, or ID of the team
        vs_player_identifier: Name or ID of the opposing player
        season: NBA season in YYYY-YY format
        season_type: Type of season (e.g., 'Regular Season', 'Playoffs')
        per_mode: Statistical mode (e.g., 'PerGame', 'Totals')
        measure_type: Measure type (e.g., 'Base', 'Advanced')
        last_n_games: Number of most recent games to include
        month: Filter by month (0 for all)
        opponent_team_id: Filter by opponent team ID
        pace_adjust: Whether to adjust for pace ('Y' or 'N')
        period: Filter by period (0 for all)
        plus_minus: Whether to include plus/minus ('Y' or 'N')
        rank: Whether to include statistical ranks ('Y' or 'N')
        vs_division_nullable: Filter by division
        vs_conference_nullable: Filter by conference
        season_segment_nullable: Filter by season segment
        outcome_nullable: Filter by game outcome ('W' or 'L')
        location_nullable: Filter by game location ('Home' or 'Road')
        league_id_nullable: League ID
        game_segment_nullable: Filter by game segment
        date_from_nullable: Start date filter (YYYY-MM-DD)
        date_to_nullable: End date filter (YYYY-MM-DD)
        player_identifier: (Optional) Player name or ID for PlayerID param
        return_dataframe: Whether to return DataFrames along with the JSON response
    Returns:
        If return_dataframe=False:
            str: A JSON string containing the team vs player data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string and a dictionary of DataFrames.
    """
    dataframes = {}
    logger.info(f"Executing fetch_teamvsplayer_logic for team '{team_identifier}', vs_player '{vs_player_identifier}', season {season}, type {season_type}, return_dataframe={return_dataframe}")

    # Validate season
    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate dates
    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate season type
    if season_type not in _VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate per_mode
    if per_mode not in _VALID_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate measure_type
    if measure_type not in _VALID_MEASURE_TYPES:
        error_msg = Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_MEASURE_TYPES)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate league_id
    if league_id_nullable and league_id_nullable not in _VALID_LEAGUE_IDS:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_LEAGUE_IDS)[:5]))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
        vs_player_id, vs_player_actual_name = find_player_id_or_error(vs_player_identifier)
        player_id = None
        if player_identifier:
            player_id, _ = find_player_id_or_error(player_identifier)
    except Exception as e:
        logger.error(f"Error finding team/player: {e}", exc_info=True)
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        endpoint = teamvsplayer.TeamVsPlayer(
            team_id=team_id,
            vs_player_id=vs_player_id,
            season=season,
            season_type_playoffs=season_type,
            per_mode_detailed=per_mode,
            measure_type_detailed_defense=measure_type,
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            league_id_nullable=league_id_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            player_id_nullable=player_id,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        # DataFrames from the API response
        overall_df = endpoint.overall.get_data_frame()
        on_off_court_df = endpoint.on_off_court.get_data_frame()
        shot_area_overall_df = endpoint.shot_area_overall.get_data_frame()
        shot_area_on_court_df = endpoint.shot_area_on_court.get_data_frame()
        shot_area_off_court_df = endpoint.shot_area_off_court.get_data_frame()
        shot_distance_overall_df = endpoint.shot_distance_overall.get_data_frame()
        shot_distance_on_court_df = endpoint.shot_distance_on_court.get_data_frame()
        shot_distance_off_court_df = endpoint.shot_distance_off_court.get_data_frame()
        vs_player_overall_df = endpoint.vs_player_overall.get_data_frame()

        if return_dataframe:
            dataframes = {
                "overall": overall_df,
                "on_off_court": on_off_court_df,
                "shot_area_overall": shot_area_overall_df,
                "shot_area_on_court": shot_area_on_court_df,
                "shot_area_off_court": shot_area_off_court_df,
                "shot_distance_overall": shot_distance_overall_df,
                "shot_distance_on_court": shot_distance_on_court_df,
                "shot_distance_off_court": shot_distance_off_court_df,
                "vs_player_overall": vs_player_overall_df
            }
            # Save DataFrames to CSV
            for df_key, df_value in dataframes.items():
                if not df_value.empty:
                    csv_path = _get_csv_path_for_team_vs_player(
                        str(team_id),
                        str(vs_player_id),
                        season,
                        season_type,
                        per_mode,
                        measure_type,
                        dashboard_type=df_key
                    )
                    _save_dataframe_to_csv(df_value, csv_path)

        # Process DataFrames for JSON response
        response_data = {
            "team_name": team_actual_name,
            "team_id": team_id,
            "vs_player_name": vs_player_actual_name,
            "vs_player_id": vs_player_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "measure_type": measure_type,
                "last_n_games": last_n_games,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "pace_adjust": pace_adjust,
                "period": period,
                "plus_minus": plus_minus,
                "rank": rank,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "league_id": league_id_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable,
                "player_id": player_id
            },
            "overall": _process_dataframe(overall_df, single_row=False),
            "on_off_court": _process_dataframe(on_off_court_df, single_row=False),
            "shot_area_overall": _process_dataframe(shot_area_overall_df, single_row=False),
            "shot_area_on_court": _process_dataframe(shot_area_on_court_df, single_row=False),
            "shot_area_off_court": _process_dataframe(shot_area_off_court_df, single_row=False),
            "shot_distance_overall": _process_dataframe(shot_distance_overall_df, single_row=False),
            "shot_distance_on_court": _process_dataframe(shot_distance_on_court_df, single_row=False),
            "shot_distance_off_court": _process_dataframe(shot_distance_off_court_df, single_row=False),
            "vs_player_overall": _process_dataframe(vs_player_overall_df, single_row=False)
        }

        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as api_error:
        logger.error(f"nba_api teamvsplayer failed: {api_error}", exc_info=True)
        # Use team_actual_name or vs_player_actual_name if available for better error context
        error_identifier_context = f"team '{team_actual_name if 'team_actual_name' in locals() else team_identifier}' vs player '{vs_player_actual_name if 'vs_player_actual_name' in locals() else vs_player_identifier}'"
        error_msg = Errors.TEAM_VS_PLAYER_API.format(identifier=error_identifier_context, error=str(api_error)) if hasattr(Errors, "TEAM_VS_PLAYER_API") else Errors.API_ERROR.format(error=str(api_error))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_dashboard_shooting.py =====
"""
Handles fetching team dashboard statistics by shooting splits.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the TeamDashboardByShootingSplits endpoint, which provides
detailed team shooting statistics broken down by various splits:
- Overall shooting stats
- Shot type (jump shots, layups, etc.)
- Shot area (restricted area, paint, mid-range, etc.)
- Shot distance (0-5 ft, 5-8 ft, etc.)
- Assisted/unassisted shots
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import teamdashboardbyshootingsplits
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
)
from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from ..utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_DASHBOARD_SHOOTING_CACHE_SIZE = 128
TEAM_DASHBOARD_SHOOTING_CSV_DIR = get_cache_dir("team_dashboard_shooting")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_dashboard(
    team_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    dashboard_type: str
) -> str:
    """
    Generates a file path for saving a dashboard DataFrame as CSV.

    Args:
        team_id: The team ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        dashboard_type: The dashboard type (e.g., overall_team_dashboard)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename
    filename = f"{team_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_{dashboard_type}.csv"

    return get_cache_file_path(filename, "team_dashboard_shooting")

# --- Parameter Validation Functions ---
def _validate_dashboard_params(
    team_identifier: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str
) -> Optional[str]:
    """
    Validates parameters for the team dashboard function.

    Args:
        team_identifier: Team name, abbreviation, or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not team_identifier:
        return Errors.TEAM_IDENTIFIER_EMPTY

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_DASHBOARD_SHOOTING_CACHE_SIZE)
def fetch_team_dashboard_shooting_splits_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team dashboard statistics by shooting splits using the TeamDashboardByShootingSplits endpoint.

    This endpoint provides detailed team shooting statistics broken down by various splits:
    - Overall shooting stats
    - Shot type (jump shots, layups, etc.)
    - Shot area (restricted area, paint, mid-range, etc.)
    - Shot distance (0-5 ft, 5-8 ft, etc.)
    - Assisted/unassisted shots

    Args:
        team_identifier (str): Name, abbreviation, or ID of the team
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with dashboard data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_team_dashboard_shooting_splits_logic for: '{team_identifier}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_dashboard_params(
        team_identifier, season, season_type, measure_type, per_mode
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Get team ID
    try:
        team_id_val, team_actual_name = find_team_id_or_error(team_identifier)
    except TeamNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "team_id": team_id_val,
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "league_id_nullable": "00",
        "last_n_games": 0,
        "month": 0,
        "opponent_team_id": 0,
        "period": 0,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling TeamDashboardByShootingSplits with parameters: {filtered_api_params}")
        dashboard_endpoint = teamdashboardbyshootingsplits.TeamDashboardByShootingSplits(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = dashboard_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = dashboard_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "OverallTeamDashboard",
            "Shot5FTTeamDashboard",
            "Shot8FTTeamDashboard",
            "ShotAreaTeamDashboard",
            "ShotTypeTeamDashboard",
            "AssitedShotTeamDashboard",
            "AssistedBy"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "team_name": team_actual_name,
            "team_id": team_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_dashboard(
                            team_id_val, season, season_type, per_mode, measure_type, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched shooting splits dashboard for {team_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_team_dashboard_shooting_splits_logic for {team_identifier}: {e}",
            exc_info=True
        )
        error_msg = Errors.TEAM_DASHBOARD_SHOOTING_API.format(
            team_identifier=team_identifier, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\team_dash_lineups.py =====
"""
Handles fetching team lineup statistics.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the TeamDashLineups endpoint, which provides
detailed team statistics broken down by different lineup combinations:
- Overall team stats
- Various lineup combinations (2-man, 3-man, 4-man, 5-man)
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import teamdashlineups
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, MeasureTypeDetailedDefense
)
from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_DASH_LINEUPS_CACHE_SIZE = 128
TEAM_DASH_LINEUPS_CSV_DIR = get_cache_dir("team_dash_lineups")

# Valid parameter values
VALID_MEASURE_TYPES = {
    "Base": MeasureTypeDetailedDefense.base,
    "Advanced": MeasureTypeDetailedDefense.advanced,
    "Misc": MeasureTypeDetailedDefense.misc,
    "Four Factors": MeasureTypeDetailedDefense.four_factors,
    "Scoring": MeasureTypeDetailedDefense.scoring,
    "Opponent": MeasureTypeDetailedDefense.opponent,
    "Usage": MeasureTypeDetailedDefense.usage,
    "Defense": MeasureTypeDetailedDefense.defense
}

VALID_PER_MODES = {
    "Totals": PerModeDetailed.totals,
    "PerGame": PerModeDetailed.per_game,
    "MinutesPer": PerModeDetailed.minutes_per,
    "Per48": PerModeDetailed.per_48,
    "Per40": PerModeDetailed.per_40,
    "Per36": PerModeDetailed.per_36,
    "PerMinute": PerModeDetailed.per_minute,
    "PerPossession": PerModeDetailed.per_possession,
    "PerPlay": PerModeDetailed.per_play,
    "Per100Possessions": PerModeDetailed.per_100_possessions,
    "Per100Plays": PerModeDetailed.per_100_plays
}

VALID_SEASON_TYPES = {
    "Regular Season": SeasonTypeAllStar.regular,
    "Playoffs": SeasonTypeAllStar.playoffs,
    "Pre Season": SeasonTypeAllStar.preseason,
    "All Star": SeasonTypeAllStar.all_star
}

VALID_GROUP_QUANTITIES = [2, 3, 4, 5]

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_lineups(
    team_id: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    group_quantity: int,
    data_set_name: str
) -> str:
    """
    Generates a file path for saving a lineups DataFrame as CSV.

    Args:
        team_id: The team ID
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        group_quantity: The number of players in the lineup (2-5)
        data_set_name: The data set name (e.g., Lineups, Overall)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_")
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename
    filename = f"{team_id}_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}_group{group_quantity}_{data_set_name}.csv"

    return get_cache_file_path(filename, "team_dash_lineups")

# --- Parameter Validation Functions ---
def _validate_lineups_params(
    team_identifier: str,
    season: str,
    season_type: str,
    measure_type: str,
    per_mode: str,
    group_quantity: int
) -> Optional[str]:
    """
    Validates parameters for the team lineups function.

    Args:
        team_identifier: Team name, abbreviation, or ID
        season: Season in YYYY-YY format
        season_type: Season type (e.g., Regular Season, Playoffs)
        measure_type: Measure type (e.g., Base, Advanced)
        per_mode: Per mode (e.g., Totals, PerGame)
        group_quantity: Number of players in the lineup (2-5)

    Returns:
        Error message if validation fails, None otherwise
    """
    if not team_identifier:
        return Errors.TEAM_IDENTIFIER_EMPTY

    if not season:
        return Errors.SEASON_EMPTY

    if season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if group_quantity not in VALID_GROUP_QUANTITIES:
        return Errors.INVALID_GROUP_QUANTITY.format(
            value=group_quantity,
            options=", ".join(map(str, VALID_GROUP_QUANTITIES))
        )

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_DASH_LINEUPS_CACHE_SIZE)
def fetch_team_lineups_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    measure_type: str = "Base",
    per_mode: str = "Totals",
    group_quantity: int = 5,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team lineup statistics using the TeamDashLineups endpoint.

    This endpoint provides detailed team statistics broken down by different lineup combinations:
    - Overall team stats
    - Various lineup combinations (2-man, 3-man, 4-man, 5-man)

    Args:
        team_identifier (str): Name, abbreviation, or ID of the team
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        per_mode (str, optional): Per mode for stats. Defaults to "Totals".
        group_quantity (int, optional): Number of players in the lineup (2-5). Defaults to 5.
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with lineups data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_team_lineups_logic for: '{team_identifier}', "
        f"Season: {season}, Measure: {measure_type}, PerMode: {per_mode}, GroupQuantity: {group_quantity}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_lineups_params(
        team_identifier, season, season_type, measure_type, per_mode, group_quantity
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Get team ID
    try:
        team_id_val, team_actual_name = find_team_id_or_error(team_identifier)
    except TeamNotFoundError as e:
        error_msg = str(e)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "team_id": team_id_val,
        "season": season,
        "season_type_all_star": VALID_SEASON_TYPES[season_type],
        "measure_type_detailed_defense": VALID_MEASURE_TYPES[measure_type],
        "per_mode_detailed": VALID_PER_MODES[per_mode],
        "plus_minus": "N",
        "pace_adjust": "N",
        "rank": "N",
        "outcome_nullable": "",
        "location_nullable": "",
        "month": 0,
        "season_segment_nullable": "",
        "date_from_nullable": "",
        "date_to_nullable": "",
        "opponent_team_id": 0,
        "vs_conference_nullable": "",
        "vs_division_nullable": "",
        "game_segment_nullable": "",
        "period": 0,
        "last_n_games": 0,
        "group_quantity": group_quantity,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out None values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v is not None and k != "timeout"}

    try:
        logger.debug(f"Calling TeamDashLineups with parameters: {filtered_api_params}")
        lineups_endpoint = teamdashlineups.TeamDashLineups(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = lineups_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = lineups_endpoint.get_data_frames()

        # Expected data set names based on documentation
        expected_data_set_names = [
            "Lineups",
            "Overall"
        ]

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected names
            data_set_names = expected_data_set_names

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "team_name": team_actual_name,
            "team_id": team_id_val,
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_lineups(
                            team_id_val, season, season_type, per_mode, measure_type, group_quantity, data_set_name
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched lineups data for {team_actual_name}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_team_lineups_logic for {team_identifier}: {e}",
            exc_info=True
        )
        error_msg = Errors.TEAM_DASH_LINEUPS_API.format(
            team_identifier=team_identifier, error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\team_dash_pt_shots.py =====
"""
Handles fetching and processing team dash pt shots data
from the TeamDashPtShots endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The TeamDashPtShots endpoint provides team shooting data (1 DataFrame):
- Team Info: TEAM_ID, TEAM_NAME, SORT_ORDER, G (4 columns)
- Shot Type: SHOT_TYPE (1 column)
- Frequency: FGA_FREQUENCY, FG2A_FREQUENCY, FG3A_FREQUENCY (3 columns)
- Shooting Stats: FGM, FGA, FG_PCT, EFG_PCT, FG2M, FG2A, FG2_PCT, FG3M, FG3A, FG3_PCT (10 columns)
- Rich shooting data: Team shooting statistics by shot type with detailed metrics (18 columns total)
- Perfect for team shooting analysis, shot type evaluation, and efficiency tracking
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import teamdashptshots
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]
    
    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return True  # Empty season is allowed (uses default)
    
    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False
    
    return False

def _validate_team_id(team_id):
    """Validate team ID format."""
    if not team_id:
        return False
    
    # Check if it's a string or integer
    if isinstance(team_id, (str, int)):
        # If string, check if it's a valid number
        if isinstance(team_id, str):
            try:
                int(team_id)
                return True
            except ValueError:
                return False
        return True
    
    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_DASH_PT_SHOTS_CACHE_SIZE = 128

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_SEASON_TYPES: Set[str] = {SeasonTypeAllStar.regular, SeasonTypeAllStar.playoffs, SeasonTypeAllStar.all_star, ""}
VALID_PER_MODES: Set[str] = {PerModeSimple.totals, PerModeSimple.per_game, ""}
VALID_LOCATIONS: Set[str] = {"Home", "Road", ""}
VALID_OUTCOMES: Set[str] = {"W", "L", ""}

# --- Cache Directory Setup ---
TEAM_DASH_PT_SHOTS_CSV_DIR = get_cache_dir("team_dash_pt_shots")

# Ensure cache directories exist
os.makedirs(TEAM_DASH_PT_SHOTS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.
    
    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')
        
        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_dash_pt_shots(
    team_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = "",
    per_mode_simple: str = "",
    league_id: str = "",
    last_n_games: str = "",
    month: str = "",
    opponent_team_id: str = "",
    period: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    data_set_name: str = "TeamDashPtShots"
) -> str:
    """
    Generates a file path for saving team dash pt shots DataFrame.
    
    Args:
        team_id: Team ID (required)
        season: Season (default: current season)
        season_type_all_star: Season type (default: "")
        per_mode_simple: Per mode (default: "")
        league_id: League ID (default: "")
        last_n_games: Last N games (default: "")
        month: Month (default: "")
        opponent_team_id: Opponent team ID (default: "")
        period: Period (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        data_set_name: Name of the data set
        
    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"team_dash_pt_shots",
        f"team{team_id}",
        f"season{season.replace('-', '_')}",
        f"type{season_type_all_star.replace(' ', '_') if season_type_all_star else 'all'}",
        f"mode{per_mode_simple.replace(' ', '_') if per_mode_simple else 'all'}",
        f"league{league_id if league_id else 'all'}",
        f"games{last_n_games if last_n_games else 'all'}",
        f"month{month if month else 'all'}",
        f"opponent{opponent_team_id if opponent_team_id else 'all'}",
        f"period{period if period else 'all'}",
        f"location{location_nullable if location_nullable else 'all'}",
        f"outcome{outcome_nullable if outcome_nullable else 'all'}",
        data_set_name
    ]
    
    filename = "_".join(filename_parts) + ".csv"
    
    return get_cache_file_path(filename, "team_dash_pt_shots")

# --- Parameter Validation ---
def _validate_team_dash_pt_shots_params(
    team_id: str,
    season: str,
    season_type_all_star: str,
    per_mode_simple: str,
    league_id: str
) -> Optional[str]:
    """Validates parameters for fetch_team_dash_pt_shots_logic."""
    if not _validate_team_id(team_id):
        return f"Invalid team_id: {team_id}. Must be a valid team ID"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type_all_star not in VALID_SEASON_TYPES:
        return f"Invalid season_type_all_star: {season_type_all_star}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    if per_mode_simple not in VALID_PER_MODES:
        return f"Invalid per_mode_simple: {per_mode_simple}. Valid options: {', '.join(VALID_PER_MODES)}"
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_DASH_PT_SHOTS_CACHE_SIZE)
def fetch_team_dash_pt_shots_logic(
    team_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = "",
    per_mode_simple: str = "",
    league_id: str = "",
    last_n_games: str = "",
    month: str = "",
    opponent_team_id: str = "",
    period: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    vs_conference_nullable: str = "",
    vs_division_nullable: str = "",
    season_segment_nullable: str = "",
    game_segment_nullable: str = "",
    date_from_nullable: str = "",
    date_to_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team dash pt shots data using the TeamDashPtShots endpoint.
    
    Provides DataFrame output capabilities and CSV caching.
    
    Args:
        team_id: Team ID (required)
        season: Season (default: current season)
        season_type_all_star: Season type (default: "")
        per_mode_simple: Per mode (default: "")
        league_id: League ID (default: "")
        last_n_games: Last N games (default: "")
        month: Month (default: "")
        opponent_team_id: Opponent team ID (default: "")
        period: Period (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        vs_conference_nullable: Vs conference (default: "")
        vs_division_nullable: Vs division (default: "")
        season_segment_nullable: Season segment (default: "")
        game_segment_nullable: Game segment (default: "")
        date_from_nullable: Date from (default: "")
        date_to_nullable: Date to (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team dash pt shots data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_team_dash_pt_shots_logic for Team: {team_id}, Season: {season}, "
        f"Type: {season_type_all_star}, Mode: {per_mode_simple}, League: {league_id}, "
        f"return_dataframe={return_dataframe}"
    )
    
    # Validate parameters
    validation_error = _validate_team_dash_pt_shots_params(
        team_id, season, season_type_all_star, per_mode_simple, league_id
    )
    if validation_error:
        logger.warning(f"Parameter validation failed for team dash pt shots: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response
    
    # Check for cached CSV file
    csv_path = _get_csv_path_for_team_dash_pt_shots(
        team_id, season, season_type_all_star, per_mode_simple, league_id,
        last_n_games, month, opponent_team_id, period, location_nullable, outcome_nullable, "TeamDashPtShots"
    )
    dataframes = {}
    
    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading team dash pt shots from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')
                
                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "team_id": team_id,
                        "season": season,
                        "season_type_all_star": season_type_all_star,
                        "per_mode_simple": per_mode_simple,
                        "league_id": league_id,
                        "last_n_games": last_n_games,
                        "month": month,
                        "opponent_team_id": opponent_team_id,
                        "period": period,
                        "location_nullable": location_nullable,
                        "outcome_nullable": outcome_nullable,
                        "vs_conference_nullable": vs_conference_nullable,
                        "vs_division_nullable": vs_division_nullable,
                        "season_segment_nullable": season_segment_nullable,
                        "game_segment_nullable": game_segment_nullable,
                        "date_from_nullable": date_from_nullable,
                        "date_to_nullable": date_to_nullable
                    },
                    "data_sets": {}
                }
                
                # Store the DataFrame
                dataframes["TeamDashPtShots"] = df
                result_dict["data_sets"]["TeamDashPtShots"] = _process_dataframe(df, single_row=False)
                
                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)
            
        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API
    
    try:
        # Prepare API parameters (only include non-empty parameters)
        api_params = {
            "team_id": team_id,
            "season": season
        }
        
        if season_type_all_star:
            api_params["season_type_all_star"] = season_type_all_star
        if per_mode_simple:
            api_params["per_mode_simple"] = per_mode_simple
        if league_id:
            api_params["league_id"] = league_id
        if last_n_games:
            api_params["last_n_games"] = last_n_games
        if month:
            api_params["month"] = month
        if opponent_team_id:
            api_params["opponent_team_id"] = opponent_team_id
        if period:
            api_params["period"] = period
        if location_nullable:
            api_params["location_nullable"] = location_nullable
        if outcome_nullable:
            api_params["outcome_nullable"] = outcome_nullable
        if vs_conference_nullable:
            api_params["vs_conference_nullable"] = vs_conference_nullable
        if vs_division_nullable:
            api_params["vs_division_nullable"] = vs_division_nullable
        if season_segment_nullable:
            api_params["season_segment_nullable"] = season_segment_nullable
        if game_segment_nullable:
            api_params["game_segment_nullable"] = game_segment_nullable
        if date_from_nullable:
            api_params["date_from_nullable"] = date_from_nullable
        if date_to_nullable:
            api_params["date_to_nullable"] = date_to_nullable
        
        logger.debug(f"Calling TeamDashPtShots with parameters: {api_params}")
        team_shots_endpoint = teamdashptshots.TeamDashPtShots(**api_params)
        
        # Get data frames
        list_of_dataframes = team_shots_endpoint.get_data_frames()
        
        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "team_id": team_id,
                "season": season,
                "season_type_all_star": season_type_all_star,
                "per_mode_simple": per_mode_simple,
                "league_id": league_id,
                "last_n_games": last_n_games,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "period": period,
                "location_nullable": location_nullable,
                "outcome_nullable": outcome_nullable,
                "vs_conference_nullable": vs_conference_nullable,
                "vs_division_nullable": vs_division_nullable,
                "season_segment_nullable": season_segment_nullable,
                "game_segment_nullable": game_segment_nullable,
                "date_from_nullable": date_from_nullable,
                "date_to_nullable": date_to_nullable
            },
            "data_sets": {}
        }
        
        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()
        
        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"TeamDashPtShots_{idx}" if idx > 0 else "TeamDashPtShots"
            
            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df
                
                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()
            
            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data
        
        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)
        
        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response
        
    except Exception as e:
        logger.error(f"Unexpected error in fetch_team_dash_pt_shots_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_team_dash_pt_shots(
    team_id: str,
    season: str = CURRENT_NBA_SEASON,
    season_type_all_star: str = "",
    per_mode_simple: str = "",
    league_id: str = "",
    last_n_games: str = "",
    month: str = "",
    opponent_team_id: str = "",
    period: str = "",
    location_nullable: str = "",
    outcome_nullable: str = "",
    vs_conference_nullable: str = "",
    vs_division_nullable: str = "",
    season_segment_nullable: str = "",
    game_segment_nullable: str = "",
    date_from_nullable: str = "",
    date_to_nullable: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get team dash pt shots data.
    
    Args:
        team_id: Team ID (required)
        season: Season (default: current season)
        season_type_all_star: Season type (default: "")
        per_mode_simple: Per mode (default: "")
        league_id: League ID (default: "")
        last_n_games: Last N games (default: "")
        month: Month (default: "")
        opponent_team_id: Opponent team ID (default: "")
        period: Period (default: "")
        location_nullable: Location (default: "")
        outcome_nullable: Outcome (default: "")
        vs_conference_nullable: Vs conference (default: "")
        vs_division_nullable: Vs division (default: "")
        season_segment_nullable: Season segment (default: "")
        game_segment_nullable: Game segment (default: "")
        date_from_nullable: Date from (default: "")
        date_to_nullable: Date to (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response
        
    Returns:
        If return_dataframe=False:
            str: JSON string with team dash pt shots data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_team_dash_pt_shots_logic(
        team_id=team_id,
        season=season,
        season_type_all_star=season_type_all_star,
        per_mode_simple=per_mode_simple,
        league_id=league_id,
        last_n_games=last_n_games,
        month=month,
        opponent_team_id=opponent_team_id,
        period=period,
        location_nullable=location_nullable,
        outcome_nullable=outcome_nullable,
        vs_conference_nullable=vs_conference_nullable,
        vs_division_nullable=vs_division_nullable,
        season_segment_nullable=season_segment_nullable,
        game_segment_nullable=game_segment_nullable,
        date_from_nullable=date_from_nullable,
        date_to_nullable=date_to_nullable,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # Test basic functionality
    print("Testing TeamDashPtShots endpoint...")
    
    # Test 1: Basic fetch
    json_response = get_team_dash_pt_shots(team_id="1610612738")  # Celtics
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")
    
    # Test 2: With DataFrame output
    json_response, dataframes = get_team_dash_pt_shots(team_id="1610612738", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")
    
    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")
    
    print("TeamDashPtShots endpoint test completed.")


===== backend\api_tools\team_details.py =====
"""
Handles fetching and processing team details data
from the TeamDetails endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The TeamDetails endpoint provides comprehensive team information with 8 DataFrames:
- TeamInfo: basic team details, arena, ownership, management (11 columns)
- TeamHistory: historical team info with year ranges (5 columns)
- SocialMediaAccounts: Facebook, Instagram, Twitter links (2 columns)
- Championships: championship history with opponents (2 columns)
- ConferenceChampionships: conference championship history (2 columns)
- DivisionChampionships: division championship history (2 columns)
- RetiredPlayers: retired jersey numbers and players (6 columns)
- HallOfFamePlayers: HOF players associated with team (6 columns)
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import teamdetails
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_DETAILS_CACHE_SIZE = 32

# Valid parameter sets
VALID_TEAM_IDS: Set[str] = {
    # NBA Teams
    "1610612737", "1610612738", "1610612739", "1610612740", "1610612741",
    "1610612742", "1610612743", "1610612744", "1610612745", "1610612746",
    "1610612747", "1610612748", "1610612749", "1610612750", "1610612751",
    "1610612752", "1610612753", "1610612754", "1610612755", "1610612756",
    "1610612757", "1610612758", "1610612759", "1610612760", "1610612761",
    "1610612762", "1610612763", "1610612764", "1610612765", "1610612766"
}

# --- Cache Directory Setup ---
TEAM_DETAILS_CSV_DIR = get_cache_dir("team_details")

# Ensure cache directories exist
os.makedirs(TEAM_DETAILS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_details(team_id: str, dataset_name: str = "TeamInfo") -> str:
    """
    Generates a file path for saving team details DataFrame.

    Args:
        team_id: Team ID
        dataset_name: Name of the dataset (e.g., "TeamInfo", "Championships", etc.)

    Returns:
        Path to the CSV file
    """
    filename = f"team_details_{team_id}_{dataset_name}.csv"
    return get_cache_file_path(filename, "team_details")

# --- Parameter Validation ---
def _validate_team_details_params(team_id: str) -> Optional[str]:
    """Validates parameters for fetch_team_details_logic."""
    if not team_id:
        return "team_id is required"
    if team_id not in VALID_TEAM_IDS:
        return f"Invalid team_id: {team_id}. Must be a valid NBA team ID"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_DETAILS_CACHE_SIZE)
def fetch_team_details_logic(
    team_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team details data using the TeamDetails endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        team_id: Team ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with team details data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(f"Executing fetch_team_details_logic for Team ID: {team_id}, return_dataframe={return_dataframe}")

    # Validate parameters
    validation_error = _validate_team_details_params(team_id)
    if validation_error:
        logger.warning(f"Parameter validation failed for team details: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV files for all datasets
    dataframes = {}
    dataframe_names = [
        "TeamInfo", "TeamHistory", "SocialMediaAccounts", "Championships",
        "ConferenceChampionships", "DivisionChampionships", "RetiredPlayers", "HallOfFamePlayers"
    ]

    # Check if all CSV files exist
    all_csvs_exist = True
    csv_paths = {}

    if return_dataframe:
        for dataset_name in dataframe_names:
            csv_path = _get_csv_path_for_team_details(team_id, dataset_name)
            csv_paths[dataset_name] = csv_path
            if not os.path.exists(csv_path):
                all_csvs_exist = False
                break

        if all_csvs_exist:
            try:
                logger.info(f"Loading team details from cached CSV files for team {team_id}")

                # Load all DataFrames from CSV
                for dataset_name in dataframe_names:
                    csv_path = csv_paths[dataset_name]
                    if os.path.exists(csv_path):
                        file_size = os.path.getsize(csv_path)
                        if file_size > 50:  # Only load if file has content
                            df = pd.read_csv(csv_path, encoding='utf-8')
                            dataframes[dataset_name] = df

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "team_id": team_id
                    },
                    "data_sets": {}
                }

                # Add all datasets to response
                for dataset_name, df in dataframes.items():
                    result_dict["data_sets"][dataset_name] = _process_dataframe(df, single_row=False)

                # Add empty datasets for missing ones
                for dataset_name in dataframe_names:
                    if dataset_name not in result_dict["data_sets"]:
                        result_dict["data_sets"][dataset_name] = []

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

            except Exception as e:
                logger.error(f"Error loading CSV files: {e}", exc_info=True)
                # If there's an error loading the CSVs, fetch from the API

    try:
        logger.debug(f"Calling TeamDetails with team_id: {team_id}")
        team_details_endpoint = teamdetails.TeamDetails(team_id=team_id)

        # Get data frames
        list_of_dataframes = team_details_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "team_id": team_id
            },
            "data_sets": {}
        }

        # Define meaningful names for the 8 DataFrames
        dataframe_names = [
            "TeamInfo",           # Basic team information
            "TeamHistory",        # Historical team information
            "SocialMediaAccounts", # Social media links
            "Championships",      # Championship history
            "ConferenceChampionships", # Conference championship history
            "DivisionChampionships",   # Division championship history
            "RetiredPlayers",     # Retired jersey numbers
            "HallOfFamePlayers"   # Hall of Fame players
        ]

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            if idx < len(dataframe_names):
                data_set_name = dataframe_names[idx]
            else:
                data_set_name = f"TeamDetails_{idx}"

            if not df.empty:
                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save each DataFrame to its own CSV file
                    csv_path = _get_csv_path_for_team_details(team_id, data_set_name)
                    _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data
            else:
                # Include empty datasets in the response
                result_dict["data_sets"][data_set_name] = []

                # Save empty CSV file for consistency
                if return_dataframe:
                    csv_path = _get_csv_path_for_team_details(team_id, data_set_name)
                    empty_df = pd.DataFrame()
                    _save_dataframe_to_csv(empty_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_team_details_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_team_details(
    team_id: str,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get team details data.

    Args:
        team_id: Team ID (required)
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with team details data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_team_details_logic(
        team_id=team_id,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing TeamDetails endpoint...")

    # Test 1: Basic fetch for Cleveland Cavaliers
    json_response = get_team_details("1610612739")
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_team_details("1610612739", return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("TeamDetails endpoint test completed.")


===== backend\api_tools\team_estimated_metrics.py =====
"""
Handles fetching and processing team estimated metrics data
from the TeamEstimatedMetrics endpoint.
Provides both JSON and DataFrame outputs with CSV caching.

The TeamEstimatedMetrics endpoint provides team estimated metrics data (1 DataFrame):
- Team Info: TEAM_NAME, TEAM_ID, GP, W, L, W_PCT, MIN (7 columns)
- Estimated Metrics: E_OFF_RATING, E_DEF_RATING, E_NET_RATING, E_PACE, E_AST_RATIO, E_OREB_PCT, E_DREB_PCT, E_REB_PCT, E_TM_TOV_PCT (9 columns)
- Rankings: All metrics have corresponding rank columns (14 columns)
- Rich metrics data: Team estimated metrics with advanced analytics (30 columns total)
- Perfect for team analytics, advanced metrics evaluation, and team comparison
"""
import logging
import os
import json
from typing import Optional, Dict, Any, Set, Union, Tuple
from functools import lru_cache

from nba_api.stats.endpoints import teamestimatedmetrics
from nba_api.stats.library.parameters import SeasonType
import pandas as pd

from utils.path_utils import get_cache_dir, get_cache_file_path

# Define utility functions here since we can't import from .utils
def _process_dataframe(df, single_row=False):
    """Process a DataFrame into a list of dictionaries."""
    if df is None or df.empty:
        return []

    # Handle multi-level columns by flattening them
    if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:
        # Flatten multi-level columns
        df = df.copy()
        df.columns = ['_'.join(str(col).strip() for col in cols if str(col).strip()) for cols in df.columns.values]

    # Convert any remaining tuple columns to strings
    if any(isinstance(col, tuple) for col in df.columns):
        df = df.copy()
        df.columns = [str(col) if isinstance(col, tuple) else col for col in df.columns]

    if single_row:
        return df.iloc[0].to_dict()

    return df.to_dict(orient="records")

def format_response(data=None, error=None):
    """Format a response as JSON."""
    if error:
        return json.dumps({"error": error})
    return json.dumps(data)

def _validate_season_format(season):
    """Validate season format (YYYY-YY)."""
    if not season:
        return True  # Empty season is allowed (uses default)

    # Check if it matches YYYY-YY format
    if len(season) == 7 and season[4] == '-':
        try:
            year1 = int(season[:4])
            year2 = int(season[5:])
            # Check if the second year is the next year
            return year2 == (year1 + 1) % 100
        except ValueError:
            return False

    return False

# Default current NBA season
CURRENT_NBA_SEASON = "2024-25"

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_ESTIMATED_METRICS_CACHE_SIZE = 64

# Valid parameter sets
VALID_LEAGUE_IDS: Set[str] = {"00", "10", ""}  # NBA, WNBA, or empty
VALID_SEASON_TYPES: Set[str] = {"Regular Season", "Playoffs", "Pre Season", ""}

# --- Cache Directory Setup ---
TEAM_ESTIMATED_METRICS_CSV_DIR = get_cache_dir("team_estimated_metrics")

# Ensure cache directories exist
os.makedirs(TEAM_ESTIMATED_METRICS_CSV_DIR, exist_ok=True)

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV with UTF-8 encoding
        df.to_csv(file_path, index=False, encoding='utf-8')

        # Log the file size and path
        file_size = os.path.getsize(file_path)
        logger.info(f"Saved DataFrame to CSV: {file_path} (Size: {file_size} bytes)")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_estimated_metrics(
    league_id: str = "",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = "",
    data_set_name: str = "TeamEstimatedMetrics"
) -> str:
    """
    Generates a file path for saving team estimated metrics DataFrame.

    Args:
        league_id: League ID (default: "")
        season: Season (default: current season)
        season_type: Season type (default: "")
        data_set_name: Name of the data set

    Returns:
        Path to the CSV file
    """
    # Create a filename based on the parameters
    filename_parts = [
        f"team_estimated_metrics",
        f"league{league_id if league_id else 'all'}",
        f"season{season.replace('-', '_')}",
        f"type{season_type.replace(' ', '_') if season_type else 'all'}",
        data_set_name
    ]

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "team_estimated_metrics")

# --- Parameter Validation ---
def _validate_team_estimated_metrics_params(
    league_id: str,
    season: str,
    season_type: str
) -> Optional[str]:
    """Validates parameters for fetch_team_estimated_metrics_logic."""
    if league_id not in VALID_LEAGUE_IDS:
        return f"Invalid league_id: {league_id}. Valid options: {', '.join(VALID_LEAGUE_IDS)}"
    if not _validate_season_format(season):
        return f"Invalid season format: {season}. Expected format: YYYY-YY"
    if season_type not in VALID_SEASON_TYPES:
        return f"Invalid season_type: {season_type}. Valid options: {', '.join(VALID_SEASON_TYPES)}"
    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_ESTIMATED_METRICS_CACHE_SIZE)
def fetch_team_estimated_metrics_logic(
    league_id: str = "",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team estimated metrics data using the TeamEstimatedMetrics endpoint.

    Provides DataFrame output capabilities and CSV caching.

    Args:
        league_id: League ID (default: "")
        season: Season (default: current season)
        season_type: Season type (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with team estimated metrics data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    logger.info(
        f"Executing fetch_team_estimated_metrics_logic for League: {league_id}, Season: {season}, "
        f"Type: {season_type}, return_dataframe={return_dataframe}"
    )

    # Validate parameters
    validation_error = _validate_team_estimated_metrics_params(league_id, season, season_type)
    if validation_error:
        logger.warning(f"Parameter validation failed for team estimated metrics: {validation_error}")
        error_response = format_response(error=validation_error)
        if return_dataframe:
            return error_response, {}
        return error_response

    # Check for cached CSV file
    csv_path = _get_csv_path_for_team_estimated_metrics(league_id, season, season_type, "TeamEstimatedMetrics")
    dataframes = {}

    if os.path.exists(csv_path) and return_dataframe:
        try:
            # Check if the file is empty or too small
            file_size = os.path.getsize(csv_path)
            if file_size < 100:  # If file is too small, it's probably empty or corrupted
                logger.warning(f"CSV file is too small ({file_size} bytes), fetching from API instead")
                # Delete the corrupted file
                try:
                    os.remove(csv_path)
                    logger.info(f"Deleted corrupted CSV file: {csv_path}")
                except Exception as e:
                    logger.error(f"Error deleting corrupted CSV file: {e}", exc_info=True)
                # Continue to API fetch
            else:
                logger.info(f"Loading team estimated metrics from CSV: {csv_path}")
                # Read CSV with appropriate data types
                df = pd.read_csv(csv_path, encoding='utf-8')

                # Process for JSON response
                result_dict = {
                    "parameters": {
                        "league_id": league_id,
                        "season": season,
                        "season_type": season_type
                    },
                    "data_sets": {}
                }

                # Store the DataFrame
                dataframes["TeamEstimatedMetrics"] = df
                result_dict["data_sets"]["TeamEstimatedMetrics"] = _process_dataframe(df, single_row=False)

                if return_dataframe:
                    return format_response(result_dict), dataframes
                return format_response(result_dict)

        except Exception as e:
            logger.error(f"Error loading CSV: {e}", exc_info=True)
            # If there's an error loading the CSV, fetch from the API

    try:
        # Prepare API parameters (only include non-empty parameters)
        api_params = {
            "season": season
        }

        if league_id:
            api_params["league_id"] = league_id
        if season_type:
            api_params["season_type"] = season_type

        logger.debug(f"Calling TeamEstimatedMetrics with parameters: {api_params}")
        team_metrics_endpoint = teamestimatedmetrics.TeamEstimatedMetrics(**api_params)

        # Get data frames
        list_of_dataframes = team_metrics_endpoint.get_data_frames()

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": {
                "league_id": league_id,
                "season": season,
                "season_type": season_type
            },
            "data_sets": {}
        }

        # Create a combined DataFrame for CSV storage
        combined_df = pd.DataFrame()

        # Process each data frame
        for idx, df in enumerate(list_of_dataframes):
            # Use a generic name for the data set
            data_set_name = f"TeamEstimatedMetrics_{idx}" if idx > 0 else "TeamEstimatedMetrics"

            # Store DataFrame if requested (even if empty for caching purposes)
            if return_dataframe:
                dataframes[data_set_name] = df

                # Use the first (main) DataFrame for CSV storage
                if idx == 0:
                    combined_df = df.copy()

            # Process for JSON response
            processed_data = _process_dataframe(df, single_row=False)
            result_dict["data_sets"][data_set_name] = processed_data

        # Save combined DataFrame to CSV (even if empty for caching purposes)
        if return_dataframe:
            _save_dataframe_to_csv(combined_df, csv_path)

        final_json_response = format_response(result_dict)
        if return_dataframe:
            return final_json_response, dataframes
        return final_json_response

    except Exception as e:
        logger.error(f"Unexpected error in fetch_team_estimated_metrics_logic: {e}", exc_info=True)
        error_response = format_response(error=f"Unexpected error: {e}")
        if return_dataframe:
            return error_response, {}
        return error_response

# --- Public API Functions ---
def get_team_estimated_metrics(
    league_id: str = "",
    season: str = CURRENT_NBA_SEASON,
    season_type: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Public function to get team estimated metrics data.

    Args:
        league_id: League ID (default: "")
        season: Season (default: current season)
        season_type: Season type (default: "")
        return_dataframe: Whether to return DataFrames along with the JSON response

    Returns:
        If return_dataframe=False:
            str: JSON string with team estimated metrics data or an error message
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames
    """
    return fetch_team_estimated_metrics_logic(
        league_id=league_id,
        season=season,
        season_type=season_type,
        return_dataframe=return_dataframe
    )

# --- Example Usage (for testing or direct script execution) ---
if __name__ == '__main__':
    # Configure logging for standalone execution
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Test basic functionality
    print("Testing TeamEstimatedMetrics endpoint...")

    # Test 1: Basic fetch
    json_response = get_team_estimated_metrics()
    data = json.loads(json_response)
    print(f"Basic test - Data sets: {list(data.get('data_sets', {}).keys())}")

    # Test 2: With DataFrame output
    json_response, dataframes = get_team_estimated_metrics(return_dataframe=True)
    data = json.loads(json_response)
    print(f"DataFrame test - DataFrames: {list(dataframes.keys())}")

    for name, df in dataframes.items():
        print(f"DataFrame '{name}' shape: {df.shape}")
        if not df.empty:
            print(f"Columns: {df.columns.tolist()}")
            print(f"Sample data: {df.head(1).to_dict('records')}")

    print("TeamEstimatedMetrics endpoint test completed.")


===== backend\api_tools\team_game_logs.py =====
"""
Handles fetching team game logs.
Provides both JSON and DataFrame outputs with CSV caching.

This module implements the TeamGameLogs endpoint, which provides
detailed game-by-game statistics for teams:
- Basic and advanced statistics for each game
- Game information (date, matchup, outcome)
- Statistical rankings
"""
import os
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import teamgamelogs
from nba_api.stats.library.parameters import (
    MeasureTypePlayerGameLogs
)
from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response
)
from utils.path_utils import get_cache_dir, get_cache_file_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_GAME_LOGS_CACHE_SIZE = 128
TEAM_GAME_LOGS_CSV_DIR = get_cache_dir("team_game_logs")

# Valid parameter values
VALID_SEASON_TYPES = {
    "Regular Season": "Regular Season",
    "Playoffs": "Playoffs",
    "Pre Season": "Pre Season",
    "All Star": "All Star"
}

VALID_PER_MODES = {
    "Totals": "Totals",
    "PerGame": "PerGame"
}

VALID_MEASURE_TYPES = {
    "Base": MeasureTypePlayerGameLogs.base,
    "Advanced": MeasureTypePlayerGameLogs.advanced
}

VALID_GAME_SEGMENTS = {
    "First Half": "First Half",
    "Second Half": "Second Half",
    "Overtime": "Overtime"
}

VALID_LOCATIONS = {
    "Home": "Home",
    "Road": "Road"
}

VALID_OUTCOMES = {
    "W": "W",
    "L": "L"
}

VALID_SEASON_SEGMENTS = {
    "Post All-Star": "Post All-Star",
    "Pre All-Star": "Pre All-Star"
}

VALID_CONFERENCES = {
    "East": "East",
    "West": "West"
}

VALID_DIVISIONS = {
    "Atlantic": "Atlantic",
    "Central": "Central",
    "Southeast": "Southeast",
    "Northwest": "Northwest",
    "Pacific": "Pacific",
    "Southwest": "Southwest"
}

# --- Helper Functions for DataFrame Processing ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file.

    Args:
        df: DataFrame to save
        file_path: Path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.info(f"Saved DataFrame to CSV: {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to CSV: {e}", exc_info=True)

def _get_csv_path_for_team_game_logs(
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    team_id: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
) -> str:
    """
    Generates a file path for saving a team game logs DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., Regular Season, Playoffs)
        per_mode: The per mode (e.g., Totals, PerGame)
        measure_type: The measure type (e.g., Base, Advanced)
        team_id: Optional team ID filter
        date_from: Optional start date filter (MM/DD/YYYY)
        date_to: Optional end date filter (MM/DD/YYYY)

    Returns:
        Path to the CSV file
    """
    # Clean up parameters for filename
    season_clean = season.replace("-", "_") if season else "all_seasons"
    season_type_clean = season_type.replace(" ", "_").lower()
    per_mode_clean = per_mode.replace(" ", "_").lower()
    measure_type_clean = measure_type.replace(" ", "_").lower()

    # Create filename with optional filters
    filename_parts = [
        f"team_game_logs_{season_clean}_{season_type_clean}_{per_mode_clean}_{measure_type_clean}"
    ]

    if team_id:
        filename_parts.append(f"team_{team_id}")

    if date_from:
        date_from_clean = date_from.replace("/", "_")
        filename_parts.append(f"from_{date_from_clean}")

    if date_to:
        date_to_clean = date_to.replace("/", "_")
        filename_parts.append(f"to_{date_to_clean}")

    filename = "_".join(filename_parts) + ".csv"

    return get_cache_file_path(filename, "team_game_logs")

# --- Parameter Validation Functions ---
def _validate_team_game_logs_params(
    season_type: str,
    per_mode: str,
    measure_type: str,
    game_segment: Optional[str] = None,
    location: Optional[str] = None,
    outcome: Optional[str] = None,
    season_segment: Optional[str] = None,
    vs_conference: Optional[str] = None,
    vs_division: Optional[str] = None
) -> Optional[str]:
    """
    Validates parameters for the team game logs function.

    Args:
        season_type: Season type (e.g., Regular Season, Playoffs)
        per_mode: Per mode (e.g., Totals, PerGame)
        measure_type: Measure type (e.g., Base, Advanced)
        game_segment: Optional game segment filter
        location: Optional location filter
        outcome: Optional outcome filter
        season_segment: Optional season segment filter
        vs_conference: Optional conference filter
        vs_division: Optional division filter

    Returns:
        Error message if validation fails, None otherwise
    """
    if season_type and season_type not in VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(
            value=season_type,
            options=", ".join(list(VALID_SEASON_TYPES.keys()))
        )

    if per_mode and per_mode not in VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(
            value=per_mode,
            options=", ".join(list(VALID_PER_MODES.keys()))
        )

    if measure_type and measure_type not in VALID_MEASURE_TYPES:
        return Errors.INVALID_MEASURE_TYPE.format(
            value=measure_type,
            options=", ".join(list(VALID_MEASURE_TYPES.keys()))
        )

    if game_segment and game_segment not in VALID_GAME_SEGMENTS:
        return f"Invalid game_segment: '{game_segment}'. Valid options: {', '.join(list(VALID_GAME_SEGMENTS.keys()))}"

    if location and location not in VALID_LOCATIONS:
        return f"Invalid location: '{location}'. Valid options: {', '.join(list(VALID_LOCATIONS.keys()))}"

    if outcome and outcome not in VALID_OUTCOMES:
        return f"Invalid outcome: '{outcome}'. Valid options: {', '.join(list(VALID_OUTCOMES.keys()))}"

    if season_segment and season_segment not in VALID_SEASON_SEGMENTS:
        return f"Invalid season_segment: '{season_segment}'. Valid options: {', '.join(list(VALID_SEASON_SEGMENTS.keys()))}"

    if vs_conference and vs_conference not in VALID_CONFERENCES:
        return f"Invalid vs_conference: '{vs_conference}'. Valid options: {', '.join(list(VALID_CONFERENCES.keys()))}"

    if vs_division and vs_division not in VALID_DIVISIONS:
        return f"Invalid vs_division: '{vs_division}'. Valid options: {', '.join(list(VALID_DIVISIONS.keys()))}"

    return None

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_GAME_LOGS_CACHE_SIZE)
def fetch_team_game_logs_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = "Regular Season",
    per_mode: str = "PerGame",
    measure_type: str = "Base",
    team_id: str = "",
    date_from: str = "",
    date_to: str = "",
    game_segment: str = "",
    last_n_games: str = "",
    league_id: str = "00",  # NBA
    location: str = "",
    month: str = "",
    opponent_team_id: str = "",
    outcome: str = "",
    po_round: str = "",
    period: str = "",
    player_id: str = "",
    season_segment: str = "",
    shot_clock_range: str = "",
    vs_conference: str = "",
    vs_division: str = "",
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team game logs using the TeamGameLogs endpoint.

    This endpoint provides detailed game-by-game statistics for teams:
    - Basic and advanced statistics for each game
    - Game information (date, matchup, outcome)
    - Statistical rankings

    Args:
        season (str, optional): Season in YYYY-YY format. Defaults to current season.
        season_type (str, optional): Season type. Defaults to "Regular Season".
        per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
        measure_type (str, optional): Statistical category. Defaults to "Base".
        team_id (str, optional): Team ID filter. Defaults to "".
        date_from (str, optional): Start date filter (MM/DD/YYYY). Defaults to "".
        date_to (str, optional): End date filter (MM/DD/YYYY). Defaults to "".
        game_segment (str, optional): Game segment filter. Defaults to "".
        last_n_games (str, optional): Last N games filter. Defaults to "".
        league_id (str, optional): League ID. Defaults to "00" (NBA).
        location (str, optional): Location filter (Home/Road). Defaults to "".
        month (str, optional): Month filter. Defaults to "".
        opponent_team_id (str, optional): Opponent team ID filter. Defaults to "".
        outcome (str, optional): Outcome filter (W/L). Defaults to "".
        po_round (str, optional): Playoff round filter. Defaults to "".
        period (str, optional): Period filter. Defaults to "".
        player_id (str, optional): Player ID filter. Defaults to "".
        season_segment (str, optional): Season segment filter. Defaults to "".
        shot_clock_range (str, optional): Shot clock range filter. Defaults to "".
        vs_conference (str, optional): Conference filter. Defaults to "".
        vs_division (str, optional): Division filter. Defaults to "".
        return_dataframe (bool, optional): Whether to return DataFrames. Defaults to False.

    Returns:
        If return_dataframe=False:
            str: JSON string with team game logs data or error.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: JSON response and dictionary of DataFrames.
    """
    logger.info(
        f"Executing fetch_team_game_logs_logic for: "
        f"Season: {season}, Type: {season_type}, Team ID: {team_id}"
    )

    dataframes: Dict[str, pd.DataFrame] = {}

    # Validate parameters
    validation_error = _validate_team_game_logs_params(
        season_type, per_mode, measure_type, game_segment, location, outcome,
        season_segment, vs_conference, vs_division
    )
    if validation_error:
        if return_dataframe:
            return format_response(error=validation_error), dataframes
        return format_response(error=validation_error)

    # Prepare API parameters - using only parameters that work based on testing
    api_params = {
        "season_nullable": season,
        "season_type_nullable": VALID_SEASON_TYPES.get(season_type, ""),
        "per_mode_simple_nullable": VALID_PER_MODES.get(per_mode, ""),
        "measure_type_player_game_logs_nullable": VALID_MEASURE_TYPES.get(measure_type, ""),
        "date_from_nullable": date_from,
        "date_to_nullable": date_to,
        "game_segment_nullable": game_segment,
        "last_n_games_nullable": last_n_games,
        "league_id_nullable": league_id,
        "location_nullable": location,
        "month_nullable": month,
        "opp_team_id_nullable": opponent_team_id,
        "outcome_nullable": outcome,
        "po_round_nullable": po_round,
        "period_nullable": period,
        "player_id_nullable": player_id,
        "season_segment_nullable": season_segment,
        "shot_clock_range_nullable": shot_clock_range,
        "team_id_nullable": team_id,
        "vs_conference_nullable": vs_conference,
        "vs_division_nullable": vs_division,
        "timeout": settings.DEFAULT_TIMEOUT_SECONDS
    }

    # Filter out empty values for cleaner logging
    filtered_api_params = {k: v for k, v in api_params.items() if v and k != "timeout"}

    try:
        logger.debug(f"Calling TeamGameLogs with parameters: {filtered_api_params}")
        game_logs_endpoint = teamgamelogs.TeamGameLogs(**api_params)

        # Get normalized dictionary for data set names
        normalized_dict = game_logs_endpoint.get_normalized_dict()

        # Get data frames
        list_of_dataframes = game_logs_endpoint.get_data_frames()

        # Expected data set name based on documentation
        expected_data_set_name = "TeamGameLogs"

        # Get data set names from the result sets
        data_set_names = []
        if "resultSets" in normalized_dict:
            data_set_names = list(normalized_dict["resultSets"].keys())
        else:
            # If no result sets found, use expected name
            data_set_names = [expected_data_set_name]

        # Process data for JSON response
        result_dict: Dict[str, Any] = {
            "parameters": filtered_api_params,
            "data_sets": {}
        }

        # Process each data set
        for idx, data_set_name in enumerate(data_set_names):
            if idx < len(list_of_dataframes):
                df = list_of_dataframes[idx]

                # Store DataFrame if requested
                if return_dataframe:
                    dataframes[data_set_name] = df

                    # Save to CSV if not empty
                    if not df.empty:
                        csv_path = _get_csv_path_for_team_game_logs(
                            season, season_type, per_mode, measure_type, team_id, date_from, date_to
                        )
                        _save_dataframe_to_csv(df, csv_path)

                # Process for JSON response
                processed_data = _process_dataframe(df, single_row=False)
                result_dict["data_sets"][data_set_name] = processed_data

        # Return response
        logger.info(f"Successfully fetched team game logs for Season: {season}, Team ID: {team_id}")
        if return_dataframe:
            return format_response(result_dict), dataframes
        return format_response(result_dict)

    except Exception as e:
        logger.error(
            f"API error in fetch_team_game_logs_logic: {e}",
            exc_info=True
        )
        error_msg = Errors.TEAM_GAME_LOGS_API.format(
            team_id=team_id or "N/A", season=season or "N/A", season_type=season_type or "N/A", error=str(e)
        )
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)


===== backend\api_tools\team_general_stats.py =====
"""
Handles fetching general team statistics, including current season dashboard stats
and historical year-by-year performance.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json # For JSONDecodeError
from typing import Optional, Dict, List, Tuple, Any, Set, Union

import pandas as pd
from nba_api.stats.endpoints import teamdashboardbygeneralsplits, teamyearbyyearstats
from nba_api.stats.library.parameters import (
    LeagueID,
    SeasonTypeAllStar,
    MeasureTypeDetailedDefense, # Note: API uses MeasureTypeDetailedDefense for teamdashboardbygeneralsplits
    PerModeDetailed,
    PerModeSimple # For teamyearbyyearstats
)

from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_GENERAL_STATS_CACHE_SIZE = 128
DEFAULT_HISTORICAL_PER_MODE = PerModeSimple.per_game # Sensible default for year-by-year

_TEAM_GENERAL_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TEAM_GENERAL_VALID_PER_MODES: Set[str] = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_TEAM_GENERAL_VALID_MEASURE_TYPES: Set[str] = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_TEAM_GENERAL_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Cache Directory Setup ---
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path
TEAM_GENERAL_CSV_DIR = get_cache_dir("team_general")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_general(
    team_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    measure_type: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving team general stats DataFrame as CSV.

    Args:
        team_name: The team's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        measure_type: The measure type (e.g., 'Base', 'Advanced')
        data_type: The type of data ('dashboard' or 'historical')

    Returns:
        Path to the CSV file
    """
    # Clean team name and data type for filename
    clean_team_name = team_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()
    clean_measure_type = measure_type.replace(" ", "_").lower()

    filename = f"{clean_team_name}_{season}_{clean_season_type}_{clean_per_mode}_{clean_measure_type}_{data_type}.csv"
    return get_cache_file_path(filename, "team_general")

# --- Helper Functions ---
def _fetch_dashboard_general_splits_data(
    team_id: int, season: str, season_type: str, per_mode: str, measure_type: str,
    opponent_team_id: int, date_from: Optional[str], date_to: Optional[str],
    team_name: str = "",
    last_n_games: int = 0,
    league_id_nullable: Optional[str] = None,
    month: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    shot_clock_range_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[Tuple[Dict[str, Any], Optional[str]], Tuple[Dict[str, Any], Optional[str], pd.DataFrame]]:
    """
    Fetches and processes data from teamdashboardbygeneralsplits.

    Args:
        team_id: The team's ID
        season: NBA season in YYYY-YY format
        season_type: Type of season (e.g., "Regular Season")
        per_mode: Statistical mode (e.g., "PerGame")
        measure_type: Type of statistics to retrieve (e.g., "Base", "Advanced")
        opponent_team_id: Filter by opponent team ID
        date_from: Start date filter (YYYY-MM-DD)
        date_to: End date filter (YYYY-MM-DD)
        team_name: The team's name (for CSV file naming)
        last_n_games: Number of games to include (0 for all games)
        league_id_nullable: League ID (e.g., "00" for NBA)
        month: Month number (0 for all months)
        period: Period number (0 for all periods)
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central")
        vs_conference_nullable: Filter by conference (e.g., "East", "West")
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star")
        outcome_nullable: Filter by game outcome (e.g., "W", "L")
        location_nullable: Filter by game location (e.g., "Home", "Road")
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half")
        pace_adjust: Whether to adjust for pace (Y/N)
        plus_minus: Whether to include plus-minus (Y/N)
        rank: Whether to include rank (Y/N)
        shot_clock_range_nullable: Filter by shot clock range
        po_round_nullable: Filter by playoff round
        return_dataframe: Whether to return the original DataFrame

    Returns:
        If return_dataframe=False:
            Tuple[Dict[str, Any], Optional[str]]: Processed data and error message (if any)
        If return_dataframe=True:
            Tuple[Dict[str, Any], Optional[str], pd.DataFrame]: Processed data, error message, and original DataFrame
    """
    dashboard_stats_dict: Dict[str, Any] = {}
    error_message: Optional[str] = None
    overall_stats_df = pd.DataFrame()  # Initialize empty DataFrame

    logger.debug(f"Fetching team dashboard stats for Team ID: {team_id}, Season: {season}, Measure: {measure_type}")
    try:
        dashboard_endpoint = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(
            team_id=team_id,
            season=season,
            season_type_all_star=season_type,
            per_mode_detailed=per_mode,
            measure_type_detailed_defense=measure_type,
            opponent_team_id=opponent_team_id,
            date_from_nullable=date_from,
            date_to_nullable=date_to,
            last_n_games=last_n_games,
            league_id_nullable=league_id_nullable,
            month=month,
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            game_segment_nullable=game_segment_nullable,
            pace_adjust=pace_adjust,
            plus_minus=plus_minus,
            rank=rank,
            shot_clock_range_nullable=shot_clock_range_nullable,
            po_round_nullable=po_round_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        overall_stats_df = dashboard_endpoint.overall_team_dashboard.get_data_frame()

        # Save DataFrame to CSV if requested and not empty
        if return_dataframe and not overall_stats_df.empty and team_name:
            csv_path = _get_csv_path_for_team_general(
                team_name, season, season_type, per_mode, measure_type, "dashboard"
            )
            _save_dataframe_to_csv(overall_stats_df, csv_path)

        dashboard_stats_dict = _process_dataframe(overall_stats_df, single_row=True) or {}

        if overall_stats_df.empty and not dashboard_stats_dict: # Check if it was empty and processing yielded empty
            logger.warning(f"No dashboard stats found for team {team_id}, season {season} with measure {measure_type} and filters.")
        elif dashboard_stats_dict is None: # Should not happen if _process_dataframe returns {} for empty
            error_message = Errors.TEAM_PROCESSING.format(data_type=f"dashboard stats ({measure_type})", identifier=str(team_id), season=season)
            logger.error(error_message)
    except json.JSONDecodeError as jde:
        logger.error(f"API JSONDecodeError for team dashboard {team_id}, measure_type {measure_type}: {jde}", exc_info=True)
        error_message = Errors.TEAM_API.format(data_type=f"team dashboard ({measure_type})", identifier=str(team_id), season=season, error=f"JSONDecodeError: {str(jde)}")
    except Exception as api_error:
        error_message = Errors.TEAM_API.format(data_type=f"team dashboard ({measure_type})", identifier=str(team_id), season=season, error=str(api_error))
        logger.error(error_message, exc_info=True)

    if return_dataframe:
        return dashboard_stats_dict, error_message, overall_stats_df
    return dashboard_stats_dict, error_message

def _fetch_historical_year_by_year_stats(
    team_id: int, league_id: str, season_type_for_hist: str, # Use a distinct season_type for clarity
    team_name: str = "",
    return_dataframe: bool = False
) -> Union[Tuple[List[Dict[str, Any]], Optional[str]], Tuple[List[Dict[str, Any]], Optional[str], pd.DataFrame]]:
    """
    Fetches and processes data from teamyearbyyearstats.

    Args:
        team_id: The team's ID
        league_id: League ID (e.g., "00" for NBA)
        season_type_for_hist: Type of season for historical stats
        team_name: The team's name (for CSV file naming)
        return_dataframe: Whether to return the original DataFrame

    Returns:
        If return_dataframe=False:
            Tuple[List[Dict[str, Any]], Optional[str]]: Processed data and error message (if any)
        If return_dataframe=True:
            Tuple[List[Dict[str, Any]], Optional[str], pd.DataFrame]: Processed data, error message, and original DataFrame
    """
    historical_stats_list: List[Dict[str, Any]] = []
    error_message: Optional[str] = None
    hist_df = pd.DataFrame()  # Initialize empty DataFrame

    logger.debug(f"Fetching historical stats for Team ID: {team_id}, League: {league_id}, PerMode: {DEFAULT_HISTORICAL_PER_MODE}")
    try:
        historical_endpoint = teamyearbyyearstats.TeamYearByYearStats(
            team_id=team_id, league_id=league_id,
            season_type_all_star=season_type_for_hist, # API takes season_type, though typically 'Regular Season' for historical
            per_mode_simple=DEFAULT_HISTORICAL_PER_MODE,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        hist_df = historical_endpoint.team_stats.get_data_frame()

        # Save DataFrame to CSV if requested and not empty
        if return_dataframe and not hist_df.empty and team_name:
            csv_path = _get_csv_path_for_team_general(
                team_name, "all_seasons", season_type_for_hist, DEFAULT_HISTORICAL_PER_MODE, "base", "historical"
            )
            _save_dataframe_to_csv(hist_df, csv_path)

        historical_stats_list = _process_dataframe(hist_df, single_row=False) or []

        if hist_df.empty and not historical_stats_list:
            logger.warning(f"No historical stats found for team {team_id}.")
        elif historical_stats_list is None: # Should not happen if _process_dataframe returns [] for empty
            error_message = Errors.TEAM_PROCESSING.format(data_type="historical stats", identifier=str(team_id), season="N/A")
            logger.error(error_message)
    except Exception as hist_api_error:
        error_message = Errors.TEAM_API.format(data_type="historical stats", identifier=str(team_id), season="N/A", error=str(hist_api_error))
        logger.warning(f"Could not fetch historical stats: {error_message}", exc_info=True)

    if return_dataframe:
        return historical_stats_list, error_message, hist_df
    return historical_stats_list, error_message

# --- Main Logic Function ---
def fetch_team_stats_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON, # Season for dashboard stats
    season_type: str = SeasonTypeAllStar.regular, # SeasonType for dashboard stats
    per_mode: str = PerModeDetailed.per_game, # PerMode for dashboard stats
    measure_type: str = MeasureTypeDetailedDefense.base,  # MeasureType for dashboard stats
    opponent_team_id: int = 0, # For dashboard stats
    date_from: Optional[str] = None, # For dashboard stats
    date_to: Optional[str] = None,   # For dashboard stats
    league_id: str = LeagueID.nba,    # LeagueID for historical stats (and dashboard if needed)
    last_n_games: int = 0,
    month: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    pace_adjust: str = "N",
    plus_minus: str = "N",
    rank: str = "N",
    shot_clock_range_nullable: Optional[str] = None,
    po_round_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches comprehensive team statistics: current season dashboard stats and historical year-by-year performance.
    Provides DataFrame output capabilities.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format for dashboard stats. Defaults to current season.
        season_type: Type of season for dashboard stats. Defaults to Regular Season.
        per_mode: Statistical mode for dashboard stats. Defaults to PerGame.
        measure_type: Type of statistics to retrieve for dashboard stats. Defaults to Base.
        opponent_team_id: Filter by opponent team ID for dashboard stats. Defaults to 0 (all).
        date_from: Start date filter for dashboard stats (YYYY-MM-DD).
        date_to: End date filter for dashboard stats (YYYY-MM-DD).
        league_id: League ID for historical stats. Defaults to NBA.
        last_n_games: Number of games to include (0 for all games).
        month: Month number (0 for all months).
        period: Period number (0 for all periods).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half").
        pace_adjust: Whether to adjust for pace (Y/N).
        plus_minus: Whether to include plus-minus (Y/N).
        rank: Whether to include rank (Y/N).
        shot_clock_range_nullable: Filter by shot clock range.
        po_round_nullable: Filter by playoff round.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with team stats or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_team_stats_logic for: '{team_identifier}', Dashboard Season: {season}, Dashboard Measure: {measure_type}, " +
              f"LastNGames: {last_n_games}, Month: {month}, Period: {period}, VsDivision: {vs_division_nullable}, " +
              f"VsConference: {vs_conference_nullable}, SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, " +
              f"Location: {location_nullable}, GameSegment: {game_segment_nullable}, PaceAdjust: {pace_adjust}, " +
              f"PlusMinus: {plus_minus}, Rank: {rank}, ShotClockRange: {shot_clock_range_nullable}, " +
              f"PORound: {po_round_nullable}, return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    # Parameter Validations
    if not team_identifier or not str(team_identifier).strip():
        if return_dataframe:
            return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY), dataframes
        return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY)

    if not season or not _validate_season_format(season): # Validates dashboard season
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season)), dataframes
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))

    if date_from and not validate_date_format(date_from):
        if return_dataframe:
            return format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from)), dataframes
        return format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from))

    if date_to and not validate_date_format(date_to):
        if return_dataframe:
            return format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to)), dataframes
        return format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to))

    if season_type not in _TEAM_GENERAL_VALID_SEASON_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_GENERAL_VALID_SEASON_TYPES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_GENERAL_VALID_SEASON_TYPES)[:5])))

    if per_mode not in _TEAM_GENERAL_VALID_PER_MODES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_TEAM_GENERAL_VALID_PER_MODES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_TEAM_GENERAL_VALID_PER_MODES)[:5])))

    if measure_type not in _TEAM_GENERAL_VALID_MEASURE_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_TEAM_GENERAL_VALID_MEASURE_TYPES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_TEAM_GENERAL_VALID_MEASURE_TYPES)[:5])))

    if league_id not in _TEAM_GENERAL_VALID_LEAGUE_IDS:
        if return_dataframe:
            return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TEAM_GENERAL_VALID_LEAGUE_IDS)[:3]))), dataframes
        return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TEAM_GENERAL_VALID_LEAGUE_IDS)[:3])))

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
        all_errors: List[str] = []

        # Fetch dashboard stats with DataFrame if requested
        if return_dataframe:
            dashboard_stats, dash_err, dashboard_df = _fetch_dashboard_general_splits_data(
                team_id, season, season_type, per_mode, measure_type, opponent_team_id, date_from, date_to,
                team_name=team_actual_name,
                last_n_games=last_n_games,
                league_id_nullable=league_id,
                month=month,
                period=period,
                vs_division_nullable=vs_division_nullable,
                vs_conference_nullable=vs_conference_nullable,
                season_segment_nullable=season_segment_nullable,
                outcome_nullable=outcome_nullable,
                location_nullable=location_nullable,
                game_segment_nullable=game_segment_nullable,
                pace_adjust=pace_adjust,
                plus_minus=plus_minus,
                rank=rank,
                shot_clock_range_nullable=shot_clock_range_nullable,
                po_round_nullable=po_round_nullable,
                return_dataframe=True
            )
            dataframes["dashboard"] = dashboard_df
        else:
            dashboard_stats, dash_err = _fetch_dashboard_general_splits_data(
                team_id, season, season_type, per_mode, measure_type, opponent_team_id, date_from, date_to,
                last_n_games=last_n_games,
                league_id_nullable=league_id,
                month=month,
                period=period,
                vs_division_nullable=vs_division_nullable,
                vs_conference_nullable=vs_conference_nullable,
                season_segment_nullable=season_segment_nullable,
                outcome_nullable=outcome_nullable,
                location_nullable=location_nullable,
                game_segment_nullable=game_segment_nullable,
                pace_adjust=pace_adjust,
                plus_minus=plus_minus,
                rank=rank,
                shot_clock_range_nullable=shot_clock_range_nullable,
                po_round_nullable=po_round_nullable
            )

        if dash_err: all_errors.append(dash_err)

        # Fetch historical stats with DataFrame if requested
        if return_dataframe:
            historical_stats, hist_err, historical_df = _fetch_historical_year_by_year_stats(
                team_id, league_id, season_type,
                team_name=team_actual_name, return_dataframe=True
            )
            dataframes["historical"] = historical_df
        else:
            historical_stats, hist_err = _fetch_historical_year_by_year_stats(
                team_id, league_id, season_type
            )

        if hist_err: all_errors.append(hist_err)

        if not dashboard_stats and not historical_stats and all_errors:
            # If both fetches failed and produced errors
            error_summary = Errors.TEAM_ALL_FAILED.format(identifier=team_identifier, season=season, errors_list=', '.join(all_errors))
            logger.error(error_summary)

            if return_dataframe:
                return format_response(error=error_summary), dataframes
            return format_response(error=error_summary)

        result = {
            "team_id": team_id, "team_name": team_actual_name,
            "parameters": {
                "season_for_dashboard": season,
                "season_type_for_dashboard": season_type,
                "per_mode_for_dashboard": per_mode,
                "measure_type_for_dashboard": measure_type,
                "opponent_team_id_for_dashboard": opponent_team_id,
                "date_from_for_dashboard": date_from,
                "date_to_for_dashboard": date_to,
                "league_id_for_historical": league_id,
                "season_type_for_historical": season_type,
                "last_n_games": last_n_games,
                "month": month,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "pace_adjust": pace_adjust,
                "plus_minus": plus_minus,
                "rank": rank,
                "shot_clock_range": shot_clock_range_nullable,
                "po_round": po_round_nullable
            },
            "current_season_dashboard_stats": dashboard_stats, # Renamed for clarity
            "historical_year_by_year_stats": historical_stats # Renamed for clarity
        }
        if all_errors:
            result["partial_errors"] = all_errors

        logger.info(f"fetch_team_stats_logic completed for Team ID: {team_id}, Dashboard Season: {season}")

        if return_dataframe:
            # Add DataFrame metadata to the response
            result["dataframe_info"] = {
                "message": "Team stats data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add dashboard DataFrame metadata if available
            if "dashboard" in dataframes and not dataframes["dashboard"].empty:
                dashboard_csv_path = _get_csv_path_for_team_general(
                    team_actual_name, season, season_type, per_mode, measure_type, "dashboard"
                )
                csv_filename = os.path.basename(dashboard_csv_path)
                relative_path = get_relative_cache_path(csv_filename, "team_general")

                result["dataframe_info"]["dataframes"]["dashboard"] = {
                    "shape": list(dataframes["dashboard"].shape),
                    "columns": dataframes["dashboard"].columns.tolist(),
                    "csv_path": relative_path
                }

            # Add historical DataFrame metadata if available
            if "historical" in dataframes and not dataframes["historical"].empty:
                historical_csv_path = _get_csv_path_for_team_general(
                    team_actual_name, "all_seasons", season_type, DEFAULT_HISTORICAL_PER_MODE, "base", "historical"
                )
                csv_filename = os.path.basename(historical_csv_path)
                relative_path = get_relative_cache_path(csv_filename, "team_general")

                result["dataframe_info"]["dataframes"]["historical"] = {
                    "shape": list(dataframes["historical"].shape),
                    "columns": dataframes["historical"].columns.tolist(),
                    "csv_path": relative_path
                }

            return format_response(result), dataframes
        return format_response(result)

    except (TeamNotFoundError, ValueError) as e: # From find_team_id_or_error
        logger.warning(f"Team lookup or initial validation failed for '{team_identifier}': {e}")

        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    except Exception as e: # Catch-all for unexpected issues in orchestration
        error_msg = Errors.TEAM_UNEXPECTED.format(identifier=team_identifier, season=season, error=str(e))
        logger.critical(error_msg, exc_info=True)

        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_historical_leaders.py =====
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple, List, Set
import pandas as pd
from nba_api.stats.endpoints import teamhistoricalleaders
from nba_api.stats.library.parameters import LeagueID # For default value
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, find_team_id_or_error, TeamNotFoundError
from ..utils.validation import _validate_league_id # Assuming this exists and is appropriate

logger = logging.getLogger(__name__)

# Define valid league IDs, similar to other tools if not using _validate_league_id directly
_VALID_LEAGUE_IDS_HISTORICAL: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}


def _validate_season_id_format(season_id: str) -> bool:
    """Validates that the season_id is a 5-digit string."""
    return isinstance(season_id, str) and len(season_id) == 5 and season_id.isdigit()

def fetch_team_historical_leaders_logic(
    team_identifier: str,
    season_id: str, # e.g., "22022" for 2022-23 season
    league_id: str = LeagueID.nba, # Default to NBA "00"
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches historical career leaders for a specific team and season.

    The 'SeasonID' parameter for this endpoint refers to the season for which
    the historical leaders are being requested (e.g., if you want to see who
    the historical leaders were as of the end of the 2022-23 season, you would use "22022").

    Args:
        team_identifier (str): The name, abbreviation, or ID of the team.
        season_id (str): The 5-digit season ID (e.g., "22022" for the 2022-23 season).
                         This specifies the point in time for which to retrieve historical leaders.
        league_id (str, optional): The league ID. Defaults to "00" (NBA).
                                   Other examples: "10" (WNBA), "20" (G-League).
        return_dataframe (bool, optional): Whether to return DataFrames along with the JSON response. 
                                           Defaults to False.

    Returns:
        Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: 
            If return_dataframe is False, a JSON string with the team historical leaders data or an error.
            If return_dataframe is True, a tuple containing the JSON response string and a dictionary 
            of DataFrames: {'career_leaders_by_team': df}.
    """
    dataframes: Dict[str, pd.DataFrame] = {}
    logger.info(
        f"Executing fetch_team_historical_leaders_logic for team: '{team_identifier}', "
        f"SeasonID: {season_id}, LeagueID: {league_id}, DataFrame: {return_dataframe}"
    )

    # Parameter Validations
    if not team_identifier or not str(team_identifier).strip():
        error_msg = Errors.TEAM_IDENTIFIER_EMPTY
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if not _validate_season_id_format(season_id):
        error_msg = Errors.INVALID_PARAMETER_FORMAT.format(param_name="SeasonID", param_value=season_id, expected_format="a 5-digit string (e.g., '22022')")
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Use _validate_league_id if it's suitable, otherwise use the set directly
    if not _validate_league_id(league_id): # Or: if league_id not in _VALID_LEAGUE_IDS_HISTORICAL:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id, options=list(_VALID_LEAGUE_IDS_HISTORICAL)[:3])
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_id_actual, team_actual_name = find_team_id_or_error(team_identifier)
    except (TeamNotFoundError, ValueError) as e:
        logger.warning(f"Team lookup failed for '{team_identifier}': {e}")
        if return_dataframe: return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        logger.debug(f"Calling TeamHistoricalLeaders API for TeamID: {team_id_actual}, SeasonID: {season_id}")
        endpoint = teamhistoricalleaders.TeamHistoricalLeaders(
            team_id=team_id_actual,
            season_id=season_id,
            league_id=league_id,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        
        career_leaders_df = endpoint.career_leaders_by_team.get_data_frame()

        if return_dataframe:
            dataframes["career_leaders_by_team"] = career_leaders_df

        processed_leaders = _process_dataframe(career_leaders_df, single_row=False) # Returns a list of dicts

        if processed_leaders is None: # Indicates a processing error in _process_dataframe
             error_msg = Errors.PROCESSING_ERROR.format(error=f"Failed to process TeamHistoricalLeaders data for team {team_actual_name}, season_id {season_id}")
             logger.error(error_msg)
             if return_dataframe: return format_response(error=error_msg), dataframes
             return format_response(error=error_msg)
        
        if not processed_leaders and career_leaders_df.empty:
            logger.info(f"No historical leaders data returned by API for team {team_actual_name}, season_id {season_id}.")
            # This is not necessarily an error, API might return empty for some valid queries.

        response_data = {
            "team_name": team_actual_name,
            "team_id": team_id_actual,
            "parameters": {
                "season_id": season_id,
                "league_id": league_id
            },
            "career_leaders_by_team": processed_leaders
        }
        
        logger.info(f"Successfully fetched TeamHistoricalLeaders for team {team_actual_name}, season_id {season_id}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except Exception as e:
        logger.error(
            f"API error in fetch_team_historical_leaders_logic for team '{team_actual_name}', season_id {season_id}: {e}",
            exc_info=True
        )
        error_msg = Errors.API_ERROR.format(error=f"Endpoint: TeamHistoricalLeaders, Team: {team_actual_name}, SeasonID: {season_id}, Details: {str(e)}")
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

# Example Error strings to add to backend/core/errors.py if they don't exist:
# class Errors:
#     ...
#     INVALID_SEASON_ID_FORMAT = "Invalid SeasonID format: '{season_id}'. Must be a 5-digit string (e.g., '22022')."
#     PROCESSING_ERROR = "An error occurred while processing data for {data_type}."
#     # NBA_API_ERROR = "NBA API request failed for endpoint {endpoint_name}: {error}" # Keep if distinct, else use API_ERROR
#     ... 

===== backend\api_tools\team_history.py =====
import logging
import json # Added for potential use if format_response changes
import os # Added for path operations
from functools import lru_cache
from typing import Union, Tuple, Dict # Added for typing
import pandas as pd # Added for DataFrame typing

from nba_api.stats.endpoints import CommonTeamYears
from nba_api.stats.library.parameters import LeagueID

from ..config import settings
from ..core.errors import Errors
from .utils import (
    _process_dataframe,
    format_response
)
from ..utils.validation import _validate_league_id
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# Define cache directory
TEAM_HISTORY_CSV_DIR = get_cache_dir("team_history")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_history(league_id: str) -> str:
    """
    Generates a file path for saving team history DataFrame as CSV.
    """
    filename = f"team_history_league_{league_id}.csv"
    return get_cache_file_path(filename, "team_history")

@lru_cache(maxsize=settings.DEFAULT_LRU_CACHE_SIZE)
def fetch_common_team_years_logic(
    league_id: str = LeagueID.nba,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: # Updated return type
    """
    Fetches a list of all team years for a given league, indicating the range of seasons each team existed.

    Args:
        league_id (str, optional): The league ID. Defaults to "00" (NBA).
        return_dataframe (bool, optional): Whether to return the DataFrame alongside JSON. Defaults to False.

    Returns:
        Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON string, Dict with DataFrame).
    """
    dataframes = {} # Initialize for potential DataFrame return
    logger.info(f"Executing fetch_common_team_years_logic for LeagueID: {league_id}, return_dataframe={return_dataframe}")

    if not _validate_league_id(league_id):
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id, options=["00", "10", "20"])
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_years_endpoint = CommonTeamYears(
            league_id=league_id,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"CommonTeamYears API call successful for LeagueID: {league_id}")

        team_years_df = team_years_endpoint.team_years.get_data_frame()

        if return_dataframe:
            dataframes["team_years"] = team_years_df
            if not team_years_df.empty:
                csv_path = _get_csv_path_for_team_history(league_id)
                _save_dataframe_to_csv(team_years_df, csv_path)

        team_years_list = _process_dataframe(team_years_df, single_row=False)

        if team_years_list is None:
            logger.error(f"DataFrame processing failed for CommonTeamYears (LeagueID: {league_id}).")
            error_msg = Errors.COMMON_TEAM_YEARS_API_ERROR.format(error="DataFrame processing returned None unexpectedly.")
            if return_dataframe:
                return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        response_data = {
            "parameters": {"league_id": league_id},
            "team_years": team_years_list
        }

        # Add DataFrame metadata to the response if returning DataFrames
        if return_dataframe and not team_years_df.empty:
            csv_path = _get_csv_path_for_team_history(league_id)
            csv_filename = os.path.basename(csv_path)
            relative_path = get_relative_cache_path(csv_filename, "team_history")

            response_data["dataframe_info"] = {
                "message": "Team history data has been converted to DataFrame and saved as CSV file",
                "dataframes": {
                    "team_years": {
                        "shape": list(team_years_df.shape),
                        "columns": team_years_df.columns.tolist(),
                        "csv_path": relative_path
                    }
                }
            }

        json_output = format_response(response_data)
        logger.info(f"Successfully fetched {len(team_years_list)} team year entries for LeagueID: {league_id}")

        if return_dataframe:
            return json_output, dataframes
        return json_output

    except Exception as e:
        logger.error(
            f"Error in fetch_common_team_years_logic for LeagueID {league_id}: {str(e)}",
            exc_info=True
        )
        error_msg = Errors.COMMON_TEAM_YEARS_API_ERROR.format(error=str(e))
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_info_roster.py =====
"""
Handles fetching comprehensive team information, including common details,
season ranks, roster, and coaching staff.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
from typing import Optional, Dict, List, Tuple, Any, Set, Union
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import teaminfocommon, commonteamroster
from nba_api.stats.library.parameters import LeagueID, SeasonTypeAllStar

from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_INFO_ROSTER_CACHE_SIZE = 128
_TEAM_INFO_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TEAM_INFO_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Cache Directory Setup ---
TEAM_INFO_CSV_DIR = get_cache_dir("team_info")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_info(
    team_name: str,
    data_type: str,
    season: str,
    league_id: str
) -> str:
    """
    Generates a file path for saving team info/roster DataFrame as CSV.

    Args:
        team_name: The team's name
        data_type: The type of data ('info', 'ranks', 'roster', 'coaches')
        season: The season in YYYY-YY format
        league_id: The league ID

    Returns:
        Path to the CSV file
    """
    # Clean team name and data type for filename
    clean_team_name = team_name.replace(" ", "_").replace(".", "").lower()

    filename = f"{clean_team_name}_{data_type}_{season}_{league_id}.csv"
    return get_cache_file_path(filename, "team_info")

# --- Helper Functions ---
def _fetch_team_details_and_ranks(
    team_id: int, season: str, season_type: str, league_id: str,
    team_name: str = "", return_dataframe: bool = False
) -> Union[Tuple[Dict[str, Any], Dict[str, Any], List[str]],
           Tuple[Dict[str, Any], Dict[str, Any], List[str], pd.DataFrame, pd.DataFrame]]:
    """
    Fetches and processes team info and season ranks from teaminfocommon.

    Args:
        team_id: The team's ID
        season: NBA season in YYYY-YY format
        season_type: Type of season (e.g., "Regular Season")
        league_id: League ID (e.g., "00" for NBA)
        team_name: The team's name (for CSV file naming)
        return_dataframe: Whether to return the original DataFrames

    Returns:
        If return_dataframe=False:
            Tuple[Dict[str, Any], Dict[str, Any], List[str]]: Processed data and error messages
        If return_dataframe=True:
            Tuple[Dict[str, Any], Dict[str, Any], List[str], pd.DataFrame, pd.DataFrame]:
                Processed data, error messages, and original DataFrames
    """
    info_dict, ranks_dict = {}, {}
    current_errors: List[str] = []
    team_info_df = pd.DataFrame()
    team_ranks_df = pd.DataFrame()

    logger.debug(f"Fetching teaminfocommon for Team ID: {team_id}, Season: {season}, Type: {season_type}, League: {league_id}")
    try:
        team_info_endpoint = teaminfocommon.TeamInfoCommon(
            team_id=team_id, season_nullable=season, league_id=league_id,
            season_type_nullable=season_type, timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        team_info_df = team_info_endpoint.team_info_common.get_data_frame()
        team_ranks_df = team_info_endpoint.team_season_ranks.get_data_frame()

        # Save DataFrames to CSV if requested and not empty
        if return_dataframe and team_name:
            if not team_info_df.empty:
                info_csv_path = _get_csv_path_for_team_info(team_name, "info", season, league_id)
                _save_dataframe_to_csv(team_info_df, info_csv_path)

            if not team_ranks_df.empty:
                ranks_csv_path = _get_csv_path_for_team_info(team_name, "ranks", season, league_id)
                _save_dataframe_to_csv(team_ranks_df, ranks_csv_path)

        info_dict = _process_dataframe(team_info_df, single_row=True) if not team_info_df.empty else {}
        ranks_dict = _process_dataframe(team_ranks_df, single_row=True) if not team_ranks_df.empty else {}

        if team_info_df.empty and team_ranks_df.empty:
            logger.warning(f"No team info/ranks data returned by API for team {team_id}, season {season}.")
        # _process_dataframe returns None on internal error, or empty dict for empty df
        if info_dict is None or ranks_dict is None: # Check for None which indicates processing failure
             current_errors.append("team info/ranks processing")
             logger.error(Errors.TEAM_PROCESSING.format(data_type="team info/ranks", identifier=str(team_id), season=season))
             info_dict = info_dict or {} # Ensure they are dicts even if one failed
             ranks_dict = ranks_dict or {}
    except Exception as api_error:
        error_msg = Errors.TEAM_API.format(data_type="teaminfocommon", identifier=str(team_id), season=season, error=str(api_error))
        logger.error(error_msg, exc_info=True)
        current_errors.append("team info/ranks API")

    if return_dataframe:
        return info_dict or {}, ranks_dict or {}, current_errors, team_info_df, team_ranks_df
    return info_dict or {}, ranks_dict or {}, current_errors

def _fetch_team_roster_and_coaches(
    team_id: int, season: str, league_id: str,
    team_name: str = "", return_dataframe: bool = False
) -> Union[Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[str]],
           Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[str], pd.DataFrame, pd.DataFrame]]:
    """
    Fetches and processes team roster and coaches from commonteamroster.

    Args:
        team_id: The team's ID
        season: NBA season in YYYY-YY format
        league_id: League ID (e.g., "00" for NBA)
        team_name: The team's name (for CSV file naming)
        return_dataframe: Whether to return the original DataFrames

    Returns:
        If return_dataframe=False:
            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[str]]: Processed data and error messages
        If return_dataframe=True:
            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[str], pd.DataFrame, pd.DataFrame]:
                Processed data, error messages, and original DataFrames
    """
    roster_list, coaches_list = [], []
    current_errors: List[str] = []
    roster_df = pd.DataFrame()
    coaches_df = pd.DataFrame()

    logger.debug(f"Fetching commonteamroster for Team ID: {team_id}, Season: {season}, League: {league_id}")
    try:
        roster_endpoint = commonteamroster.CommonTeamRoster(
            team_id=team_id, season=season, league_id_nullable=league_id, timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        roster_df = roster_endpoint.common_team_roster.get_data_frame()
        coaches_df = roster_endpoint.coaches.get_data_frame()

        # Save DataFrames to CSV if requested and not empty
        if return_dataframe and team_name:
            if not roster_df.empty:
                roster_csv_path = _get_csv_path_for_team_info(team_name, "roster", season, league_id)
                _save_dataframe_to_csv(roster_df, roster_csv_path)

            if not coaches_df.empty:
                coaches_csv_path = _get_csv_path_for_team_info(team_name, "coaches", season, league_id)
                _save_dataframe_to_csv(coaches_df, coaches_csv_path)

        roster_list = _process_dataframe(roster_df, single_row=False) if not roster_df.empty else []
        coaches_list = _process_dataframe(coaches_df, single_row=False) if not coaches_df.empty else []

        if roster_df.empty and coaches_df.empty:
            logger.warning(f"No roster/coaches data returned by API for team {team_id}, season {season}.")
        if roster_list is None or coaches_list is None: # Check for None which indicates processing failure
            current_errors.append("roster/coaches processing")
            logger.error(Errors.TEAM_PROCESSING.format(data_type="roster/coaches", identifier=str(team_id), season=season))
            roster_list = roster_list or [] # Ensure they are lists even if one failed
            coaches_list = coaches_list or []
    except Exception as api_error:
        error_msg = Errors.TEAM_API.format(data_type="commonteamroster", identifier=str(team_id), season=season, error=str(api_error))
        logger.error(error_msg, exc_info=True)
        current_errors.append("roster/coaches API")

    if return_dataframe:
        return roster_list or [], coaches_list or [], current_errors, roster_df, coaches_df
    return roster_list or [], coaches_list or [], current_errors

# --- Main Logic Function ---
@lru_cache(maxsize=TEAM_INFO_ROSTER_CACHE_SIZE)
def fetch_team_info_and_roster_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    league_id: str = LeagueID.nba,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches comprehensive team information including basic details, conference/division ranks,
    current season roster, and coaching staff for a specified team and season.
    Provides DataFrame output capabilities.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        league_id: League ID. Defaults to NBA.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with team information or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_team_info_and_roster_logic for: '{team_identifier}', Season: {season}, Type: {season_type}, League: {league_id}, DataFrame: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not team_identifier or not str(team_identifier).strip():
        if return_dataframe:
            return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY), dataframes
        return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY)

    if not season or not _validate_season_format(season):
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season)), dataframes
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))

    if season_type not in _TEAM_INFO_VALID_SEASON_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_INFO_VALID_SEASON_TYPES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_INFO_VALID_SEASON_TYPES)[:5])))

    if league_id not in _TEAM_INFO_VALID_LEAGUE_IDS:
        if return_dataframe:
            return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TEAM_INFO_VALID_LEAGUE_IDS)[:3]))), dataframes
        return format_response(error=Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TEAM_INFO_VALID_LEAGUE_IDS)[:3])))

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
        all_errors: List[str] = []

        # Fetch team details and ranks with DataFrame if requested
        if return_dataframe:
            team_info_dict, team_ranks_dict, info_errors, team_info_df, team_ranks_df = _fetch_team_details_and_ranks(
                team_id, season, season_type, league_id, team_name=team_actual_name, return_dataframe=True
            )
            dataframes["team_info"] = team_info_df
            dataframes["team_ranks"] = team_ranks_df
        else:
            team_info_dict, team_ranks_dict, info_errors = _fetch_team_details_and_ranks(
                team_id, season, season_type, league_id
            )
        all_errors.extend(info_errors)

        # Fetch roster and coaches with DataFrame if requested
        if return_dataframe:
            roster_list, coaches_list, roster_errors, roster_df, coaches_df = _fetch_team_roster_and_coaches(
                team_id, season, league_id, team_name=team_actual_name, return_dataframe=True
            )
            dataframes["roster"] = roster_df
            dataframes["coaches"] = coaches_df
        else:
            roster_list, coaches_list, roster_errors = _fetch_team_roster_and_coaches(
                team_id, season, league_id
            )
        all_errors.extend(roster_errors)

        if not team_info_dict and not team_ranks_dict and not roster_list and not coaches_list and all_errors:
            error_summary = Errors.TEAM_ALL_FAILED.format(identifier=team_identifier, season=season, errors_list=', '.join(all_errors))
            logger.error(error_summary)
            if return_dataframe:
                return format_response(error=error_summary), dataframes
            return format_response(error=error_summary)

        result = {
            "team_id": team_id, "team_name": team_actual_name, "season": season,
            "season_type": season_type, "league_id": league_id,
            "info": team_info_dict, "ranks": team_ranks_dict,
            "roster": roster_list, "coaches": coaches_list
        }
        if all_errors:
            result["partial_errors"] = all_errors

        # Add DataFrame metadata to the response if returning DataFrames
        if return_dataframe:
            result["dataframe_info"] = {
                "message": "Team information data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add metadata for each DataFrame if not empty
            for df_key, df in dataframes.items():
                if not df.empty:
                    csv_path = _get_csv_path_for_team_info(team_actual_name, df_key, season, league_id)
                    csv_filename = os.path.basename(csv_path)
                    relative_path = get_relative_cache_path(csv_filename, "team_info")

                    result["dataframe_info"]["dataframes"][df_key] = {
                        "shape": list(df.shape),
                        "columns": df.columns.tolist(),
                        "csv_path": relative_path
                    }

        logger.info(f"fetch_team_info_and_roster_logic completed for Team ID: {team_id}, Season: {season}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except (TeamNotFoundError, ValueError) as e: # Handles find_team_id_or_error issues
        logger.warning(f"Team lookup or validation failed for '{team_identifier}': {e}")
        if return_dataframe:
            return format_response(error=str(e)), dataframes
        return format_response(error=str(e))
    except Exception as e: # Catch-all for unexpected issues in the main orchestration
        error_msg = Errors.TEAM_UNEXPECTED.format(identifier=team_identifier, season=season, error=str(e))
        logger.critical(error_msg, exc_info=True)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_passing_analytics.py =====
"""
Handles fetching team passing statistics, including passes made and received.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Dict, List, Any, Union, Tuple
from functools import lru_cache
import pandas as pd

from nba_api.stats.endpoints import teamdashptpass
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from config import settings
from core.errors import Errors
from api_tools.utils import (
    _process_dataframe,
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from utils.validation import _validate_season_format
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Cache Directory Setup ---
TEAM_PASSING_CSV_DIR = get_cache_dir("team_passing")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_passing(
    team_name: str,
    data_type: str,
    season: str,
    season_type: str,
    per_mode: str
) -> str:
    """
    Generates a file path for saving team passing DataFrame as CSV.

    Args:
        team_name: The team's name
        data_type: The type of data ('passes_made' or 'passes_received')
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')

    Returns:
        Path to the CSV file
    """
    # Clean team name and data type for filename
    clean_team_name = team_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_team_name}_{data_type}_{season}_{clean_season_type}_{clean_per_mode}.csv"
    return get_cache_file_path(filename, "team_passing")

@lru_cache(maxsize=128)
def fetch_team_passing_stats_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    opponent_team_id: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team passing statistics, including passes made and received.
    Provides DataFrame output capabilities.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode. Defaults to PerGame.
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        opponent_team_id: Filter by opponent team ID (0 for all teams).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        date_to_nullable: End date filter in format YYYY-MM-DD.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with team passing statistics or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_team_passing_stats_logic for: '{team_identifier}', Season: {season}, PerMode: {per_mode}, " +
              f"LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, OpponentTeamID: {opponent_team_id}, " +
              f"VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, " +
              f"SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, Location: {location_nullable}, " +
              f"DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, DataFrame: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    if not team_identifier or not str(team_identifier).strip():
        if return_dataframe:
            return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY), dataframes
        return format_response(error=Errors.TEAM_IDENTIFIER_EMPTY)

    if not season or not _validate_season_format(season):
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season)), dataframes
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))

    VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
    if season_type not in VALID_SEASON_TYPES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(VALID_SEASON_TYPES)[:5]))), dataframes
        return format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(VALID_SEASON_TYPES)[:5])))

    VALID_PER_MODES = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}
    if per_mode not in VALID_PER_MODES:
        if return_dataframe:
            return format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(VALID_PER_MODES)[:3]))), dataframes
        return format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(VALID_PER_MODES)[:3])))

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
        logger.debug(f"Fetching teamdashptpass for Team ID: {team_id}, Season: {season}, PerMode: {per_mode}")
        try:
            passing_stats_endpoint = teamdashptpass.TeamDashPtPass(
                team_id=team_id,
                season=season,
                season_type_all_star=season_type,
                per_mode_simple=per_mode,
                last_n_games=last_n_games,
                league_id=league_id,
                month=month,
                opponent_team_id=opponent_team_id,
                vs_division_nullable=vs_division_nullable,
                vs_conference_nullable=vs_conference_nullable,
                season_segment_nullable=season_segment_nullable,
                outcome_nullable=outcome_nullable,
                location_nullable=location_nullable,
                date_from_nullable=date_from_nullable,
                date_to_nullable=date_to_nullable,
                timeout=settings.DEFAULT_TIMEOUT_SECONDS
            )
            logger.debug(f"teamdashptpass API call successful for ID: {team_id}, Season: {season}")
            passes_made_df = passing_stats_endpoint.passes_made.get_data_frame()
            passes_received_df = passing_stats_endpoint.passes_received.get_data_frame()
        except Exception as api_error:
            logger.error(f"nba_api teamdashptpass failed for ID {team_id}, Season {season}: {api_error}", exc_info=True)
            error_msg = Errors.TEAM_PASSING_API.format(identifier=str(team_id), season=season, error=str(api_error))
            if return_dataframe:
                return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

        # Save DataFrames to CSV if requested and not empty
        if return_dataframe:
            # Add DataFrames to the dictionary
            dataframes["passes_made"] = passes_made_df
            dataframes["passes_received"] = passes_received_df

            # Save passes_made DataFrame to CSV if not empty
            if not passes_made_df.empty:
                passes_made_csv_path = _get_csv_path_for_team_passing(
                    team_actual_name, "passes_made", season, season_type, per_mode
                )
                _save_dataframe_to_csv(passes_made_df, passes_made_csv_path)

            # Save passes_received DataFrame to CSV if not empty
            if not passes_received_df.empty:
                passes_received_csv_path = _get_csv_path_for_team_passing(
                    team_actual_name, "passes_received", season, season_type, per_mode
                )
                _save_dataframe_to_csv(passes_received_df, passes_received_csv_path)

        passes_made_list = _process_dataframe(passes_made_df, single_row=False)
        passes_received_list = _process_dataframe(passes_received_df, single_row=False)

        if passes_made_list is None or passes_received_list is None:
            if passes_made_df.empty and passes_received_df.empty:
                logger.warning(f"No passing stats found for team {team_actual_name} ({team_id}), season {season}.")
                passes_made_list, passes_received_list = [], []
            else:
                logger.error(f"DataFrame processing failed for team passing stats of {team_actual_name} ({season}).")
                error_msg = Errors.TEAM_PASSING_PROCESSING.format(identifier=str(team_id), season=season)
                if return_dataframe:
                    return format_response(error=error_msg), dataframes
                return format_response(error=error_msg)

        result = {
            "team_name": team_actual_name,
            "team_id": team_id,
            "season": season,
            "season_type": season_type,
            "parameters": {
                "per_mode": per_mode,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "passes_made": passes_made_list or [],
            "passes_received": passes_received_list or []
        }

        # Add DataFrame metadata to the response if returning DataFrames
        if return_dataframe:
            result["dataframe_info"] = {
                "message": "Team passing data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add metadata for passes_made DataFrame if not empty
            if not passes_made_df.empty:
                passes_made_csv_path = _get_csv_path_for_team_passing(
                    team_actual_name, "passes_made", season, season_type, per_mode
                )
                csv_filename = os.path.basename(passes_made_csv_path)
                relative_path = get_relative_cache_path(csv_filename, "team_passing")

                result["dataframe_info"]["dataframes"]["passes_made"] = {
                    "shape": list(passes_made_df.shape),
                    "columns": passes_made_df.columns.tolist(),
                    "csv_path": relative_path
                }

            # Add metadata for passes_received DataFrame if not empty
            if not passes_received_df.empty:
                passes_received_csv_path = _get_csv_path_for_team_passing(
                    team_actual_name, "passes_received", season, season_type, per_mode
                )
                csv_filename = os.path.basename(passes_received_csv_path)
                relative_path = get_relative_cache_path(csv_filename, "team_passing")

                result["dataframe_info"]["dataframes"]["passes_received"] = {
                    "shape": list(passes_received_df.shape),
                    "columns": passes_received_df.columns.tolist(),
                    "csv_path": relative_path
                }

        logger.info(f"fetch_team_passing_stats_logic completed for '{team_actual_name}'")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except (TeamNotFoundError, ValueError) as lookup_error:
        logger.warning(f"Team lookup or validation failed for '{team_identifier}': {lookup_error}")
        if return_dataframe:
            return format_response(error=str(lookup_error)), dataframes
        return format_response(error=str(lookup_error))
    except Exception as unexpected_error:
        error_msg = Errors.TEAM_PASSING_UNEXPECTED.format(identifier=team_identifier, season=season, error=str(unexpected_error))
        logger.critical(error_msg, exc_info=True)
        if return_dataframe:
            return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_player_dashboard.py =====
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd
from nba_api.stats.endpoints import teamplayerdashboard
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense
)
from config import settings
from core.errors import Errors
from api_tools.utils import format_response, _process_dataframe, find_team_id_or_error
from utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

_VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
# Note: TeamPlayerDashboard uses MeasureTypeDetailedDefense for its MeasureType parameter
_VALID_MEASURE_TYPES = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

def fetch_team_player_dashboard_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.totals,
    measure_type: str = MeasureTypeDetailedDefense.base,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N", # Valid: Y, N
    period: int = 0,
    plus_minus: str = "N", # Valid: Y, N
    rank: str = "N", # Valid: Y, N
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    shot_clock_range_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    po_round_nullable: Optional[int] = None, # Playoff round
    outcome_nullable: Optional[str] = None, # Valid: W, L
    location_nullable: Optional[str] = None, # Valid: Home, Road
    league_id_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None, # Valid: First Half, Second Half, Overtime
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team player dashboard statistics (PlayersSeasonTotals and TeamOverall) from the NBA API.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format.
        season_type: Type of season (e.g., 'Regular Season', 'Playoffs').
        per_mode: Statistical mode (e.g., 'PerGame', 'Totals').
        measure_type: Measure type (e.g., 'Base', 'Advanced'). Used by the API as MeasureTypeDetailedDefense.
        last_n_games: Number of most recent games to include.
        month: Filter by month (0 for all).
        opponent_team_id: Filter by opponent team ID.
        pace_adjust: Whether to adjust for pace ('Y' or 'N').
        period: Filter by period (0 for all).
        plus_minus: Whether to include plus/minus ('Y' or 'N').
        rank: Whether to include statistical ranks ('Y' or 'N').
        vs_division_nullable: Filter by division.
        vs_conference_nullable: Filter by conference.
        shot_clock_range_nullable: Filter by shot clock range.
        season_segment_nullable: Filter by season segment.
        po_round_nullable: Filter by playoff round.
        outcome_nullable: Filter by game outcome ('W' or 'L').
        location_nullable: Filter by game location ('Home' or 'Road').
        league_id_nullable: League ID.
        game_segment_nullable: Filter by game segment.
        date_from_nullable: Start date filter (YYYY-MM-DD).
        date_to_nullable: End date filter (YYYY-MM-DD).
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: A JSON string containing the team player dashboard data or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames {'players_season_totals': df, 'team_overall': df}.
    """
    dataframes = {}
    logger.info(f"Executing fetch_team_player_dashboard_logic for team '{team_identifier}', season {season}, measure_type {measure_type}, return_dataframe={return_dataframe}")

    # Parameter Validations
    if not team_identifier or not str(team_identifier).strip():
        error_msg = Errors.TEAM_IDENTIFIER_EMPTY
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if season_type not in _VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if per_mode not in _VALID_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if measure_type not in _VALID_MEASURE_TYPES: # Uses MeasureTypeDetailedDefense
        error_msg = Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_MEASURE_TYPES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if league_id_nullable and league_id_nullable not in _VALID_LEAGUE_IDS:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_LEAGUE_IDS)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate PaceAdjust, PlusMinus, Rank (Y/N)
    for param_name, param_value in [("pace_adjust", pace_adjust), ("plus_minus", plus_minus), ("rank", rank)]:
        if param_value not in ["Y", "N"]:
            error_msg = f"Invalid {param_name} value: '{param_value}'. Must be 'Y' or 'N'."
            if return_dataframe: return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

    # Validate Outcome (W/L)
    if outcome_nullable and outcome_nullable not in ["W", "L"]:
        error_msg = f"Invalid outcome_nullable value: '{outcome_nullable}'. Must be 'W' or 'L'."
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate Location (Home/Road)
    if location_nullable and location_nullable not in ["Home", "Road"]:
        error_msg = f"Invalid location_nullable value: '{location_nullable}'. Must be 'Home' or 'Road'."
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate GameSegment (First Half, Second Half, Overtime)
    if game_segment_nullable and game_segment_nullable not in ["First Half", "Second Half", "Overtime"]:
        error_msg = f"Invalid game_segment_nullable value: '{game_segment_nullable}'. Must be 'First Half', 'Second Half', or 'Overtime'."
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_id, team_actual_name = find_team_id_or_error(team_identifier)
    except Exception as e:
        logger.error(f"Error finding team '{team_identifier}': {e}", exc_info=True)
        if return_dataframe: return format_response(error=str(e)), dataframes
        return format_response(error=str(e))

    try:
        endpoint = teamplayerdashboard.TeamPlayerDashboard(
            team_id=team_id,
            season=season,
            season_type_all_star=season_type, # Corrected from season_type_playoffs
            per_mode_detailed=per_mode,
            measure_type_detailed_defense=measure_type, # Corrected param name
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            shot_clock_range_nullable=shot_clock_range_nullable,
            season_segment_nullable=season_segment_nullable,
            po_round_nullable=po_round_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            league_id_nullable=league_id_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )

        players_season_totals_df = endpoint.players_season_totals.get_data_frame()
        team_overall_df = endpoint.team_overall.get_data_frame()

        if return_dataframe:
            dataframes = {
                "players_season_totals": players_season_totals_df,
                "team_overall": team_overall_df
            }

        response_data = {
            "team_name": team_actual_name,
            "team_id": team_id,
            "parameters": {
                "season": season,
                "season_type": season_type,
                "per_mode": per_mode,
                "measure_type": measure_type,
                "last_n_games": last_n_games,
                "month": month,
                "opponent_team_id": opponent_team_id,
                "pace_adjust": pace_adjust,
                "period": period,
                "plus_minus": plus_minus,
                "rank": rank,
                "vs_division_nullable": vs_division_nullable,
                "vs_conference_nullable": vs_conference_nullable,
                "shot_clock_range_nullable": shot_clock_range_nullable,
                "season_segment_nullable": season_segment_nullable,
                "po_round_nullable": po_round_nullable,
                "outcome_nullable": outcome_nullable,
                "location_nullable": location_nullable,
                "league_id_nullable": league_id_nullable,
                "game_segment_nullable": game_segment_nullable,
                "date_from_nullable": date_from_nullable,
                "date_to_nullable": date_to_nullable
            },
            "players_season_totals": _process_dataframe(players_season_totals_df, single_row=False),
            "team_overall": _process_dataframe(team_overall_df, single_row=True) # TeamOverall is usually a single row
        }

        logger.info(f"Successfully fetched TeamPlayerDashboard for team_id {team_id}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except json.JSONDecodeError as jde:
        logger.error(f"NBA API JSONDecodeError for TeamPlayerDashboard, team '{team_actual_name}': {jde}", exc_info=True)
        error_msg = Errors.NBA_API_TIMEOUT_OR_DECODE_ERROR.format(endpoint_name="TeamPlayerDashboard", details=str(jde))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    except Exception as api_error:
        logger.error(f"NBA API error for TeamPlayerDashboard, team '{team_actual_name}': {api_error}", exc_info=True)
        error_msg = Errors.TEAM_PLAYER_DASHBOARD_API.format(identifier=team_actual_name, error=str(api_error)) if hasattr(Errors, "TEAM_PLAYER_DASHBOARD_API") else f"API error for TeamPlayerDashboard: {str(api_error)}"
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

===== backend\api_tools\team_player_on_off_details.py =====
import logging
import json
from typing import Optional, Dict, Any, Union, Tuple
import pandas as pd
from nba_api.stats.endpoints import teamplayeronoffdetails
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense # MeasureType is MeasureTypeDetailedDefense for this endpoint
)
from ..config import settings
from ..core.errors import Errors
from .utils import format_response, _process_dataframe, find_team_id_or_error
from ..utils.validation import _validate_season_format, validate_date_format

logger = logging.getLogger(__name__)

_VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_VALID_PER_MODES = {getattr(PerModeDetailed, attr) for attr in dir(PerModeDetailed) if not attr.startswith('_') and isinstance(getattr(PerModeDetailed, attr), str)}
_VALID_MEASURE_TYPES = {getattr(MeasureTypeDetailedDefense, attr) for attr in dir(MeasureTypeDetailedDefense) if not attr.startswith('_') and isinstance(getattr(MeasureTypeDetailedDefense, attr), str)}
_VALID_LEAGUE_IDS = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# Specific validation for nullable string choice parameters
_VALID_VS_DIVISIONS = {None, "", "Atlantic", "Central", "Northwest", "Pacific", "Southeast", "Southwest", "East", "West"}
_VALID_VS_CONFERENCES = {None, "", "East", "West"}
_VALID_SEASON_SEGMENTS = {None, "", "Post All-Star", "Pre All-Star"}
_VALID_OUTCOMES = {None, "", "W", "L"}
_VALID_LOCATIONS = {None, "", "Home", "Road"}
_VALID_GAME_SEGMENTS = {None, "", "First Half", "Overtime", "Second Half"}


def fetch_team_player_on_off_details_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeDetailed.totals,
    measure_type: str = MeasureTypeDetailedDefense.base,
    last_n_games: int = 0,
    month: int = 0,
    opponent_team_id: int = 0,
    pace_adjust: str = "N",
    period: int = 0,
    plus_minus: str = "N",
    rank: str = "N",
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    league_id_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches detailed on/off court statistics for players of a specific team.

    Retrieves three main data sets:
    - OverallTeamPlayerOnOffDetails: Summary stats for the team.
    - PlayersOffCourtTeamPlayerOnOffDetails: Team stats when specific players are OFF the court.
    - PlayersOnCourtTeamPlayerOnOffDetails: Team stats when specific players are ON the court.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format.
        season_type: Type of season (e.g., 'Regular Season', 'Playoffs').
        per_mode: Statistical mode (e.g., 'PerGame', 'Totals').
        measure_type: Measure type (e.g., 'Base', 'Advanced'). API uses MeasureTypeDetailedDefense.
        last_n_games: Filter by the last N games.
        month: Filter by month (1-12, 0 for all).
        opponent_team_id: Filter by opponent team ID (0 for all).
        pace_adjust: Adjust for pace ('Y' or 'N').
        period: Filter by period (0 for all, 1-4 for quarters, 5+ for OT).
        plus_minus: Include plus/minus data ('Y' or 'N').
        rank: Include rank data ('Y' or 'N').
        vs_division_nullable: Filter by opponent's division.
        vs_conference_nullable: Filter by opponent's conference.
        season_segment_nullable: Filter by season segment (e.g., 'Pre All-Star').
        outcome_nullable: Filter by game outcome ('W' or 'L').
        location_nullable: Filter by game location ('Home' or 'Road').
        league_id_nullable: League ID (e.g., '00' for NBA).
        game_segment_nullable: Filter by game segment (e.g., 'First Half').
        date_from_nullable: Start date (YYYY-MM-DD).
        date_to_nullable: End date (YYYY-MM-DD).
        return_dataframe: If True, returns a tuple: (JSON response, Dict of DataFrames).
                          Otherwise, returns JSON response string.

    Returns:
        str or Tuple[str, Dict[str, pd.DataFrame]]: JSON string or (JSON string, dict of DataFrames)
                                                    The dict of DataFrames will contain:
                                                    'overall': OverallTeamPlayerOnOffDetails
                                                    'off_court': PlayersOffCourtTeamPlayerOnOffDetails
                                                    'on_court': PlayersOnCourtTeamPlayerOnOffDetails
    """
    dataframes = {}
    logger.info(f"Executing fetch_team_player_on_off_details_logic for team '{team_identifier}', season {season}")

    # Parameter Validations
    if not team_identifier or not str(team_identifier).strip():
        error_msg = Errors.TEAM_IDENTIFIER_EMPTY
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if not season or not _validate_season_format(season):
        error_msg = Errors.INVALID_SEASON_FORMAT.format(season=season)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_msg = Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable)
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if season_type not in _VALID_SEASON_TYPES:
        error_msg = Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_VALID_SEASON_TYPES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if per_mode not in _VALID_PER_MODES:
        error_msg = Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_VALID_PER_MODES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if measure_type not in _VALID_MEASURE_TYPES:
        error_msg = Errors.INVALID_MEASURE_TYPE.format(value=measure_type, options=", ".join(list(_VALID_MEASURE_TYPES)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    if league_id_nullable and league_id_nullable not in _VALID_LEAGUE_IDS:
        error_msg = Errors.INVALID_LEAGUE_ID.format(value=league_id_nullable, options=", ".join(list(_VALID_LEAGUE_IDS)[:5]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    # Validate Y/N parameters
    for param_name, param_value in [("pace_adjust", pace_adjust), ("plus_minus", plus_minus), ("rank", rank)]:
        if param_value.upper() not in ["Y", "N"]:
            error_msg = f"Invalid {param_name} value: '{param_value}'. Must be 'Y' or 'N'."
            if return_dataframe: return format_response(error=error_msg), dataframes
            return format_response(error=error_msg)

    # Validate nullable choice parameters
    if vs_division_nullable not in _VALID_VS_DIVISIONS:
        error_msg = Errors.INVALID_DIVISION.format(value=vs_division_nullable, options=", ".join([opt for opt in _VALID_VS_DIVISIONS if opt][:3]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    if vs_conference_nullable not in _VALID_VS_CONFERENCES:
        error_msg = Errors.INVALID_CONFERENCE.format(value=vs_conference_nullable, options=", ".join([opt for opt in _VALID_VS_CONFERENCES if opt][:2]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    if season_segment_nullable not in _VALID_SEASON_SEGMENTS:
        error_msg = Errors.INVALID_SEASON_SEGMENT.format(value=season_segment_nullable, options=", ".join([opt for opt in _VALID_SEASON_SEGMENTS if opt][:2]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    if outcome_nullable not in _VALID_OUTCOMES:
        error_msg = Errors.INVALID_OUTCOME.format(value=outcome_nullable, options=", ".join([opt for opt in _VALID_OUTCOMES if opt][:2]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    if location_nullable not in _VALID_LOCATIONS:
        error_msg = Errors.INVALID_LOCATION.format(value=location_nullable, options=", ".join([opt for opt in _VALID_LOCATIONS if opt][:2]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    if game_segment_nullable not in _VALID_GAME_SEGMENTS:
        error_msg = Errors.INVALID_GAME_SEGMENT.format(value=game_segment_nullable, options=", ".join([opt for opt in _VALID_GAME_SEGMENTS if opt][:3]))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)

    try:
        team_id_actual, team_actual_name = find_team_id_or_error(team_identifier)
    except Exception as ve: # Changed from ValueError to general Exception to catch TeamNotFoundError etc.
        logger.error(f"Error finding team '{team_identifier}': {ve}", exc_info=True)
        if return_dataframe: return format_response(error=str(ve)), dataframes
        return format_response(error=str(ve))

    try:
        api_params = {
            "team_id": team_id_actual,
            "season": season,
            "season_type_all_star": season_type, # API uses season_type_all_star
            "per_mode_detailed": per_mode,
            "measure_type_detailed_defense": measure_type, # API uses measure_type_detailed_defense
            "last_n_games": last_n_games,
            "month": month,
            "opponent_team_id": opponent_team_id,
            "pace_adjust": pace_adjust.upper(),
            "period": period,
            "plus_minus": plus_minus.upper(),
            "rank": rank.upper(),
            "vs_division_nullable": vs_division_nullable if vs_division_nullable else None, # Ensure empty strings become None
            "vs_conference_nullable": vs_conference_nullable if vs_conference_nullable else None,
            "season_segment_nullable": season_segment_nullable if season_segment_nullable else None,
            "outcome_nullable": outcome_nullable if outcome_nullable else None,
            "location_nullable": location_nullable if location_nullable else None,
            "league_id_nullable": league_id_nullable if league_id_nullable else None,
            "game_segment_nullable": game_segment_nullable if game_segment_nullable else None,
            "date_from_nullable": date_from_nullable if date_from_nullable else None,
            "date_to_nullable": date_to_nullable if date_to_nullable else None,
            "timeout": settings.DEFAULT_TIMEOUT_SECONDS
        }
        endpoint = teamplayeronoffdetails.TeamPlayerOnOffDetails(**api_params)

        overall_df = endpoint.overall_team_player_on_off_details.get_data_frame()
        off_court_df = endpoint.players_off_court_team_player_on_off_details.get_data_frame()
        on_court_df = endpoint.players_on_court_team_player_on_off_details.get_data_frame()

        if return_dataframe:
            dataframes = {
                "overall": overall_df,
                "off_court": off_court_df,
                "on_court": on_court_df
            }

        response_data = {
            "team_name": team_actual_name,
            "team_id": team_id_actual,
            "parameters": {k: v for k, v in api_params.items() if k != "timeout"}, # Exclude timeout for cleaner output
            "overall_team_player_on_off_details": _process_dataframe(overall_df, single_row=False),
            "players_off_court_team_player_on_off_details": _process_dataframe(off_court_df, single_row=False),
            "players_on_court_team_player_on_off_details": _process_dataframe(on_court_df, single_row=False)
        }
        
        logger.info(f"Successfully fetched TeamPlayerOnOffDetails for team_id {team_id_actual}")
        if return_dataframe:
            return format_response(response_data), dataframes
        return format_response(response_data)

    except json.JSONDecodeError as jde:
        logger.error(f"NBA API JSONDecodeError for TeamPlayerOnOffDetails, team '{team_actual_name}': {jde}", exc_info=True)
        error_msg = Errors.NBA_API_TIMEOUT_OR_DECODE_ERROR.format(endpoint_name="TeamPlayerOnOffDetails", details=str(jde))
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg)
    except Exception as api_error:
        logger.error(f"NBA API error for TeamPlayerOnOffDetails, team '{team_actual_name}': {api_error}", exc_info=True)
        # Attempt to use a specific error message if defined in Errors
        error_key_base = "TEAM_PLAYER_ON_OFF_DETAILS_API"
        specific_error_key = f"{error_key_base}_{type(api_error).__name__.upper()}"
        
        if hasattr(Errors, specific_error_key):
            error_msg = getattr(Errors, specific_error_key).format(identifier=team_actual_name, error=str(api_error))
        elif hasattr(Errors, error_key_base):
            error_msg = getattr(Errors, error_key_base).format(identifier=team_actual_name, error=str(api_error))
        else:
            error_msg = f"API error for TeamPlayerOnOffDetails team '{team_actual_name}': {str(api_error)}"
            
        if return_dataframe: return format_response(error=error_msg), dataframes
        return format_response(error=error_msg) 

===== backend\api_tools\team_rebounding_tracking.py =====
"""
Handles fetching team-level rebounding tracking statistics using the TeamDashPtReb endpoint.
Utilizes shared utilities for team identification and parameter validation.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
from typing import Optional, Set, Dict, Union, Tuple # For type hinting validation sets
from functools import lru_cache

import pandas as pd
from nba_api.stats.endpoints import teamdashptreb
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from config import settings
from core.errors import Errors
from api_tools.utils import _process_dataframe, format_response
from utils.validation import validate_date_format
from api_tools.team_tracking_utils import _validate_team_tracking_params, _get_team_info_for_tracking
from api_tools.http_client import nba_session # For session patching

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_REBOUNDING_TRACKING_CACHE_SIZE = 128
NBA_API_DEFAULT_OPPONENT_TEAM_ID = 0 # Standard value for no specific opponent filter

_TEAM_REB_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TEAM_REB_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

# Apply session patch to the endpoint class for custom HTTP client usage
teamdashptreb.requests = nba_session

# --- Cache Directory Setup ---
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path
TEAM_REBOUNDING_CSV_DIR = get_cache_dir("team_rebounding")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_rebounding(
    team_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving team rebounding DataFrame as CSV.

    Args:
        team_name: The team's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data ('overall', 'shot_type', 'contest', 'shot_distance', 'reb_distance')

    Returns:
        Path to the CSV file
    """
    # Clean team name and data type for filename
    clean_team_name = team_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_team_name}_{season}_{clean_season_type}_{clean_per_mode}_{data_type}.csv"
    return get_cache_file_path(filename, "team_rebounding")

# --- Main Logic Function ---
def fetch_team_rebounding_stats_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    opponent_team_id: int = NBA_API_DEFAULT_OPPONENT_TEAM_ID,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team rebounding statistics, categorized by various factors like shot type,
    contest level, shot distance, and rebound distance, using the TeamDashPtReb endpoint.
    Provides DataFrame output capabilities.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode. Defaults to PerGame.
        opponent_team_id: Filter by opponent team ID. Defaults to 0 (all).
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        period: Filter by period (0 for all, 1-4 for quarters, 5+ for OT).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half", "Overtime").
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        date_to_nullable: End date filter in format YYYY-MM-DD.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with team rebounding stats or an error message.
                 Includes 'overall', 'by_shot_type', 'by_contest', 'by_shot_distance',
                 and 'by_rebound_distance' categories.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_team_rebounding_stats_logic for: {team_identifier}, Season: {season}, PerMode: {per_mode}, " +
              f"OpponentTeamID: {opponent_team_id}, LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, " +
              f"Period: {period}, VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, " +
              f"SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, Location: {location_nullable}, " +
              f"GameSegment: {game_segment_nullable}, DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, " +
              f"return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    validation_error_msg = _validate_team_tracking_params(team_identifier, season)
    if validation_error_msg:
        if return_dataframe:
            return validation_error_msg, dataframes
        return validation_error_msg

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _TEAM_REB_VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_REB_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _TEAM_REB_VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_TEAM_REB_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    error_resp, team_id_resolved, team_name_resolved = _get_team_info_for_tracking(team_identifier, None)
    if error_resp:
        if return_dataframe:
            return error_resp, dataframes
        return error_resp

    if team_id_resolved is None or team_name_resolved is None: # Should be caught by error_resp
        error_response = format_response(error=Errors.TEAM_INFO_RESOLUTION_FAILED.format(identifier=team_identifier))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        logger.debug(f"Fetching teamdashptreb for Team ID: {team_id_resolved}, Season: {season}")
        reb_stats_endpoint = teamdashptreb.TeamDashPtReb(
            team_id=team_id_resolved,
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            opponent_team_id=opponent_team_id,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"teamdashptreb API call successful for {team_name_resolved}")

        overall_df = reb_stats_endpoint.overall_rebounding.get_data_frame()
        shot_type_df = reb_stats_endpoint.shot_type_rebounding.get_data_frame()
        contested_df = reb_stats_endpoint.num_contested_rebounding.get_data_frame()
        distances_df = reb_stats_endpoint.shot_distance_rebounding.get_data_frame()
        reb_dist_df = reb_stats_endpoint.reb_distance_rebounding.get_data_frame()

        if return_dataframe:
            dataframes["overall"] = overall_df
            dataframes["shot_type"] = shot_type_df
            dataframes["contest"] = contested_df
            dataframes["shot_distance"] = distances_df
            dataframes["reb_distance"] = reb_dist_df

            # Save DataFrames to CSV if not empty
            if not overall_df.empty:
                csv_path = _get_csv_path_for_team_rebounding(
                    team_name_resolved, season, season_type, per_mode, "overall"
                )
                _save_dataframe_to_csv(overall_df, csv_path)

            if not shot_type_df.empty:
                csv_path = _get_csv_path_for_team_rebounding(
                    team_name_resolved, season, season_type, per_mode, "shot_type"
                )
                _save_dataframe_to_csv(shot_type_df, csv_path)

            if not contested_df.empty:
                csv_path = _get_csv_path_for_team_rebounding(
                    team_name_resolved, season, season_type, per_mode, "contest"
                )
                _save_dataframe_to_csv(contested_df, csv_path)

            if not distances_df.empty:
                csv_path = _get_csv_path_for_team_rebounding(
                    team_name_resolved, season, season_type, per_mode, "shot_distance"
                )
                _save_dataframe_to_csv(distances_df, csv_path)

            if not reb_dist_df.empty:
                csv_path = _get_csv_path_for_team_rebounding(
                    team_name_resolved, season, season_type, per_mode, "reb_distance"
                )
                _save_dataframe_to_csv(reb_dist_df, csv_path)

        overall_data = _process_dataframe(overall_df, single_row=True)
        shot_type_list = _process_dataframe(shot_type_df, single_row=False)
        contested_list = _process_dataframe(contested_df, single_row=False)
        distances_list = _process_dataframe(distances_df, single_row=False)
        reb_dist_list = _process_dataframe(reb_dist_df, single_row=False)

        if all(data is None for data in [overall_data, shot_type_list, contested_list, distances_list, reb_dist_list]):
            if all(df.empty for df in [overall_df, shot_type_df, contested_df, distances_df, reb_dist_df]):
                logger.warning(f"No rebounding stats found for team {team_name_resolved} with given filters.")

                response_data = {
                    "team_id": team_id_resolved,
                    "team_name": team_name_resolved,
                    "season": season,
                    "season_type": season_type,
                    "parameters": {
                        "per_mode": per_mode,
                        "opponent_team_id": opponent_team_id,
                        "last_n_games": last_n_games,
                        "league_id": league_id,
                        "month": month,
                        "period": period,
                        "vs_division": vs_division_nullable,
                        "vs_conference": vs_conference_nullable,
                        "season_segment": season_segment_nullable,
                        "outcome": outcome_nullable,
                        "location": location_nullable,
                        "game_segment": game_segment_nullable,
                        "date_from": date_from_nullable,
                        "date_to": date_to_nullable
                    },
                    "overall": {},
                    "by_shot_type": [],
                    "by_contest": [],
                    "by_shot_distance": [],
                    "by_rebound_distance": []
                }

                if return_dataframe:
                    return format_response(response_data), dataframes
                return format_response(response_data)
            else:
                logger.error(f"DataFrame processing failed for rebounding stats of {team_name_resolved}.")
                error_msg = Errors.TEAM_REBOUNDING_PROCESSING.format(identifier=str(team_id_resolved))
                error_response = format_response(error=error_msg)

                if return_dataframe:
                    return error_response, dataframes
                return error_response

        result = {
            "team_id": team_id_resolved,
            "team_name": team_name_resolved,
            "season": season,
            "season_type": season_type,
            "parameters": {
                "per_mode": per_mode,
                "opponent_team_id": opponent_team_id,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "overall": overall_data or {},
            "by_shot_type": shot_type_list or [],
            "by_contest": contested_list or [],
            "by_shot_distance": distances_list or [],
            "by_rebound_distance": reb_dist_list or []
        }

        # Add DataFrame metadata to the response if returning DataFrames
        if return_dataframe:
            result["dataframe_info"] = {
                "message": "Team rebounding tracking data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add metadata for each DataFrame if not empty
            dataframe_types = {
                "overall": overall_df,
                "shot_type": shot_type_df,
                "contest": contested_df,
                "shot_distance": distances_df,
                "reb_distance": reb_dist_df
            }

            for df_key, df in dataframe_types.items():
                if not df.empty:
                    csv_path = _get_csv_path_for_team_rebounding(
                        team_name_resolved, season, season_type, per_mode, df_key
                    )
                    csv_filename = os.path.basename(csv_path)
                    relative_path = get_relative_cache_path(csv_filename, "team_rebounding")

                    result["dataframe_info"]["dataframes"][df_key] = {
                        "shape": list(df.shape),
                        "columns": df.columns.tolist(),
                        "csv_path": relative_path
                    }

        logger.info(f"fetch_team_rebounding_stats_logic completed for {team_name_resolved}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except Exception as e:
        logger.error(f"Error fetching rebounding stats for team {team_identifier}: {str(e)}", exc_info=True)
        error_msg = Errors.TEAM_REBOUNDING_UNEXPECTED.format(identifier=team_identifier, error=str(e))
        error_response = format_response(error=error_msg)

        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\team_shooting_tracking.py =====
"""
Handles fetching team-level shooting tracking statistics using the TeamDashPtShots endpoint.
Utilizes shared utilities for team identification and parameter validation.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import os
import json
import pandas as pd
from typing import Optional, Set, Dict, Union, Tuple # For type hinting validation sets
from functools import lru_cache

from nba_api.stats.endpoints import teamdashptshots
from nba_api.stats.library.parameters import SeasonTypeAllStar, PerModeSimple

from config import settings
from core.errors import Errors
from api_tools.utils import _process_dataframe, format_response
from utils.validation import validate_date_format
from api_tools.team_tracking_utils import _validate_team_tracking_params, _get_team_info_for_tracking
from api_tools.http_client import nba_session # For session patching

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
TEAM_SHOOTING_TRACKING_CACHE_SIZE = 128
NBA_API_DEFAULT_OPPONENT_TEAM_ID = 0 # Standard value for no specific opponent filter

_TEAM_SHOOTING_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TEAM_SHOOTING_VALID_PER_MODES: Set[str] = {getattr(PerModeSimple, attr) for attr in dir(PerModeSimple) if not attr.startswith('_') and isinstance(getattr(PerModeSimple, attr), str)}

# Apply session patch to the endpoint class for custom HTTP client usage
teamdashptshots.requests = nba_session

# --- Cache Directory Setup ---
from utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path
TEAM_SHOOTING_CSV_DIR = get_cache_dir("team_shooting")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_team_shooting(
    team_name: str,
    season: str,
    season_type: str,
    per_mode: str,
    data_type: str
) -> str:
    """
    Generates a file path for saving team shooting DataFrame as CSV.

    Args:
        team_name: The team's name
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        data_type: The type of data ('general', 'shot_clock', 'dribble', 'defender', 'touch_time')

    Returns:
        Path to the CSV file
    """
    # Clean team name and data type for filename
    clean_team_name = team_name.replace(" ", "_").replace(".", "").lower()
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"{clean_team_name}_{season}_{clean_season_type}_{clean_per_mode}_{data_type}.csv"
    return get_cache_file_path(filename, "team_shooting")

# --- Main Logic Function ---
def fetch_team_shooting_stats_logic(
    team_identifier: str,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerModeSimple.per_game,
    opponent_team_id: int = NBA_API_DEFAULT_OPPONENT_TEAM_ID,
    last_n_games: int = 0,
    league_id: str = "00",
    month: int = 0,
    period: int = 0,
    vs_division_nullable: Optional[str] = None,
    vs_conference_nullable: Optional[str] = None,
    season_segment_nullable: Optional[str] = None,
    outcome_nullable: Optional[str] = None,
    location_nullable: Optional[str] = None,
    game_segment_nullable: Optional[str] = None,
    date_from_nullable: Optional[str] = None,
    date_to_nullable: Optional[str] = None,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches team shooting statistics, categorized by various factors like shot clock,
    number of dribbles, defender distance, and touch time, using the TeamDashPtShots endpoint.
    Provides DataFrame output capabilities.

    The 'general_shooting' dataset from the API often contains an overall summary row
    followed by breakdown rows (e.g., by shot type). This logic separates these.

    Args:
        team_identifier: Name, abbreviation, or ID of the team.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode. Defaults to PerGame.
        opponent_team_id: Filter by opponent team ID. Defaults to 0 (all).
        last_n_games: Number of games to include (0 for all games).
        league_id: League ID (default: "00" for NBA).
        month: Month number (0 for all months).
        period: Filter by period (0 for all, 1-4 for quarters, 5+ for OT).
        vs_division_nullable: Filter by division (e.g., "Atlantic", "Central").
        vs_conference_nullable: Filter by conference (e.g., "East", "West").
        season_segment_nullable: Filter by season segment (e.g., "Post All-Star", "Pre All-Star").
        outcome_nullable: Filter by game outcome (e.g., "W", "L").
        location_nullable: Filter by game location (e.g., "Home", "Road").
        game_segment_nullable: Filter by game segment (e.g., "First Half", "Second Half", "Overtime").
        date_from_nullable: Start date filter in format YYYY-MM-DD.
        date_to_nullable: End date filter in format YYYY-MM-DD.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with team shooting stats or an error message.
                 Includes 'overall_shooting', 'general_shooting_splits', 'by_shot_clock',
                 'by_dribble', 'by_defender_distance', and 'by_touch_time'.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_team_shooting_stats_logic for: {team_identifier}, Season: {season}, PerMode: {per_mode}, " +
              f"OpponentTeamID: {opponent_team_id}, LastNGames: {last_n_games}, LeagueID: {league_id}, Month: {month}, " +
              f"Period: {period}, VsDivision: {vs_division_nullable}, VsConference: {vs_conference_nullable}, " +
              f"SeasonSegment: {season_segment_nullable}, Outcome: {outcome_nullable}, Location: {location_nullable}, " +
              f"GameSegment: {game_segment_nullable}, DateFrom: {date_from_nullable}, DateTo: {date_to_nullable}, " +
              f"return_dataframe={return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    validation_error_msg = _validate_team_tracking_params(team_identifier, season)
    if validation_error_msg:
        if return_dataframe:
            return validation_error_msg, dataframes
        return validation_error_msg

    if date_from_nullable and not validate_date_format(date_from_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_from_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if date_to_nullable and not validate_date_format(date_to_nullable):
        error_response = format_response(error=Errors.INVALID_DATE_FORMAT.format(date=date_to_nullable))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if season_type not in _TEAM_SHOOTING_VALID_SEASON_TYPES:
        error_response = format_response(error=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TEAM_SHOOTING_VALID_SEASON_TYPES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    if per_mode not in _TEAM_SHOOTING_VALID_PER_MODES:
        error_response = format_response(error=Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_TEAM_SHOOTING_VALID_PER_MODES)[:5])))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    error_resp, team_id_resolved, team_name_resolved = _get_team_info_for_tracking(team_identifier, None)
    if error_resp:
        if return_dataframe:
            return error_resp, dataframes
        return error_resp

    if team_id_resolved is None or team_name_resolved is None: # Should be caught by error_resp
        error_response = format_response(error=Errors.TEAM_INFO_RESOLUTION_FAILED.format(identifier=team_identifier))
        if return_dataframe:
            return error_response, dataframes
        return error_response

    try:
        logger.debug(f"Fetching teamdashptshots for Team ID: {team_id_resolved}, Season: {season}")
        shot_stats_endpoint = teamdashptshots.TeamDashPtShots(
            team_id=team_id_resolved,
            season=season,
            season_type_all_star=season_type,
            per_mode_simple=per_mode,
            opponent_team_id=opponent_team_id,
            last_n_games=last_n_games,
            league_id=league_id,
            month=month,
            period=period,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            timeout=settings.DEFAULT_TIMEOUT_SECONDS
        )
        logger.debug(f"teamdashptshots API call successful for {team_name_resolved}")

        general_df = shot_stats_endpoint.general_shooting.get_data_frame()
        shot_clock_df = shot_stats_endpoint.shot_clock_shooting.get_data_frame()
        dribbles_df = shot_stats_endpoint.dribble_shooting.get_data_frame()
        defender_df = shot_stats_endpoint.closest_defender_shooting.get_data_frame()
        touch_time_df = shot_stats_endpoint.touch_time_shooting.get_data_frame()

        if return_dataframe:
            # Store the original DataFrames
            dataframes["general"] = general_df
            dataframes["shot_clock"] = shot_clock_df
            dataframes["dribble"] = dribbles_df
            dataframes["defender"] = defender_df
            dataframes["touch_time"] = touch_time_df

            # Save DataFrames to CSV if not empty
            if not general_df.empty:
                csv_path = _get_csv_path_for_team_shooting(
                    team_name_resolved, season, season_type, per_mode, "general"
                )
                _save_dataframe_to_csv(general_df, csv_path)

            if not shot_clock_df.empty:
                csv_path = _get_csv_path_for_team_shooting(
                    team_name_resolved, season, season_type, per_mode, "shot_clock"
                )
                _save_dataframe_to_csv(shot_clock_df, csv_path)

            if not dribbles_df.empty:
                csv_path = _get_csv_path_for_team_shooting(
                    team_name_resolved, season, season_type, per_mode, "dribble"
                )
                _save_dataframe_to_csv(dribbles_df, csv_path)

            if not defender_df.empty:
                csv_path = _get_csv_path_for_team_shooting(
                    team_name_resolved, season, season_type, per_mode, "defender"
                )
                _save_dataframe_to_csv(defender_df, csv_path)

            if not touch_time_df.empty:
                csv_path = _get_csv_path_for_team_shooting(
                    team_name_resolved, season, season_type, per_mode, "touch_time"
                )
                _save_dataframe_to_csv(touch_time_df, csv_path)

        # Process DataFrames for JSON response
        overall_shooting_data = _process_dataframe(general_df.head(1) if not general_df.empty else pd.DataFrame(), single_row=True)
        general_splits_list = _process_dataframe(general_df.iloc[1:] if len(general_df) > 1 else pd.DataFrame(), single_row=False)

        shot_clock_list = _process_dataframe(shot_clock_df, single_row=False)
        dribbles_list = _process_dataframe(dribbles_df, single_row=False)
        defender_list = _process_dataframe(defender_df, single_row=False)
        touch_time_list = _process_dataframe(touch_time_df, single_row=False)

        if overall_shooting_data is None and not general_df.empty:
            logger.error(f"DataFrame processing failed for general shooting stats of {team_name_resolved}.")
            error_msg = Errors.TEAM_SHOOTING_PROCESSING.format(identifier=str(team_id_resolved))
            error_response = format_response(error=error_msg)

            if return_dataframe:
                return error_response, dataframes
            return error_response

        if general_df.empty:
            logger.warning(f"No shooting stats found for team {team_name_resolved} with given filters.")

            response_data = {
                "team_id": team_id_resolved,
                "team_name": team_name_resolved,
                "season": season,
                "season_type": season_type,
                "parameters": {
                    "per_mode": per_mode,
                    "opponent_team_id": opponent_team_id,
                    "last_n_games": last_n_games,
                    "league_id": league_id,
                    "month": month,
                    "period": period,
                    "vs_division": vs_division_nullable,
                    "vs_conference": vs_conference_nullable,
                    "season_segment": season_segment_nullable,
                    "outcome": outcome_nullable,
                    "location": location_nullable,
                    "game_segment": game_segment_nullable,
                    "date_from": date_from_nullable,
                    "date_to": date_to_nullable
                },
                "overall_shooting": {},
                "general_shooting_splits": [],
                "by_shot_clock": [],
                "by_dribble": [],
                "by_defender_distance": [],
                "by_touch_time": []
            }

            # Add DataFrame metadata to the response if returning DataFrames
            if return_dataframe:
                response_data["dataframe_info"] = {
                    "message": "No team shooting tracking data found for the specified parameters",
                    "dataframes": {}
                }

                return format_response(response_data), dataframes
            return format_response(response_data)

        result = {
            "team_id": team_id_resolved,
            "team_name": team_name_resolved,
            "season": season,
            "season_type": season_type,
            "parameters": {
                "per_mode": per_mode,
                "opponent_team_id": opponent_team_id,
                "last_n_games": last_n_games,
                "league_id": league_id,
                "month": month,
                "period": period,
                "vs_division": vs_division_nullable,
                "vs_conference": vs_conference_nullable,
                "season_segment": season_segment_nullable,
                "outcome": outcome_nullable,
                "location": location_nullable,
                "game_segment": game_segment_nullable,
                "date_from": date_from_nullable,
                "date_to": date_to_nullable
            },
            "overall_shooting": overall_shooting_data or {},
            "general_shooting_splits": general_splits_list or [],
            "by_shot_clock": shot_clock_list or [],
            "by_dribble": dribbles_list or [],
            "by_defender_distance": defender_list or [],
            "by_touch_time": touch_time_list or []
        }

        # Add DataFrame metadata to the response if returning DataFrames
        if return_dataframe:
            result["dataframe_info"] = {
                "message": "Team shooting tracking data has been converted to DataFrames and saved as CSV files",
                "dataframes": {}
            }

            # Add metadata for each DataFrame if not empty
            dataframe_types = {
                "general": general_df,
                "shot_clock": shot_clock_df,
                "dribble": dribbles_df,
                "defender": defender_df,
                "touch_time": touch_time_df
            }

            for df_key, df in dataframe_types.items():
                if not df.empty:
                    csv_path = _get_csv_path_for_team_shooting(
                        team_name_resolved, season, season_type, per_mode, df_key
                    )
                    csv_filename = os.path.basename(csv_path)
                    relative_path = get_relative_cache_path(csv_filename, "team_shooting")

                    result["dataframe_info"]["dataframes"][df_key] = {
                        "shape": list(df.shape),
                        "columns": df.columns.tolist(),
                        "csv_path": relative_path
                    }

        logger.info(f"fetch_team_shooting_stats_logic completed for {team_name_resolved}")

        if return_dataframe:
            return format_response(result), dataframes
        return format_response(result)

    except Exception as e:
        logger.error(f"Error fetching shooting stats for team {team_identifier}: {str(e)}", exc_info=True)
        error_msg = Errors.TEAM_SHOOTING_UNEXPECTED.format(identifier=team_identifier, error=str(e))
        error_response = format_response(error=error_msg)

        if return_dataframe:
            return error_response, dataframes
        return error_response

===== backend\api_tools\team_tracking_utils.py =====
import logging
from typing import Optional, Tuple

from core.errors import Errors
from api_tools.utils import (
    format_response,
    find_team_id_or_error,
    TeamNotFoundError
)
from utils.validation import _validate_season_format

logger = logging.getLogger(__name__)

def _validate_team_tracking_params(
    team_identifier: Optional[str],
    season: str,
    team_id: Optional[int] = None
) -> Optional[str]:
    """Validate common parameters for team tracking stats functions."""
    if not team_identifier and team_id is None:
        return format_response(error=Errors.TEAM_IDENTIFIER_OR_ID_REQUIRED)
    if not season or not _validate_season_format(season):
        return format_response(error=Errors.INVALID_SEASON_FORMAT.format(season=season))
    return None

def _get_team_info_for_tracking(team_identifier: Optional[str], team_id_input: Optional[int]) -> Tuple[Optional[str], Optional[int], Optional[str]]:
    """
    Resolves team_id and team_name from either team_identifier or a direct team_id_input.
    Returns a tuple: (error_response_json_str, resolved_team_id, resolved_team_name).
    If successful, error_response_json_str is None.
    """
    if team_id_input is not None:
        try:
            resolved_id, resolved_name = find_team_id_or_error(str(team_id_input))
            if resolved_id == team_id_input:
                 return None, resolved_id, resolved_name
            else:
                 logger.warning(f"Team ID {team_id_input} resolved to a different ID {resolved_id} or name. Using original ID.")
                 return None, team_id_input, f"Team_{team_id_input}"
        except (TeamNotFoundError, ValueError) as e:
            logger.warning(f"Could not find/validate team name for provided ID {team_id_input}: {e}")
            return None, team_id_input, f"Team_{team_id_input}"
    elif team_identifier:
        try:
            resolved_id, resolved_name = find_team_id_or_error(team_identifier)
            return None, resolved_id, resolved_name
        except (TeamNotFoundError, ValueError) as e:
            logger.warning(f"Team lookup failed for identifier '{team_identifier}': {e}")
            return format_response(error=str(e)), None, None
    else:
        return format_response(error=Errors.TEAM_IDENTIFIER_OR_ID_REQUIRED), None, None

===== backend\api_tools\trending_team_tools.py =====
"""
Provides logic to fetch and determine top-performing (trending) teams
based on league standings data.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import json
import os
import pandas as pd
from datetime import datetime
from typing import Any, Tuple, List, Dict, Optional, Set, Union
from functools import lru_cache

from nba_api.stats.library.parameters import SeasonTypeAllStar, LeagueID
from .league_standings import fetch_league_standings_logic
from ..config import settings
from ..core.errors import Errors
from .utils import format_response
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
CACHE_TTL_SECONDS_TRENDING_TEAMS = 3600 * 4  # Cache standings for trending teams for 4 hours
TRENDING_TEAMS_RAW_STANDINGS_CACHE_SIZE = 16
TOP_TEAMS_PROCESSED_CACHE_SIZE = 64
DEFAULT_TOP_N_TEAMS = 5

# --- Cache Directory Setup ---
TRENDING_TEAMS_CSV_DIR = get_cache_dir("trending_teams")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_trending_teams(
    season: str,
    season_type: str,
    league_id: str,
    top_n: int
) -> str:
    """
    Generates a file path for saving trending teams DataFrame as CSV.

    Args:
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        league_id: The league ID (e.g., '00' for NBA)
        top_n: The number of top teams to include

    Returns:
        Path to the CSV file
    """
    # Clean season type for filename
    clean_season_type = season_type.replace(" ", "_").lower()

    filename = f"top_{top_n}_teams_{season}_{clean_season_type}_{league_id}.csv"
    return get_cache_file_path(filename, "trending_teams")

_TRENDING_TEAMS_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TRENDING_TEAMS_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

_ESSENTIAL_TEAM_FIELDS_FOR_TRENDING = [
    "TeamID", "TeamName", "Conference", "PlayoffRank", "WinPct",
    "WINS", "LOSSES", "Record", "LeagueRank"
]

# --- Helper Functions ---
@lru_cache(maxsize=TRENDING_TEAMS_RAW_STANDINGS_CACHE_SIZE)
def get_cached_standings_for_trending_teams(
    cache_key: Tuple, # e.g., (season, season_type, league_id)
    timestamp_bucket: str,
    **kwargs: Any # Parameters for fetch_league_standings_logic
) -> str:
    """Cached wrapper for `fetch_league_standings_logic`."""
    logger.info(f"Cache miss/expiry for standings (trending_teams, ts: {timestamp_bucket}) - fetching. Params: {kwargs}")
    try:
        standings_logic_args = {k: v for k, v in kwargs.items() if v is not None}
        return fetch_league_standings_logic(**standings_logic_args)
    except Exception as e:
        logger.error(f"fetch_league_standings_logic failed within cache wrapper: {e}", exc_info=True)
        raise

def _validate_trending_teams_params(
    season: str, season_type: str, league_id: str, top_n: int
) -> Optional[str]:
    """Validates parameters for fetch_top_teams_logic."""
    if not _validate_season_format(season):
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if not isinstance(top_n, int) or top_n < 1:
        return Errors.INVALID_TOP_N.format(value=top_n)
    if season_type not in _TRENDING_TEAMS_VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TRENDING_TEAMS_VALID_SEASON_TYPES)[:5]))
    if league_id not in _TRENDING_TEAMS_VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TRENDING_TEAMS_VALID_LEAGUE_IDS)[:3]))
    return None

def _extract_and_format_top_teams(
    standings_list: List[Dict[str, Any]], top_n: int
) -> List[Dict[str, Any]]:
    """Sorts standings by WinPct and extracts essential fields for the top N teams."""
    if not standings_list:
        return []
    try:
        # Handle potential None or non-floatable WinPct values during sort
        standings_sorted_list = sorted(
            standings_list,
            key=lambda x: float(x.get("WinPct", 0.0) or 0.0), # Default to 0.0 if None or empty
            reverse=True
        )
    except (ValueError, TypeError) as sort_err:
        logger.warning(f"Error sorting standings by WinPct: {sort_err}. Proceeding with unsorted top N from original list.")
        standings_sorted_list = standings_list # Fallback

    top_teams_formatted = []
    for team_data in standings_sorted_list[:top_n]:
        top_team_info = {field: team_data.get(field) for field in _ESSENTIAL_TEAM_FIELDS_FOR_TRENDING if field in team_data}
        top_teams_formatted.append(top_team_info)
    return top_teams_formatted

# --- Main Logic Function ---
@lru_cache(maxsize=TOP_TEAMS_PROCESSED_CACHE_SIZE)
def fetch_top_teams_logic(
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    league_id: str = LeagueID.nba,
    top_n: int = DEFAULT_TOP_N_TEAMS,
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches the top N performing teams based on win percentage.
    Utilizes cached league standings data.
    Provides DataFrame output capabilities.

    Args:
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        league_id: League ID. Defaults to NBA.
        top_n: Number of top teams to return. Defaults to 5.
        bypass_cache: Whether to bypass the cache and fetch fresh data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with top teams or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Executing fetch_top_teams_logic for season: {season}, type: {season_type}, league: {league_id}, top_n: {top_n}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    param_error = _validate_trending_teams_params(season, season_type, league_id, top_n)
    if param_error:
        if return_dataframe:
            return format_response(error=param_error), dataframes
        return format_response(error=param_error)

    cache_key_for_standings = (season, season_type, league_id)
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_TRENDING_TEAMS))
    standings_params = {"season": season, "season_type": season_type, "league_id": league_id}

    try:
        standings_json_str: str
        if bypass_cache:
            logger.info(f"Bypassing cache, fetching fresh standings for trending teams: {standings_params}")
            standings_json_str = fetch_league_standings_logic(**standings_params)
        else:
            standings_json_str = get_cached_standings_for_trending_teams(
                cache_key=cache_key_for_standings,
                timestamp_bucket=timestamp_bucket, # Corrected arg name
                **standings_params
            )

        try:
            standings_data_response = json.loads(standings_json_str)
        except json.JSONDecodeError as json_err:
            logger.error(f"Failed to decode JSON from standings logic: {json_err}. Response (first 500): {standings_json_str[:500]}")
            error_response = format_response(error=Errors.PROCESSING_ERROR.format(error="invalid JSON from standings"))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if isinstance(standings_data_response, dict) and "error" in standings_data_response:
            logger.error(f"Error received from upstream standings fetch: {standings_data_response['error']}")
            if return_dataframe:
                return standings_json_str, dataframes # Propagate the error JSON
            return standings_json_str # Propagate the error JSON

        standings_list = standings_data_response.get("standings", [])
        if not standings_list:
            logger.warning(f"No standings data available for {season}, type {season_type}, league {league_id}.")

            response_data = {
                "season": season, "season_type": season_type, "league_id": league_id,
                "requested_top_n": top_n, "top_teams": []
            }

            # Add DataFrame metadata to the response if returning DataFrames
            if return_dataframe:
                response_data["dataframe_info"] = {
                    "message": "No standings data available for the specified parameters",
                    "dataframes": {}
                }

                # Create an empty DataFrame for top teams
                dataframes["top_teams"] = pd.DataFrame(columns=_ESSENTIAL_TEAM_FIELDS_FOR_TRENDING)

                return format_response(response_data), dataframes
            return format_response(response_data)

        top_teams_list = _extract_and_format_top_teams(standings_list, top_n)

        result_payload = {
            "season": season, "season_type": season_type, "league_id": league_id,
            "requested_top_n": top_n, "top_teams": top_teams_list
        }

        if return_dataframe:
            # Create a DataFrame from the top teams list
            top_teams_df = pd.DataFrame(top_teams_list)
            dataframes["top_teams"] = top_teams_df

            # Save DataFrame to CSV if not empty
            if not top_teams_df.empty:
                csv_path = _get_csv_path_for_trending_teams(season, season_type, league_id, top_n)
                _save_dataframe_to_csv(top_teams_df, csv_path)

                # Add DataFrame metadata to the response
                result_payload["dataframe_info"] = {
                    "message": "Top teams data has been converted to DataFrame and saved as CSV file",
                    "dataframes": {
                        "top_teams": {
                            "shape": list(top_teams_df.shape),
                            "columns": top_teams_df.columns.tolist(),
                            "csv_path": get_relative_cache_path(os.path.basename(csv_path), "trending_teams")
                        }
                    }
                }

        logger.info(f"Successfully determined top {len(top_teams_list)} teams for {season}, type {season_type}, league {league_id}.")

        if return_dataframe:
            return format_response(result_payload), dataframes
        return format_response(result_payload)

    except Exception as e:
        logger.error(f"Unexpected error in fetch_top_teams_logic: {e}", exc_info=True)
        error_response = format_response(error=Errors.TRENDING_TEAMS_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response


===== backend\api_tools\trending_tools.py =====
"""
Provides logic to fetch and determine top-performing (trending) players
based on league leaders data for various statistical categories.
Provides both JSON and DataFrame outputs with CSV caching.
"""
import logging
import json
import os
import pandas as pd
from typing import Any, Tuple, Optional, Set, Dict, Union
from functools import lru_cache
from datetime import datetime

from nba_api.stats.library.parameters import SeasonTypeAllStar, StatCategoryAbbreviation, PerMode48, Scope, LeagueID
from ..core.errors import Errors
from .league_leaders_data import fetch_league_leaders_logic
from .utils import format_response
from ..utils.validation import _validate_season_format
from ..utils.path_utils import get_cache_dir, get_cache_file_path, get_relative_cache_path
from ..config import settings

logger = logging.getLogger(__name__)

# --- Module-Level Constants ---
CACHE_TTL_SECONDS_TRENDING = 14400  # 4 hours
TRENDING_PLAYERS_RAW_LEADERS_CACHE_SIZE = 64
TOP_PERFORMERS_PROCESSED_CACHE_SIZE = 128
DEFAULT_TOP_N_PERFORMERS = 5

# --- Cache Directory Setup ---
TRENDING_PLAYERS_CSV_DIR = get_cache_dir("trending_players")

# --- Helper Functions for CSV Caching ---
def _save_dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:
    """
    Saves a DataFrame to a CSV file, creating the directory if it doesn't exist.

    Args:
        df: The DataFrame to save
        file_path: The path to save the CSV file
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Save DataFrame to CSV
        df.to_csv(file_path, index=False)
        logger.debug(f"Saved DataFrame to {file_path}")
    except Exception as e:
        logger.error(f"Error saving DataFrame to {file_path}: {e}", exc_info=True)

def _get_csv_path_for_trending_players(
    category: str,
    season: str,
    season_type: str,
    per_mode: str,
    scope: str,
    league_id: str,
    top_n: int
) -> str:
    """
    Generates a file path for saving trending players DataFrame as CSV.

    Args:
        category: The statistical category (e.g., 'PTS', 'AST')
        season: The season in YYYY-YY format
        season_type: The season type (e.g., 'Regular Season', 'Playoffs')
        per_mode: The per mode (e.g., 'PerGame', 'Totals')
        scope: The scope (e.g., 'S' for season, 'RS' for recent)
        league_id: The league ID (e.g., '00' for NBA)
        top_n: The number of top performers to include

    Returns:
        Path to the CSV file
    """
    # Clean parameters for filename
    clean_season_type = season_type.replace(" ", "_").lower()
    clean_per_mode = per_mode.replace(" ", "_").lower()

    filename = f"top_{top_n}_{category}_{season}_{clean_season_type}_{clean_per_mode}_{scope}_{league_id}.csv"
    return get_cache_file_path(filename, "trending_players")

# --- Valid Parameter Sets ---
_TRENDING_VALID_STAT_CATEGORIES: Set[str] = {getattr(StatCategoryAbbreviation, attr) for attr in dir(StatCategoryAbbreviation) if not attr.startswith('_') and isinstance(getattr(StatCategoryAbbreviation, attr), str)}
_TRENDING_VALID_SEASON_TYPES: Set[str] = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_') and isinstance(getattr(SeasonTypeAllStar, attr), str)}
_TRENDING_VALID_PER_MODES: Set[str] = {getattr(PerMode48, attr) for attr in dir(PerMode48) if not attr.startswith('_') and isinstance(getattr(PerMode48, attr), str)}
_TRENDING_VALID_SCOPES: Set[str] = {getattr(Scope, attr) for attr in dir(Scope) if not attr.startswith('_') and isinstance(getattr(Scope, attr), str)}
_TRENDING_VALID_LEAGUE_IDS: Set[str] = {getattr(LeagueID, attr) for attr in dir(LeagueID) if not attr.startswith('_') and isinstance(getattr(LeagueID, attr), str)}

# --- Helper Functions ---
@lru_cache(maxsize=TRENDING_PLAYERS_RAW_LEADERS_CACHE_SIZE)
def get_cached_league_leaders_for_trending(
    cache_key: Tuple, # Unique tuple of all relevant params for fetch_league_leaders_logic
    timestamp_bucket: str, # For time-based invalidation
    **kwargs: Any # Parameters for fetch_league_leaders_logic
) -> str:
    """Cached wrapper for `fetch_league_leaders_logic`."""
    logger.info(f"Cache miss/expiry for league leaders (trending, ts: {timestamp_bucket}) - fetching. Params: {kwargs}")
    try:
        return fetch_league_leaders_logic(**kwargs)
    except Exception as e:
        logger.error(f"fetch_league_leaders_logic failed within cache wrapper: {e}", exc_info=True)
        raise

def _validate_top_performers_params(
    category: str, season: str, season_type: str, per_mode: str,
    scope: str, league_id: str, top_n: int
) -> Optional[str]:
    """Validates parameters for fetch_top_performers_logic."""
    if not _validate_season_format(season):
        return Errors.INVALID_SEASON_FORMAT.format(season=season)
    if not isinstance(top_n, int) or top_n <= 0: # top_n already defaulted if invalid by main func
        # This specific log/defaulting is handled in main func, here just check type for safety
        pass # Assuming top_n is already validated/defaulted by caller
    if category not in _TRENDING_VALID_STAT_CATEGORIES:
        return Errors.INVALID_STAT_CATEGORY.format(value=category, options=", ".join(list(_TRENDING_VALID_STAT_CATEGORIES)[:7]))
    if season_type not in _TRENDING_VALID_SEASON_TYPES:
        return Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(_TRENDING_VALID_SEASON_TYPES)[:5]))
    if per_mode not in _TRENDING_VALID_PER_MODES:
        return Errors.INVALID_PER_MODE.format(value=per_mode, options=", ".join(list(_TRENDING_VALID_PER_MODES)[:5]))
    if scope not in _TRENDING_VALID_SCOPES:
        return Errors.INVALID_SCOPE.format(value=scope, options=", ".join(list(_TRENDING_VALID_SCOPES)[:5]))
    if league_id not in _TRENDING_VALID_LEAGUE_IDS:
        return Errors.INVALID_LEAGUE_ID.format(value=league_id, options=", ".join(list(_TRENDING_VALID_LEAGUE_IDS)[:3]))
    return None

# --- Main Logic Function ---
def fetch_top_performers_logic(
    category: str = StatCategoryAbbreviation.pts,
    season: str = settings.CURRENT_NBA_SEASON,
    season_type: str = SeasonTypeAllStar.regular,
    per_mode: str = PerMode48.per_game,
    scope: str = Scope.s,
    league_id: str = LeagueID.nba,
    top_n: int = DEFAULT_TOP_N_PERFORMERS,
    bypass_cache: bool = False,
    return_dataframe: bool = False
) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
    """
    Fetches the top N performing players for a given statistical category and criteria.
    Utilizes `fetch_league_leaders_logic` and applies caching.
    Provides DataFrame output capabilities.

    Args:
        category: Statistical category to rank players by. Defaults to points.
        season: NBA season in YYYY-YY format. Defaults to current season.
        season_type: Type of season. Defaults to Regular Season.
        per_mode: Statistical mode. Defaults to PerGame.
        scope: Data scope (e.g., 'S' for season, 'RS' for recent). Defaults to season.
        league_id: League ID. Defaults to NBA.
        top_n: Number of top performers to return. Defaults to 5.
        bypass_cache: Whether to bypass the cache and fetch fresh data. Defaults to False.
        return_dataframe: Whether to return DataFrames along with the JSON response.

    Returns:
        If return_dataframe=False:
            str: JSON string with top performers or an error message.
        If return_dataframe=True:
            Tuple[str, Dict[str, pd.DataFrame]]: A tuple containing the JSON response string
                                               and a dictionary of DataFrames.
    """
    logger.info(f"Fetching top {top_n} performers for {category} in season {season}, type {season_type}, mode {per_mode}, scope {scope}, league {league_id}, return_dataframe: {return_dataframe}")

    # Store DataFrames if requested
    dataframes = {}

    # Validate top_n separately as it has a default and specific logging
    if not isinstance(top_n, int) or top_n <= 0:
        logger.warning(f"Invalid top_n value '{top_n}'. Must be a positive integer. Defaulting to {DEFAULT_TOP_N_PERFORMERS}.")
        top_n = DEFAULT_TOP_N_PERFORMERS

    param_error = _validate_top_performers_params(category, season, season_type, per_mode, scope, league_id, top_n)
    if param_error:
        if return_dataframe:
            return format_response(error=param_error), dataframes
        return format_response(error=param_error)

    # Parameters for the underlying league leaders call
    params_for_league_leaders = {
        "season": season, "stat_category": category, "season_type": season_type,
        "per_mode": per_mode, "scope": scope, "league_id": league_id,
        "top_n": top_n # fetch_league_leaders_logic already handles limiting to top_n
    }
    # Cache key for the raw league leaders data
    raw_leaders_cache_key = tuple(sorted(params_for_league_leaders.items()))
    timestamp_bucket = str(int(datetime.now().timestamp() // CACHE_TTL_SECONDS_TRENDING))

    try:
        leaders_json_str: str
        if bypass_cache:
            logger.info(f"Bypassing cache, fetching fresh league leaders data: {params_for_league_leaders}")
            leaders_json_str = fetch_league_leaders_logic(**params_for_league_leaders)
        else:
            leaders_json_str = get_cached_league_leaders_for_trending(
                cache_key=raw_leaders_cache_key,
                timestamp_bucket=timestamp_bucket, # Corrected arg name
                **params_for_league_leaders
            )

        try:
            data = json.loads(leaders_json_str)
        except json.JSONDecodeError as json_err:
            logger.error(f"Failed to decode JSON from fetch_league_leaders_logic: {json_err}. Response (first 500): {leaders_json_str[:500]}")
            error_response = format_response(error=Errors.PROCESSING_ERROR.format(error="invalid JSON from league leaders"))
            if return_dataframe:
                return error_response, dataframes
            return error_response

        if isinstance(data, dict) and "error" in data:
            logger.error(f"Error received from upstream league leaders fetch: {data['error']}")
            if return_dataframe:
                return leaders_json_str, dataframes # Propagate the error JSON
            return leaders_json_str # Propagate the error JSON

        leaders_list = data.get("leaders", []) # fetch_league_leaders_logic already returns the top_n

        result_payload = {
            "season": season, "stat_category": category, "season_type": season_type,
            "per_mode": per_mode, "scope": scope, "league_id": league_id,
            "requested_top_n": top_n, # The number originally requested
            "top_performers": leaders_list # This list is already the top_n (or fewer if less available)
        }

        if return_dataframe:
            # Create a DataFrame from the leaders list
            if leaders_list:
                top_performers_df = pd.DataFrame(leaders_list)
                dataframes["top_performers"] = top_performers_df

                # Save DataFrame to CSV if not empty
                if not top_performers_df.empty:
                    csv_path = _get_csv_path_for_trending_players(
                        category, season, season_type, per_mode, scope, league_id, top_n
                    )
                    _save_dataframe_to_csv(top_performers_df, csv_path)

                    # Add DataFrame metadata to the response
                    result_payload["dataframe_info"] = {
                        "message": "Top performers data has been converted to DataFrame and saved as CSV file",
                        "dataframes": {
                            "top_performers": {
                                "shape": list(top_performers_df.shape),
                                "columns": top_performers_df.columns.tolist(),
                                "csv_path": get_relative_cache_path(os.path.basename(csv_path), "trending_players")
                            }
                        }
                    }
            else:
                # Create an empty DataFrame for top performers
                dataframes["top_performers"] = pd.DataFrame()

                # Add DataFrame metadata to the response
                result_payload["dataframe_info"] = {
                    "message": "No top performers data available for the specified parameters",
                    "dataframes": {}
                }

        logger.info(f"Successfully prepared top {len(leaders_list)} performers for {category} in {season}.")

        if return_dataframe:
            return format_response(result_payload), dataframes
        return format_response(result_payload)

    except Exception as e:
        logger.error(f"Unexpected error in fetch_top_performers_logic: {e}", exc_info=True)
        error_response = format_response(error=Errors.TRENDING_UNEXPECTED.format(error=str(e)))
        if return_dataframe:
            return error_response, dataframes
        return error_response


===== backend\api_tools\utils.py =====
import logging
import json
import time
from typing import Optional, Union, List, Dict, Any, Callable, Tuple
import pandas as pd
import numpy as np
from datetime import datetime, date
from requests.exceptions import ReadTimeout, ConnectionError

from core.errors import Errors
from nba_api.stats.static import players, teams
logger = logging.getLogger(__name__)

# Constants
DEFAULT_RETRY_ATTEMPTS = 3
DEFAULT_RETRY_INITIAL_DELAY = 1.0
DEFAULT_RETRY_MAX_DELAY = 8.0
MAX_LOG_VALUE_LENGTH = 100

def retry_on_timeout(func: Callable[[], Any], max_retries: int = DEFAULT_RETRY_ATTEMPTS, initial_delay: float = DEFAULT_RETRY_INITIAL_DELAY, max_delay: float = DEFAULT_RETRY_MAX_DELAY) -> Any:
    """
    Retries a function call with exponential backoff if a `ReadTimeout` or `ConnectionError` occurs.
    """
    delay = initial_delay
    last_exception: Optional[Exception] = None

    for attempt in range(max_retries):
        try:
            return func()
        except (ReadTimeout, ConnectionError) as e:
            last_exception = e
            if attempt < max_retries - 1:
                logger.warning(f"Attempt {attempt + 1} of {max_retries} failed with {type(e).__name__}: {str(e)}. Retrying in {delay:.2f} seconds...")
                time.sleep(delay)
                delay = min(delay * 2, max_delay)
            else:
                logger.error(f"All {max_retries} attempts failed. Last error: {type(e).__name__}: {str(e)}")
                raise
        except Exception as e:
            logger.error(f"Function call failed with non-retryable error: {type(e).__name__}: {str(e)}", exc_info=True)
            raise

    if last_exception:
        raise last_exception
    # This part of the code should ideally not be reached if the loop always returns or raises.
    # Adding a more specific error message or ensuring all paths lead to a return/raise.
    logger.error("retry_on_timeout completed all retries without returning or raising a final exception. This indicates an issue with the retried function or retry logic itself.")
    # Depending on expected behavior, could raise a generic error here or return a specific sentinel value.
    # For now, returning None as per original, but this path is problematic.
    return None


def format_response(data: Optional[Dict[str, Any]] = None, error: Optional[str] = None) -> str:
    """
    Formats the API response as a JSON string.
    """
    if error:
        return json.dumps({"error": error})
    elif data is not None:
        # Using default=str for any non-serializable types (like datetime objects not handled by _convert_value_for_json)
        return json.dumps(data, default=str)
    else:
        # Return an empty JSON object if no data and no error
        return json.dumps({})

def _convert_value_for_json(value: Any, col_name: str, context_for_log: str) -> Any:
    """
    Converts a single DataFrame cell value to a JSON-serializable native Python type.
    Handles NaN/NaT, numpy types, and datetime objects.
    """
    try:
        if pd.isna(value):
            return None
        elif isinstance(value, np.integer):
            return int(value)
        elif isinstance(value, np.floating):
            # Check for NaN again for float types, as pd.isna might miss some np.nan if not pre-converted
            return None if np.isnan(value) else float(value)
        elif isinstance(value, np.bool_):
            return bool(value)
        elif isinstance(value, (datetime, date, pd.Timestamp)):
            return value.isoformat()
        elif isinstance(value, (int, float, bool, str)):
            return value
        else:
            # Fallback for other types, log and convert to string
            logger.debug(f"Utils: _convert_value_for_json ({context_for_log}) - Fallback to str for col '{col_name}', type '{type(value)}', value: '{str(value)[:MAX_LOG_VALUE_LENGTH]}'")
            return str(value)
    except Exception as val_e:
        logger.error(f"Utils: _convert_value_for_json ({context_for_log}) - Error converting value for col '{col_name}', type '{type(value)}', value: '{str(value)[:MAX_LOG_VALUE_LENGTH]}'. Error: {val_e}", exc_info=True)
        return None # Return None on conversion error to prevent breaking JSON serialization

def _process_dataframe(df: Optional[pd.DataFrame], single_row: bool = True) -> Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]:
    """
    Processes a pandas DataFrame into a dictionary or list of dictionaries,
    with robust handling of data types for JSON serialization.
    """
    if df is None or df.empty:
        return {} if single_row else []

    # No need to use df.copy() if we iterate and build new dicts/lists
    # df_copy = df.copy() # Work on a copy to avoid modifying the original DataFrame

    # Step 1: Convert all pandas/numpy NaNs/NaTs to Python None
    # This is now handled more granularly by _convert_value_for_json using pd.isna and np.isnan
    # try:
    #     df_copy = df_copy.where(pd.notnull(df_copy), None)
    # except Exception as e:
    #     logger.warning(f"Utils: _process_dataframe - Error during df.where(pd.notnull(df), None): {e}. Proceeding with original df_copy.")

    try:
        if single_row:
            if len(df) > 0: # Use original df length
                row_dict = {}
                # Use df.iloc[0] directly from the original DataFrame
                for col_name, value in df.iloc[0].items():
                    row_dict[col_name] = _convert_value_for_json(value, col_name, "single_row")
                return row_dict
            else:
                return {}
        else: # For single_row == False
            records = []
            # Iterate directly over the original DataFrame
            for _, row_series in df.iterrows():
                record = {}
                for col_name, value in row_series.items():
                    record[col_name] = _convert_value_for_json(value, col_name, "multi_row")
                records.append(record)
            return records
    except Exception as e:
        logger.error(f"Error processing DataFrame (outer logic in _process_dataframe): {str(e)}", exc_info=True)
        return None

# --- Custom Exceptions ---

class PlayerNotFoundError(Exception):
    """Custom exception raised when a player cannot be found by the lookup utilities."""
    def __init__(self, player_identifier: str):
        self.player_identifier = player_identifier
        message = Errors.PLAYER_NOT_FOUND.format(identifier=player_identifier) if hasattr(Errors, 'PLAYER_NOT_FOUND') else f"Player '{player_identifier}' not found."
        super().__init__(message)

class TeamNotFoundError(Exception):
    """Custom exception raised when a team cannot be found by the lookup utilities."""
    def __init__(self, team_identifier: str):
        self.team_identifier = team_identifier
        message = Errors.TEAM_NOT_FOUND.format(identifier=team_identifier) if hasattr(Errors, 'TEAM_NOT_FOUND') else f"Team '{team_identifier}' not found."
        super().__init__(message)

# --- Lookup Helpers ---

def get_player_id_from_name(player_name: str) -> Union[int, Dict[str, str]]:
    """
    Gets a player's ID from their name.

    Args:
        player_name (str): The name of the player to look up.

    Returns:
        Union[int, Dict[str, str]]: The player's ID if found, or an error dictionary if not found.
    """
    try:
        player_id, _ = find_player_id_or_error(player_name)
        return player_id
    except PlayerNotFoundError:
        return {"error": f"Player '{player_name}' not found."}
    except Exception as e:
        logger.error(f"Error in get_player_id_from_name for {player_name}: {str(e)}", exc_info=True)
        return {"error": f"Failed to get player ID for {player_name}: {str(e)}"}

def find_player_id_or_error(player_name: str) -> Tuple[int, str]:
    """
    Finds a player's unique ID and their canonical full name.
    """
    if not player_name or not player_name.strip():
        error_msg = Errors.PLAYER_NAME_EMPTY if hasattr(Errors, 'PLAYER_NAME_EMPTY') else "Player name cannot be empty."
        logger.error(error_msg)
        raise ValueError(error_msg)

    try:
        logger.debug(f"Searching for player ID for: '{player_name}'")

        # Attempt to treat as ID first if it's all digits
        if player_name.isdigit():
            player_id_int = int(player_name)
            all_players_list = players.get_players() # Get all players
            for p in all_players_list:
                if p['id'] == player_id_int:
                    logger.info(f"Found player by ID: {p['full_name']} (ID: {p['id']}) for input '{player_name}' (interpreted as ID)")
                    return p['id'], p['full_name']
            # If not found by ID, it might be a name that happens to be all digits, or an invalid ID.
            # Proceed to name lookup, or let PlayerNotFoundError be raised if it's truly not found by name either.

        player_list_results = players.find_players_by_full_name(player_name)
        if player_list_results:
            player_info_dict = player_list_results[0]
            player_id_found = int(player_info_dict['id'])
            player_actual_name_found = player_info_dict['full_name']
            logger.info(f"Found player by name: {player_actual_name_found} (ID: {player_id_found}) for input '{player_name}'")
            return player_id_found, player_actual_name_found
        else:
            logger.warning(f"Player not found for identifier: '{player_name}' using nba_api.stats.static.players (tried as ID if applicable, then as name)")
            raise PlayerNotFoundError(player_name)

    except PlayerNotFoundError:
        raise
    except Exception as e:
        logger.error(f"Unexpected error finding player ID for '{player_name}': {e}", exc_info=True)
        raise Exception(f"An unexpected error occurred while searching for player '{player_name}'.") from e

def find_team_id_or_error(team_identifier: str) -> Tuple[int, str]:
    """
    Finds a team's unique ID and its canonical full name.
    """
    if not team_identifier or not str(team_identifier).strip():
        error_msg = Errors.TEAM_IDENTIFIER_EMPTY if hasattr(Errors, 'TEAM_IDENTIFIER_EMPTY') else "Team identifier cannot be empty."
        logger.error(error_msg)
        raise ValueError(error_msg)

    identifier_str_cleaned = str(team_identifier).strip()
    logger.debug(f"Searching for team ID using identifier: '{identifier_str_cleaned}'")

    try:
        if identifier_str_cleaned.isdigit():
            team_id_int_input = int(identifier_str_cleaned)
            all_teams_list_static = teams.get_teams()
            for team_dict_static in all_teams_list_static:
                if team_dict_static['id'] == team_id_int_input:
                    logger.info(f"Found team by ID: {team_dict_static['full_name']} (ID: {team_dict_static['id']})")
                    return team_dict_static['id'], team_dict_static['full_name']

        team_info_by_abbr_static = teams.find_team_by_abbreviation(identifier_str_cleaned.upper())
        if team_info_by_abbr_static:
            logger.info(f"Found team by abbreviation: {team_info_by_abbr_static['full_name']} (ID: {team_info_by_abbr_static['id']})")
            return team_info_by_abbr_static['id'], team_info_by_abbr_static['full_name']

        team_list_by_name_static = teams.find_teams_by_full_name(identifier_str_cleaned)
        if team_list_by_name_static:
            team_info_first_match = team_list_by_name_static[0]
            logger.info(f"Found team by full name: {team_info_first_match['full_name']} (ID: {team_info_first_match['id']})")
            return team_info_first_match['id'], team_info_first_match['full_name']

        all_teams_for_nickname_search = teams.get_teams()
        identifier_lower_case = identifier_str_cleaned.lower()
        for team_item in all_teams_for_nickname_search:
            if team_item['nickname'].lower() == identifier_lower_case:
                logger.info(f"Found team by nickname: {team_item['full_name']} (ID: {team_item['id']})")
                return team_item['id'], team_item['full_name']

        logger.warning(f"Team not found for identifier: '{identifier_str_cleaned}'")
        raise TeamNotFoundError(identifier_str_cleaned)

    except TeamNotFoundError:
        raise
    except Exception as e:
        logger.error(f"Unexpected error finding team ID for '{identifier_str_cleaned}': {e}", exc_info=True)
        raise Exception(f"An unexpected error occurred while searching for team '{identifier_str_cleaned}'.") from e

===== backend\api_tools\visualization.py =====
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Any
import os

def create_shotchart(shot_data: Dict[str, Any], output_dir: str) -> str:
    """
    Create a shot chart visualization from the shot data.
    
    Args:
        shot_data: Dictionary containing shot chart data
        output_dir: Directory to save the visualization
        
    Returns:
        str: Path to the saved visualization file
    """
    # Create figure and axis
    fig = plt.figure(figsize=(12, 11))
    ax = fig.add_subplot(111)
    
    # Draw court
    draw_court(ax)
    
    # Plot shots
    for shot in shot_data["shot_locations"]:
        # NBA coordinates are in inches from center court
        # We need to flip both x and y coordinates for proper orientation
        x = -shot["x"]  # Flip x coordinate
        y = shot["y"]   # Keep y as is
        made = shot["made"]
        
        if made:
            ax.scatter(x, y, c='#2ECC71', alpha=0.7, s=50, 
                      marker='o', edgecolor='white', linewidth=0.5, zorder=2)
        else:
            ax.scatter(x, y, c='#E74C3C', alpha=0.6, s=40, 
                      marker='x', linewidth=1.5, zorder=2)
    
    # Add title and stats
    stats = shot_data["overall_stats"]
    title = f"{shot_data['player_name']} Shot Chart {shot_data['season']}\n"
    title += f"FG: {stats['made_shots']}/{stats['total_shots']} ({stats['field_goal_percentage']}%)"
    plt.title(title, pad=20, size=14, weight='bold')
    
    # Add zone percentages with adjusted positioning
    zone_positions = {
        'Above the Break 3': {'offset_x': 0, 'offset_y': 30},
        'Left Corner 3': {'offset_x': -20, 'offset_y': 20},
        'Right Corner 3': {'offset_x': 20, 'offset_y': 20},
        'Mid-Range': {'offset_x': 0, 'offset_y': 25},
        'In The Paint (Non-RA)': {'offset_x': 0, 'offset_y': 20},
        'Restricted Area': {'offset_x': 0, 'offset_y': -30}
    }
    
    for zone, stats in shot_data["zone_breakdown"].items():
        # Calculate average position for the zone with flipped x coordinates
        zone_shots = [(-shot["x"], shot["y"]) for shot in shot_data["shot_locations"] 
                     if shot["zone"] == zone]
        if zone_shots:
            avg_x = sum(x for x, _ in zone_shots) / len(zone_shots)
            avg_y = sum(y for _, y in zone_shots) / len(zone_shots)
            
            # Apply zone-specific positioning
            offset = zone_positions.get(zone, {'offset_x': 0, 'offset_y': 25})
            text_x = avg_x + offset['offset_x']
            text_y = avg_y + offset['offset_y']
            
            # Add text with zone stats
            text = f"{zone}\n{stats['made']}/{stats['attempts']}\n{stats['percentage']}%"
            ax.text(text_x, text_y, text, ha='center', va='bottom', 
                   bbox=dict(facecolor='white', alpha=0.7, edgecolor='#222222', 
                            boxstyle='round,pad=0.5'),
                   size=8, weight='bold', zorder=3)
    
    # Add legend
    made_patch = plt.scatter([], [], c='#2ECC71', alpha=0.7, s=50, 
                           marker='o', edgecolor='white', label='Made')
    missed_patch = plt.scatter([], [], c='#E74C3C', alpha=0.6, s=40, 
                             marker='x', linewidth=1.5, label='Missed')
    ax.legend(handles=[made_patch, missed_patch], loc='upper right',
             bbox_to_anchor=(1, 1), framealpha=1)
    
    # Set axis limits to show only the offensive half-court
    ax.set_xlim(-250, 250)
    ax.set_ylim(-50, 420)
    
    # Save plot
    os.makedirs(output_dir, exist_ok=True)
    filename = f"shotchart_{shot_data['player_name'].replace(' ', '_')}_{shot_data['season']}.png"
    filepath = os.path.join(output_dir, filename)
    plt.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    return filepath

def draw_court(ax: plt.Axes):
    """Draw an NBA basketball court."""
    # Court dimensions (in inches)
    court_width = 500
    court_height = 470
    three_point_radius = 237.5
    three_point_side_radius = 220
    three_point_side_height = 140
    paint_width = 160
    paint_height = 190
    backboard_width = 60
    hoop_radius = 7.5
    restricted_area_radius = 40
    
    # Main court outline
    court = plt.Rectangle((-court_width/2, -court_height/2), court_width, court_height, 
                         color='#FFFFFF', fill=True, zorder=0)
    ax.add_patch(court)
    
    # Paint area
    paint = plt.Rectangle((-paint_width/2, 0), paint_width, paint_height, 
                         fill=False, color='black', linewidth=1, zorder=1)
    ax.add_patch(paint)
    
    # Three-point line
    three_point_left = plt.Rectangle((-court_width/2, 0), 
                                   0, three_point_side_height, color='black', 
                                   linewidth=1, zorder=1)
    three_point_right = plt.Rectangle((court_width/2, 0), 
                                    0, three_point_side_height, color='black', 
                                    linewidth=1, zorder=1)
    ax.add_patch(three_point_left)
    ax.add_patch(three_point_right)
    
    # Three point arc
    theta = np.linspace(0, np.pi, 50)
    three_point_arc_x = three_point_radius * np.cos(theta)
    three_point_arc_y = three_point_radius * np.sin(theta)
    ax.plot(three_point_arc_x, three_point_arc_y, color='black', linewidth=1, zorder=1)
    
    # Backboard
    ax.plot([-backboard_width/2, backboard_width/2], [0, 0], 
           color='black', linewidth=1, zorder=1)
    
    # Hoop
    hoop = plt.Circle((0, 0), hoop_radius, color='black', 
                     fill=False, linewidth=1, zorder=1)
    ax.add_patch(hoop)
    
    # Restricted area
    restricted = plt.Circle((0, 0), restricted_area_radius, 
                          color='black', fill=False, linewidth=1, zorder=1)
    ax.add_patch(restricted)
    
    # Set aspect ratio and remove axes
    ax.set_aspect('equal')
    ax.set_xticks([])
    ax.set_yticks([]) 

===== backend\api_tools\visualization_cache.py =====
"""
Caching module for visualizations to improve performance.
"""

import os
import json
import hashlib
import logging
import time
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

# Cache directory
CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "cache", "visualizations")
# Cache expiration time (24 hours in seconds)
CACHE_EXPIRATION = 24 * 60 * 60

class VisualizationCache:
    """
    Cache for visualizations to improve performance.
    Uses a file-based cache with JSON metadata and image files.
    """
    
    @staticmethod
    def _ensure_cache_dir() -> None:
        """Ensure the cache directory exists."""
        os.makedirs(CACHE_DIR, exist_ok=True)
    
    @staticmethod
    def _generate_cache_key(params: Dict[str, Any]) -> str:
        """
        Generate a cache key from the parameters.
        
        Args:
            params: Dictionary of parameters
            
        Returns:
            Cache key string
        """
        # Convert params to a sorted string representation for consistent hashing
        param_str = json.dumps(params, sort_keys=True)
        # Generate MD5 hash
        return hashlib.md5(param_str.encode()).hexdigest()
    
    @staticmethod
    def _get_cache_metadata_path(cache_key: str) -> str:
        """Get the path to the cache metadata file."""
        return os.path.join(CACHE_DIR, f"{cache_key}.json")
    
    @staticmethod
    def _get_cache_file_path(cache_key: str, file_ext: str) -> str:
        """Get the path to the cached file."""
        return os.path.join(CACHE_DIR, f"{cache_key}.{file_ext}")
    
    @classmethod
    def get(cls, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Get a cached visualization if it exists and is not expired.
        
        Args:
            params: Dictionary of parameters
            
        Returns:
            Cached visualization data or None if not found or expired
        """
        cls._ensure_cache_dir()
        cache_key = cls._generate_cache_key(params)
        metadata_path = cls._get_cache_metadata_path(cache_key)
        
        # Check if metadata file exists
        if not os.path.exists(metadata_path):
            return None
        
        try:
            # Load metadata
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # Check if cache is expired
            if time.time() - metadata.get('timestamp', 0) > CACHE_EXPIRATION:
                logger.info(f"Cache expired for key: {cache_key}")
                return None
            
            # Check if cached file exists
            file_path = metadata.get('file_path')
            if not file_path or not os.path.exists(file_path):
                logger.warning(f"Cached file not found: {file_path}")
                return None
            
            # For base64 data, load it from the metadata
            if 'image_data' in metadata or 'animation_data' in metadata:
                return metadata
            
            # For file paths, return the metadata
            return metadata
            
        except Exception as e:
            logger.error(f"Error reading cache: {e}")
            return None
    
    @classmethod
    def set(cls, params: Dict[str, Any], result: Dict[str, Any]) -> None:
        """
        Cache a visualization.
        
        Args:
            params: Dictionary of parameters
            result: Visualization result to cache
        """
        cls._ensure_cache_dir()
        cache_key = cls._generate_cache_key(params)
        metadata_path = cls._get_cache_metadata_path(cache_key)
        
        try:
            # Create metadata
            metadata = {
                'params': params,
                'timestamp': time.time(),
                'chart_type': result.get('chart_type', 'unknown')
            }
            
            # Handle different result types
            if 'image_data' in result:
                metadata['image_data'] = result['image_data']
            elif 'animation_data' in result:
                metadata['animation_data'] = result['animation_data']
            elif 'file_path' in result:
                # For file paths, store the path and copy the file to the cache
                src_path = result['file_path']
                file_ext = os.path.splitext(src_path)[1][1:]  # Get extension without dot
                dst_path = cls._get_cache_file_path(cache_key, file_ext)
                
                # Copy file if it exists
                if os.path.exists(src_path):
                    import shutil
                    shutil.copy2(src_path, dst_path)
                    metadata['file_path'] = dst_path
            
            # Save metadata
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f)
                
            logger.info(f"Cached visualization with key: {cache_key}")
            
        except Exception as e:
            logger.error(f"Error caching visualization: {e}")
    
    @classmethod
    def clear_expired(cls) -> int:
        """
        Clear expired cache entries.
        
        Returns:
            Number of entries cleared
        """
        cls._ensure_cache_dir()
        cleared_count = 0
        
        try:
            # Get all metadata files
            metadata_files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.json')]
            
            for metadata_file in metadata_files:
                metadata_path = os.path.join(CACHE_DIR, metadata_file)
                
                try:
                    # Load metadata
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    
                    # Check if cache is expired
                    if time.time() - metadata.get('timestamp', 0) > CACHE_EXPIRATION:
                        # Remove file if it exists
                        file_path = metadata.get('file_path')
                        if file_path and os.path.exists(file_path):
                            os.remove(file_path)
                        
                        # Remove metadata file
                        os.remove(metadata_path)
                        cleared_count += 1
                        
                except Exception as e:
                    logger.error(f"Error clearing cache entry {metadata_file}: {e}")
            
            logger.info(f"Cleared {cleared_count} expired cache entries")
            return cleared_count
            
        except Exception as e:
            logger.error(f"Error clearing expired cache: {e}")
            return 0


===== backend\api_tools\__init__.py =====
"""NBA API tools package."""

===== backend\core\constants.py =====
"""
Core constants for the NBA analytics backend application.
This file contains static, non-configurable values used across various modules.
It does not include environment-specific settings, which are handled by `backend.config`.
"""
from typing import List

# --- General Application Constants ---
HEADSHOT_BASE_URL: str = "https://ak-static.cms.nba.com/wp-content/uploads/headshots/nba/latest/260x190"
DEFAULT_PLAYER_SEARCH_LIMIT: int = 10
MIN_PLAYER_SEARCH_LENGTH: int = 2
MAX_SEARCH_RESULTS: int = 25  # General limit for search results if not specified otherwise
MAX_GAMES_TO_RETURN: int = 50  # Max games for endpoints like find_games
MAX_PLAYERS_TO_RETURN: int = 25  # Max players for endpoints like player search

# --- Supported Targets for Endpoints ---
# These define valid string identifiers used in some route/logic parameters.

SUPPORTED_FETCH_TARGETS: List[str] = [
    "player_info",
    "player_gamelog",
    "team_info",
    "player_career_stats",
    "find_games"
]

SUPPORTED_SEARCH_TARGETS: List[str] = [
    "players",
    "teams",
    "games"
]

# --- Illustrative Examples (Not actively used unless integrated elsewhere) ---
# Example of other potential constants if needed in the future:
# DEFAULT_LEAGUE_ID: str = "00" # NBA
# SEASON_TYPE_REGULAR: str = "Regular Season"
# SEASON_TYPE_PLAYOFFS: str = "Playoffs"

# Example Team IDs (can be expanded or moved to a dynamic source if many are needed):
# These are illustrative and should be properly managed if used actively.
TEAM_ID_LAKERS: int = 1610612747
TEAM_ID_CELTICS: int = 1610612738
TEAM_ID_WARRIORS: int = 1610612744

===== backend\core\errors.py =====
"""
Central repository for error message string constants used throughout the application.
These constants help maintain consistency in error reporting.
"""

# --- Error Message Constants ---
class Errors:
    # Generic Errors
    INVALID_SEASON_FORMAT: str = "Invalid season format: '{season}'. Expected YYYY-YY (e.g., 2023-24)."
    SEASON_EMPTY: str = "Season cannot be empty."
    REQUEST_TIMEOUT: str = "Request timed out after {timeout} seconds."
    API_ERROR: str = "NBA API request failed: {error}" # General NBA API error
    PROCESSING_ERROR: str = "Failed to process data: {error}"
    UNEXPECTED_ERROR: str = "An unexpected error occurred: {error}"
    DATA_NOT_FOUND: str = "No data found for the specified criteria."
    INVALID_DATE_FORMAT: str = "Invalid date format: '{date}'. Expected YYYY-MM-DD."
    MISSING_REQUIRED_PARAMS: str = "Missing required parameters for the request: {missing_params}"
    TEAM_ID_NOT_FOUND: str = "Could not determine Team ID for identifier '{identifier}'."
    EMPTY_SEARCH_QUERY: str = "Search query cannot be empty."
    SEARCH_QUERY_TOO_SHORT: str = "Search query must be at least {min_length} characters long."
    JSON_PROCESSING_ERROR: str = "Internal server error: Invalid data format from an underlying service."
    SSE_GENERATION_ERROR: str = "Error during SSE stream generation: {error_details}"
    TOPIC_EMPTY: str = "Research topic cannot be empty."
    PROMPT_SUGGESTION_ERROR: str = "Failed to generate prompt suggestions due to an internal error."
    UNSUPPORTED_FETCH_TARGET: str = "Unsupported fetch target: '{target}'. Supported targets are: {supported_targets}."
    UNSUPPORTED_SEARCH_TARGET: str = "Unsupported search target: '{target}'. Supported targets are: {supported_targets}."

    # Player Errors
    PLAYER_NAME_EMPTY: str = "Player name cannot be empty."
    PLAYER_ID_EMPTY: str = "Player ID cannot be empty."
    INVALID_PLAYER_ID_FORMAT: str = "Invalid player ID format: '{player_id}'. Expected digits only."
    PLAYER_NOT_FOUND: str = "Player '{identifier}' not found."
    PLAYER_INFO_API: str = "API error fetching player info for {identifier}: {error}"
    PLAYER_INFO_PROCESSING: str = "Failed to process player info for {identifier}."
    PLAYER_INFO_UNEXPECTED: str = "Unexpected error fetching player info for {identifier}: {error}"
    PLAYER_GAMELOG_API: str = "API error fetching gamelog for {identifier} (Season: {season}): {error}"
    PLAYER_GAMELOG_PROCESSING: str = "Failed to process gamelog for {identifier} (Season: {season})."
    PLAYER_GAMELOG_UNEXPECTED: str = "Unexpected error fetching gamelog for {identifier} (Season: {season}): {error}"
    PLAYER_CAREER_STATS_API: str = "API error fetching career stats for {identifier}: {error}"
    PLAYER_CAREER_STATS_PROCESSING: str = "Failed to process career stats for {identifier}."
    PLAYER_CAREER_STATS_UNEXPECTED: str = "Unexpected error fetching career stats for {identifier}: {error}"
    PLAYER_AWARDS_API: str = "API error fetching awards for {identifier}: {error}"
    PLAYER_AWARDS_PROCESSING: str = "Failed to process awards for {identifier}."
    PLAYER_AWARDS_UNEXPECTED: str = "Unexpected error fetching awards for {identifier}: {error}"
    PLAYER_SHOTCHART_API: str = "API error fetching shot chart for {identifier} (Season: {season}): {error}"
    PLAYER_SHOTCHART_PROCESSING: str = "Failed to process shot chart data for {identifier} (Season: {season})."
    PLAYER_SHOTCHART_UNEXPECTED: str = "Unexpected error fetching shot chart for {identifier} (Season: {season}): {error}"
    PLAYER_DEFENSE_API: str = "API error fetching defense stats for {identifier} (Season: {season}): {error}"
    PLAYER_DEFENSE_PROCESSING: str = "Failed to process defense stats for {identifier} (Season: {season})."
    PLAYER_DEFENSE_UNEXPECTED: str = "Unexpected error fetching defense stats for {identifier} (Season: {season}): {error}"
    PLAYER_CLUTCH_API: str = "API error fetching clutch stats for {identifier} (Season: {season}): {error}"
    PLAYER_CLUTCH_PROCESSING: str = "Failed to process clutch stats for {identifier} (Season: {season})."
    PLAYER_CLUTCH_UNEXPECTED: str = "Unexpected error fetching clutch stats for {identifier} (Season: {season}): {error}"
    PLAYER_PASSING_API: str = "API error fetching passing stats for {identifier} (Season: {season}): {error}"
    PLAYER_PASSING_PROCESSING: str = "Failed to process passing stats for {identifier} (Season: {season})."
    PLAYER_PASSING_UNEXPECTED: str = "Unexpected error fetching passing stats for {identifier} (Season: {season}): {error}"
    PLAYER_REBOUNDING_API: str = "API error fetching rebounding stats for {identifier} (Season: {season}): {error}"
    PLAYER_REBOUNDING_PROCESSING: str = "Failed to process rebounding stats for {identifier} (Season: {season})."
    PLAYER_REBOUNDING_UNEXPECTED: str = "Unexpected error fetching rebounding stats for {identifier} (Season: {season}): {error}"
    PLAYER_SHOTS_TRACKING_API: str = "API error fetching shot tracking stats for {identifier} (Season: {season}): {error}"
    PLAYER_SHOTS_TRACKING_PROCESSING: str = "Failed to process shot tracking stats for {identifier} (Season: {season})."
    PLAYER_SHOTS_TRACKING_UNEXPECTED: str = "Unexpected error fetching shot tracking stats for {identifier} (Season: {season}): {error}"
    PLAYER_ID_REQUIRED: str = "player_id is required when searching by player."

    PLAYER_PROFILE_API: str = "API error fetching player profile for {identifier}: {error}"
    PLAYER_PROFILE_PROCESSING: str = "Failed to process essential profile data for {identifier}"
    PLAYER_PROFILE_UNEXPECTED: str = "Unexpected error fetching player profile for {identifier}: {error}"
    PLAYER_HUSTLE_API: str = "API error fetching hustle stats: {error}"
    PLAYER_HUSTLE_PROCESSING: str = "Data processing error for hustle stats."
    PLAYER_HUSTLE_UNEXPECTED: str = "Unexpected error fetching hustle stats: {error}"

    PLAYER_ANALYSIS_API: str = "API error analyzing stats for player {identifier}: {error}"
    PLAYER_ANALYSIS_PROCESSING: str = "Failed to process analysis stats for player {identifier}."
    PLAYER_ANALYSIS_UNEXPECTED: str = "Unexpected error analyzing stats for player {identifier}: {error}"
    PLAYER_SEARCH_UNEXPECTED: str = "Unexpected error searching for players: {error}"

    # Team Errors
    TEAM_IDENTIFIER_EMPTY: str = "Team identifier (name, abbreviation, or ID) cannot be empty."
    TEAM_NOT_FOUND: str = "Team '{identifier}' not found."
    TEAM_API: str = "API error fetching {data_type} for team {identifier} (Season: {season}): {error}"
    TEAM_PROCESSING: str = "Failed to process {data_type} for team {identifier} (Season: {season})."
    TEAM_UNEXPECTED: str = "Unexpected error fetching team data for {identifier} (Season: {season}): {error}"
    TEAM_ALL_FAILED: str = "Failed to fetch any data (info, ranks, roster, coaches) for team {identifier} (Season: {season}). Errors: {errors_list}"
    TEAM_ID_REQUIRED: str = "team_id is required when searching by team."
    TEAM_PASSING_API: str = "API error fetching passing stats for team {identifier} (Season: {season}): {error}"
    TEAM_PASSING_PROCESSING: str = "Failed to process passing stats for team {identifier} (Season: {season})."
    TEAM_PASSING_UNEXPECTED: str = "Unexpected error fetching passing stats for team {identifier} (Season: {season}): {error}"
    TEAM_SHOOTING_API: str = "API error fetching shooting stats for team {identifier} (Season: {season}): {error}"
    TEAM_SHOOTING_PROCESSING: str = "Failed to process shooting stats for team {identifier} (Season: {season})."
    TEAM_SHOOTING_UNEXPECTED: str = "Unexpected error fetching shooting stats for team {identifier} (Season: {season}): {error}"
    TEAM_REBOUNDING_API: str = "API error fetching rebounding stats for team {identifier} (Season: {season}): {error}"
    TEAM_REBOUNDING_PROCESSING: str = "Failed to process rebounding stats for team {identifier} (Season: {season})."
    TEAM_REBOUNDING_UNEXPECTED: str = "Unexpected error fetching rebounding stats for team {identifier} (Season: {season}): {error}"
    TEAM_SEARCH_UNEXPECTED: str = "Unexpected error searching for teams: {error}"
    TEAM_IDENTIFIER_OR_ID_REQUIRED: str = "Either team_identifier or team_id must be provided."
    INVALID_TEAM_IDENTIFIER: str = "Invalid team identifier: '{identifier}'. Must be a valid team name, abbreviation, or ID."
    INVALID_TEAM_ID_VALUE: str = "Invalid Team ID: '{team_id}'. Team ID must be a valid positive integer."

    # Game Errors
    GAME_ID_EMPTY: str = "Game ID cannot be empty."
    INVALID_GAME_ID_FORMAT: str = "Invalid game ID format: '{game_id}'. Expected 10 digits."
    GAME_NOT_FOUND: str = "Game with ID '{game_id}' not found."
    BOXSCORE_API: str = "API error fetching boxscore for game {game_id}: {error}"
    BOXSCORE_TRADITIONAL_API: str = "API error fetching traditional boxscore for game {game_id}: {error}"
    BOXSCORE_ADVANCED_API: str = "API error fetching advanced boxscore for game {game_id}: {error}"
    BOXSCORE_FOURFACTORS_API: str = "Error fetching BoxScoreFourFactorsV3 for game {game_id}: {error}"
    BOXSCORE_USAGE_API: str = "Error fetching BoxScoreUsageV3 for game {game_id}: {error}"
    BOXSCORE_DEFENSIVE_API: str = "Error fetching BoxScoreDefensiveV2 for game {game_id}: {error}"
    BOXSCORE_SUMMARY_API: str = "Error fetching BoxScoreSummaryV2 for game {game_id}: {error}"
    BOXSCORE_MATCHUPS_API: str = "Error fetching BoxScoreMatchupsV3 for game {game_id}: {error}"
    WINPROBABILITY_API: str = "API error fetching win probability for game {game_id}: {error}"
    PLAYBYPLAY_API: str = "API error fetching play-by-play for game {game_id}: {error}"
    SHOTCHART_API: str = "API error fetching shot chart for game {game_id}: {error}"
    GAME_UNEXPECTED: str = "Unexpected error fetching data for game {game_id}: {error}"
    SHOTCHART_PROCESSING: str = "Failed to process shot chart data for game {game_id}."
    INVALID_SEASON_FOR_GAME_SEARCH: str = "Invalid season provided for game search: {season}."
    DATE_ONLY_GAME_FINDER_UNSUPPORTED: str = "Searching games by date range only is not supported for leaguegamefinder. Please also provide a season, team ID, or player ID for a more stable query."
    GAME_SEARCH_UNEXPECTED: str = "Unexpected error searching for games: {error}"

    # Synergy Errors
    SYNERGY_API: str = "API error fetching synergy play types: {error}"
    SYNERGY_PROCESSING: str = "Failed to process synergy play types data."
    SYNERGY_UNEXPECTED: str = "Unexpected error fetching synergy play types: {error}"
    INVALID_PLAY_TYPE: str = "Invalid play type: '{play_type}'. Valid options: {options}"
    INVALID_TYPE_GROUPING: str = "Invalid type grouping: '{type_grouping}'. Valid options: {options}"
    SYNERGY_PLAY_TYPE_REQUIRED: str = "A specific play_type is required to fetch Synergy data. Valid options: {options}. General queries without a play type are not supported by the NBA API."

    # League Errors
    LEAGUE_STANDINGS_API: str = "API error fetching league standings for Season {season} (Type: {season_type}): {error}"
    LEAGUE_STANDINGS_PROCESSING: str = "Failed to process league standings for Season {season} (Type: {season_type})."
    LEAGUE_STANDINGS_UNEXPECTED: str = "Unexpected error fetching league standings for Season {season} (Type: {season_type}): {error}"
    LEAGUE_SCOREBOARD_API: str = "API error fetching scoreboard for date {game_date}: {error}"
    LEAGUE_SCOREBOARD_UNEXPECTED: str = "Unexpected error fetching scoreboard for date {game_date}: {error}"
    DRAFT_HISTORY_API: str = "API error fetching draft history for year {year}: {error}"
    DRAFT_HISTORY_UNEXPECTED: str = "Unexpected error fetching draft history for year {year}: {error}"
    LEAGUE_LEADERS_API: str = "API error fetching league leaders for {stat_category} (Season: {season}): {error}"
    LEAGUE_LEADERS_PROCESSING: str = "Failed to process league leaders data for {stat_category} (Season: {season})."
    LEAGUE_LEADERS_UNEXPECTED: str = "Unexpected error fetching league leaders for {stat_category} (Season: {season}): {error}"
    INVALID_DRAFT_YEAR_FORMAT: str = "Invalid draft year format: '{year}'. Expected YYYY."
    DRAFT_HISTORY_PROCESSING: str = "Failed to process draft history for year {year}."
    LEAGUE_GAMES_API: str = "API error fetching league games: {error}"
    LEAGUE_GAMES_UNEXPECTED: str = "Unexpected error fetching league games: {error}"

    # Trending Stats Errors
    INVALID_TOP_N: str = "Invalid top_n parameter: must be a positive integer > 0, got {value}"
    TRENDING_UNEXPECTED: str = "Unexpected error fetching trending player data: {error}"
    TRENDING_TEAMS_UNEXPECTED: str = "Unexpected error fetching trending teams data: {error}"

    # Matchup Errors
    MISSING_PLAYER_IDENTIFIER: str = "Player identifier (name or ID) cannot be empty."
    MATCHUPS_API: str = "Error fetching matchup data: {error}"
    MATCHUPS_ROLLUP_API: str = "Error fetching matchup rollup data: {error}"
    MATCHUPS_PROCESSING: str = "Failed to process matchup data."
    MATCHUPS_UNEXPECTED: str = "Unexpected error fetching matchup data: {error}"
    MATCHUPS_ROLLUP_PROCESSING: str = "Failed to process matchup rollup data."
    MATCHUPS_ROLLUP_UNEXPECTED: str = "Unexpected error fetching matchup rollup data: {error}"

    # Odds Errors
    ODDS_API_UNEXPECTED: str = "Unexpected error fetching odds data: {error}"

    # Parameter Validation Errors
    INVALID_SEASON_TYPE: str = "Invalid season_type: '{value}'. Valid options: {options}"
    INVALID_STAT_CATEGORY: str = "Invalid stat_category: '{value}'. Valid options: {options}"
    INVALID_PER_MODE: str = "Invalid per_mode: '{value}'. Valid options: {options}"
    INVALID_GROUP_QUANTITY: str = "Invalid group_quantity: '{value}'. Valid options: {options}"
    INVALID_LEAGUE_ID: str = "Invalid league_id: '{value}'. Valid options: {options}"
    INVALID_MEASURE_TYPE: str = "Invalid measure_type: '{value}'. Valid options: {options}"
    INVALID_PLAYER_POSITION: str = "Invalid player_position: '{value}'. Valid options: {options}"
    INVALID_PLAYER_EXPERIENCE: str = "Invalid player_experience: '{value}'. Valid options: {options}"
    INVALID_STARTER_BENCH: str = "Invalid starter_bench: '{value}'. Valid options: {options}"
    INVALID_CLUTCH_TIME: str = "Invalid clutch_time: '{value}'. Valid options: {options}"
    INVALID_AHEAD_BEHIND: str = "Invalid ahead_behind: '{value}'. Valid options: {options}"
    INVALID_POINT_DIFF: str = "Invalid point_diff: '{value}'. Must be a positive integer."
    INVALID_SCOPE: str = "Invalid scope: '{value}'. Valid options: {options}"
    INVALID_PLAYER_OR_TEAM_ABBREVIATION: str = "Invalid player_or_team_abbreviation: '{value}'. Must be 'P' or 'T'."
    INVALID_DEFENSE_CATEGORY: str = "Invalid defense_category: '{value}'. Valid options: {options}"
    INVALID_SHOT_CLOCK_RANGE: str = "Invalid shot_clock_range: '{value}'. Valid options: {options}"
    INVALID_PLUS_MINUS: str = "Invalid plus_minus: '{value}'. Must be 'Y' or 'N'."
    INVALID_PACE_ADJUST: str = "Invalid pace_adjust: '{value}'. Must be 'Y' or 'N'."
    INVALID_RANK: str = "Invalid rank: '{value}'. Must be 'Y' or 'N'."
    INVALID_GAME_SEGMENT: str = "Invalid game_segment: '{value}'. Valid options: {options}"
    INVALID_LOCATION: str = "Invalid location: '{value}'. Valid options: {options}"
    INVALID_OUTCOME: str = "Invalid outcome: '{value}'. Valid options: {options}"
    INVALID_CONFERENCE: str = "Invalid vs_conference: '{value}'. Valid options: {options}"
    INVALID_DIVISION: str = "Invalid vs_division: '{value}'. Valid options: {options}"
    INVALID_SEASON_SEGMENT: str = "Invalid season_segment: '{value}'. Valid options: {options}"
    INVALID_RUN_TYPE: str = "Invalid RunType: '{value}'. Valid options: {options}"
    INVALID_PARAMETER_FORMAT: str = "Invalid format for parameter '{param_name}': '{param_value}'. Expected format: {expected_format}"
    INVALID_CONTEXT_MEASURE: str = "Invalid context_measure: '{value}'. Valid options: {options}"

    # League Player On Details Errors
    LEAGUE_PLAYER_ON_DETAILS_PROCESSING: str = "Processing failed for league player on details (TeamID: {team_id}, Season: {season})."
    LEAGUE_PLAYER_ON_DETAILS_UNEXPECTED: str = "Unexpected error fetching league player on details for TeamID {team_id}, Season {season}: {error}"

    # Specific NBA API interaction errors
    NBA_API_TIMEOUT: str = "NBA API request to endpoint '{endpoint_name}' timed out. Details: {details}"
    NBA_API_CONNECTION_ERROR: str = "NBA API request to endpoint '{endpoint_name}' failed due to a connection error. Details: {details}"
    NBA_API_GENERAL_ERROR: str = "NBA API request to endpoint '{endpoint_name}' failed with an unexpected error. Details: {details}"

    # Scoreboard specific
    UNEXPECTED_ERROR_SCOREBOARD: str = "Unexpected error fetching/formatting scoreboard data for date {date}: {error_details}"

    # Advanced Player Stats (PlayerEstimatedMetrics, TeamEstimatedMetrics, TeamPlayerOnOffDetails)
    PLAYER_ESTIMATED_METRICS_API: str = "API error fetching player estimated metrics (Season: {season}, Type: {season_type}): {error}"
    PLAYER_DASHBOARD_GENERAL_API: str = "API error fetching player dashboard general splits for {player_name}: {error}"
    PLAYER_DASHBOARD_SHOOTING_API: str = "API error fetching player dashboard shooting splits for {player_name}: {error}"
    PLAYER_DASHBOARD_GAME_API: str = "API error fetching player dashboard game splits for {player_name}: {error}"
    PLAYER_DASHBOARD_LASTN_API: str = "API error fetching player dashboard last N games for {player_name}: {error}"
    TEAM_DASHBOARD_SHOOTING_API: str = "API error fetching team dashboard shooting splits for {team_identifier}: {error}"
    TEAM_DASH_LINEUPS_API: str = "API error fetching team lineups for {team_identifier}: {error}"
    LEAGUE_DASH_PLAYER_BIO_API: str = "API error fetching player bio stats (Season: {season}, Type: {season_type}): {error}"
    LEAGUE_DASH_PLAYER_CLUTCH_API: str = "API error fetching player clutch stats (Season: {season}, Type: {season_type}, Clutch Time: {clutch_time}): {error}"
    LEAGUE_DASH_TEAM_CLUTCH_API: str = "API error fetching team clutch stats (Season: {season}, Type: {season_type}, Clutch Time: {clutch_time}): {error}"
    PLAYER_VS_PLAYER_API: str = "API error fetching player vs player stats (Player: {player_id}, Vs Player: {vs_player_id}, Season: {season}): {error}"
    PLAYER_GAME_LOGS_API: str = "API error fetching player game logs (Player ID: {player_id}, Team ID: {team_id}, Season: {season}): {error}"
    LEAGUE_DASH_PLAYER_STATS_API: str = "API error fetching league player stats (Season: {season}, Type: {season_type}, Measure Type: {measure_type}): {error}"
    TEAM_GAME_LOGS_API: str = "API error fetching team game logs (Team ID: {team_id}, Season: {season}, Type: {season_type}): {error}"
    LEAGUE_DASH_TEAM_STATS_API: str = "API error fetching league team stats (Season: {season}, Type: {season_type}, Measure Type: {measure_type}): {error}"
    PLAYER_CAREER_STATS_API: str = "API error fetching player career stats (Player ID: {player_id}, Per Mode: {per_mode}): {error}"

    PLAYER_SHOT_CHART_API: str = "Error fetching shot chart data for player {player_id} in season {season}: {error}"
    PLAYER_SHOT_CHART_NO_DATA: str = "No shot chart data found for player {player_id} in season {season}."
    TEAM_SHOT_CHART_API: str = "Error fetching shot chart data for game {game_id}: {error}"
    TEAM_SHOT_CHART_NO_DATA: str = "No shot chart data found for game {game_id}."
    COMMON_ALL_PLAYERS_API_ERROR: str = "Error fetching data from CommonAllPlayers endpoint: {error}"
    COMMON_PLAYOFF_SERIES_API_ERROR: str = "Error fetching data from CommonPlayoffSeries endpoint: {error}"
    COMMON_TEAM_YEARS_API_ERROR: str = "Error fetching data from CommonTeamYears endpoint: {error}"
    LEAGUE_DASH_LINEUPS_API_ERROR: str = "Error fetching data from LeagueDashLineups endpoint: {error}"
    LEAGUE_DASH_OPP_PT_SHOT_API_ERROR: str = "Error fetching data from LeagueDashOppPtShot endpoint: {error}"

===== backend\core\knowledge_base.py =====
import sys
from pathlib import Path
import os
# import csv # No longer directly used in this version
if __name__ == "__main__" and __package__ is None:
    PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
    if str(PROJECT_ROOT) not in sys.path:
        sys.path.insert(0, str(PROJECT_ROOT))

from typing import List, Optional, Union, Type, Any, Tuple
import re
from urllib.parse import urlparse, parse_qs
# import shutil # No longer directly used if temp dirs are handled by routes

from agno.knowledge.combined import CombinedKnowledgeBase
from agno.knowledge.pdf import PDFKnowledgeBase
from agno.knowledge.csv import CSVKnowledgeBase
from agno.knowledge.text import TextKnowledgeBase
# Removed: from agno.knowledge.wikipedia import WikipediaKnowledgeBase
# Removed: from agno.knowledge.youtube import YouTubeKnowledgeBase
from agno.document.chunking.fixed import FixedSizeChunking

import logging
from agno.vectordb.chroma import ChromaDb
from agno.embedder.google import GeminiEmbedder
from config import settings
from agno.document import Document
logger = logging.getLogger(__name__)

class KnowledgeSourceAdditionError(Exception):
    """Custom exception for errors during knowledge source addition."""
    pass

CHROMA_DB_PERSIST_PATH = settings.CHROMA_DB_NBA_AGENT
Path(CHROMA_DB_PERSIST_PATH).mkdir(parents=True, exist_ok=True)

DEFAULT_CHUNKING_STRATEGY = FixedSizeChunking(chunk_size=1000, overlap=200)

def sanitize_for_collection_name(name: str) -> str:
    name = re.sub(r"[^a-zA-Z0-9_-]", "_", name)
    if len(name) > 63:
        name = name[:63]
    if len(name) < 3:
        name = f"{name}___"
    if not name[0].isalnum():
        name = "a" + name[1:]
    if len(name) > 1 and not name[-1].isalnum():
        name = name[:-1] + "a"
    if len(name) < 3:
        name = f"{name:<3}".replace(" ", "_")
    if len(name) > 63:
        name = name[:63]
        if not name[-1].isalnum() and len(name) > 0:
             name = name[:-1] + 'z'
    name = name.lower()
    if ".." in name:
        name = name.replace("..", "_")
    if len(name) < 3:
        name = (name + "pad")[:3]
    if len(name) > 63:
        name = name[:63]
        if not name[-1].isalnum() and len(name) > 0:
             name = name[:-1] + 'z'
    return name

# Removed get_youtube_video_id as it's no longer used for special KB handling

class NBAnalyzerKnowledgeBase:
    def __init__(self, recreate_collections: bool = False):
        self.recreate_collections = recreate_collections
        self.embedder = GeminiEmbedder(
            id="text-embedding-004",
            api_key=settings.GOOGLE_API_KEY,
            dimensions=768
        )
        self.chroma_db_path = settings.CHROMA_DB_NBA_AGENT
        os.makedirs(self.chroma_db_path, exist_ok=True) # Ensures path exists
        self.local_pdf_sources: List[PDFKnowledgeBase] = []
        self.local_csv_sources: List[CSVKnowledgeBase] = []
        self.local_txt_sources: List[TextKnowledgeBase] = []
        
        self.combined_kb: Optional[CombinedKnowledgeBase] = None
        self._initialize_combined_kb()

    def _initialize_combined_kb(self):
        logger.info(f"[KB_DEBUG_COMBINED] Entering _initialize_combined_kb. Current sources: PDF({len(self.local_pdf_sources)}), CSV({len(self.local_csv_sources)}), TXT({len(self.local_txt_sources)})")
        all_sources = (
            self.local_pdf_sources + 
            self.local_csv_sources + 
            self.local_txt_sources
        )
        if not all_sources:
            logger.info("[KB_LOG] No sources to combine yet.")
            self.combined_kb = None
            logger.info(f"[KB_DEBUG_COMBINED] Exiting _initialize_combined_kb early (no sources).")
            return

        logger.info(f"[KB_LOG] Attempting to initialize CombinedKnowledgeBase with {len(all_sources)} sources.")
        try:
            combined_collection_name = self._get_collection_name("combined", "orchestrator")
            combined_vector_db = ChromaDb(
                collection=combined_collection_name,
                path=self.chroma_db_path,
                embedder=self.embedder
            )
            self.combined_kb = CombinedKnowledgeBase(
                sources=all_sources,
                vector_db=combined_vector_db,
                embedder=self.embedder
            )
            logger.info(f"[KB_DEBUG_COMBINED] Loading CombinedKnowledgeBase (recreate={self.recreate_collections}).")
            self.combined_kb.load(recreate=self.recreate_collections)
            logger.info(f"[KB_LOG] CombinedKnowledgeBase initialized and loaded with {len(self.combined_kb.sources)} sources, main collection: {combined_collection_name}")
        except Exception as e:
            logger.error(f"Failed to initialize or load CombinedKnowledgeBase: {e}", exc_info=True)
            self.combined_kb = None

    def _get_collection_name(self, prefix: str, identifier: str) -> str:
        s_prefix = sanitize_for_collection_name(prefix)
        s_identifier = sanitize_for_collection_name(identifier)
        
        max_identifier_len = 63 - len(s_prefix) - 1
        if len(s_identifier) > max_identifier_len:
            s_identifier = s_identifier[:max_identifier_len]
        
        if not s_identifier: # Simplified check for empty
            s_identifier = "default"
        
        if len(s_identifier) > 0 and not s_identifier[-1].isalnum():
            s_identifier = s_identifier[:-1] + 'z'

        full_name = f"{s_prefix}_{s_identifier}"
        return sanitize_for_collection_name(full_name)

    def add_local_pdf_directory(self, dir_path: str):
        logger.info(f"[KB_DEBUG] Attempting to add PDF directory: {dir_path}")
        if not os.path.isdir(dir_path):
            logger.error(f"PDF directory not found: {dir_path}")
            return f"Failed to add PDF directory: Directory not found." # Return message

        dir_name = os.path.basename(dir_path)
        collection_name_str = self._get_collection_name("pdfdir", dir_name)
        
        if any(kb.vector_db.collection_name == collection_name_str for kb in self.local_pdf_sources):
            logger.info(f"PDF directory {dir_path} with collection {collection_name_str} already added. Skipping.")
            return f"PDF directory {dir_path} already added." # Return message

        try:
            pdf_vector_db = ChromaDb(collection=collection_name_str, path=self.chroma_db_path, embedder=self.embedder)
            pdf_kb = PDFKnowledgeBase(
                path=dir_path,
                vector_db=pdf_vector_db,
                embedder=self.embedder,
            )
            logger.info(f"[KB_DEBUG] Loading PDFKnowledgeBase (recreate={self.recreate_collections})")
            pdf_kb.load(recreate=self.recreate_collections)
            logger.info(f"[KB_DEBUG] PDFKnowledgeBase loaded successfully.")
            
            self.local_pdf_sources.append(pdf_kb)
            logger.info(f"[KB_DEBUG] PDFKnowledgeBase object created and appended to local sources.")
            self._initialize_combined_kb()
            logger.info(f"PDF directory {dir_path} processed and loaded into collection: {collection_name_str}.")
            return f"Successfully added PDF directory: {dir_path}."
        except Exception as e:
            logger.error(f"Failed to add PDF directory {dir_path} during KB object lifecycle: {e}", exc_info=True)
            return f"Failed to add PDF directory: {e}"

    def add_local_csv_directory(self, dir_path: str):
        logger.info(f"[KB_DEBUG] Attempting to add CSV directory: {dir_path}")
        if not os.path.isdir(dir_path):
            logger.error(f"CSV directory not found: {dir_path}")
            return f"Failed to add CSV directory: Directory not found."

        dir_name = os.path.basename(dir_path)
        collection_name_str = self._get_collection_name("csvdir", dir_name)

        if any(kb.vector_db.collection_name == collection_name_str for kb in self.local_csv_sources):
            logger.info(f"CSV directory {dir_path} with collection {collection_name_str} already added.")
            return f"CSV directory {dir_path} already added."
        
        try:
            csv_kb = CSVKnowledgeBase(
                path=dir_path,
                vector_db=ChromaDb(collection=collection_name_str, path=self.chroma_db_path, embedder=self.embedder),
                embedder=self.embedder,
            )
            csv_kb.load(recreate=self.recreate_collections)
            logger.info(f"[KB_DEBUG] CSVKnowledgeBase loaded successfully.")
            self.local_csv_sources.append(csv_kb)
            logger.info(f"[KB_DEBUG] CSVKnowledgeBase object created and appended to local sources. Load is currently skipped.")
            self._initialize_combined_kb()
            logger.info(f"CSV directory {dir_path} processed and loaded into collection: {collection_name_str}.")
            return f"Successfully added CSV directory: {dir_path}."
        except Exception as e:
            logger.error(f"Failed to add CSV directory {dir_path}: {e}", exc_info=True)
            return f"Failed to add CSV directory: {e}"

    def add_local_txt_directory(self, dir_path: str):
        logger.info(f"[KB_DEBUG] Attempting to add TXT directory: {dir_path}")
        if not os.path.isdir(dir_path):
            logger.error(f"TXT directory not found: {dir_path}")
            return f"Failed to add TXT directory: Directory not found."

        dir_name = os.path.basename(dir_path)
        collection_name_str = self._get_collection_name("txtdir", dir_name)

        if any(kb.vector_db.collection_name == collection_name_str for kb in self.local_txt_sources):
            logger.info(f"TXT directory {dir_path} with collection {collection_name_str} already added.")
            return f"TXT directory {dir_path} already added."
        
        try:
            txt_kb = TextKnowledgeBase(
                path=dir_path,
                vector_db=ChromaDb(collection=collection_name_str, path=self.chroma_db_path, embedder=self.embedder),
                embedder=self.embedder,
            )
            txt_kb.load(recreate=self.recreate_collections)
            logger.info(f"[KB_DEBUG] TextKnowledgeBase loaded successfully.")
            self.local_txt_sources.append(txt_kb)
            logger.info(f"[KB_DEBUG] TextKnowledgeBase object created and appended to local sources. Load is currently skipped.")
            self._initialize_combined_kb()
            logger.info(f"TXT directory {dir_path} processed and loaded into collection: {collection_name_str}.")
            return f"Successfully added TXT directory: {dir_path}."
        except Exception as e:
            logger.error(f"Failed to add TXT directory {dir_path}: {e}", exc_info=True)
            return f"Failed to add TXT directory: {e}"

    def add_website_url(self, url: str, max_links: int = 5): # max_links is unused currently
        # All URLs are now treated as generic links for the agent to handle with tools.
        # No direct KB ingestion for websites, YouTube, or Wikipedia here.
        logger.info(f"Received website URL: {url}. This URL type is noted but not processed for direct knowledge base ingestion. Agent should use tools like web crawlers or YouTube tools if needed.")
        # No KB object is created, no collection name needed for this.
        # self._initialize_combined_kb() # No new KB source added, so no need to reinitialize combined_kb.
        return f"Website URL {url} noted. Agent will use tools to access content if required."

    def add_website_urls(self, urls: List[str], collection_suffix: str = "websites_batch", chunking_strategy: Optional[Any] = None, max_links_per_url: int = 1, max_depth_per_url: int = 1):
        if not urls:
            logger.warning("[KB_LOG] add_website_urls called with empty URL list.")
            return "No URLs provided."
        
        processed_messages = []
        for i, url_item in enumerate(urls):
            if not url_item or not isinstance(url_item, str):
                logger.warning(f"[KB_LOG] Invalid URL at index {i}: {url_item}. Skipping.")
                processed_messages.append(f"Skipped invalid URL at index {i}: {url_item}")
                continue
            try:
                # Delegate to the simplified add_website_url
                message = self.add_website_url(url_item, max_links=max_links_per_url)
                processed_messages.append(message)
            except Exception as e: # Should be caught by add_website_url, but as a fallback
                logger.error(f"[KB_LOG] Unexpected error adding website URL {url_item} from batch: {e}", exc_info=True)
                processed_messages.append(f"Error adding URL {url_item}: {e}")
        
        logger.info(f"[KB_LOG] Processed {len(urls)} URLs from the batch via add_website_urls.")
        return "Batch URL processing complete. " + " | ".join(processed_messages)


    def query(self, query_text: str, n_results: int = 5) -> List[Tuple[Document, float]]:
        if not self.combined_kb:
            logger.warning("CombinedKnowledgeBase is not initialized. No sources added or loaded yet.")
            return []
        try:
            # Assuming combined_kb.search is robust enough or we add checks
            # The actual .load() calls for individual KBs are currently commented out, 
            # so this query will likely return empty results or results from an empty/stale combined_kb.
            raw_results = self.combined_kb.search(query=query_text, num_documents=n_results) 
            
            processed_results = []
            for res in raw_results:
                if isinstance(res, tuple) and len(res) == 2 and hasattr(res[0], 'content'):
                    processed_results.append(res)
                elif hasattr(res, 'content'):
                    processed_results.append((res, 0.0)) 
                else:
                    logger.warning(f"Query raw_result is an unexpected format, skipping: {type(res)}.")
            
            logger.info(f"Query: '{query_text}', Processed {len(processed_results)} results.")
            return processed_results
        except Exception as e:
            logger.error(f"Error querying CombinedKnowledgeBase: {e}", exc_info=True)
            return []

    def get_underlying_kb_object(self) -> Optional[CombinedKnowledgeBase]:
        return self.combined_kb

    def list_available_collections(self) -> List[str]:
        collections = []
        # Only PDF, CSV, TXT sources remain
        for kb_list in [self.local_pdf_sources, self.local_csv_sources, self.local_txt_sources]:
            for kb in kb_list:
                if hasattr(kb, 'vector_db') and hasattr(kb.vector_db, 'collection_name'):
                    collections.append(kb.vector_db.collection_name)
        return list(set(collections))

    # Removed async_search as it wasn't fully aligned and query can be run in threadpool by FastAPI
    # Removed _get_or_create_kb_for_source as it was not implemented
    # Removed add_youtube_video_urls method (now handled by add_website_urls -> add_website_url)
    # Removed add_wikipedia_topics method

    async def initialize_default_knowledge(self):
        default_pdf_dir = Path(__file__).resolve().parent.parent / "pdfs"
        if default_pdf_dir.is_dir() and any(default_pdf_dir.glob("*.pdf")):
            logger.info(f"[KB_LOG] Initializing with default PDFs from: {default_pdf_dir}")
            try:
                self.add_local_pdf_directory(str(default_pdf_dir)) # Will also have its load skipped
                logger.info(f"[KB_LOG] Successfully initiated loading of default PDFs.")
            except Exception as e:
                logger.error(f"[KB_LOG] Error initiating loading of default PDFs: {e}", exc_info=True)
        else:
            logger.info(f"[KB_LOG] Default PDF directory not found or empty: {default_pdf_dir}")

        default_csv_dir = Path(__file__).resolve().parent.parent / "csvs"
        if default_csv_dir.is_dir() and any(default_csv_dir.glob("*.csv")):
            logger.info(f"[KB_LOG] Initializing with default CSVs from: {default_csv_dir}")
            try:
                self.add_local_csv_directory(str(default_csv_dir)) # Will also have its load skipped
                logger.info(f"[KB_LOG] Successfully initiated loading of default CSVs.")
            except Exception as e:
                logger.error(f"[KB_LOG] Error initiating loading of default CSVs: {e}", exc_info=True)
        else:
            logger.info(f"[KB_LOG] Default CSV directory not found or empty: {default_csv_dir}")

# Main block for testing removed as it's better to use dedicated test scripts.


===== backend\core\__init__.py =====
# This file makes the 'core' directory a Python package.

===== backend\routes\analyze.py =====
import logging
import asyncio
import json
from fastapi import APIRouter, HTTPException, Body, Query
from typing import Dict, Any, Optional, List
from schemas import PlayerAnalysisRequest
from backend.api_tools.analyze import analyze_player_stats_logic
from backend.api_tools.advanced_metrics import fetch_player_advanced_analysis_logic
from backend.api_tools.shot_charts import fetch_player_shot_chart
from backend.api_tools.advanced_shot_charts import process_shot_data_for_visualization # Moved from local import
from backend.api_tools.player_comparison import compare_player_shots as compare_shots # Moved from local import
from backend.api_tools.visualization_cache import VisualizationCache # Moved from local import
from backend.core.errors import Errors
from config import settings # Already at top, but ensure it's used if needed locally

# --- Module-Level Constants ---
MIN_PLAYERS_FOR_COMPARISON = 2
MAX_PLAYERS_FOR_COMPARISON = 4

# Router for player analysis related endpoints
router = APIRouter(
    tags=["Analysis", "Players"]
)
logger = logging.getLogger(__name__)

@router.post(
    "/player",
    response_model=Dict[str, Any],
    summary="Analyze Player Statistics",
    description="Fetches and returns overall dashboard statistics for a specified player and season. "
                "The underlying logic uses the `PlayerDashboardByYearOverYear` NBA API endpoint, "
                "focusing on the 'OverallPlayerDashboard' data for the given season."
)
async def analyze_player_stats_endpoint(
    request: PlayerAnalysisRequest = Body(...)
) -> Dict[str, Any]:
    """
    Analyzes player statistics based on the provided player name and season.

    This endpoint retrieves overall dashboard statistics for a player for a specific season.
    It leverages a logic function that calls the NBA API.

    Request Body (`PlayerAnalysisRequest`):
    - **player_name** (str, required): The full name of the player to analyze (e.g., "LeBron James").
    - **season** (str, optional): The NBA season identifier in YYYY-YY format (e.g., "2023-24").
      If not provided, the logic layer defaults to the `CURRENT_SEASON` defined in the backend configuration.
    - **season_type** (str, optional): The type of season (e.g., "Regular Season", "Playoffs").
      Defaults to "Regular Season" in the logic layer.
    - **per_mode** (str, optional): The statistical mode (e.g., "PerGame", "Totals").
      Defaults to "PerGame" in the logic layer.
    - **league_id** (str, optional): The league ID (e.g., "00" for NBA).
      Defaults to "00" (NBA) in the logic layer.

    Successful Response (200 OK):
    Returns a dictionary containing the player's analysis. Example structure:
    ```json
    {
        "player_name": "LeBron James",
        "player_id": 2544,
        "season": "2023-24",
        // ... other fields from PlayerAnalysisRequest if returned by logic
        "overall_dashboard_stats": {
            "GROUP_SET": "Overall",
            "PLAYER_ID": 2544,
            // ... many other statistical fields
        }
    }
    ```
    If no data is found, `overall_dashboard_stats` might be empty or an error might be indicated.

    Error Responses:
    - **400 Bad Request**: If input validation fails (e.g., invalid season format, missing player_name).
    - **404 Not Found**: If the specified player is not found by the logic layer.
    - **500 Internal Server Error**: For unexpected errors during API calls or data processing.
    """
    logger.info(f"Received POST /analyze/player request for: {request.player_name}, Season: {request.season}")

    # Pydantic model `PlayerAnalysisRequest` handles initial validation of field types and presence.
    # Further business logic validation (e.g., season format if not covered by regex in Pydantic)
    # can be in the logic layer or here if simple.
    # The `pattern` in Pydantic Field for `season` should handle format validation.

    try:
        result_json_string = await asyncio.to_thread(
            analyze_player_stats_logic,
            request.player_name,
            request.season,
            request.season_type.value if request.season_type else None, # Pass enum value
            request.per_mode.value if request.per_mode else None,       # Pass enum value
            request.league_id.value if request.league_id else None      # Pass enum value
        )
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from analyze_player_stats_logic for {request.player_name}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid player" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() or "missing parameter" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)

        logger.info(f"Successfully analyzed stats for player: {request.player_name}")
        return result_data

    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from logic layer for {request.player_name}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in /analyze/player endpoint for {request.player_name}: {str(e)}", exc_info=True)
        detail_msg = Errors.UNEXPECTED_ERROR.format(error=f"analyzing player stats for {request.player_name}")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/player/{player_name}/advanced",
    response_model=Dict[str, Any],
    summary="Get Advanced Player Metrics",
    description="Fetches advanced metrics, skill grades, and similar players for a specified player."
)
async def get_player_advanced_metrics(
    player_name: str
) -> Dict[str, Any]:
    """
    Retrieves advanced metrics and analysis for a player.

    This endpoint provides comprehensive advanced analytics including:
    - Advanced metrics (EPM, RAPTOR, DARKO, LEBRON, etc.)
    - Skill grades (shooting, defense, playmaking, etc.)
    - Similar players with similarity scores

    Path Parameters:
    - **player_name** (str, required): The full name of the player to analyze (e.g., "LeBron James").

    Successful Response (200 OK):
    Returns a dictionary containing the player's advanced analysis. Example structure:
    ```json
    {
        "player_name": "LeBron James",
        "player_id": 2544,
        "advanced_metrics": {
            "EPM": 5.2,
            "EPM_OFF": 4.1,
            "EPM_DEF": 1.1,
            // ... other advanced metrics
        },
        "skill_grades": {
            "perimeter_shooting": "B",
            "interior_scoring": "A",
            // ... other skill grades
        },
        "similar_players": [
            {"player_id": 201142, "player_name": "Kevin Durant", "similarity_score": 0.92},
            // ... other similar players
        ]
    }
    ```

    Error Responses:
    - **404 Not Found**: If the specified player is not found.
    - **500 Internal Server Error**: For unexpected errors during data processing.
    """
    logger.info(f"Received GET /analyze/player/{player_name}/advanced request")

    try:
        result_json_string = await asyncio.to_thread(
            fetch_player_advanced_analysis_logic,
            player_name
        )
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from fetch_player_advanced_analysis_logic for {player_name}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)

        logger.info(f"Successfully fetched advanced metrics for player: {player_name}")
        return result_data

    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from logic layer for {player_name}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in /analyze/player/{player_name}/advanced endpoint: {str(e)}", exc_info=True)
        detail_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching advanced metrics for {player_name}")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/player/{player_name}/shots",
    response_model=Dict[str, Any],
    summary="Get Player Shot Chart Data",
    description="Fetches shot chart data and zone analysis for a specified player."
)
async def get_player_shot_chart(
    player_name: str,
    season: Optional[str] = Query(None, description="NBA season in format YYYY-YY (e.g., '2023-24')"),
    season_type: str = Query("Regular Season", description="Type of season: 'Regular Season', 'Playoffs', 'Pre Season', 'All Star'"),
    last_n_games: int = Query(0, description="Number of most recent games to include (0 for all games)")
) -> Dict[str, Any]:
    """
    Retrieves shot chart data for a player.

    This endpoint provides comprehensive shot data including:
    - Individual shots with coordinates, shot type, and outcome
    - Zone analysis with shooting percentages and league comparisons

    Path Parameters:
    - **player_name** (str, required): The full name of the player to analyze (e.g., "LeBron James").

    Query Parameters:
    - **season** (str, optional): NBA season in format YYYY-YY (e.g., "2023-24"). Defaults to current season.
    - **season_type** (str, optional): Type of season. Options: "Regular Season", "Playoffs", "Pre Season", "All Star".
    - **last_n_games** (int, optional): Number of most recent games to include (0 for all games).

    Successful Response (200 OK):
    Returns a dictionary containing the player's shot data. Example structure:
    ```json
    {
        "player_name": "LeBron James",
        "player_id": 2544,
        "team_name": "Los Angeles Lakers",
        "team_id": 1610612747,
        "season": "2023-24",
        "season_type": "Regular Season",
        "shots": [
            {
                "x": -80.0,
                "y": 120.0,
                "made": true,
                "value": 2,
                "shot_type": "Jump Shot",
                "shot_zone": "Mid-Range - Left Side",
                "distance": 14.0,
                "game_date": "2023-10-24",
                "period": 1
            },
            // ... other shots
        ],
        "zones": [
            {
                "zone": "Restricted Area",
                "attempts": 120,
                "made": 84,
                "percentage": 0.7,
                "leaguePercentage": 0.65,
                "relativePercentage": 0.05
            },
            // ... other zones
        ]
    }
    ```

    Error Responses:
    - **404 Not Found**: If the specified player is not found.
    - **500 Internal Server Error**: For unexpected errors during data processing.
    """
    logger.info(f"Received GET /analyze/player/{player_name}/shots request")

    try:
        result_json_string = await asyncio.to_thread(
            fetch_player_shot_chart,
            player_name=player_name,
            season=season,
            season_type=season_type,
            last_n_games=last_n_games
        )
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from fetch_player_shot_chart for {player_name}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)

        logger.info(f"Successfully fetched shot chart data for player: {player_name}")
        return result_data

    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from logic layer for {player_name}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in /analyze/player/{player_name}/shots endpoint: {str(e)}", exc_info=True)
        detail_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching shot chart data for {player_name}")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/player/{player_name}/advanced-shotchart",
    summary="Get Advanced Shot Chart Visualization for a Player",
    description="Generates advanced shot chart visualizations for a player with various chart types and formats.",
    response_model=Dict[str, Any]
)
async def get_player_advanced_shotchart(
    player_name: str,
    season: Optional[str] = Query(None, description="NBA season in format YYYY-YY (e.g., '2023-24')"),
    season_type: str = Query("Regular Season", description="Type of season: 'Regular Season', 'Playoffs', 'Pre Season', 'All Star'"),
    chart_type: str = Query("scatter", description="Type of chart: 'scatter', 'heatmap', 'hexbin', 'animated', 'frequency', 'distance'"),
    output_format: str = Query("base64", description="Output format: 'base64', 'file'"),
    use_cache: bool = Query(True, description="Whether to use cached visualizations")
) -> Dict[str, Any]:
    """
    Generates advanced shot chart visualizations for a player.

    This endpoint provides various visualization types:
    - Scatter plot: Traditional shot chart with made/missed shots
    - Heatmap: Heat map of shot frequency
    - Hexbin: Hexagonal binning of shots
    - Animated: Animated sequence of shots
    - Frequency: Shot frequency visualization
    - Distance: Shot efficiency by distance

    Path Parameters:
    - **player_name** (str, required): The full name of the player to analyze (e.g., "LeBron James").

    Query Parameters:
    - **season** (str, optional): NBA season in format YYYY-YY (e.g., "2023-24"). Defaults to current season.
    - **season_type** (str, optional): Type of season. Options: "Regular Season", "Playoffs", "Pre Season", "All Star".
    - **chart_type** (str, optional): Type of chart to generate. Options: "scatter", "heatmap", "hexbin", "animated", "frequency", "distance".
    - **output_format** (str, optional): Output format. Options: "base64", "file".
    - **use_cache** (bool, optional): Whether to use cached visualizations. Defaults to True.

    Returns:
    - **200 OK**: Shot chart visualization data.
    - **404 Not Found**: If player is not found or no shot data exists.
    - **500 Internal Server Error**: For server-side errors.
    """
    # process_shot_data_for_visualization and settings are now imported at the top

    logger.info(f"Received GET /analyze/player/{player_name}/advanced-shotchart request with season={season}, "
                f"season_type={season_type}, chart_type={chart_type}, output_format={output_format}, use_cache={use_cache}")

    try:
        # Use the current season if not specified
        effective_season = season if season else settings.CURRENT_NBA_SEASON # settings is imported at top

        # Process the shot data and create visualization
        result = await asyncio.to_thread(
            process_shot_data_for_visualization, # Now imported at top
            player_name=player_name,
            season=effective_season,
            season_type=season_type,
            chart_type=chart_type,
            output_format=output_format,
            use_cache=use_cache
        )

        if 'error' in result:
            error_detail = result['error']
            logger.error(f"Error generating advanced shot chart for {player_name}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "no shot data" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)

        logger.info(f"Successfully generated {chart_type} shot chart for player: {player_name}")
        return result

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.critical(f"Unexpected error in /analyze/player/{player_name}/advanced-shotchart endpoint: {str(e)}", exc_info=True)
        detail_msg = Errors.UNEXPECTED_ERROR.format(error=f"generating shot chart visualization for {player_name}")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/players/compare-shots",
    summary="Compare Shot Charts for Multiple Players",
    description="Generates comparative shot chart visualizations for multiple players.",
    response_model=Dict[str, Any]
)
async def compare_player_shots(
    player_names: List[str] = Query(..., description="List of player names to compare (2-4 players)"),
    season: Optional[str] = Query(None, description="NBA season in format YYYY-YY (e.g., '2023-24')"),
    season_type: str = Query("Regular Season", description="Type of season: 'Regular Season', 'Playoffs', 'Pre Season', 'All Star'"),
    chart_type: str = Query("scatter", description="Type of chart: 'scatter', 'heatmap', 'zones'"),
    output_format: str = Query("base64", description="Output format: 'base64', 'file'"),
    use_cache: bool = Query(True, description="Whether to use cached visualizations")
) -> Dict[str, Any]:
    """
    Generates comparative shot chart visualizations for multiple players.

    This endpoint provides various comparison visualization types:
    - Scatter: Side-by-side shot charts for each player
    - Heatmap: Side-by-side heatmaps for each player
    - Zones: Bar chart comparing shooting percentages by zone

    Query Parameters:
    - **player_names** (List[str], required): List of player names to compare (2-4 players).
    - **season** (str, optional): NBA season in format YYYY-YY (e.g., "2023-24"). Defaults to current season.
    - **season_type** (str, optional): Type of season. Options: "Regular Season", "Playoffs", "Pre Season", "All Star".
    - **chart_type** (str, optional): Type of chart to generate. Options: "scatter", "heatmap", "zones".
    - **output_format** (str, optional): Output format. Options: "base64", "file".
    - **use_cache** (bool, optional): Whether to use cached visualizations. Defaults to True.

    Returns:
    - **200 OK**: Comparison visualization data.
    - **400 Bad Request**: If invalid parameters are provided.
    - **404 Not Found**: If players are not found or no shot data exists.
    - **500 Internal Server Error**: For server-side errors.
    """
    # compare_shots, VisualizationCache, and settings are now imported at the top

    logger.info(f"Received GET /analyze/players/compare-shots request with player_names={player_names}, "
                f"season={season}, season_type={season_type}, chart_type={chart_type}, "
                f"output_format={output_format}, use_cache={use_cache}")

    try:
        # Validate parameters
        if len(player_names) < MIN_PLAYERS_FOR_COMPARISON: # Use constant
            raise HTTPException(status_code=400, detail=f"At least {MIN_PLAYERS_FOR_COMPARISON} players are required for comparison")

        if len(player_names) > MAX_PLAYERS_FOR_COMPARISON: # Use constant
            raise HTTPException(status_code=400, detail=f"Maximum of {MAX_PLAYERS_FOR_COMPARISON} players can be compared at once")

        # Use the current season if not specified
        effective_season = season if season else settings.CURRENT_NBA_SEASON # settings is imported at top

        # Check cache first if enabled
        if use_cache:
            cache_params = {
                "player_names": sorted(player_names),  # Sort for consistent caching
                "season": effective_season,
                "season_type": season_type,
                "chart_type": chart_type,
                "output_format": output_format
            }

            cached_result = VisualizationCache.get(cache_params)
            if cached_result:
                logger.info(f"Using cached comparison visualization for {player_names}, {effective_season}, {chart_type}")
                return cached_result

        # Process the comparison and create visualization
        result = await asyncio.to_thread(
            compare_shots,
            player_names=player_names,
            season=effective_season,
            season_type=season_type,
            chart_type=chart_type,
            output_format=output_format
        )

        if 'error' in result:
            error_detail = result['error']
            logger.error(f"Error generating comparison visualization: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "no shot data" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)

        # Cache the result if successful
        if use_cache:
            cache_params = {
                "player_names": sorted(player_names),  # Sort for consistent caching
                "season": effective_season,
                "season_type": season_type,
                "chart_type": chart_type,
                "output_format": output_format
            }
            VisualizationCache.set(cache_params, result)

        logger.info(f"Successfully generated {chart_type} comparison visualization for players: {player_names}")
        return result

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.critical(f"Unexpected error in /analyze/players/compare-shots endpoint: {str(e)}", exc_info=True)
        detail_msg = Errors.UNEXPECTED_ERROR.format(error=f"generating comparison visualization")
        raise HTTPException(status_code=500, detail=detail_msg)

===== backend\routes\fetch.py =====
"""
Provides a generic FastAPI route for fetching various types of NBA data.
This router acts as a dispatcher to specific data fetching logic functions
based on the 'target' specified in the request.
"""
from fastapi import APIRouter, HTTPException, Body, status
import json
import logging
import asyncio
from typing import Dict, Any

from schemas import FetchRequest
from backend.api_tools.player_common_info import fetch_player_info_logic
from backend.api_tools.player_gamelogs import fetch_player_gamelog_logic
from backend.api_tools.player_career_data import fetch_player_career_stats_logic
from backend.api_tools.team_info_roster import fetch_team_info_and_roster_logic
from backend.api_tools.game_finder import fetch_league_games_logic
from nba_api.stats.library.parameters import PerMode36, SeasonTypeAllStar
from backend.core.constants import SUPPORTED_FETCH_TARGETS
from backend.core.errors import Errors

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/fetch",
    tags=["Generic Fetch"]
)

async def _handle_fetch_logic_call(
    logic_function: callable,
    endpoint_name: str, # For logging/error context
    *args: Any, # Explicitly type *args
    **kwargs: Any # Explicitly type **kwargs
) -> Dict[str, Any]:
    """Helper to call underlying logic, parse JSON, and handle errors for /fetch routes."""
    try:
        # Filter out None kwargs so logic functions can use their defaults if they support it
        # However, for this generic fetch, params are explicitly passed based on target.
        
        result_json_string = await asyncio.to_thread(logic_function, *args, **kwargs)
        
        if not result_json_string: # Handle case where logic might return None or empty string
            logger.error(f"Logic function {logic_function.__name__} returned no data for {endpoint_name} with args {args}, kwargs {kwargs}.")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="No data returned from underlying service.")

        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for {endpoint_name} ({args}, {kwargs}): {error_detail}")
            
            status_code_to_raise = status.HTTP_500_INTERNAL_SERVER_ERROR # Default
            if "not found" in error_detail.lower():
                status_code_to_raise = status.HTTP_404_NOT_FOUND
            elif "invalid parameter" in error_detail.lower() or "missing" in error_detail.lower() or "invalid format" in error_detail.lower():
                status_code_to_raise = status.HTTP_400_BAD_REQUEST
            elif "nba api error" in error_detail.lower() or "external api" in error_detail.lower():
                status_code_to_raise = status.HTTP_502_BAD_GATEWAY
            
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data # Can be Dict or List (e.g. for gamelog)
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for {endpoint_name} ({args}, {kwargs}): {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for {endpoint_name} ({args}, {kwargs}): {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing fetch request for {endpoint_name}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.post(
    "/data", # Changed path from /fetch_data to /data under /fetch prefix
    summary="Generic Data Fetcher",
    description="A versatile endpoint to fetch various types of NBA data based on a specified `target` "
                "and target-specific `params`. Refer to the `FetchRequest` schema and the documentation "
                "for each target to understand required and optional parameters.",
    response_model=Dict[str, Any] # Response structure varies, Dict[str, Any] is a general placeholder
)
async def fetch_data_endpoint( # Renamed for clarity
    request: FetchRequest = Body(...)
) -> Dict[str, Any]:
    """
    Generic endpoint to fetch various NBA data.

    Request Body (`FetchRequest`):
    - **target** (str, required): Specifies the type of data to fetch. Supported values are defined in
      `SUPPORTED_FETCH_TARGETS` (e.g., "player_info", "player_gamelog", "team_info", 
      "player_career_stats", "find_games").
    - **params** (Dict[str, Any], required): A dictionary of parameters specific to the chosen `target`.

      **Parameter details by target:**
      - **target="player_info"**:
        - `params`: `{"player_name": str (required)}`
      - **target="player_gamelog"**:
        - `params`: `{"player_name": str (required), "season": str (required, YYYY-YY), "season_type"?: str (optional, defaults to Regular Season)}`
      - **target="team_info"**:
        - `params`: `{"team_identifier": str (required, name/abbr/ID), "season"?: str (optional, YYYY-YY, defaults to current)}`
      - **target="player_career_stats"**:
        - `params`: `{"player_name": str (required), "per_mode"?: str (optional, e.g., "PerGame", defaults to "PerGame")}`
          (Note: API uses `per_mode36` but common term is `per_mode`)
      - **target="find_games"**: (LeagueGameFinder)
        - `params`: `{
            "player_or_team_abbreviation": "P" | "T" (required),
            "player_id_nullable"?: int (optional, required if player_or_team_abbreviation='P'),
            "team_id_nullable"?: int (optional, required if player_or_team_abbreviation='T'),
            "season_nullable"?: str (optional, YYYY-YY),
            "season_type_nullable"?: str (optional),
            "league_id_nullable"?: str (optional, e.g., "00"),
            "date_from_nullable"?: str (optional, YYYY-MM-DD),
            "date_to_nullable"?: str (optional, YYYY-MM-DD)
          }`

    Successful Response (200 OK):
    Returns a dictionary containing the fetched data. The structure of this dictionary
    depends on the `target` and the data returned by the corresponding logic function.
    (e.g., for "player_info", it will be player details; for "player_gamelog", a list of games).

    Error Responses:
    - 400 Bad Request: If `target` is unsupported, or if required `params` for the target are missing/invalid.
    - 404 Not Found: If the requested resource (e.g., player, team) is not found by the logic layer.
    - 500 Internal Server Error: For unexpected errors or issues in the underlying service.
    - 502 Bad Gateway: If an external API call made by the logic layer fails.
    """
    target = request.target
    params = request.params if request.params is not None else {} # Ensure params is a dict
    logger.info(f"Received POST /fetch/data request for target: {target}, params: {params}")

    if target not in SUPPORTED_FETCH_TARGETS:
        logger.warning(f"Unsupported fetch target: {target}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail=Errors.UNSUPPORTED_FETCH_TARGET.format(target=target, supported_targets=", ".join(SUPPORTED_FETCH_TARGETS))
        )

    if target == "player_info":
        player_name = params.get("player_name")
        if not player_name:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'player_name' in params for target 'player_info'.")
        return await _handle_fetch_logic_call(fetch_player_info_logic, "player_info", player_name)

    elif target == "player_gamelog":
        player_name = params.get("player_name")
        season = params.get("season")
        if not player_name or not season:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'player_name' or 'season' in params for target 'player_gamelog'.")
        season_type = params.get("season_type", SeasonTypeAllStar.regular) # Default here or in logic
        return await _handle_fetch_logic_call(fetch_player_gamelog_logic, "player_gamelog", player_name, season, season_type=season_type)

    elif target == "team_info":
        team_identifier = params.get("team_identifier")
        if not team_identifier:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'team_identifier' in params for target 'team_info'.")
        season = params.get("season") # Optional, logic handles default
        return await _handle_fetch_logic_call(fetch_team_info_and_roster_logic, "team_info", team_identifier, season=season)

    elif target == "player_career_stats":
        player_name = params.get("player_name")
        if not player_name:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'player_name' in params for target 'player_career_stats'.")
        # API uses per_mode36, but common term is per_mode. Logic function expects per_mode_param.
        per_mode_param = params.get("per_mode", PerMode36.per_game) # Default here or in logic
        return await _handle_fetch_logic_call(fetch_player_career_stats_logic, "player_career_stats", player_name, per_mode_param=per_mode_param)

    elif target == "find_games":
        # Extract all possible params for fetch_league_games_logic
        required_player_or_team = params.get("player_or_team_abbreviation")
        if not required_player_or_team or required_player_or_team not in ['P', 'T']:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing or invalid 'player_or_team_abbreviation' (must be 'P' or 'T') in params for target 'find_games'.")
        
        find_games_kwargs = {
            "player_or_team_abbreviation": required_player_or_team,
            "player_id_nullable": params.get("player_id_nullable"),
            "team_id_nullable": params.get("team_id_nullable"),
            "season_nullable": params.get("season_nullable"),
            "season_type_nullable": params.get("season_type_nullable"),
            "league_id_nullable": params.get("league_id_nullable"),
            "date_from_nullable": params.get("date_from_nullable"),
            "date_to_nullable": params.get("date_to_nullable")
        }
        if required_player_or_team == 'P' and find_games_kwargs["player_id_nullable"] is None:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'player_id_nullable' in params when player_or_team_abbreviation is 'P'.")
        if required_player_or_team == 'T' and find_games_kwargs["team_id_nullable"] is None:
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Missing 'team_id_nullable' in params when player_or_team_abbreviation is 'T'.")
            
        return await _handle_fetch_logic_call(fetch_league_games_logic, "find_games", **find_games_kwargs)
    
    else:
        # Should be caught by SUPPORTED_FETCH_TARGETS check, but as a safeguard
        logger.error(f"Logic error: Reached else block for fetch target: {target}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Internal server error: Invalid fetch target processing.")

===== backend\routes\game.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, Path # Added Path
from typing import Dict, Any, Optional
import json

# Updated imports for modularized game logic
from backend.api_tools.game_boxscores import (
    fetch_boxscore_traditional_logic,
    fetch_boxscore_advanced_logic,
    fetch_boxscore_four_factors_logic,
    fetch_boxscore_usage_logic,
    fetch_boxscore_defensive_logic
)
from backend.api_tools.game_playbyplay import fetch_playbyplay_logic
from backend.api_tools.game_visuals_analytics import (
    fetch_shotchart_logic,
    fetch_win_probability_logic
)
# fetch_league_games_logic is now in game_finder.py and likely used by a different route or tool.

import backend.api_tools.utils as api_utils
from backend.core.errors import Errors

router = APIRouter(
    prefix="/game", # Add prefix for all routes in this file
    tags=["Games"]  # Tag for OpenAPI documentation
)
logger = logging.getLogger(__name__)

async def _handle_logic_call(logic_function: callable, *args, **kwargs) -> Dict[str, Any]:
    """Helper to call logic function, parse JSON, and handle errors."""
    try:
        result_json_string = await asyncio.to_thread(logic_function, *args, **kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid id" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from {logic_function.__name__} for args {args}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        log_args = args[:2]
        logger.critical(f"Unexpected error in API route calling {func_name} with args {log_args}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing game data via {func_name}")
        raise HTTPException(status_code=500, detail=detail_msg)


@router.get(
    "/boxscore/traditional/{game_id}",
    summary="Get Traditional Box Score for a Game",
    description="Fetches the traditional box score (points, rebounds, assists, etc.) for a specified NBA game ID. "
                "Data includes player stats, team stats, and starter/bench splits.",
    response_model=Dict[str, Any]
)
async def get_game_boxscore_traditional(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game (e.g., '0022300161').", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """
    Endpoint to get the traditional box score for a game.
    Uses `fetch_boxscore_traditional_logic`.

    Path Parameters:
    - **game_id** (str, required): The 10-digit ID of the game.

    Successful Response (200 OK):
    Returns a dictionary containing traditional box score data.
    Refer to `fetch_boxscore_traditional_logic` in `game_tools.py` for detailed structure.
    Includes `teams`, `players`, and `starters_bench` statistics.

    Error Responses:
    - 400 Bad Request: If `game_id` is invalid.
    - 404 Not Found: If game data is not found.
    - 500 Internal Server Error: For other processing issues.
    """
    logger.info(f"Received GET /game/boxscore/traditional/{game_id} request.")
    return await _handle_logic_call(fetch_boxscore_traditional_logic, game_id)

@router.get(
    "/playbyplay/{game_id}",
    summary="Get Play-by-Play Data for a Game",
    description="Fetches play-by-play data for a specified NBA game ID. "
                "Attempts to retrieve live data first; if unavailable, falls back to historical data. "
                "Period filtering applies only to historical data.",
    response_model=Dict[str, Any]
)
async def get_game_playbyplay(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$"),
    start_period: int = Query(0, description="Filter Play-by-Play starting from this period (1-4, 5+ for OT, 0 for all). Only applies to historical data fallback.", ge=0),
    end_period: int = Query(0, description="Filter Play-by-Play ending at this period (1-4, 5+ for OT, 0 for all). Only applies to historical data fallback.", ge=0)
) -> Dict[str, Any]:
    """
    Endpoint to get Play-by-Play data for a game.
    Uses `fetch_playbyplay_logic`.

    Path Parameters:
    - **game_id** (str, required): The 10-digit ID of the game.

    Query Parameters:
    - **start_period** (int, optional): Starting period for filtering (0 for all). Default: 0.
    - **end_period** (int, optional): Ending period for filtering (0 for all). Default: 0.

    Successful Response (200 OK):
    Returns a dictionary containing play-by-play data.
    Refer to `fetch_playbyplay_logic` in `game_tools.py` for detailed structure.
    Includes `source` (live/historical) and `periods` with plays.

    Error Responses:
    - 400 Bad Request: If `game_id` is invalid.
    - 404 Not Found: If game data is not found.
    - 500 Internal Server Error: For other processing issues.
    """
    logger.info(f"Received GET /game/playbyplay/{game_id} request with start_period={start_period}, end_period={end_period}.")
    return await _handle_logic_call(fetch_playbyplay_logic, game_id, start_period, end_period)

@router.get(
    "/shotchart/{game_id}",
    summary="Get Shot Chart Data for a Game",
    description="Fetches shot chart data for all players in a specified NBA game ID, "
                "including shot locations, outcomes, and league averages for zones.",
    response_model=Dict[str, Any]
)
async def get_game_shotchart(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """
    Endpoint to get shot chart data for a game.
    Uses `fetch_shotchart_logic`.

    Path Parameters:
    - **game_id** (str, required): The 10-digit ID of the game.

    Successful Response (200 OK):
    Returns a dictionary containing shot chart data.
    Refer to `fetch_shotchart_logic` in `game_tools.py` for detailed structure.
    Includes `teams` with player shots and `league_averages`.

    Error Responses:
    - 400 Bad Request: If `game_id` is invalid.
    - 404 Not Found: If game data is not found.
    - 500 Internal Server Error: For other processing issues.
    """
    logger.info(f"Received GET /game/shotchart/{game_id} request.")
    return await _handle_logic_call(fetch_shotchart_logic, game_id)

@router.get(
    "/boxscore/advanced/{game_id}",
    summary="Get Advanced Box Score for a Game",
    description="Fetches advanced box score statistics (e.g., Offensive/Defensive Rating, Pace, PIE) for a specified NBA game ID.",
    response_model=Dict[str, Any]
)
async def get_advanced_boxscore(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """Endpoint to get advanced box score data. Uses `fetch_boxscore_advanced_logic`."""
    logger.info(f"Received GET /game/boxscore/advanced/{game_id}")
    return await _handle_logic_call(fetch_boxscore_advanced_logic, game_id)

@router.get(
    "/boxscore/fourfactors/{game_id}",
    summary="Get Four Factors Box Score for a Game",
    description="Fetches Four Factors box score statistics (Effective FG%, Turnover Rate, Rebound Rate, Free Throw Rate) for a specified NBA game ID.",
    response_model=Dict[str, Any]
)
async def get_four_factors_boxscore(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """Endpoint to get Four Factors box score data. Uses `fetch_boxscore_four_factors_logic`."""
    logger.info(f"Received GET /game/boxscore/fourfactors/{game_id}")
    return await _handle_logic_call(fetch_boxscore_four_factors_logic, game_id)

@router.get(
    "/boxscore/usage/{game_id}",
    summary="Get Usage Statistics Box Score for a Game",
    description="Fetches usage statistics (e.g., USG%, %FGA, %AST) for all players in a specified NBA game ID.",
    response_model=Dict[str, Any]
)
async def get_usage_boxscore(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """Endpoint to get usage box score data. Uses `fetch_boxscore_usage_logic`."""
    logger.info(f"Received GET /game/boxscore/usage/{game_id}")
    return await _handle_logic_call(fetch_boxscore_usage_logic, game_id)

@router.get(
    "/boxscore/defensive/{game_id}",
    summary="Get Defensive Box Score for a Game",
    description="Fetches defensive statistics (e.g., opponent shooting percentages when player is defending) for a specified NBA game ID.",
    response_model=Dict[str, Any]
)
async def get_defensive_boxscore(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """Endpoint to get defensive box score data. Uses `fetch_boxscore_defensive_logic`."""
    logger.info(f"Received GET /game/boxscore/defensive/{game_id}")
    return await _handle_logic_call(fetch_boxscore_defensive_logic, game_id)

@router.get(
    "/winprobability/{game_id}",
    summary="Get Win Probability Data for a Game",
    description="Fetches win probability data points throughout a specified NBA game ID, showing how win likelihood changed with each play.",
    response_model=Dict[str, Any]
)
async def get_win_probability(
    game_id: str = Path(..., description="The 10-digit ID of the NBA game.", regex=r"^\d{10}$")
) -> Dict[str, Any]:
    """Endpoint to get win probability data. Uses `fetch_win_probability_logic`."""
    logger.info(f"Received GET /game/winprobability/{game_id}")
    # fetch_win_probability_logic takes an optional run_type, defaulting to "0".
    return await _handle_logic_call(fetch_win_probability_logic, game_id)

===== backend\routes\knowledge_routes.py =====
from fastapi import APIRouter, UploadFile, File, HTTPException, Form, Body
from fastapi.concurrency import run_in_threadpool # Import run_in_threadpool
from pydantic import BaseModel, HttpUrl, Field
import shutil
from pathlib import Path
import logging
from typing import List, Dict, Any, Optional
import uuid # For creating unique temp dirs for text uploads
import os

from backend.core.knowledge_base import NBAnalyzerKnowledgeBase, KnowledgeSourceAdditionError
from config import settings
from backend.core.models import QueryRequest, QueryResponse, AddKnowledgeSourceRequest, GenericResponse, AddWikipediaRequest, AddTextFileRequest, AddUrlRequest, AddYouTubeVideosRequest

router = APIRouter()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Define a temporary directory for uploads within the backend structure
TEMP_UPLOAD_DIR = Path(settings.CHROMA_DB_NBA_AGENT) / "temp_uploads_kb"
TEMP_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
logger.info(f"Temporary upload directory ensured at: {TEMP_UPLOAD_DIR}")


# Initialize the global knowledge base instance
# GLOBAL_KB_RECREATE_COLLECTIONS = os.getenv("GLOBAL_KB_RECREATE_COLLECTIONS", "False").lower() == "true"
GLOBAL_KB_RECREATE_COLLECTIONS = False # Set to False for standard operation
logger.info(f"Initializing Global NBAnalyzerKnowledgeBase with recreate_collections={GLOBAL_KB_RECREATE_COLLECTIONS}")

global_knowledge_base_instance: Optional[NBAnalyzerKnowledgeBase] = None
try:
    global_knowledge_base_instance = NBAnalyzerKnowledgeBase(
        recreate_collections=GLOBAL_KB_RECREATE_COLLECTIONS
    )
    logger.info(f"Global KnowledgeBase initialized. recreate_collections={GLOBAL_KB_RECREATE_COLLECTIONS}")
except Exception as e:
    logger.error(f"Failed to initialize Global KnowledgeBase: {e}", exc_info=True)
    # global_knowledge_base_instance remains None, endpoints should check

@router.post("/add_pdf_file/", summary="Upload a PDF file to the Knowledge Base")
async def add_pdf_file(file: UploadFile = File(...)):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")

    if not file.filename:
        raise HTTPException(status_code=400, detail="No file name provided.")

    if not file.filename.endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Invalid file type. Only PDF files are accepted.")

    unique_dir_name = str(uuid.uuid4())
    temp_pdf_upload_dir = TEMP_UPLOAD_DIR / "pdfs" / unique_dir_name
    temp_pdf_upload_dir.mkdir(parents=True, exist_ok=True)
    
    safe_filename = Path(file.filename).name
    temp_file_path = temp_pdf_upload_dir / safe_filename

    try:
        logger.info(f"Receiving PDF file: {safe_filename} into {temp_pdf_upload_dir}")
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logger.info(f"PDF file {safe_filename} saved to {temp_file_path}")
        
        await run_in_threadpool(global_knowledge_base_instance.add_local_pdf_directory, str(temp_pdf_upload_dir))
        logger.info(f"PDF processing for {safe_filename} initiated.")
        
        return {"message": f"PDF file '{safe_filename}' received and processing initiated."}
    except KnowledgeSourceAdditionError as e:
        logger.error(f"KnowledgeSourceAdditionError preparing PDF file {safe_filename} for processing: {e}", exc_info=True)
        if temp_pdf_upload_dir.exists():
            shutil.rmtree(temp_pdf_upload_dir)
            logger.info(f"Cleaned up temporary PDF directory due to processing error: {temp_pdf_upload_dir}")
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing PDF file {safe_filename}: {e}", exc_info=True)
        if temp_file_path.exists():
            temp_file_path.unlink(missing_ok=True)
        if temp_pdf_upload_dir.exists() and not any(temp_pdf_upload_dir.iterdir()):
            shutil.rmtree(temp_pdf_upload_dir)
        raise HTTPException(status_code=500, detail=f"Error processing PDF file: {str(e)}")
    finally:
        if hasattr(file, 'file') and hasattr(file.file, 'close'): 
            file.file.close()

@router.post("/add_csv_file/", summary="Upload a CSV file to the Knowledge Base")
async def add_csv_file(file: UploadFile = File(...)):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")

    if not file.filename:
        raise HTTPException(status_code=400, detail="No file name provided.")

    if not file.filename.endswith(".csv"):
        raise HTTPException(status_code=400, detail="Invalid file type. Only CSV files are accepted.")

    unique_dir_name = str(uuid.uuid4())
    temp_csv_upload_dir = TEMP_UPLOAD_DIR / "csvs" / unique_dir_name
    temp_csv_upload_dir.mkdir(parents=True, exist_ok=True)

    safe_filename = Path(file.filename).name
    temp_file_path = temp_csv_upload_dir / safe_filename

    try:
        logger.info(f"Receiving CSV file: {safe_filename} into {temp_csv_upload_dir}")
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logger.info(f"CSV file {safe_filename} saved to {temp_file_path}")

        await run_in_threadpool(global_knowledge_base_instance.add_local_csv_directory, str(temp_csv_upload_dir))
        logger.info(f"CSV processing for {safe_filename} initiated.")
        
        return {"message": f"CSV file '{safe_filename}' received and processing initiated."}
    except KnowledgeSourceAdditionError as e:
        logger.error(f"KnowledgeSourceAdditionError preparing CSV file {safe_filename} for processing: {e}", exc_info=True)
        if temp_csv_upload_dir.exists(): 
            shutil.rmtree(temp_csv_upload_dir)
            logger.info(f"Cleaned up temporary CSV directory due to processing error: {temp_csv_upload_dir}")
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing CSV file {safe_filename}: {e}", exc_info=True)
        if temp_file_path.exists(): temp_file_path.unlink(missing_ok=True)
        if temp_csv_upload_dir.exists() and not any(temp_csv_upload_dir.iterdir()): shutil.rmtree(temp_csv_upload_dir)
        raise HTTPException(status_code=500, detail=f"Error processing CSV file: {str(e)}")
    finally:
        if hasattr(file, 'file') and hasattr(file.file, 'close'): file.file.close()

@router.post("/add_txt_file/", summary="Upload a TXT file to the Knowledge Base")
async def add_txt_file(file: UploadFile = File(...)):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")

    if not file.filename:
        raise HTTPException(status_code=400, detail="No file name provided.")

    if not file.filename.endswith(".txt"):
        raise HTTPException(status_code=400, detail="Invalid file type. Only TXT files are accepted.")

    unique_dir_name = str(uuid.uuid4())
    temp_text_upload_dir = TEMP_UPLOAD_DIR / "texts" / unique_dir_name
    temp_text_upload_dir.mkdir(parents=True, exist_ok=True)
    
    safe_filename = Path(file.filename).name
    temp_file_path = temp_text_upload_dir / safe_filename

    try:
        logger.info(f"Receiving TXT file: {safe_filename} into {temp_text_upload_dir}")
        with temp_file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        logger.info(f"TXT file {safe_filename} saved to {temp_file_path}")
        
        await run_in_threadpool(global_knowledge_base_instance.add_local_txt_directory, str(temp_text_upload_dir))
        logger.info(f"TXT file {safe_filename} processing initiated.")
        
        return {"message": f"TXT file '{safe_filename}' received and processing initiated."}
    except KnowledgeSourceAdditionError as e:
        logger.error(f"KnowledgeSourceAdditionError preparing TXT file {safe_filename} for processing: {e}", exc_info=True)
        if temp_text_upload_dir.exists(): 
            shutil.rmtree(temp_text_upload_dir)
            logger.info(f"Cleaned up temporary TXT directory due to processing error: {temp_text_upload_dir}")
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing TXT file {safe_filename}: {e}", exc_info=True)
        if temp_file_path.exists(): temp_file_path.unlink(missing_ok=True)
        if temp_text_upload_dir.exists() and not any(temp_text_upload_dir.iterdir()): shutil.rmtree(temp_text_upload_dir)
        raise HTTPException(status_code=500, detail=f"Error processing TXT file: {str(e)}")
    finally:
        if hasattr(file, 'file') and hasattr(file.file, 'close'): file.file.close()

@router.post("/add_website_url/", response_model=GenericResponse, summary="Add website URLs to the Knowledge Base")
async def add_website_url_route(request: AddUrlRequest):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    try:
        str_urls = [str(url) for url in request.urls]
        logger.info(f"Received request to add website URLs: {str_urls}")
        
        await run_in_threadpool(global_knowledge_base_instance.add_website_urls, str_urls, collection_suffix="uploaded_website_batch")
        logger.info(f"Website URLs {str_urls} submitted for processing in background.")
        return GenericResponse(status="success", message=f"Website URLs {str_urls} submitted for background processing.")
    except KnowledgeSourceAdditionError as e:
        logger.error(f"KnowledgeSourceAdditionError preparing website URLs {request.urls} for processing: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing website URLs {request.urls}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing website URLs: {str(e)}")

@router.post("/add_wikipedia_topics/", response_model=GenericResponse, summary="Add Wikipedia topics to the Knowledge Base")
async def add_wikipedia_topics_route(request: AddWikipediaRequest):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    try:
        logger.info(f"Received request to add Wikipedia topics: {request.topics}")
        await run_in_threadpool(global_knowledge_base_instance.add_wikipedia_topics, request.topics, collection_suffix="uploaded_wiki_topics")
        logger.info(f"Wikipedia topics {request.topics} submitted for background processing.")
        return GenericResponse(status="success", message=f"Wikipedia topics {request.topics} submitted for background processing.")
    except KnowledgeSourceAdditionError as e:
        logger.error(f"KnowledgeSourceAdditionError preparing Wikipedia topics {request.topics} for processing: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing Wikipedia topics {request.topics}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing Wikipedia topics: {str(e)}")

@router.get("/status/", summary="Get the status of the Knowledge Base")
async def get_kb_status():
    if not global_knowledge_base_instance:
        return {"status": "KnowledgeBase not initialized", "sources_count": 0, "main_collection_prefix": "N/A"}
    
    # The get_underlying_kb_object might not be needed if NBAnalyzerKnowledgeBase directly exposes source counts
    # combined_kb = global_knowledge_base_instance.get_underlying_kb_object() 
    # source_count = 0
    # if combined_kb and hasattr(combined_kb, 'sources'):
    #     source_count = len(combined_kb.sources)
        
    return {
        "status": "KnowledgeBase Initialized",
        "main_collection_prefix": global_knowledge_base_instance.main_collection_prefix,
        "persist_path": str(global_knowledge_base_instance.vector_db_path), # Assuming vector_db_path exists
        # "combined_sources_count": source_count, # Re-evaluate how to get total sources
        "pdf_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("pdf")),
        "csv_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("csv")),
        "website_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("website")),
        "text_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("text")),
        "youtube_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("youtube")),
        "wikipedia_source_collections_count": len(global_knowledge_base_instance.get_collections_by_type("wikipedia")),
        "all_managed_collections": global_knowledge_base_instance.list_all_managed_collections()
    }

@router.post("/query/", response_model=QueryResponse, summary="Query the Knowledge Base")
async def query_kb(request: QueryRequest):
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    try:
        logger.info(f"Received query: {request.query} with num_documents: {request.num_documents}")
        results = await run_in_threadpool(global_knowledge_base_instance.query, request.query, n_results=request.num_documents)
        
        processed_results = []
        if results: 
            for doc_tuple in results: 
                processed_results.append({
                    "content": doc_tuple.get("content", ""), 
                    "metadata": doc_tuple.get("metadata", {}),
                    "score": doc_tuple.get("score") 
                })

        return QueryResponse(documents=processed_results)

    except Exception as e:
        logger.error(f"Error querying KnowledgeBase: {e}", exc_info=True)
        # Construct a more informative error detail
        error_detail = f"Error querying KnowledgeBase: {type(e).__name__} - {str(e)}"
        # Check if it's a TypeError from the query method itself
        if isinstance(e, TypeError) and "query() got an unexpected keyword argument" in str(e):
            error_detail = f"Error processing query: Mismatch in query parameters. {str(e)}"
        elif isinstance(e, TypeError):
             error_detail = f"Error processing query: Type error in query processing. {str(e)}"

        raise HTTPException(status_code=500, detail=error_detail)


@router.post("/initialize_default_pdfs/", summary="Initialize KB with PDFs from a default directory")
async def initialize_default_pdfs():
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    
    default_pdf_path_str = settings.DEFAULT_PDFS_PATH
    if not default_pdf_path_str:
        logger.warning("DEFAULT_PDFS_PATH not set in settings. Skipping default PDF initialization.")
        return {"message": "DEFAULT_PDFS_PATH not set. No default PDFs initialized."}

    default_pdf_path = Path(default_pdf_path_str)
    
    if not default_pdf_path.exists() or not default_pdf_path.is_dir():
        logger.warning(f"Default PDF path {default_pdf_path} does not exist or is not a directory.")
        raise HTTPException(status_code=404, detail=f"Default PDF directory not found at {default_pdf_path}")

    try:
        logger.info(f"Initializing default PDFs from: {default_pdf_path}")
        # Assuming add_local_pdf_directory is thread-safe or called appropriately
        await run_in_threadpool(global_knowledge_base_instance.add_local_pdf_directory, str(default_pdf_path))
        logger.info(f"Default PDF initialization initiated from {default_pdf_path}.")
        return {"message": f"Default PDF initialization from '{default_pdf_path}' initiated."}
    except KnowledgeSourceAdditionError as e: # Catch specific KB error
        logger.error(f"Failed to load PDF source from {default_pdf_path}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f"Error during default PDF initialization from {default_pdf_path}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error initializing default PDFs: {str(e)}")

@router.post("/add_youtube_videos/", response_model=GenericResponse, summary="Add YouTube video URLs to the Knowledge Base")
async def add_youtube_videos_route(request: AddYouTubeVideosRequest): # Model expects `urls`
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    try:
        # Ensure access is request.urls as per AddYouTubeVideosRequest model
        str_video_urls = [str(url) for url in request.urls] 
        logger.info(f"Received request to add YouTube videos: {str_video_urls}")
        
        await run_in_threadpool(global_knowledge_base_instance.add_website_urls, str_video_urls, collection_suffix="uploaded_youtube_batch")
        logger.info(f"YouTube videos {str_video_urls} submitted for processing in background.")
        return GenericResponse(status="success", message=f"YouTube videos {str_video_urls} submitted for background processing.")
    except KnowledgeSourceAdditionError as e:
        # Ensure logging uses request.urls
        logger.error(f"KnowledgeSourceAdditionError preparing YouTube videos {request.urls} for processing: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        # Ensure logging uses request.urls
        logger.error(f"Error processing YouTube videos {request.urls}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error processing YouTube videos: {str(e)}")

@router.get("/list_collections/", summary="List all active KB collections")
async def list_collections() -> List[str]: 
    if not global_knowledge_base_instance:
        raise HTTPException(status_code=500, detail="KnowledgeBase not initialized.")
    try:
        collection_names = await run_in_threadpool(global_knowledge_base_instance.list_all_managed_collections)
        return collection_names
    except Exception as e:
        logger.error(f"Error listing collections: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error listing collections: {str(e)}")

# To make these routes available, they need to be included in the main FastAPI app (e.g., in backend/main.py) 

===== backend\routes\leaders.py =====
import logging
import json
import asyncio
from typing import Optional, Dict, Any
from fastapi import APIRouter, Query, HTTPException, status

from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, 
    PerMode48, 
    LeagueID, 
    Scope,
    StatCategoryAbbreviation
)
from backend.api_tools.league_leaders_data import fetch_league_leaders_logic
from backend.utils.validation import validate_season_format
from backend.core.errors import Errors
from config import settings

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/league",  # Standard prefix for league-related endpoints
    tags=["League"]    # Consistent tag
)

async def _handle_league_route_logic_call(
    logic_function: callable, 
    endpoint_name: str,
    *args, 
    **kwargs
) -> Dict[str, Any]:
    """Helper to call league-related logic, parse JSON, and handle errors."""
    try:
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        result_json_string = await asyncio.to_thread(logic_function, *args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} with args {args}, kwargs {filtered_kwargs}: {error_detail}")
            status_code_to_raise = status.HTTP_404_NOT_FOUND if "not found" in error_detail.lower() else \
                                   status.HTTP_400_BAD_REQUEST if "invalid" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR # Default to 500, can be 502 if upstream API error
            if "nba api error" in error_detail.lower() or "external api" in error_detail.lower(): # Check for upstream API issues
                status_code_to_raise = status.HTTP_502_BAD_GATEWAY
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}, kwargs {filtered_kwargs}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for args {args}, kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.get(
    "/leaders", 
    summary="Get League Leaders by Statistical Category",
    description="Fetches a list of top NBA players based on a specified statistical category, season, and other criteria. "
                "Data is sourced from the NBA's official stats API.",
    response_model=Dict[str, Any]
)
async def get_league_leaders_endpoint(
    stat_category: str = Query(StatCategoryAbbreviation.pts, description="The statistical category to rank leaders by (e.g., 'PTS', 'REB', 'AST'). See `nba_api.stats.library.parameters.StatCategoryAbbreviation` for all valid options."),
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic if not provided.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: str = Query(SeasonTypeAllStar.regular, description="Type of season (e.g., 'Regular Season', 'Playoffs'). See `nba_api.stats.library.parameters.SeasonTypeAllStar`."),
    per_mode: str = Query(PerMode48.per_game, description="Statistical mode (e.g., 'PerGame', 'Totals', 'Per36'). See `nba_api.stats.library.parameters.PerMode48`."),
    league_id: str = Query(LeagueID.nba, description="League ID. See `nba_api.stats.library.parameters.LeagueID` (e.g., '00' for NBA)."),
    scope: str = Query(Scope.s, description="Scope of the statistics (e.g., 'S' for Season, 'RS' for Regular Season, 'PO' for Playoffs). See `nba_api.stats.library.parameters.Scope`."),
    top_n: Optional[int] = Query(10, description="Number of top leaders to return. Logic layer defaults if not provided or invalid.", ge=1, le=200) # Added ge/le
) -> Dict[str, Any]:
    """
    Endpoint to retrieve top league leaders for a given statistical category.
    Uses `fetch_league_leaders_logic` from `league_tools.py`.

    Query Parameters:
    - **stat_category** (str, required): e.g., "PTS", "REB", "AST". Default: "PTS".
    - **season** (str, optional): YYYY-YY format. Defaults to current season in logic.
    - **season_type** (str, optional): e.g., "Regular Season". Default: "Regular Season".
    - **per_mode** (str, optional): e.g., "PerGame". Default: "PerGame".
    - **league_id** (str, optional): e.g., "00" for NBA. Default: "00".
    - **scope** (str, optional): e.g., "S" for Season. Default: "S".
    - **top_n** (int, optional): Number of leaders. Default: 10. Min:1, Max:200 (API might have its own limits).

    Successful Response (200 OK):
    Returns a dictionary containing parameters and a list of league leaders.
    Example:
    ```json
    {
        "parameters": {
            "stat_category": "PTS",
            "season": "2023-24",
            // ... other params
        },
        "leaders": [
            {
                "PLAYER_ID": 201939,
                "RANK": 1,
                "PLAYER": "Stephen Curry",
                "TEAM_ID": 1610612744,
                "TEAM": "GSW",
                "GP": 70,
                "MIN": 35.0,
                "PTS": 30.1, // Actual stat category value
                // ... other common stats like FGM, FGA, FG_PCT etc.
            },
            // ... more players
        ]
    }
    ```
    If no leaders are found, `leaders` will be an empty list.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid (e.g., bad season format, invalid stat category).
    - 500 Internal Server Error / 502 Bad Gateway: For unexpected errors or issues fetching/processing data.
    """
    logger.info(f"Received GET /league/leaders request. Category: {stat_category}, Season: {season}, Type: {season_type}, PerMode: {per_mode}, League: {league_id}, Scope: {scope}, TopN: {top_n}")

    season_to_use = season or settings.CURRENT_NBA_SEASON # Changed
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    # Validate enum-like string parameters
    VALID_STAT_CATEGORIES = {getattr(StatCategoryAbbreviation, attr) for attr in dir(StatCategoryAbbreviation) if not attr.startswith('_')}
    if stat_category not in VALID_STAT_CATEGORIES:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_STAT_CATEGORY.format(value=stat_category, options=", ".join(list(VALID_STAT_CATEGORIES)[:5]))) # Show some options
    
    # Similar validation for season_type, per_mode, league_id, scope can be added here
    # For brevity, assuming logic layer or FastAPI's default Query validation (if enums were used) handles them.
    # Example for season_type:
    VALID_SEASON_TYPES = {getattr(SeasonTypeAllStar, attr) for attr in dir(SeasonTypeAllStar) if not attr.startswith('_')}
    if season_type not in VALID_SEASON_TYPES:
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_TYPE.format(value=season_type, options=", ".join(list(VALID_SEASON_TYPES)[:5])))


    leaders_kwargs = {
        "season": season_to_use,
        "stat_category": stat_category,
        "season_type": season_type,
        "per_mode": per_mode,
        "league_id": league_id,
        "scope": scope,
        "top_n": top_n if top_n is not None else 10 # Logic layer handles default if None
    }

    return await _handle_league_route_logic_call(
        fetch_league_leaders_logic, "league leaders",
        **leaders_kwargs
    )


===== backend\routes\league_stats.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, status
from typing import Dict, Any, Optional
import json

from nba_api.stats.library.parameters import (
    SeasonTypeAllStar,
    PerMode48,
    LeagueID,
    MeasureTypeDetailedDefense,
    Conference,
    Division
)

from api_tools.league_dash_team_stats import fetch_league_team_stats_logic
from api_tools.league_dash_player_stats import fetch_league_player_stats_logic
from api_tools.player_estimated_metrics import fetch_player_estimated_metrics_logic
from api_tools.team_estimated_metrics import fetch_team_estimated_metrics_logic
from api_tools.comprehensive_analytics import get_comprehensive_league_data, get_player_advanced_analytics
from core.errors import Errors
from utils.validation import validate_season_format
from config import settings

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/league",
    tags=["League Statistics"]
)

async def _handle_league_stats_logic_call(
    logic_function: callable,
    endpoint_name: str,
    *args,
    **kwargs
) -> Dict[str, Any]:
    """Helper to call league-related logic, parse JSON, and handle errors."""
    try:
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        result_json_string = await asyncio.to_thread(logic_function, *args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} with args {args}, kwargs {filtered_kwargs}: {error_detail}")
            status_code_to_raise = status.HTTP_404_NOT_FOUND if "not found" in error_detail.lower() else \
                                   status.HTTP_400_BAD_REQUEST if "invalid" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR
            if "nba api error" in error_detail.lower() or "external api" in error_detail.lower():
                status_code_to_raise = status.HTTP_502_BAD_GATEWAY
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}, kwargs {filtered_kwargs}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for {endpoint_name}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.get(
    "/team-stats",
    summary="Get League-Wide Team Statistics",
    description="Fetches comprehensive league-wide team statistics for a specified season and criteria.",
    response_model=Dict[str, Any]
)
async def get_league_team_stats_endpoint(
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query(SeasonTypeAllStar.regular, description="Type of season (e.g., 'Regular Season', 'Playoffs')."),
    per_mode: Optional[str] = Query(PerMode48.per_game, description="Statistical mode (e.g., 'PerGame', 'Totals', 'MinutesPer')."),
    measure_type: Optional[str] = Query(MeasureTypeDetailedDefense.base, description="Category of stats (e.g., 'Base', 'Advanced', 'Scoring')."),
    league_id: Optional[str] = Query(LeagueID.nba, description="League ID (e.g., '00' for NBA)."),
    conference: Optional[str] = Query(None, description=f"Conference filter (e.g., '{Conference.east}', '{Conference.west}')."),
    division: Optional[str] = Query(None, description=f"Division filter (e.g., '{Division.east}', '{Division.west}')."),
    team_id: Optional[int] = Query(None, description="Filter by specific team ID."),
    date_from: Optional[str] = Query(None, description="Start date (MM/DD/YYYY) for filtering games.", regex=r"^\d{2}/\d{2}/\d{4}$"),
    date_to: Optional[str] = Query(None, description="End date (MM/DD/YYYY) for filtering games.", regex=r"^\d{2}/\d{2}/\d{4}$"),
    location: Optional[str] = Query(None, description="Filter by game location (e.g., 'Home', 'Road')."),
    outcome: Optional[str] = Query(None, description="Filter by game outcome (e.g., 'W', 'L')."),
    season_segment: Optional[str] = Query(None, description="Filter by season segment (e.g., 'Post All-Star', 'Pre All-Star')."),
    vs_conference: Optional[str] = Query(None, description=f"Filter by opponent conference (e.g., '{Conference.east}', '{Conference.west}')."),
    vs_division: Optional[str] = Query(None, description=f"Filter by opponent division (e.g., '{Division.east}', '{Division.west}').")
) -> Dict[str, Any]:
    """
    Endpoint to retrieve league-wide team statistics.
    Uses `fetch_league_team_stats_logic` from `league_dash_team_stats.py`.

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.
    - **season_type** (str, optional): e.g., "Regular Season". Default: "Regular Season".
    - **per_mode** (str, optional): e.g., "PerGame". Default: "PerGame".
    - **measure_type** (str, optional): e.g., "Base". Default: "Base".
    - **league_id** (str, optional): e.g., "00" for NBA. Default: "00".
    - **conference** (str, optional): e.g., "East".
    - **division** (str, optional): e.g., "Atlantic".
    - **team_id** (int, optional): Specific team ID.
    - **date_from** (str, optional): MM/DD/YYYY.
    - **date_to** (str, optional): MM/DD/YYYY.
    - **location** (str, optional): "Home" or "Road".
    - **outcome** (str, optional): "W" or "L".
    - **season_segment** (str, optional): "Post All-Star" or "Pre All-Star".
    - **vs_conference** (str, optional): Opponent conference.
    - **vs_division** (str, optional): Opponent division.

    Successful Response (200 OK):
    Returns a dictionary containing parameters and league-wide team statistics.
    Refer to `fetch_league_team_stats_logic` docstring in `league_dash_team_stats.py` for detailed structure.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid.
    - 500 Internal Server Error / 502 Bad Gateway: For unexpected errors or issues fetching/processing data.
    """
    logger.info(f"Received GET /league/team-stats request. Season: {season}, Type: {season_type}, PerMode: {per_mode}, Measure: {measure_type}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    stats_kwargs = {
        "season": season_to_use,
        "season_type": season_type,
        "per_mode": per_mode,
        "measure_type": measure_type,
        "league_id": league_id,
        "conference": conference,
        "division": division,
        "team_id": team_id,
        "date_from": date_from,
        "date_to": date_to,
        "location": location,
        "outcome": outcome,
        "season_segment": season_segment,
        "vs_conference": vs_conference,
        "vs_division": vs_division
    }

    return await _handle_league_stats_logic_call(
        fetch_league_team_stats_logic, "league team stats",
        **stats_kwargs
    )

@router.get(
    "/player-stats",
    summary="Get League-Wide Player Statistics",
    description="Fetches comprehensive league-wide player statistics for a specified season and criteria.",
    response_model=Dict[str, Any]
)
async def get_league_player_stats_endpoint(
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query("Regular Season", description="Type of season (e.g., 'Regular Season', 'Playoffs')."),
    per_mode: Optional[str] = Query("PerGame", description="Statistical mode (e.g., 'PerGame', 'Totals', 'Per36')."),
    measure_type: Optional[str] = Query("Base", description="Category of stats (e.g., 'Base', 'Advanced', 'Scoring')."),
    team_id: Optional[str] = Query("", description="Filter by specific team ID."),
    player_position: Optional[str] = Query("", description="Filter by player position (e.g., 'G', 'F', 'C')."),
    player_experience: Optional[str] = Query("", description="Filter by player experience (e.g., 'Rookie', 'Veteran')."),
    starter_bench: Optional[str] = Query("", description="Filter by starter/bench status (e.g., 'Starters', 'Bench')."),
    date_from: Optional[str] = Query("", description="Start date (MM/DD/YYYY) for filtering games."),
    date_to: Optional[str] = Query("", description="End date (MM/DD/YYYY) for filtering games."),
    game_segment: Optional[str] = Query("", description="Filter by game segment (e.g., 'First Half', 'Second Half')."),
    last_n_games: Optional[int] = Query(0, description="Filter by last N games (0 for all games)."),
    league_id: Optional[str] = Query("00", description="League ID (e.g., '00' for NBA)."),
    location: Optional[str] = Query("", description="Filter by game location (e.g., 'Home', 'Road')."),
    month: Optional[int] = Query(0, description="Filter by month (0-12, 0 for all months)."),
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID."),
    outcome: Optional[str] = Query("", description="Filter by game outcome (e.g., 'W', 'L')."),
    period: Optional[int] = Query(0, description="Filter by period (0-4, 0 for all periods)."),
    season_segment: Optional[str] = Query("", description="Filter by season segment (e.g., 'Post All-Star', 'Pre All-Star')."),
    vs_conference: Optional[str] = Query("", description="Filter by opponent conference (e.g., 'East', 'West')."),
    vs_division: Optional[str] = Query("", description="Filter by opponent division.")
) -> Dict[str, Any]:
    """
    Endpoint to retrieve league-wide player statistics.
    Uses `fetch_league_player_stats_logic` from `league_dash_player_stats.py`.

    This endpoint provides comprehensive player statistics across the league:
    - Basic and advanced statistics
    - Scoring and defensive metrics
    - Player rankings
    - Filtering by team, position, experience, etc.

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.
    - **season_type** (str, optional): e.g., "Regular Season". Default: "Regular Season".
    - **per_mode** (str, optional): e.g., "PerGame". Default: "PerGame".
    - **measure_type** (str, optional): e.g., "Base". Default: "Base".
    - **team_id** (str, optional): Specific team ID filter.
    - **player_position** (str, optional): Position filter (G, F, C).
    - **player_experience** (str, optional): Experience filter.
    - **starter_bench** (str, optional): Role filter.
    - **date_from** (str, optional): MM/DD/YYYY.
    - **date_to** (str, optional): MM/DD/YYYY.
    - **game_segment** (str, optional): Game segment filter.
    - **last_n_games** (int, optional): Last N games filter.
    - **league_id** (str, optional): League ID.
    - **location** (str, optional): "Home" or "Road".
    - **month** (int, optional): Month filter (0-12).
    - **opponent_team_id** (int, optional): Opponent team filter.
    - **outcome** (str, optional): "W" or "L".
    - **period** (int, optional): Period filter (0-4).
    - **season_segment** (str, optional): Season segment filter.
    - **vs_conference** (str, optional): Opponent conference.
    - **vs_division** (str, optional): Opponent division.

    Successful Response (200 OK):
    Returns a dictionary containing parameters and league-wide player statistics.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid.
    - 500 Internal Server Error / 502 Bad Gateway: For unexpected errors.
    """
    logger.info(f"Received GET /league/player-stats request. Season: {season}, Type: {season_type}, PerMode: {per_mode}, Measure: {measure_type}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    player_stats_kwargs = {
        "season": season_to_use,
        "season_type": season_type,
        "per_mode": per_mode,
        "measure_type": measure_type,
        "team_id": team_id,
        "player_position": player_position,
        "player_experience": player_experience,
        "starter_bench": starter_bench,
        "date_from": date_from,
        "date_to": date_to,
        "game_segment": game_segment,
        "last_n_games": last_n_games,
        "league_id": league_id,
        "location": location,
        "month": month,
        "opponent_team_id": opponent_team_id,
        "outcome": outcome,
        "period": period,
        "season_segment": season_segment,
        "vs_conference": vs_conference,
        "vs_division": vs_division
    }

    return await _handle_league_stats_logic_call(
        fetch_league_player_stats_logic, "league player stats",
        **player_stats_kwargs
    )

@router.get(
    "/player-estimated-metrics",
    summary="Get League-Wide Player Estimated Metrics",
    description="Fetches estimated metrics (E_OFF_RATING, E_DEF_RATING, E_NET_RATING, etc.) for all players.",
    response_model=Dict[str, Any]
)
async def get_league_player_estimated_metrics_endpoint(
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query("Regular Season", description="Type of season (e.g., 'Regular Season', 'Playoffs')."),
    league_id: Optional[str] = Query("00", description="League ID (e.g., '00' for NBA).")
) -> Dict[str, Any]:
    """
    Endpoint to retrieve league-wide player estimated metrics.
    Uses `fetch_player_estimated_metrics_logic` from `player_estimated_metrics.py`.

    This endpoint provides estimated metrics for all players:
    - E_OFF_RATING: Estimated Offensive Rating
    - E_DEF_RATING: Estimated Defensive Rating
    - E_NET_RATING: Estimated Net Rating
    - E_PACE: Estimated Pace
    - E_AST_RATIO: Estimated Assist Ratio
    - E_OREB_PCT: Estimated Offensive Rebound Percentage
    - E_DREB_PCT: Estimated Defensive Rebound Percentage
    - E_REB_PCT: Estimated Total Rebound Percentage
    - E_TM_TOV_PCT: Estimated Team Turnover Percentage
    - E_USG_PCT: Estimated Usage Percentage

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.
    - **season_type** (str, optional): e.g., "Regular Season". Default: "Regular Season".
    - **league_id** (str, optional): League ID. Default: "00" (NBA).

    Successful Response (200 OK):
    Returns a dictionary containing parameters and player estimated metrics.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid.
    - 500 Internal Server Error / 502 Bad Gateway: For unexpected errors.
    """
    logger.info(f"Received GET /league/player-estimated-metrics request. Season: {season}, Type: {season_type}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    estimated_metrics_kwargs = {
        "season": season_to_use,
        "season_type": season_type,
        "league_id": league_id
    }

    return await _handle_league_stats_logic_call(
        fetch_player_estimated_metrics_logic, "player estimated metrics",
        **estimated_metrics_kwargs
    )

@router.get(
    "/team-estimated-metrics",
    summary="Get League-Wide Team Estimated Metrics",
    description="Fetches estimated metrics for all teams including offensive/defensive ratings, pace, etc.",
    response_model=Dict[str, Any]
)
async def get_league_team_estimated_metrics_endpoint(
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query("", description="Type of season. Default: '' (Regular Season)."),
    league_id: Optional[str] = Query("00", description="League ID. Default: '00' (NBA).")
) -> Dict[str, Any]:
    """
    Endpoint to retrieve league-wide team estimated metrics.
    Uses `fetch_team_estimated_metrics_logic` from `team_estimated_metrics.py`.

    This endpoint provides estimated metrics for all teams:
    - E_OFF_RATING: Estimated Offensive Rating
    - E_DEF_RATING: Estimated Defensive Rating
    - E_NET_RATING: Estimated Net Rating
    - E_PACE: Estimated Pace
    - E_AST_RATIO: Estimated Assist Ratio
    - E_OREB_PCT: Estimated Offensive Rebound Percentage
    - E_DREB_PCT: Estimated Defensive Rebound Percentage
    - E_REB_PCT: Estimated Total Rebound Percentage
    - E_TM_TOV_PCT: Estimated Team Turnover Percentage

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.
    - **season_type** (str, optional): Season type. Default: "".
    - **league_id** (str, optional): League ID. Default: "00".

    Successful Response (200 OK):
    Returns a dictionary containing parameters and team estimated metrics.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid.
    - 500 Internal Server Error / 502 Bad Gateway: For unexpected errors.
    """
    logger.info(f"Received GET /league/team-estimated-metrics request. Season: {season}, Type: {season_type}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    team_estimated_metrics_kwargs = {
        "season": season_to_use,
        "season_type": season_type,
        "league_id": league_id
    }

    return await _handle_league_stats_logic_call(
        fetch_team_estimated_metrics_logic, "team estimated metrics",
        **team_estimated_metrics_kwargs
    )

@router.get(
    "/comprehensive-data",
    summary="Get Comprehensive League Data",
    description="Loads all league data efficiently for team/player pages to avoid multiple API calls.",
    response_model=Dict[str, Any]
)
async def get_comprehensive_league_data_endpoint(
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON}.", regex=r"^\d{4}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to load comprehensive league data efficiently.

    This endpoint loads all player and team data at once to avoid multiple API calls:
    - Basic player statistics
    - Advanced player statistics
    - Player estimated metrics
    - Team statistics
    - Team estimated metrics
    - League averages

    This data can then be used to populate individual team/player pages efficiently.

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.

    Successful Response (200 OK):
    Returns comprehensive league data with status and counts.

    Error Responses:
    - 400 Bad Request: If season format is invalid.
    - 500 Internal Server Error: For unexpected errors.
    """
    logger.info(f"Received GET /league/comprehensive-data request. Season: {season}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    try:
        result = await get_comprehensive_league_data(season_to_use)
        return result
    except Exception as e:
        logger.error(f"Error in comprehensive league data endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error loading comprehensive data: {str(e)}")

@router.get(
    "/player-advanced-analytics/{player_id}",
    summary="Get Advanced Player Analytics",
    description="Get comprehensive advanced analytics for a specific player including custom metrics and percentiles.",
    response_model=Dict[str, Any]
)
async def get_player_advanced_analytics_endpoint(
    player_id: int,
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON}.", regex=r"^\d{4}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get advanced analytics for a specific player.

    This endpoint provides comprehensive advanced metrics:
    - True Shooting Percentage
    - Effective Field Goal Percentage
    - Points/Rebounds/Assists per minute
    - Usage Rate and Player Impact Estimate
    - Estimated offensive/defensive ratings
    - Percentile rankings vs league

    Path Parameters:
    - **player_id** (int): NBA player ID.

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.

    Successful Response (200 OK):
    Returns advanced analytics and percentile rankings for the player.

    Error Responses:
    - 400 Bad Request: If season format is invalid.
    - 404 Not Found: If player not found.
    - 500 Internal Server Error: For unexpected errors.
    """
    logger.info(f"Received GET /league/player-advanced-analytics/{player_id} request. Season: {season}")

    season_to_use = season or settings.CURRENT_NBA_SEASON
    if not validate_season_format(season_to_use):
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=Errors.INVALID_SEASON_FORMAT.format(season=season_to_use))

    try:
        result = await get_player_advanced_analytics(player_id, season_to_use)

        if 'error' in result.get('advanced_metrics', {}):
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Player {player_id} not found")

        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in player advanced analytics endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error calculating advanced analytics: {str(e)}")

===== backend\routes\live_game.py =====
import asyncio
from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect, status
import logging
import json
from typing import Dict, Any 
from backend.api_tools.live_game_tools import fetch_league_scoreboard_logic
from backend.core.errors import Errors
from starlette.websockets import WebSocketState

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/live",
    tags=["Live Games"]
)

async def _handle_live_game_logic_call(
    logic_function: callable, 
    endpoint_name: str,
    *args, 
    **kwargs
) -> Dict[str, Any]:
    """Helper to call live game logic, parse JSON, and handle errors."""
    try:
        result_json_string = await asyncio.to_thread(logic_function, *args, **kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for {endpoint_name}: {error_detail}")
            status_code_to_raise = status.HTTP_502_BAD_GATEWAY if "external api" in error_detail.lower() or "provider error" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for {endpoint_name}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for {endpoint_name}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.get(
    "/scoreboard",
    summary="Get Current Live NBA Scoreboard",
    description="Fetches the current live NBA scoreboard data. (Consider using `/scoreboard/` in `scoreboard.py` for more flexibility).",
    response_model=Dict[str, Any]
)
async def get_current_live_scoreboard_endpoint() -> Dict[str, Any]:
    logger.info("Request received for GET /live/scoreboard (current live data)")
    return await _handle_live_game_logic_call(fetch_league_scoreboard_logic, "live scoreboard")

@router.websocket("/scoreboard/ws")
async def live_scoreboard_websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info(f"Live scoreboard WebSocket connection accepted from {websocket.client.host}:{websocket.client.port}")
    
    last_client_ping_time = asyncio.get_event_loop().time()
    stop_event = asyncio.Event()

    async def send_scoreboard_updates():
        nonlocal stop_event
        while not stop_event.is_set():
            try:
                if websocket.application_state != WebSocketState.CONNECTED:
                    logger.warning(f"WebSocket {websocket.client} no longer connected in send_scoreboard_updates. Stopping.")
                    stop_event.set()
                    break
                
                scoreboard_json_str = await asyncio.to_thread(fetch_league_scoreboard_logic)
                try:
                    temp_data = json.loads(scoreboard_json_str)
                    if isinstance(temp_data, dict) and "error" in temp_data:
                        logger.warning(f"Live scoreboard logic returned error: {temp_data['error']}. Skipping send for this cycle.")
                        await asyncio.sleep(3) 
                        continue
                except json.JSONDecodeError:
                    logger.error(f"Failed to decode JSON from live scoreboard logic: {scoreboard_json_str[:200]}. Skipping send.")
                    await asyncio.sleep(3)
                    continue
                
                await websocket.send_text(scoreboard_json_str)
                logger.debug(f"Sent live scoreboard update to {websocket.client.host}:{websocket.client.port}")
            except WebSocketDisconnect: 
                logger.info(f"WebSocket client {websocket.client.host}:{websocket.client.port} disconnected during send. Stopping updates.")
                stop_event.set()
                break
            except Exception as send_error:
                logger.error(f"Error sending live scoreboard update to {websocket.client.host}:{websocket.client.port}: {send_error}", exc_info=True)
                stop_event.set() 
                break
            await asyncio.sleep(3) 

    async def client_ping_monitor():
        nonlocal last_client_ping_time, stop_event
        while not stop_event.is_set():
            await asyncio.sleep(5) 
            if (asyncio.get_event_loop().time() - last_client_ping_time) > 40: 
                logger.warning(f"No ping from client {websocket.client.host}:{websocket.client.port} in 40s. Closing connection.")
                stop_event.set()
                if websocket.application_state == WebSocketState.CONNECTED:
                    await websocket.close(code=status.WS_1008_POLICY_VIOLATION) 
                break
    
    update_task = asyncio.create_task(send_scoreboard_updates())
    monitor_task = asyncio.create_task(client_ping_monitor())

    try:
        while not stop_event.is_set():
            if websocket.application_state != WebSocketState.CONNECTED:
                logger.warning(f"WebSocket {websocket.client} disconnected in main receive loop. Stopping.")
                stop_event.set()
                break
            try:
                message = await asyncio.wait_for(websocket.receive_text(), timeout=45) 
                if message.strip().lower() == '{"type":"ping"}': 
                    last_client_ping_time = asyncio.get_event_loop().time()
                    logger.debug(f"Received ping from {websocket.client.host}:{websocket.client.port}")
                    await websocket.send_text('{"type":"pong"}') 
            except asyncio.TimeoutError:
                logger.warning(f"Timeout waiting for message/ping from {websocket.client.host}:{websocket.client.port}. Last ping: {last_client_ping_time:.0f}s ago.")
            except WebSocketDisconnect:
                logger.info(f"Client {websocket.client.host}:{websocket.client.port} disconnected (WebSocketDisconnect in receive loop).")
                stop_event.set()
                break 
    except WebSocketDisconnect: 
        logger.info(f"WebSocket connection (outer) closed by client {websocket.client.host}:{websocket.client.port}.")
    except Exception as e:
        logger.error(f"Unexpected error in live scoreboard WebSocket handler for {websocket.client.host}:{websocket.client.port}: {str(e)}", exc_info=True)
    finally:
        logger.info(f"Stopping tasks and closing live scoreboard WebSocket for {websocket.client.host}:{websocket.client.port}.")
        stop_event.set() 
        for task in [update_task, monitor_task]:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    logger.debug(f"Task {task.get_name()} cancelled for WebSocket {websocket.client.host}:{websocket.client.port}") # Use task.get_name()
                except Exception as task_ex:
                    logger.error(f"Error during task cleanup for {task.get_name()}: {task_ex}")
        
        if websocket.application_state == WebSocketState.CONNECTED:
            try:
                await websocket.close(code=status.WS_1000_NORMAL_CLOSURE)
            except RuntimeError as rt_err:
                 logger.debug(f"RuntimeError during final WebSocket close (live_game) for {websocket.client} (likely already closing/closed): {rt_err}")
            except Exception as final_close_err:
                 logger.warning(f"Error during final WebSocket close (live_game) for {websocket.client}: {final_close_err}")
        logger.info(f"Live scoreboard WebSocket handler finished for {websocket.client.host}:{websocket.client.port}")

===== backend\routes\odds.py =====
import logging
import asyncio
import json
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, status

from backend.api_tools.odds_tools import fetch_odds_data_logic
from backend.core.errors import Errors
from backend.utils.path_utils import get_relative_cache_path

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/odds",  # Standard prefix for odds-related endpoints
    tags=["Odds"]    # Tag for OpenAPI documentation
)

async def _handle_odds_route_logic_call(
    logic_function: callable,
    endpoint_name: str,
    *args,
    **kwargs
) -> Dict[str, Any]:
    """Helper to call odds-related logic, parse JSON, and handle errors."""
    try:
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}

        result_json_string = await asyncio.to_thread(logic_function, *args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} with args {args}, kwargs {filtered_kwargs}: {error_detail}")
            status_code_to_raise = status.HTTP_502_BAD_GATEWAY if "external api" in error_detail.lower() or "provider error" in error_detail.lower() else \
                                   status.HTTP_400_BAD_REQUEST if "invalid parameter" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR # Default
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}, kwargs {filtered_kwargs}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for args {args}, kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.get(
    "/live",
    summary="Get Live Betting Odds for NBA Games",
    description="Fetches live betting odds for upcoming or in-progress NBA games from configured odds providers. "
                "The specific games and markets returned depend on the provider and data availability.",
    response_model=Dict[str, Any]
)
async def get_live_betting_odds_endpoint(
    as_dataframe: bool = False
) -> Dict[str, Any]:
    """
    Endpoint to retrieve live betting odds for NBA games.
    Uses `fetch_odds_data_logic` from `odds_tools.py`.

    Query Parameters:
    - as_dataframe (bool, optional): If True, returns a flattened representation of the odds data
      and saves it to a CSV file in the cache directory. The DataFrame is structured to make it more
      usable for analysis. Defaults to False.

    Successful Response (200 OK):
    Returns a dictionary containing odds data. The exact structure depends
    on the `fetch_odds_data_logic` implementation and the odds provider's API.

    Standard response (as_dataframe=False):
    ```json
    {
        "games": [
            {
                "gameId": "0042400216",
                "homeTeamId": 1610612752,
                "awayTeamId": 1610612738,
                "gameTime": "2023-05-18T19:30:00Z",
                "gameStatus": 1,
                "gameStatusText": "Scheduled",
                "markets": [
                    {
                        "marketId": "spread",
                        "name": "Point Spread",
                        "books": [
                            {
                                "bookId": "fanduel",
                                "name": "FanDuel",
                                "outcomes": [
                                    {
                                        "type": "home",
                                        "odds": "-110",
                                        "value": "-5.5"
                                    },
                                    {
                                        "type": "away",
                                        "odds": "-110",
                                        "value": "+5.5"
                                    }
                                ]
                            }
                        ]
                    }
                ]
            }
        ]
    }
    ```

    DataFrame response (as_dataframe=True):
    ```json
    {
        "games": [...],  // Same as standard response
        "dataframe_info": {
            "message": "Odds data has been converted to a DataFrame and saved as CSV",
            "dataframe_shape": [40, 14],
            "dataframe_columns": ["gameId", "awayTeamId", "homeTeamId", "gameTime", "gameStatus", "gameStatusText",
                                 "marketId", "marketName", "bookId", "bookName", "outcomeType", "odds", "openingOdds", "value"],
            "csv_path": "backend/cache/odds_data.csv",
            "sample_data": [
                {
                    "gameId": "0042400216",
                    "awayTeamId": 1610612738,
                    "homeTeamId": 1610612752,
                    "marketName": "Point Spread",
                    "bookName": "FanDuel",
                    "outcomeType": "home",
                    "odds": "-110",
                    "value": "-5.5"
                }
            ]
        }
    }
    ```

    If no odds are available, it might return an empty list or a structure indicating no data.

    Error Responses:
    - 500 Internal Server Error: For unexpected errors.
    - 502 Bad Gateway: If there's an issue fetching data from the external odds provider.
    - 400 Bad Request: If query parameters were added and they are invalid.
    """
    logger.info(f"Request received for GET /odds/live with as_dataframe={as_dataframe}")

    odds_kwargs = {
        "return_dataframe": as_dataframe
    }

    if as_dataframe:
        # For DataFrame output, we need to handle the response differently
        try:
            filtered_kwargs = {k: v for k, v in odds_kwargs.items() if v is not None}

            # Call the logic function with return_dataframe=True
            json_response, df = await asyncio.to_thread(fetch_odds_data_logic, **filtered_kwargs)
            result_data = json.loads(json_response)

            if isinstance(result_data, dict) and 'error' in result_data:
                error_detail = result_data['error']
                logger.error(f"Error from fetch_odds_data_logic with kwargs {filtered_kwargs}: {error_detail}")
                status_code_to_raise = status.HTTP_502_BAD_GATEWAY if "external api" in error_detail.lower() or "provider error" in error_detail.lower() else \
                                      status.HTTP_400_BAD_REQUEST if "invalid parameter" in error_detail.lower() else \
                                      status.HTTP_500_INTERNAL_SERVER_ERROR # Default
                raise HTTPException(status_code=status_code_to_raise, detail=error_detail)

            # Add DataFrame info to the response
            df_info = {
                "message": "Odds data has been converted to a DataFrame and saved as CSV",
                "dataframe_shape": df.shape,
                "dataframe_columns": df.columns.tolist(),
                "csv_path": get_relative_cache_path("odds_data.csv"),  # Using path_utils for consistent paths
                "sample_data": df.head(5).to_dict(orient="records") if not df.empty else []
            }
            result_data["dataframe_info"] = df_info

            return result_data

        except HTTPException as http_exc:
            raise http_exc
        except json.JSONDecodeError as json_err:
            logger.error(f"Failed to parse JSON response from fetch_odds_data_logic for kwargs {filtered_kwargs}: {json_err}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
        except Exception as e:
            logger.critical(f"Unexpected error in API route calling fetch_odds_data_logic for kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
            error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
            detail_msg = error_msg_template.format(error=f"processing live betting odds")
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)
    else:
        # For standard JSON output, use the existing helper function
        return await _handle_odds_route_logic_call(
            fetch_odds_data_logic, "live betting odds",
            **odds_kwargs
        )


===== backend\routes\player.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, Path # Added Path
from typing import Optional, List, Dict, Any
import json

from backend.api_tools.player_common_info import get_player_headshot_url
from backend.api_tools.player_aggregate_stats import fetch_player_stats_logic
from backend.api_tools.player_dashboard_stats import fetch_player_profile_logic

from backend.api_tools.search import find_players_by_name_fragment
from backend.core.errors import Errors
from backend.utils.validation import _validate_season_format

router = APIRouter(
    prefix="/player", # Add prefix for all player routes
    tags=["Players"]  # Tag for OpenAPI documentation
)
logger = logging.getLogger(__name__)

async def _handle_player_route_logic_call(logic_function: callable, *args, **kwargs) -> Dict[str, Any]:
    """Helper to call player-related logic, parse JSON, and handle errors for player routes."""
    try:
        result_json_string = await asyncio.to_thread(logic_function, *args, **kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid player" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        log_args = args[:1] # Log first arg (usually player identifier)
        logger.critical(f"Unexpected error in API route calling {func_name} with args {log_args}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing player data via {func_name}")
        raise HTTPException(status_code=500, detail=detail_msg)


@router.get(
    "/{player_id}/headshot",
    summary="Get Player Headshot URL",
    description="Retrieves the URL for a player's official NBA headshot image based on their player ID.",
    response_model=Dict[str, Any]
)
async def get_player_headshot_by_id_endpoint( # Renamed for clarity
    player_id: int = Path(..., description="The unique 7-to-10 digit ID of the NBA player.", gt=0) # Added gt=0 validation
) -> Dict[str, Any]:
    """
    Endpoint to get a player's headshot URL by their ID.

    Path Parameters:
    - **player_id** (int, required): The unique ID of the player. Must be a positive integer.

    Successful Response (200 OK):
    ```json
    {
        "player_id": 2544,
        "headshot_url": "https://ak-static.cms.nba.com/wp-content/uploads/headshots/nba/latest/260x190/2544.png"
    }
    ```

    Error Responses:
    - 400 Bad Request: If `player_id` is invalid (e.g., not a positive integer).
    - 500 Internal Server Error: For unexpected errors.
    """
    logger.info(f"Received GET /player/{player_id}/headshot request.")
    try:
        headshot_url_str = await asyncio.to_thread(get_player_headshot_url, player_id)
        result = {"player_id": player_id, "headshot_url": headshot_url_str}
        return result
    except ValueError as ve: # Catch specific ValueError from get_player_headshot_url if player_id is invalid
        logger.warning(f"Invalid player_id for headshot: {player_id}. Error: {ve}")
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.critical(f"Unexpected error fetching headshot for player ID {player_id}: {str(e)}", exc_info=True)
        error_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching headshot for player ID {player_id}")
        raise HTTPException(status_code=500, detail=error_msg)

@router.get(
    "/search",
    summary="Search for Players by Name Fragment",
    description="Searches for NBA players whose full names contain the provided query string. "
                "Returns a list of matching players with basic information.",
    response_model=List[Dict[str, Any]]
)
async def search_players_by_name_endpoint(
    q: str = Query(..., description="Search query string for player name. Minimum 2 characters.", min_length=2),
    limit: int = Query(10, description="Maximum number of search results to return.", ge=1, le=50)
) -> List[Dict[str, Any]]:
    """
    Endpoint to search for players by a name fragment.
    Uses `find_players_by_name_fragment` from `search.py`.

    Query Parameters:
    - **q** (str, required): The search query for the player's name. Minimum length is 2 characters.
    - **limit** (int, optional): Maximum number of results to return. Default: 10. Min: 1, Max: 50.

    Successful Response (200 OK):
    Returns a list of player objects. Each object contains:
    ```json
    [
        {
            "id": 2544,
            "full_name": "LeBron James",
            "first_name": "LeBron",
            "last_name": "James",
            "is_active": true
        },
        ...
    ]
    ```
    Returns an empty list if no players match or the query is too short.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid (e.g., `q` too short, `limit` out of range - handled by FastAPI).
    - 500 Internal Server Error: For unexpected errors during the search.
    """
    logger.info(f"Received GET /player/search request with query: '{q}', limit: {limit}")
    try:
        results_list = await asyncio.to_thread(find_players_by_name_fragment, q, limit=limit)
        logger.info(f"Returning {len(results_list)} players for search query '{q}'")
        return results_list
    except Exception as e:
        logger.critical(f"Unexpected error during player search for query '{q}': {str(e)}", exc_info=True)
        error_msg = Errors.UNEXPECTED_ERROR.format(error=f"searching for player '{q}'")
        raise HTTPException(status_code=500, detail=error_msg)

@router.get(
    "/stats",
    summary="Get Aggregated Player Statistics",
    description="Fetches a comprehensive set of aggregated statistics for a player, "
                "including player info, headline stats, career totals, season game logs, and awards.",
    response_model=Dict[str, Any]
)
async def fetch_player_stats_endpoint(
    player_name: str = Query(..., description="Full name of the player (e.g., 'LeBron James')."),
    season: Optional[str] = Query(None, description="NBA season in YYYY-YY format (e.g., '2023-24') for game logs. Defaults to current season in logic.", regex=r"^\d{4}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get aggregated player statistics.
    Uses `fetch_player_stats_logic`.

    Query Parameters:
    - **player_name** (str, required): Full name of the player.
    - **season** (str, optional): Season for game logs (YYYY-YY). Defaults to current season in the logic layer.

    Successful Response (200 OK):
    Returns a dictionary with aggregated player stats.
    Refer to `fetch_player_stats_logic` in `player_tools.py` for detailed structure.
    Includes `info`, `headline_stats`, `career_stats`, `season_gamelog`, `awards`.

    Error Responses:
    - 400 Bad Request: If `player_name` is missing or `season` format is invalid.
    - 404 Not Found: If the player is not found.
    - 500 Internal Server Error: For other processing issues.
    """
    logger.info(f"Received GET /player/stats request for player: '{player_name}', season: {season}")
    if season and not _validate_season_format(season): # Use directly imported util
        raise HTTPException(status_code=400, detail=Errors.INVALID_SEASON_FORMAT.format(season=season))
    return await _handle_player_route_logic_call(fetch_player_stats_logic, player_name, season)

@router.get(
    "/profile",
    summary="Get Player Profile",
    description="Fetches a comprehensive player profile including biographical info, career/season highs, "
                "next game details, and career/season totals.",
    response_model=Dict[str, Any]
)
async def fetch_player_profile_endpoint(
    player_name: str = Query(..., description="Full name of the player (e.g., 'Stephen Curry')."),
    per_mode: Optional[str] = Query(None, description="Statistical mode for career/season totals (e.g., 'PerGame', 'Totals'). Defaults to 'PerGame' in logic.")
) -> Dict[str, Any]:
    """
    Endpoint to get a player's profile.
    Uses `fetch_player_profile_logic`.

    Query Parameters:
    - **player_name** (str, required): Full name of the player.
    - **per_mode** (str, optional): Statistical mode for totals. Defaults to 'PerGame' in logic layer.

    Successful Response (200 OK):
    Returns a dictionary with the player's profile.
    Refer to `fetch_player_profile_logic` in `player_tools.py` for detailed structure.
    Includes `player_info`, `career_highs`, `season_highs`, `next_game`, `career_totals`, `season_totals`.

    Error Responses:
    - 400 Bad Request: If `player_name` is missing or `per_mode` is invalid.
    - 404 Not Found: If the player is not found.
    - 500 Internal Server Error: For other processing issues.
    """
    logger.info(f"Received GET /player/profile request for player: '{player_name}', per_mode: {per_mode}")
    return await _handle_player_route_logic_call(fetch_player_profile_logic, player_name, per_mode)

===== backend\routes\player_tracking.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, Path
from typing import Dict, Any, Optional
import json
import re

from backend.api_tools.player_clutch import fetch_player_clutch_stats_logic
from backend.api_tools.player_shooting_tracking import fetch_player_shots_tracking_logic
from backend.api_tools.player_rebounding import fetch_player_rebounding_stats_logic
from backend.api_tools.player_passing import fetch_player_passing_stats_logic

from backend.core.errors import Errors
from config import settings

router = APIRouter(
    prefix="/player", 
    tags=["Player Tracking"]
)
logger = logging.getLogger(__name__)

season_regex_pattern = r"(^\d{4}-\d{2}$)|(^" + re.escape(settings.CURRENT_NBA_SEASON) + r"$)"

async def _handle_tracking_logic_call(
    logic_function: callable,
    player_name: str,
    endpoint_name: str, 
    *args, 
    **kwargs
) -> Dict[str, Any]:
    """Helper to call player tracking logic, parse JSON, and handle errors."""
    try:
        all_logic_args = (player_name,) + args
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        
        result_json_string = await asyncio.to_thread(logic_function, *all_logic_args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for {player_name}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid player" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for {player_name}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        log_args_repr = args[:1] 
        logger.critical(f"Unexpected error in API route calling {func_name} for '{player_name}' with args {log_args_repr} and kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()} for player '{player_name}'")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/{player_name}/tracking/stats",
    summary="Get All Player Tracking Statistics (Combined)",
    description="Fetches a comprehensive set of player tracking statistics including clutch performance, "
                "detailed shooting, rebounding, and passing data for a specified player and season. "
                "This endpoint makes multiple underlying API calls; individual data categories might "
                "return empty or error states if specific data is unavailable.",
    response_model=Dict[str, Any]
)
async def get_all_player_tracking_stats(
    player_name: str = Path(..., description="Full name of the player (e.g., 'LeBron James')."),
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic if not provided.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Type of season (e.g., 'Regular Season', 'Playoffs'). Defaults to 'Regular Season' in logic."),
    measure_type_clutch: Optional[str] = Query(None, description="Measure type for clutch stats (e.g., 'Base', 'Advanced')."),
    per_mode_clutch: Optional[str] = Query(None, description="Per mode for clutch stats (e.g., 'Totals', 'PerGame')."),
    opponent_team_id_shots: Optional[int] = Query(0, description="Opponent team ID for shots tracking. Default 0 (all).", ge=0),
    date_from_shots: Optional[str] = Query(None, description="Start date (YYYY-MM-DD) for shots tracking.", regex=r"^\d{4}-\d{2}-\d{2}$"),
    date_to_shots: Optional[str] = Query(None, description="End date (YYYY-MM-DD) for shots tracking.", regex=r"^\d{4}-\d{2}-\d{2}$"),
    per_mode_reb: Optional[str] = Query(None, description="Per mode for rebounding (e.g., 'PerGame', 'Totals')."),
    per_mode_pass: Optional[str] = Query(None, description="Per mode for passing (e.g., 'PerGame', 'Totals').")
) -> Dict[str, Any]:
    logger.info(f"Received GET /player/{player_name}/tracking/stats request, Season: {season}")

    clutch_kwargs = {"season": season, "season_type": season_type, "measure_type": measure_type_clutch, "per_mode": per_mode_clutch}
    shots_kwargs = {"season": season, "season_type": season_type, "opponent_team_id": opponent_team_id_shots, "date_from": date_from_shots, "date_to": date_to_shots}
    reb_kwargs = {"season": season, "season_type": season_type, "per_mode": per_mode_reb}
    pass_kwargs = {"season": season, "season_type": season_type, "per_mode": per_mode_pass}

    try:
        results = await asyncio.gather(
            _handle_tracking_logic_call(fetch_player_clutch_stats_logic, player_name, "clutch stats", **clutch_kwargs),
            _handle_tracking_logic_call(fetch_player_shots_tracking_logic, player_name, "shots tracking", **shots_kwargs),
            _handle_tracking_logic_call(fetch_player_rebounding_stats_logic, player_name, "rebounding stats", **reb_kwargs),
            _handle_tracking_logic_call(fetch_player_passing_stats_logic, player_name, "passing stats", **pass_kwargs),
            return_exceptions=False
        )
        clutch_data, shots_data, rebounding_data, passing_data = results
        
        player_id_resolved = clutch_data.get("player_id")
        player_name_resolved = clutch_data.get("player_name", player_name)

        combined_result = {
            "player_name": player_name_resolved,
            "player_id": player_id_resolved,
            "season_requested": season,
            "clutch_stats": clutch_data,
            "shots_tracking": shots_data,
            "rebounding_stats": rebounding_data,
            "passing_stats": passing_data
        }
        return combined_result
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.critical(f"Unexpected error in combined tracking stats for {player_name}: {str(e)}", exc_info=True)
        error_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching combined tracking stats for player '{player_name}'")
        raise HTTPException(status_code=500, detail=error_msg)

@router.get(
    "/{player_name}/tracking/clutch",
    summary="Get Player Clutch Statistics",
    description="Fetches player statistics in clutch situations (e.g., last 5 minutes of a close game). "
                "Allows filtering by various clutch parameters.",
    response_model=Dict[str, Any]
)
async def get_player_clutch_stats_endpoint(
    player_name: str = Path(..., description="Full name of the player."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY or '{settings.CURRENT_NBA_SEASON}'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=season_regex_pattern, example="2023-24"), # Changed
    season_type: Optional[str] = Query(None, description="Season type (e.g., 'Regular Season', 'Playoffs'). Logic default: 'Regular Season'."),
    measure_type: Optional[str] = Query(None, description="Measure type (e.g., 'Base', 'Advanced'). Logic default: 'Base'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'Totals', 'PerGame'). Logic default: 'Totals'."),
    clutch_time_nullable: Optional[str] = Query(None, description="Clutch time window (e.g., 'Last 5 Minutes', 'Last 1 Minute')."),
    ahead_behind_nullable: Optional[str] = Query(None, description="Score margin (e.g., 'Ahead or Behind', 'Tied', 'Behind by 1-3')."),
    point_diff_nullable: Optional[int] = Query(None, description="Maximum point differential for clutch definition (e.g., 5 for within 5 points).", ge=0),
    plus_minus: Optional[str] = Query(None, description="Include plus-minus stats ('Y' or 'N').", pattern=r"^[YN]$"), # Corrected regex
    pace_adjust: Optional[str] = Query(None, description="Adjust stats for pace ('Y' or 'N').", pattern=r"^[YN]$"), # Corrected regex
    rank: Optional[str] = Query(None, description="Include stat rankings ('Y' or 'N').", pattern=r"^[YN]$"), # Corrected regex
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID. Default 0 (all).", ge=0)
) -> Dict[str, Any]:
    logger.info(f"Received GET /player/{player_name}/tracking/clutch with params: season={season}, measure_type={measure_type}, per_mode={per_mode}")
    clutch_kwargs = {
        "season": season, "season_type": season_type, "measure_type": measure_type, "per_mode": per_mode,
        "clutch_time_nullable": clutch_time_nullable, "ahead_behind_nullable": ahead_behind_nullable,
        "point_diff_nullable": point_diff_nullable, "plus_minus": plus_minus, "pace_adjust": pace_adjust,
        "rank": rank, "opponent_team_id": opponent_team_id
    }
    return await _handle_tracking_logic_call(fetch_player_clutch_stats_logic, player_name, "clutch stats", **clutch_kwargs)

@router.get(
    "/{player_name}/tracking/shots",
    summary="Get Player Shots Tracking Statistics",
    description="Fetches detailed player shooting statistics, categorized by factors like shot clock, dribbles, and defender distance.",
    response_model=Dict[str, Any]
)
async def get_player_shots_tracking_endpoint(
    player_name: str = Path(..., description="Full name of the player."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY or '{settings.CURRENT_NBA_SEASON}'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=season_regex_pattern, example="2023-24"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID. Default: 0 (all).", ge=0),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD).", pattern=r"^\d{4}-\d{2}-\d{2}$"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD).", pattern=r"^\d{4}-\d{2}-\d{2}$")
) -> Dict[str, Any]:
    logger.info(f"Received GET /player/{player_name}/tracking/shots, Season: {season}")
    shots_kwargs = {
        "season": season, "season_type": season_type,
        "opponent_team_id": opponent_team_id, "date_from": date_from, "date_to": date_to
    }
    return await _handle_tracking_logic_call(fetch_player_shots_tracking_logic, player_name, "shots tracking", **shots_kwargs)

@router.get(
    "/{player_name}/tracking/rebounding",
    summary="Get Player Rebounding Statistics",
    description="Fetches detailed player rebounding statistics, categorized by shot type, contest, and distances.",
    response_model=Dict[str, Any]
)
async def get_player_rebounding_stats_endpoint(
    player_name: str = Path(..., description="Full name of the player."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY or '{settings.CURRENT_NBA_SEASON}'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=season_regex_pattern, example="2023-24"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'PerGame'.")
) -> Dict[str, Any]:
    logger.info(f"Received GET /player/{player_name}/tracking/rebounding, Season: {season}")
    reb_kwargs = {"season": season, "season_type": season_type, "per_mode": per_mode}
    return await _handle_tracking_logic_call(fetch_player_rebounding_stats_logic, player_name, "rebounding stats", **reb_kwargs)

@router.get(
    "/{player_name}/tracking/passing",
    summary="Get Player Passing Statistics",
    description="Fetches detailed player passing statistics, including passes made to and received from teammates.",
    response_model=Dict[str, Any]
)
async def get_player_passing_stats_endpoint(
    player_name: str = Path(..., description="Full name of the player."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY or '{settings.CURRENT_NBA_SEASON}'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=season_regex_pattern, example="2023-24"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'PerGame'.")
) -> Dict[str, Any]:
    logger.info(f"Received GET /player/{player_name}/tracking/passing, Season: {season}")
    pass_kwargs = {"season": season, "season_type": season_type, "per_mode": per_mode}
    return await _handle_tracking_logic_call(fetch_player_passing_stats_logic, player_name, "passing stats", **pass_kwargs)

===== backend\routes\scoreboard.py =====
from fastapi import APIRouter, Query, HTTPException, status, WebSocket, WebSocketDisconnect
import logging
from typing import Any, Dict, Optional
from datetime import date
import json
import asyncio
from starlette.websockets import WebSocketState

from backend.api_tools.scoreboard_tools import fetch_scoreboard_data_logic
from backend.utils.validation import validate_date_format
from backend.core.errors import Errors

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/scoreboard", # Add prefix for scoreboard routes
    tags=["Scoreboard"]    # Tag for OpenAPI documentation
)

@router.websocket("/ws")
async def scoreboard_websocket_endpoint(websocket: WebSocket):
    """
    WebSocket endpoint for real-time NBA scoreboard updates.

    Connects a client via WebSocket and streams scoreboard data approximately every 30 seconds.
    The data streamed is the same as the `GET /scoreboard/` endpoint (for the current day).

    Message Format (JSON, sent from server to client):
    ```json
    {
        "game_date": "YYYY-MM-DD",
        "games": [
            {
                "game_id": "0022300161",
                "game_status_text": "Final",
                "home_team": { "team_id": ..., "team_name": "Nuggets", "score": 110, ... },
                "away_team": { "team_id": ..., "team_name": "Lakers", "score": 99, ... },
                // ... other game details
            },
            // ... more games
        ]
    }
    ```
    If an error occurs while fetching data on the server, updates might be skipped for that cycle,
    or the connection might be closed if the error is persistent or critical.
    """
    logger.info(f"New WebSocket connection request from {websocket.client}")
    await websocket.accept()
    logger.info(f"WebSocket connection accepted for {websocket.client}")
    try:
        while True:
            if websocket.application_state != WebSocketState.CONNECTED:
                logger.warning(f"WebSocket {websocket.client} no longer connected (application_state). Stopping updates.")
                break
            
            try:
                # Fetch latest scoreboard data (logic defaults to today if game_date is None)
                data_json_string = await asyncio.to_thread(
                    fetch_scoreboard_data_logic, 
                    game_date=None, 
                    bypass_cache=True # Ensure fresh data for WebSocket
                )
                
                try:
                    data_dict_parsed = json.loads(data_json_string)
                    
                    if isinstance(data_dict_parsed, dict) and 'error' not in data_dict_parsed:
                        logger.debug(f"Sending scoreboard update to WebSocket {websocket.client}")
                        await websocket.send_json(data_dict_parsed)
                    elif isinstance(data_dict_parsed, dict) and 'error' in data_dict_parsed:
                        logger.warning(f"Scoreboard logic returned an error: {data_dict_parsed['error']}. Not sending to WebSocket client {websocket.client}.")
                    else:
                        logger.warning(f"Scoreboard logic returned unexpected data structure. Not sending to WebSocket client {websocket.client}. Data (first 100 chars): {str(data_dict_parsed)[:100]}")
                
                except json.JSONDecodeError:
                    logger.error(f"Failed to decode JSON from scoreboard logic for WebSocket {websocket.client}. Response: {data_json_string[:200]}...")
                except Exception as send_err:
                    logger.error(f"Error processing/sending scoreboard data to WebSocket {websocket.client}: {send_err}", exc_info=True)
                    # Decide if to break or continue based on error type
                    if not (websocket.application_state == WebSocketState.CONNECTED): break # Break if disconnected during send
                    # Otherwise, log and continue to next update cycle for transient send issues.

                await asyncio.sleep(2) # Interval for updates, changed from 30 to 10

            except WebSocketDisconnect:
                logger.info(f"WebSocket client {websocket.client} disconnected.")
                break
            except asyncio.CancelledError:
                logger.info(f"WebSocket update task cancelled for {websocket.client}.")
                break
            except Exception as loop_err: # Catch errors within the loop (e.g., from fetch_scoreboard_data_logic if it raises)
                logger.error(f"Error during scoreboard update loop for {websocket.client}: {str(loop_err)}", exc_info=True)
                # Attempt to close gracefully if still connected
                if websocket.application_state == WebSocketState.CONNECTED:
                    await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
                break # Exit loop on significant error

    except Exception as e: # Catch errors during initial accept or setup
        logger.error(f"WebSocket connection error for {websocket.client}: {str(e)}", exc_info=True)
        if websocket.application_state != WebSocketState.DISCONNECTED:
            await websocket.close(code=status.WS_1011_INTERNAL_ERROR)
    finally:
        logger.info(f"Closing WebSocket connection for {websocket.client}")
        if websocket.application_state != WebSocketState.DISCONNECTED:
            try:
                await websocket.close(code=status.WS_1000_NORMAL_CLOSURE)
            except RuntimeError as rt_err:
                 logger.debug(f"RuntimeError during final WebSocket close for {websocket.client} (likely already closing/closed): {rt_err}")
            except Exception as final_close_err:
                 logger.warning(f"Error during final WebSocket close for {websocket.client}: {final_close_err}")


@router.get(
    "/", 
    summary="Get Scoreboard by Date", 
    description="Fetches NBA scoreboard data (list of games, status, scores) for a specific date. "
                "If no date is provided, it defaults to the current day's scoreboard.",
    response_model=Dict[str, Any]
)
async def get_scoreboard_by_date_endpoint(
    game_date: Optional[str] = Query(
        default=None, 
        description="The date for which to fetch the scoreboard, in YYYY-MM-DD format. "
                    "If not provided, defaults to the current date.",
        regex=r"^\d{4}-\d{2}-\d{2}$"
    )
) -> Dict[str, Any]:
    """
    API endpoint to retrieve scoreboard data for a given date.
    Uses `fetch_scoreboard_data_logic`.

    Query Parameters:
    - **game_date** (str, optional): Date in YYYY-MM-DD format. Defaults to today.

    Successful Response (200 OK):
    Returns a dictionary containing the game date and a list of games.
    Example:
    ```json
    {
        "game_date": "2023-10-25",
        "games": [
            {
                "game_id": "0022300001",
                "game_status_text": "Final",
                "home_team": {"team_id": 1610612738, "team_name": "Celtics", "score": 120, ...},
                "away_team": {"team_id": 1610612752, "team_name": "Knicks", "score": 110, ...},
                // ... other game details like period, game_time_utc
            }
        ]
    }
    ```
    If no games are found for the date, `games` will be an empty list.

    Error Responses:
    - 400 Bad Request: If `game_date` format is invalid.
    - 500 Internal Server Error: For unexpected errors or issues fetching/processing data.
    - 502 Bad Gateway: If the underlying NBA API call fails. (Handled by logic layer, may result in 500 here)
    """
    effective_game_date_str = game_date or date.today().strftime('%Y-%m-%d')
    logger.info(f"Received GET /scoreboard/ request for date: {effective_game_date_str}.")

    if game_date and not validate_date_format(game_date):
         raise HTTPException(
             status_code=status.HTTP_400_BAD_REQUEST, 
             detail=Errors.INVALID_DATE_FORMAT.format(date=game_date)
         )

    try:
        result_json_string = await asyncio.to_thread(fetch_scoreboard_data_logic, game_date=game_date)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from fetch_scoreboard_data_logic for date {effective_game_date_str}: {error_detail}")
            # Map common errors from logic to appropriate HTTP status codes
            status_code_to_raise = status.HTTP_500_INTERNAL_SERVER_ERROR # Default
            if "not found" in error_detail.lower():
                status_code_to_raise = status.HTTP_404_NOT_FOUND
            elif "invalid date" in error_detail.lower() or "invalid format" in error_detail.lower():
                status_code_to_raise = status.HTTP_400_BAD_REQUEST
            
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        
        logger.info(f"Successfully processed scoreboard data for {result_data.get('game_date', effective_game_date_str)}.")
        return result_data

    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from scoreboard logic for date {effective_game_date_str}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in scoreboard GET endpoint for date {effective_game_date_str}: {str(e)}", exc_info=True)
        error_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching scoreboard for date {effective_game_date_str}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_msg)

===== backend\routes\search.py =====
from fastapi import APIRouter, HTTPException, Body, status # Added Body, status
import logging
import asyncio
import json
from typing import Any

from schemas import SearchRequest
from backend.api_tools.search import (
    search_players_logic, 
    search_teams_logic, 
    search_games_logic
)
from backend.core.constants import SUPPORTED_SEARCH_TARGETS
from backend.core.errors import Errors

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/search", # Add prefix for search routes
    tags=["Search"]   # Tag for OpenAPI documentation
)

async def _handle_search_logic_call(
    logic_function: callable, 
    endpoint_name: str,
    *args, 
    **kwargs
) -> Any:
    """Helper to call search logic, parse JSON, and handle errors."""
    try:
        # Filter out None kwargs so logic functions can use their defaults
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        
        result_json_string = await asyncio.to_thread(logic_function, *args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for args {args}, kwargs {filtered_kwargs}: {error_detail}")
            # Determine status code based on error
            status_code_to_raise = status.HTTP_404_NOT_FOUND if "not found" in error_detail.lower() else \
                                   status.HTTP_400_BAD_REQUEST if "invalid" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)        
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}, kwargs {filtered_kwargs}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for args {args}, kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing search request via {func_name}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.post(
    "/",
    summary="Perform a Search",
    description="Searches for players, teams, or games based on the provided query string and target type. "
                "The structure of the results depends on the search target.",
)
async def perform_search_endpoint(
    request: SearchRequest = Body(...)
) -> Any:
    """
    Search for players, teams, or games.

    Request Body (`SearchRequest`):
    - **target** (str, required): The type of entity to search for.
      Supported values: "players", "teams", "games".
      (Refer to `SUPPORTED_SEARCH_TARGETS` in config).
    - **query** (str, required): The search query string (e.g., player name, team name, game keyword).
    - **limit** (int, optional): Maximum number of results to return. Defaults vary by logic function (typically 5-10).
    - **season** (Optional[str]): For "games" target. Season in YYYY-YY format (e.g., "2023-24").
    - **season_type** (Optional[str]): For "games" target. Type of season (e.g., "Regular Season", "Playoffs").

    Successful Response (200 OK):
    The response structure varies based on the `target`:
    - **target="players"**: Returns a list of player objects.
      ```json
      [
          {"id": 2544, "full_name": "LeBron James", "is_active": true, ...},
          ...
      ]
      ```
    - **target="teams"**: Returns a list of team objects.
      ```json
      [
          {"id": 1610612738, "full_name": "Boston Celtics", "abbreviation": "BOS", ...},
          ...
      ]
      ```
    - **target="games"**: Returns a dictionary, typically containing a list of game objects.
      (Refer to `search_games_logic` in `api_tools/search.py` for exact structure).
      Example:
      ```json
      {
          "query": "Lakers vs Celtics",
          "season": "2023-24",
          "games_found": [
              { "game_id": "0022300001", "home_team_name": "Celtics", "away_team_name": "Lakers", ... },
              ...
          ]
      }
      ```
    If no results are found, an empty list or a structure indicating no results (e.g., `{"games_found": []}`) is returned.

    Error Responses:
    - 400 Bad Request: If `target` is unsupported, or other request parameters are invalid.
    - 404 Not Found: If the search logic for a valid target explicitly returns a "not found" error.
    - 500 Internal Server Error: For unexpected errors during the search process.
    """
    logger.info(f"Received POST /search/ request with target: {request.target}, query: '{request.query}'")

    if request.target not in SUPPORTED_SEARCH_TARGETS:
        logger.warning(f"Unsupported search target: {request.target}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, 
            detail=Errors.UNSUPPORTED_SEARCH_TARGET.format(target=request.target, supported_targets=", ".join(SUPPORTED_SEARCH_TARGETS))
        )

    if not request.query or len(request.query.strip()) < getattr(request, 'min_query_length', 2): # Assuming min_query_length might be in SearchRequest
        logger.warning(f"Search query too short: '{request.query}' for target {request.target}")
        pass


    if request.target == "players":
        return await _handle_search_logic_call(
            search_players_logic, "player search",
            request.query, limit=request.limit
        )
    elif request.target == "teams":
        return await _handle_search_logic_call(
            search_teams_logic, "team search",
            request.query, limit=request.limit
        )
    elif request.target == "games":
        return await _handle_search_logic_call(
            search_games_logic, "game search",
            request.query, 
            season=request.season, 
            season_type=request.season_type, 
            limit=request.limit
        )
    else:
        logger.error(f"Reached unexpected else block for search target: {request.target}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Internal server error: Search target processing failed.")

===== backend\routes\sse.py =====
"""
Handles Server-Sent Events (SSE) for streaming responses from the AI agent.
This module includes utilities for formatting SSE messages, recursively converting
complex objects to dictionaries for JSON serialization, and the main SSE route.
"""
import asyncio
import json
import logging
import re # For thinking pattern extraction
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from rich.pretty import pprint # For test_agent_stream
from agents import nba_agent
from dataclasses import is_dataclass # No need for asdict if using custom recursive one
from agno.agent import RunResponse # For type hinting in test_agent_stream
from typing import AsyncIterator, Dict, Any, Optional # Added Optional
from agno.utils.common import dataclass_to_dict # For test_agent_stream

logger = logging.getLogger(__name__)

router = APIRouter()

# --- Constants ---
MAX_RECURSION_DEPTH = 20  # Limit recursion depth for object serialization
SSE_LOG_TRUNCATE_LIMIT = 1000 # Max characters to log for an SSE message

STAT_CARD_MARKER = "STAT_CARD_JSON::"
CHART_DATA_MARKER = "CHART_DATA_JSON::"
TABLE_DATA_MARKER = "TABLE_DATA_JSON::"

# --- Helper Functions ---
def recursive_asdict(obj: Any, _depth: int = 0) -> Any:
    """
    Recursively converts an object (especially dataclasses and nested structures)
    into a dictionary suitable for JSON serialization, with a depth limit.
    Handles basic types, lists, dicts, dataclasses, and other objects with __dict__.
    Provides fallbacks for unserializable types.
    """
    if _depth > MAX_RECURSION_DEPTH:
        return f"<Max Recursion Depth Exceeded ({MAX_RECURSION_DEPTH})>"

    if obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    elif is_dataclass(obj):
        result = {}
        # Iterate through fields defined in __init__ or __slots__ if available,
        # otherwise fallback to __dict__ but be mindful of potential issues.
        # For simplicity, __dict__ is used here as in the original.
        for key, value in obj.__dict__.items():
            result[key] = recursive_asdict(value, _depth + 1)
        return result
    elif isinstance(obj, list):
        return [recursive_asdict(item, _depth + 1) for item in obj]
    elif isinstance(obj, dict) or type(obj).__name__ == 'mappingproxy':
        current_dict = dict(obj) # Ensure mutable dict
        return {key: recursive_asdict(value, _depth + 1) for key, value in current_dict.items()}
    elif hasattr(obj, "__dict__"): # For general objects
        try:
            obj_dict = dict(vars(obj))
            return recursive_asdict(obj_dict, _depth + 1)
        except Exception as e:
            logger.warning(f"Could not serialize object part using vars() for {type(obj).__name__}: {e}")
            return f"<Object of type {type(obj).__name__} partially unserializable>"
    else: # Fallback for other types
        try:
            return str(obj)
        except Exception:
            return f"<Unserializable Type: {type(obj).__name__}>"

def format_sse(data: str, event: Optional[str] = None) -> str:
    """Formats data into an SSE message string."""
    msg = f"data: {data}\n\n"
    if event is not None:
        msg = f"event: {event}\n{msg}"
    return msg

def format_message_data(chunk_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    Transforms a raw agent chunk dictionary into a structured message dictionary
    suitable for sending to the frontend via SSE.

    This function extracts relevant information like event type, content, tool calls,
    and metadata. It also formats tool calls for better display, calculates
    a simple progress indicator, and attempts to extract structured data
    (stat cards, charts, tables) if markers are present in the content.
    """
    event_type = chunk_dict.get("event")
    content = chunk_dict.get("content", "")
    tools = chunk_dict.get("tools", [])

    # Extract additional metadata from the chunk
    agent_id = chunk_dict.get("agent_id")
    session_id = chunk_dict.get("session_id")
    run_id = chunk_dict.get("run_id")
    model = chunk_dict.get("model")
    thinking = chunk_dict.get("thinking")
    reasoning_content = chunk_dict.get("reasoning_content")
    created_at = chunk_dict.get("created_at")

    # Extract any rich data markers from content if it's a string
    data_type = None
    data_payload = None

    if isinstance(content, str):
        # Check for rich data markers
        for marker, marker_type in [
            (STAT_CARD_MARKER, "STAT_CARD"),
            (CHART_DATA_MARKER, "CHART_DATA"),
            (TABLE_DATA_MARKER, "TABLE_DATA")
        ]:
            if marker in content:
                try:
                    marker_start = content.find(marker)
                    json_start = marker_start + len(marker)
                    # Extract the content before the marker
                    pre_marker_content = content[:marker_start].strip()
                    # Extract the JSON data after the marker
                    json_data = content[json_start:].strip()

                    # Set the data type and payload
                    data_type = marker_type
                    data_payload = json_data

                    # Update content to only include pre-marker text
                    content = pre_marker_content
                    break
                except Exception as e:
                    logger.warning(f"Failed to extract {marker_type} data: {e}")

    # Format tool calls with more details
    formatted_tool_calls = []
    if tools:
        for tool in tools:
            tool_name = tool.get("tool_name", "Unknown Tool")
            tool_status = "started" if event_type == "ToolCallStarted" else "completed"
            tool_args = tool.get("args", {})
            tool_content = tool.get("content", "")
            tool_error = None

            # Check if tool content contains an error
            if isinstance(tool_content, str) and tool_content.startswith('{"error":'):
                try:
                    error_data = json.loads(tool_content)
                    tool_error = error_data.get("error")
                    tool_status = "error"
                except:
                    pass

            formatted_tool_calls.append({
                "tool_name": tool_name,
                "status": tool_status,
                "args": tool_args,
                "content": tool_content,
                "error": tool_error
            })

    # Determine message status based on event type
    status = "thinking"
    if event_type == "RunCompleted":
        status = "complete"
    elif event_type == "Error":
        status = "error"
    elif event_type == "ToolCallStarted":
        status = "tool_calling"

    # Calculate progress
    progress = 0
    if event_type in ["RunStarted", "ToolCallStarted", "ToolCallCompleted", "RunCompleted"]:
        progress_map = {
            "RunStarted": 10,
            "ToolCallStarted": 30,
            "ToolCallCompleted": 70,
            "RunCompleted": 100
        }
        progress = progress_map.get(event_type, 0)

    # Build the enhanced message data structure
    message_data = {
        "role": "assistant",
        "content": content,
        "event": event_type,
        "status": status,
        "progress": progress,
        "toolCalls": formatted_tool_calls,
        "dataType": data_type,
        "dataPayload": data_payload,
        "metadata": {
            "agent_id": agent_id,
            "session_id": session_id,
            "run_id": run_id,
            "model": model,
            "timestamp": created_at
        }
    }

    # Extract thinking patterns from content
    thinking_patterns = {}
    if isinstance(content, str):
        # Look for patterns like **Thinking:** or **Planning:** or **Analyzing:** in string content
        # The 're' import is now at the top of the file.
        patterns = {
            "thinking": r"\*\*Thinking:\*\*(.*?)(?=\*\*|$)",
            "planning": r"\*\*Planning:\*\*(.*?)(?=\*\*|$)",
            "analyzing": r"\*\*Analyzing:\*\*(.*?)(?=\*\*|$)"
        }
        for pattern_type, pattern_regex in patterns.items(): # Renamed pattern to pattern_regex
            matches = re.findall(pattern_regex, content, re.DOTALL | re.IGNORECASE)
            if matches:
                thinking_patterns[pattern_type] = "\n".join([match.strip() for match in matches])

    # Add thinking/reasoning if available
    if thinking or reasoning_content or thinking_patterns: # Check if any thinking data exists
        message_data["reasoning"] = {
            "thinking": thinking,
            "content": reasoning_content,
            "patterns": thinking_patterns
        }

    # Set default content for certain events if no content is provided
    if not content:
        if event_type == "RunStarted":
            message_data["content"] = "Starting to process your request..."
        elif event_type == "ToolCallStarted" and formatted_tool_calls:
            message_data["content"] = f"Calling tool: {formatted_tool_calls[0]['tool_name']}..."
        elif event_type == "ToolCallCompleted" and formatted_tool_calls:
            if any(tool.get("error") for tool in formatted_tool_calls):
                message_data["content"] = "Tool call encountered an error."
            else:
                message_data["content"] = "Tool call completed successfully."
        elif event_type == "Error":
            message_data["content"] = "An error occurred during processing."

    return message_data

# --- SSE Route ---
@router.get("/ask")
async def ask_agent_keepalive_sse(request: Request, prompt: str) -> StreamingResponse:
    """
    Handles requests to the AI agent and streams responses back using SSE.
    It processes agent run chunks, formats them, and sends them to the client.
    Includes keep-alive messages and a final event upon completion.
    """
    logger.info(f"Received GET /ask request with prompt: '{prompt}'")
    workflow_instance = nba_agent # Assuming nba_agent is correctly initialized elsewhere

    async def keepalive_sse_generator() -> AsyncIterator[str]:
        """Generates SSE messages from the agent's run stream."""
        try:
            logger.info(f"Starting streaming NBA analysis workflow for prompt: {prompt}")
            async for chunk in await workflow_instance.arun(prompt): # Ensure arun is awaited if it's an async gen
                chunk_dict = recursive_asdict(chunk)
                message_data = format_message_data(chunk_dict)
                sse_message_string = format_sse(json.dumps(message_data))

                log_msg_preview = sse_message_string[:SSE_LOG_TRUNCATE_LIMIT]
                if len(sse_message_string) > SSE_LOG_TRUNCATE_LIMIT:
                    log_msg_preview += "..."
                logger.debug(f"Yielding SSE string (len={len(sse_message_string)}): {log_msg_preview}")

                yield sse_message_string
                await asyncio.sleep(0.01) # Small sleep to ensure flushing and allow other tasks

                if chunk_dict.get("event") == "RunCompleted":
                    logger.info("Detected RunCompleted event, preparing final SSE event.")
                    final_event_data = {
                        "role": "assistant",
                        "content": message_data.get("content", "Analysis complete."), # Use existing content or default
                        "event": "final",
                        "status": "complete",
                        "progress": 100,
                        "reasoning": message_data.get("reasoning"),
                        "metadata": message_data.get("metadata")
                    }
                    yield format_sse(json.dumps(final_event_data), event="final") # Send with 'final' event type
                    await asyncio.sleep(0.01) # Ensure final message is sent
                    logger.info("Final SSE event sent.")
                    break # Explicitly break after RunCompleted and final event

        except Exception as e:
            logger.exception("Error during SSE generation for /ask endpoint.")
            error_content = {
                "role": "assistant", "content": f"An error occurred: {str(e)}",
                "status": "error", "progress": 0, "event": "error"
            }
            yield format_sse(json.dumps(error_content), event="error")
        finally:
            logger.info("SSE generator for /ask finished.")

    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache, no-transform", # Important for SSE
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no", # Useful for Nginx
        # "Transfer-Encoding": "chunked" # Not typically set manually for StreamingResponse
    }
    return StreamingResponse(keepalive_sse_generator(), headers=headers, media_type="text/event-stream")

@router.get("/test_stream")
async def test_agent_stream(request: Request, prompt: str) -> None: # Returns None as it prints
    logger.info(f"Received GET /test_streaming request with prompt: '{prompt}'")
    run_stream: AsyncIterator[RunResponse] = await nba_agent.arun(prompt)
    async for chunk in run_stream:
        pprint(dataclass_to_dict(chunk, exclude={"messages"}))
        print("---" * 20)

===== backend\routes\standings.py =====
from fastapi import APIRouter, HTTPException, Query, status
from typing import Optional, Dict, Any
import json
import asyncio
from nba_api.stats.library.parameters import SeasonTypeAllStar, LeagueID
from backend.api_tools.league_standings import fetch_league_standings_logic
import logging
from backend.core.errors import Errors
from config import settings
from backend.utils.validation import validate_date_format, validate_season_format

logger = logging.getLogger(__name__)
router = APIRouter(
    prefix="/league",  # Standard prefix for league-related endpoints
    tags=["League"]    # Tag for OpenAPI documentation
)

async def _handle_league_route_logic_call(
    logic_function: callable, 
    endpoint_name: str,
    *args, 
    **kwargs
) -> Dict[str, Any]:
    """Helper to call league-related logic, parse JSON, and handle errors."""
    try:
        # Filter out None kwargs so logic functions can use their defaults
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        
        result_json_string = await asyncio.to_thread(logic_function, *args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} with args {args}, kwargs {filtered_kwargs}: {error_detail}")
            status_code_to_raise = status.HTTP_404_NOT_FOUND if "not found" in error_detail.lower() else \
                                   status.HTTP_400_BAD_REQUEST if "invalid" in error_detail.lower() else \
                                   status.HTTP_500_INTERNAL_SERVER_ERROR
            raise HTTPException(status_code=status_code_to_raise, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for args {args}, kwargs {filtered_kwargs}: {json_err}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for args {args}, kwargs {filtered_kwargs}: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()}")
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=detail_msg)

@router.get(
    "/standings",
    summary="Get League Standings",
    description="Fetches NBA league standings. Can be filtered by season, season type, league, and a specific date. "
                "If `date` is provided, `season` and `season_type` are ignored by the underlying API for V2 standings.",
    response_model=Dict[str, Any]
)
async def get_league_standings_endpoint(
    league_id: Optional[str] = Query(LeagueID.nba, description="League ID (e.g., '00' for NBA, '10' for WNBA, '20' for G-League). Defaults to NBA."),
    season: Optional[str] = Query(None, description=f"Season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic if date is not set.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(SeasonTypeAllStar.regular, description="Season type (e.g., 'Regular Season', 'Playoffs'). Defaults to Regular Season."),
    date: Optional[str] = Query(None, description="Specific date for standings in YYYY-MM-DD format. If provided, season/season_type might be ignored by API. Defaults to None.", regex=r"^\d{4}-\d{2}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get NBA league standings.
    Uses `fetch_league_standings_logic` from `league_tools.py`.

    Query Parameters:
    - **league_id** (str, optional): Defaults to "00" (NBA).
    - **season** (str, optional): YYYY-YY format. Defaults to current season in logic if `date` is not specified.
    - **season_type** (str, optional): e.g., "Regular Season", "Playoffs". Defaults to "Regular Season".
      Valid values are from `nba_api.stats.library.parameters.SeasonTypeAllStar`.
    - **date** (str, optional): YYYY-MM-DD format. If provided, fetches standings for this specific date.

    Successful Response (200 OK):
    Returns a dictionary containing parameters and a list of team standings.
    Example:
    ```json
    {
        "parameters": {
            "league_id": "00",
            "season": "2023-24",
            "season_type": "Regular Season",
            "date": null
        },
        "standings": [
            {
                "TeamID": 1610612738,
                "LeagueID": "00",
                "SeasonID": "22023",
                "TeamCity": "Boston",
                "TeamName": "Celtics",
                "Conference": "East",
                "PlayoffRank": 1,
                "WINS": 64,
                "LOSSES": 18,
                // ... many other fields like WinPct, Record, HomeRecord, RoadRecord, etc.
            },
            // ... more teams
        ]
    }
    ```
    If no standings are found for the criteria, `standings` will be an empty list.

    Error Responses:
    - 400 Bad Request: If query parameters are invalid (e.g., bad date/season format).
    - 500 Internal Server Error: For unexpected errors or issues fetching/processing data.
    """
    logger.info(f"Received GET /league/standings request. League: {league_id}, Season: {season}, Type: {season_type}, Date: {date}")

    # Validate date format if provided
    if date and not validate_date_format(date):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=Errors.INVALID_DATE_FORMAT.format(date=date)
        )
    if season and not date and not validate_season_format(season):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=Errors.INVALID_SEASON_FORMAT.format(season=season)
        )
    standings_kwargs_for_logic = {
        "season": season,
        "season_type": season_type,
        "date": date
    }
    
    return await _handle_league_route_logic_call(
        fetch_league_standings_logic, "league standings",
        **standings_kwargs_for_logic
    )

===== backend\routes\team.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, Path, status
from typing import Dict, Any, Optional
import json

from api_tools.team_info_roster import fetch_team_info_and_roster_logic
from api_tools.team_general_stats import fetch_team_stats_logic
from api_tools.team_player_dashboard import fetch_team_player_dashboard_logic
from api_tools.team_game_logs import fetch_team_game_logs_logic
from api_tools.team_dash_lineups import fetch_team_lineups_logic

from api_tools.team_passing_analytics import fetch_team_passing_stats_logic
from api_tools.team_rebounding_tracking import fetch_team_rebounding_stats_logic
from api_tools.team_shooting_tracking import fetch_team_shooting_stats_logic
from api_tools.team_estimated_metrics import fetch_team_estimated_metrics_logic
from services.team_analytics_service import get_team_comprehensive_analytics

from core.errors import Errors
from config import settings

router = APIRouter(
    prefix="/team", # Add prefix for all team routes
    tags=["Teams"]  # Tag for OpenAPI documentation
)
logger = logging.getLogger(__name__)

async def _handle_team_route_logic_call(
    logic_function: callable,
    team_identifier: str, # All team routes will have this
    endpoint_name: str,
    *args, # Positional args for the logic function (e.g., season)
    **kwargs # Keyword args for the logic function
) -> Dict[str, Any]:
    """Helper to call team-related logic, parse JSON, and handle errors."""
    try:
        # Ensure all args are passed correctly; logic functions handle defaults for None
        all_logic_args = (team_identifier,) + args
        # Filter out None kwargs so logic functions can use their defaults
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}

        result_json_string = await asyncio.to_thread(logic_function, *all_logic_args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for {team_identifier}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid team" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for {team_identifier}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in API route calling {func_name} for '{team_identifier}': {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing team data via {func_name}")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/{team_identifier}/stats",
    summary="Get Comprehensive Team Statistics",
    description="Fetches comprehensive team statistics for a specified team and season. "
                "Includes current season dashboard stats (based on `measure_type`) and historical year-by-year performance.",
    response_model=Dict[str, Any]
)
async def get_team_stats_endpoint( # Renamed for clarity
    team_identifier: str = Path(..., description="Team name (e.g., 'Boston Celtics'), abbreviation (e.g., 'BOS'), or ID (e.g., '1610612738')."),
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24') for dashboard stats. Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Type of season for dashboard stats (e.g., 'Regular Season', 'Playoffs'). Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Statistical mode for dashboard and historical stats (e.g., 'PerGame', 'Totals'). Logic default: 'PerGame'."),
    measure_type: Optional[str] = Query(None, description="Category of dashboard stats (e.g., 'Base', 'Advanced', 'Scoring'). Logic default: 'Base'."),
    opponent_team_id: Optional[int] = Query(0, description="Filter dashboard stats against a specific opponent team ID. Default: 0 (all).", ge=0),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD) for filtering dashboard games.", regex=r"^\d{4}-\d{2}-\d{2}$"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD) for filtering dashboard games.", regex=r"^\d{4}-\d{2}-\d{2}$"),
    league_id: Optional[str] = Query(None, description="League ID for historical stats (e.g., '00' for NBA). Logic default: '00'.")
) -> Dict[str, Any]:
    """
    Endpoint to get comprehensive team statistics.
    Uses `fetch_team_stats_logic` from `team_tools.py`.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**: For dashboard stats.
    - **season_type**: For dashboard stats.
    - **per_mode**: For dashboard and historical stats.
    - **measure_type**: For dashboard stats.
    - **opponent_team_id**: For dashboard stats.
    - **date_from**: For dashboard stats.
    - **date_to**: For dashboard stats.
    - **league_id**: For historical stats.

    Successful Response (200 OK):
    Returns a dictionary with team stats. Refer to `fetch_team_stats_logic` docstring in `team_tools.py`.
    Includes `current_stats` (dashboard) and `historical_stats`.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/stats request, Season: {season}, Measure: {measure_type}")
    return await _handle_team_route_logic_call(
        fetch_team_stats_logic, team_identifier, "team stats",
        season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type,
        opponent_team_id=opponent_team_id, date_from=date_from, date_to=date_to, league_id=league_id
    )

@router.get(
    "/{team_identifier}/info_roster",
    summary="Get Team Information and Roster",
    description="Fetches detailed team information, including conference/division ranks, current season roster, and coaching staff.",
    response_model=Dict[str, Any]
)
async def get_team_info_and_roster_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"NBA season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    league_id: Optional[str] = Query(None, description="League ID. Logic default: '00' (NBA).")
) -> Dict[str, Any]:
    """
    Endpoint to get team information and roster.
    Uses `fetch_team_info_and_roster_logic` from `team_tools.py`.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**
    - **season_type**
    - **league_id**

    Successful Response (200 OK):
    Returns a dictionary with team info and roster. Refer to `fetch_team_info_and_roster_logic` docstring.
    Includes `info`, `ranks`, `roster`, `coaches`.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/info_roster request, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_info_and_roster_logic, team_identifier, "team info/roster",
        season=season, season_type=season_type, league_id=league_id
    )

@router.get(
    "/{team_identifier}/players",
    summary="Get Team Player Dashboard",
    description="Fetches player statistics for all players on a team, including season totals and team overall stats.",
    response_model=Dict[str, Any]
)
async def get_team_player_dashboard_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"NBA season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'Totals'."),
    measure_type: Optional[str] = Query(None, description="Measure type (e.g., 'Base', 'Advanced'). Logic default: 'Base'.")
) -> Dict[str, Any]:
    """
    Endpoint to get team player dashboard statistics.
    Uses `fetch_team_player_dashboard_logic` from `team_player_dashboard.py`.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**
    - **season_type**
    - **per_mode**
    - **measure_type**

    Successful Response (200 OK):
    Returns a dictionary with player stats. Includes `players_season_totals` and `team_overall`.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/players request, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_player_dashboard_logic, team_identifier, "team player dashboard",
        season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type
    )

@router.get(
    "/{team_identifier}/schedule",
    summary="Get Team Schedule/Game Logs",
    description="Fetches game logs for a team, which includes schedule information and game results.",
    response_model=Dict[str, Any]
)
async def get_team_schedule_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"NBA season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'PerGame'."),
    measure_type: Optional[str] = Query(None, description="Measure type (e.g., 'Base', 'Advanced'). Logic default: 'Base'.")
) -> Dict[str, Any]:
    """
    Endpoint to get team schedule/game logs.
    Uses `fetch_team_game_logs_logic` from `team_game_logs.py`.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**
    - **season_type**
    - **per_mode**
    - **measure_type**

    Successful Response (200 OK):
    Returns a dictionary with game logs/schedule data.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/schedule request, Season: {season}")

    # Special handling for team_game_logs_logic which doesn't take team_identifier as first param
    try:
        # Filter out None kwargs so logic functions can use their defaults
        filtered_kwargs = {k: v for k, v in {
            'season': season,
            'season_type': season_type,
            'per_mode': per_mode,
            'measure_type': measure_type,
            'team_id': team_identifier  # Pass team_identifier as team_id
        }.items() if v is not None}

        result_json_string = await asyncio.to_thread(fetch_team_game_logs_logic, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from fetch_team_game_logs_logic for {team_identifier}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid team" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from fetch_team_game_logs_logic for {team_identifier}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in API route calling fetch_team_game_logs_logic for '{team_identifier}': {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing team schedule data")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/{team_identifier}/lineups",
    summary="Get Team Lineups",
    description="Fetches team lineup statistics and combinations.",
    response_model=Dict[str, Any]
)
async def get_team_lineups_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"NBA season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'Totals'."),
    measure_type: Optional[str] = Query(None, description="Measure type (e.g., 'Base', 'Advanced'). Logic default: 'Base'."),
    group_quantity: Optional[int] = Query(None, description="Number of players in lineup (2-5). Logic default: 5.")
) -> Dict[str, Any]:
    """
    Endpoint to get team lineup statistics.
    Uses `fetch_team_lineups_logic` from `team_dash_lineups.py`.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**
    - **season_type**
    - **per_mode**
    - **measure_type**
    - **group_quantity**

    Successful Response (200 OK):
    Returns a dictionary with lineup statistics data.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/lineups request, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_lineups_logic, team_identifier, "team lineups",
        season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, group_quantity=group_quantity
    )

@router.get(
    "/{team_identifier}/tracking/passing",
    summary="Get Team Passing Statistics",
    description="Fetches team passing statistics, detailing passes made and received among players.",
    response_model=Dict[str, Any]
)
async def get_team_passing_stats_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode (e.g., 'PerGame', 'Totals'). Logic default: 'PerGame'.")
) -> Dict[str, Any]:
    """
    Endpoint to get team passing statistics.
    Uses `fetch_team_passing_stats_logic` from `team_tracking.py`.

    Path Parameters:
    - **team_identifier** (str, required).

    Query Parameters (all optional, logic functions use defaults):
    - **season**
    - **season_type**
    - **per_mode**

    Successful Response (200 OK):
    Returns dict with `passes_made` and `passes_received`. Refer to `fetch_team_passing_stats_logic` docstring.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/tracking/passing request, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_passing_stats_logic, team_identifier, "team passing stats",
        season=season, season_type=season_type, per_mode=per_mode
    )

@router.get(
    "/{team_identifier}/tracking/rebounding",
    summary="Get Team Rebounding Statistics",
    description="Fetches team rebounding statistics, categorized by shot type, contest, and distances.",
    response_model=Dict[str, Any]
)
async def get_team_rebounding_stats_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode. Logic default: 'PerGame'."),
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID. Default: 0 (all).", ge=0),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD).", regex=r"^\d{4}-\d{2}-\d{2}$"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD).", regex=r"^\d{4}-\d{2}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get team rebounding statistics.
    Uses `fetch_team_rebounding_stats_logic` from `team_tracking.py`.

    Path Parameters:
    - **team_identifier** (str, required).

    Query Parameters (all optional, logic functions use defaults):
    - **season**, **season_type**, **per_mode**
    - **opponent_team_id**, **date_from**, **date_to**

    Successful Response (200 OK):
    Returns dict with rebounding splits. Refer to `fetch_team_rebounding_stats_logic` docstring.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/tracking/rebounding, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_rebounding_stats_logic, team_identifier, "team rebounding stats",
        season=season, season_type=season_type, per_mode=per_mode,
        opponent_team_id=opponent_team_id, date_from=date_from, date_to=date_to
    )

@router.get(
    "/{team_identifier}/tracking/shooting",
    summary="Get Team Shooting Statistics",
    description="Fetches team shooting statistics, categorized by shot clock, dribbles, defender distance, etc.",
    response_model=Dict[str, Any]
)
async def get_team_shooting_stats_endpoint(
    team_identifier: str = Path(..., description="Team name, abbreviation, or ID."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Season type. Logic default: 'Regular Season'."),
    per_mode: Optional[str] = Query(None, description="Per mode. Logic default: 'PerGame'."),
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID. Default: 0 (all).", ge=0),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD).", regex=r"^\d{4}-\d{2}-\d{2}$"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD).", regex=r"^\d{4}-\d{2}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get team shooting statistics.
    Uses `fetch_team_shooting_stats_logic` from `team_tracking.py`.

    Path Parameters:
    - **team_identifier** (str, required).

    Query Parameters (all optional, logic functions use defaults):
    - **season**, **season_type**, **per_mode**
    - **opponent_team_id**, **date_from**, **date_to**

    Successful Response (200 OK):
    Returns dict with shooting splits. Refer to `fetch_team_shooting_stats_logic` docstring.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/tracking/shooting, Season: {season}")
    return await _handle_team_route_logic_call(
        fetch_team_shooting_stats_logic, team_identifier, "team shooting stats",
        season=season, season_type=season_type, per_mode=per_mode,
        opponent_team_id=opponent_team_id, date_from=date_from, date_to=date_to
    )

@router.get(
    "/estimated-metrics",
    summary="Get All Teams Estimated Metrics",
    description="Fetches estimated metrics for all NBA teams including offensive/defensive ratings, pace, etc.",
    response_model=Dict[str, Any]
)
async def get_all_teams_estimated_metrics_endpoint(
    league_id: Optional[str] = Query("00", description="League ID. Default: '00' (NBA)."),
    season: Optional[str] = Query(None, description=f"Season (YYYY-YY). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"),
    season_type: Optional[str] = Query("", description="Season type. Default: '' (Regular Season).")
) -> Dict[str, Any]:
    """
    Endpoint to get estimated metrics for all teams.
    Uses `fetch_team_estimated_metrics_logic` from `team_estimated_metrics.py`.

    Query Parameters (all optional):
    - **league_id** (str): League ID. Default: "00".
    - **season** (str): Season in YYYY-YY format. Defaults to current season.
    - **season_type** (str): Season type. Default: "".

    Successful Response (200 OK):
    Returns a dictionary with estimated metrics for all teams.

    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/estimated-metrics request, Season: {season}")

    try:
        # Filter out None kwargs so logic functions can use their defaults
        filtered_kwargs = {k: v for k, v in {
            'league_id': league_id,
            'season': season,
            'season_type': season_type
        }.items() if v is not None}

        result_json_string = await asyncio.to_thread(fetch_team_estimated_metrics_logic, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from fetch_team_estimated_metrics_logic: {error_detail}")
            status_code = 400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        logger.error(f"Failed to parse JSON response from fetch_team_estimated_metrics_logic: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        logger.critical(f"Unexpected error in API route calling fetch_team_estimated_metrics_logic: {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing team estimated metrics")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/{team_id}/comprehensive-analytics",
    summary="Get Comprehensive Team Analytics",
    description="Fetches comprehensive team analytics including advanced metrics, player analysis, and league rankings.",
    response_model=Dict[str, Any]
)
async def get_team_comprehensive_analytics_endpoint(
    team_id: int = Path(..., description="Team ID (e.g., 1610612739 for Cleveland Cavaliers)."),
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON}.", regex=r"^\d{4}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get comprehensive team analytics.

    This endpoint provides:
    - Team basic and advanced statistics
    - Player roster with advanced metrics
    - Schedule analysis and performance trends
    - Lineup analysis and combinations
    - League rankings and comparisons
    - Advanced metrics similar to dunksandthrees.com and craftednba.com

    Path Parameters:
    - **team_id** (int, required): NBA team ID.

    Query Parameters:
    - **season** (str, optional): YYYY-YY format. Defaults to current season.

    Successful Response (200 OK):
    Returns comprehensive team analytics with all data efficiently loaded.

    Error Responses:
    - 400 Bad Request: If team ID or season format is invalid.
    - 404 Not Found: If team not found.
    - 500 Internal Server Error: For unexpected errors.
    """
    logger.info(f"Received GET /team/{team_id}/comprehensive-analytics request. Season: {season}")

    season_to_use = season or settings.CURRENT_NBA_SEASON

    try:
        result = await get_team_comprehensive_analytics(team_id, season_to_use)

        if 'error' in result:
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Team {team_id} not found or error loading data: {result['error']}")

        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in comprehensive team analytics endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Error loading comprehensive analytics: {str(e)}")

===== backend\routes\team_tracking.py =====
import logging
import asyncio
from fastapi import APIRouter, HTTPException, Query, Path
from typing import Dict, Any, Optional
import json

from backend.api_tools.team_passing_analytics import fetch_team_passing_stats_logic
from backend.api_tools.team_rebounding_tracking import fetch_team_rebounding_stats_logic
from backend.api_tools.team_shooting_tracking import fetch_team_shooting_stats_logic

from backend.core.errors import Errors
from config import settings

router = APIRouter(
    prefix="/team", 
    tags=["Team Tracking"]
)
logger = logging.getLogger(__name__)

async def _handle_team_tracking_logic_call(
    logic_function: callable,
    team_identifier: str,
    endpoint_name: str,
    *args,
    **kwargs
) -> Dict[str, Any]:
    """Helper to call team tracking logic, parse JSON, and handle errors."""
    try:
        all_logic_args = (team_identifier,) + args
        filtered_kwargs = {k: v for k, v in kwargs.items() if v is not None}
        
        result_json_string = await asyncio.to_thread(logic_function, *all_logic_args, **filtered_kwargs)
        result_data = json.loads(result_json_string)

        if isinstance(result_data, dict) and 'error' in result_data:
            error_detail = result_data['error']
            logger.error(f"Error from {logic_function.__name__} for {team_identifier}: {error_detail}")
            status_code = 404 if "not found" in error_detail.lower() or "invalid team" in error_detail.lower() else \
                          400 if "invalid" in error_detail.lower() else 500
            raise HTTPException(status_code=status_code, detail=error_detail)
        return result_data
    except HTTPException as http_exc:
        raise http_exc
    except json.JSONDecodeError as json_err:
        func_name = logic_function.__name__
        logger.error(f"Failed to parse JSON response from {func_name} for {team_identifier}: {json_err}", exc_info=True)
        raise HTTPException(status_code=500, detail=Errors.JSON_PROCESSING_ERROR)
    except Exception as e:
        func_name = logic_function.__name__
        logger.critical(f"Unexpected error in {endpoint_name} for '{team_identifier}': {str(e)}", exc_info=True)
        error_msg_template = getattr(Errors, 'UNEXPECTED_ERROR', "An unexpected server error occurred: {error}")
        detail_msg = error_msg_template.format(error=f"processing {endpoint_name.lower()} for team '{team_identifier}'")
        raise HTTPException(status_code=500, detail=detail_msg)

@router.get(
    "/{team_identifier}/tracking/stats_all",
    summary="Get All Team Tracking Statistics (Combined)",
    description="Fetches a comprehensive set of team tracking statistics including passing, rebounding, "
                "and shooting data for a specified team and season. This endpoint makes multiple "
                "underlying API calls.",
    response_model=Dict[str, Any]
)
async def get_all_team_tracking_stats_endpoint(
    team_identifier: str = Path(..., description="Team name (e.g., 'Boston Celtics'), abbreviation (e.g., 'BOS'), or ID (e.g., '1610612738')."),
    season: Optional[str] = Query(None, description=f"NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to {settings.CURRENT_NBA_SEASON} in logic.", regex=r"^\d{4}-\d{2}$"), # Changed
    season_type: Optional[str] = Query(None, description="Type of season (e.g., 'Regular Season', 'Playoffs'). Defaults to 'Regular Season' in logic."),
    per_mode: Optional[str] = Query(None, description="Per mode for all tracking types (e.g., 'PerGame', 'Totals'). Logic functions use their own defaults if not specified here."),
    opponent_team_id: Optional[int] = Query(0, description="Filter by opponent team ID for applicable tracking types (rebounding, shooting). Default 0 (all).", ge=0),
    date_from: Optional[str] = Query(None, description="Start date (YYYY-MM-DD) for applicable tracking types (rebounding, shooting).", regex=r"^\d{4}-\d{2}-\d{2}$"),
    date_to: Optional[str] = Query(None, description="End date (YYYY-MM-DD) for applicable tracking types (rebounding, shooting).", regex=r"^\d{4}-\d{2}-\d{2}$")
) -> Dict[str, Any]:
    """
    Endpoint to get a combined set of team tracking statistics.

    Path Parameters:
    - **team_identifier** (str, required): Team name, abbreviation, or ID.

    Query Parameters (all optional, logic functions use defaults):
    - **season**, **season_type**, **per_mode**
    - **opponent_team_id** (for rebounding, shooting)
    - **date_from** (for rebounding, shooting)
    - **date_to** (for rebounding, shooting)

    Successful Response (200 OK):
    Returns a dictionary containing all tracking stats. Structure:
    ```json
    {
        "team_identifier_requested": "Boston Celtics",
        "team_id_resolved": 1610612738, // Example
        "team_name_resolved": "Boston Celtics", // Example
        "season_requested": "2023-24", // Or null
        "passing_stats": { /* ... data from fetch_team_passing_stats_logic ... */ },
        "rebounding_stats": { /* ... data from fetch_team_rebounding_stats_logic ... */ },
        "shooting_stats": { /* ... data from fetch_team_shooting_stats_logic ... */ }
    }
    ```
    Error Responses: 400, 404, 500.
    """
    logger.info(f"Received GET /team/{team_identifier}/tracking/stats_all request, Season: {season}")

    common_kwargs = {"season": season, "season_type": season_type, "per_mode": per_mode}
    reb_shoot_kwargs = {**common_kwargs, "opponent_team_id": opponent_team_id, "date_from": date_from, "date_to": date_to}

    try:
        results = await asyncio.gather(
            _handle_team_tracking_logic_call(fetch_team_passing_stats_logic, team_identifier, "team passing stats", **common_kwargs),
            _handle_team_tracking_logic_call(fetch_team_rebounding_stats_logic, team_identifier, "team rebounding stats", **reb_shoot_kwargs),
            _handle_team_tracking_logic_call(fetch_team_shooting_stats_logic, team_identifier, "team shooting stats", **reb_shoot_kwargs),
            return_exceptions=False
        )
        passing_data, rebounding_data, shooting_data = results
        
        team_id_resolved = passing_data.get("team_id") or rebounding_data.get("team_id") or shooting_data.get("team_id")
        team_name_resolved = passing_data.get("team_name", team_identifier) 

        combined_result = {
            "team_identifier_requested": team_identifier,
            "team_id_resolved": team_id_resolved,
            "team_name_resolved": team_name_resolved,
            "season_requested": season,
            "passing_stats": passing_data,
            "rebounding_stats": rebounding_data,
            "shooting_stats": shooting_data
        }
        return combined_result
    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        logger.critical(f"Unexpected error in combined team tracking stats for {team_identifier}: {str(e)}", exc_info=True)
        error_msg = Errors.UNEXPECTED_ERROR.format(error=f"fetching combined tracking stats for team '{team_identifier}'")
        raise HTTPException(status_code=500, detail=error_msg)

===== backend\routes\__init__.py =====
"""Routes for the NBA agent backend."""

===== backend\smoke_tests\test_game_boxscores.py =====
"""
Smoke test for the game_boxscores module.
Tests the functionality of fetching various types of box score data for NBA games.
"""
import os
import sys # Import sys

# Add the project root directory to the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(backend_dir)
sys.path.insert(0, project_root)

import json
import pandas as pd
from datetime import datetime



from backend.api_tools.game_boxscores import (
    fetch_boxscore_traditional_logic,
    fetch_boxscore_advanced_logic,
    fetch_boxscore_four_factors_logic,
    fetch_boxscore_usage_logic,
    fetch_boxscore_defensive_logic,
    fetch_boxscore_summary_logic,
    fetch_boxscore_misc_logic,
    fetch_boxscore_playertrack_logic,
    fetch_boxscore_scoring_logic,
    fetch_boxscore_hustle_logic,
    BOXSCORE_CSV_DIR
)

# Sample game ID for testing (2023-24 regular season game)
SAMPLE_GAME_ID = "0022300161"  # Change this to a valid game ID if needed

def test_fetch_boxscore_traditional():
    """Test fetching traditional box score data."""
    print("\n=== Testing fetch_boxscore_traditional_logic ===")

    # Test with default parameters (JSON output)
    json_response = fetch_boxscore_traditional_logic(SAMPLE_GAME_ID)

    # Parse the JSON response
    data = json.loads(json_response)

    # Check if the response has the expected structure
    assert isinstance(data, dict), "Response should be a dictionary"

    # Check if there's an error in the response
    if "error" in data:
        print(f"API returned an error: {data['error']}")
        print("This might be expected if the NBA API is unavailable or rate-limited.")
        print("Continuing with other tests...")
    else:
        # Check if the game_id field exists and matches the input
        assert "game_id" in data, "Response should have a 'game_id' field"
        assert data["game_id"] == SAMPLE_GAME_ID, f"game_id should be {SAMPLE_GAME_ID}"

        # Check if the teams field exists and is a list
        assert "teams" in data, "Response should have a 'teams' field"
        assert isinstance(data["teams"], list), "'teams' field should be a list"

        # Check if the players field exists and is a list
        assert "players" in data, "Response should have a 'players' field"
        assert isinstance(data["players"], list), "'players' field should be a list"

        # Print some information about the data
        print(f"Number of teams: {len(data['teams'])}")
        print(f"Number of players: {len(data['players'])}")

        if data["players"]:
            # Print details of the first player
            first_player = data["players"][0]
            print("\nFirst player details:")
            print(f"Player Name: {first_player.get('PLAYER', 'N/A')}")
            print(f"Team: {first_player.get('TEAM', 'N/A')}")
            print(f"Minutes: {first_player.get('MIN', 'N/A')}")
            print(f"Points: {first_player.get('PTS', 'N/A')}")
            print(f"Rebounds: {first_player.get('REB', 'N/A')}")
            print(f"Assists: {first_player.get('AST', 'N/A')}")

    print("\n=== JSON test completed ===")

    # Test with return_dataframe=True
    print("\n=== Testing fetch_boxscore_traditional_logic with DataFrame output ===")
    result = fetch_boxscore_traditional_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"
    assert len(result) == 2, "Result tuple should have 2 elements"

    json_response, dataframes = result

    # Check if the first element is a JSON string
    assert isinstance(json_response, str), "First element should be a JSON string"

    # Check if the second element is a dictionary of DataFrames
    assert isinstance(dataframes, dict), "Second element should be a dictionary of DataFrames"

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        print(f"\nDataFrame '{key}' shape: {df.shape}")
        print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns

    # Check if the CSV files were created
    csv_files = [f for f in os.listdir(BOXSCORE_CSV_DIR) if f.startswith(SAMPLE_GAME_ID)]
    print(f"\nCSV files created: {csv_files}")

    # Display a sample of one DataFrame if not empty
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nSample of DataFrame '{key}' (first 3 rows):")
            print(df.head(3))
            break

    print("\n=== DataFrame test completed ===")
    return json_response, dataframes

def test_fetch_boxscore_advanced():
    """Test fetching advanced box score data with DataFrame output."""
    print("\n=== Testing fetch_boxscore_advanced_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_boxscore_advanced_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"

    json_response, dataframes = result

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nDataFrame '{key}' shape: {df.shape}")
            print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns
            print(f"Sample data (first row):")
            # Print the first row with all columns
            first_row_dict = {col: df.iloc[0][col] for col in df.columns[:5]}  # First 5 columns
            print(first_row_dict)

    print("\n=== Advanced box score test completed ===")
    return json_response, dataframes

def test_other_boxscore_types():
    """Test fetching other types of box score data."""
    print("\n=== Testing other box score types ===")

    # Test Four Factors
    print("\nTesting Four Factors box score:")
    result = fetch_boxscore_four_factors_logic(SAMPLE_GAME_ID, return_dataframe=True)
    json_response, dataframes = result
    print(f"Four Factors DataFrames returned: {list(dataframes.keys())}")

    # Test Usage
    print("\nTesting Usage box score:")
    result = fetch_boxscore_usage_logic(SAMPLE_GAME_ID, return_dataframe=True)
    json_response, dataframes = result
    print(f"Usage DataFrames returned: {list(dataframes.keys())}")

    # Test Defensive
    print("\nTesting Defensive box score:")
    result = fetch_boxscore_defensive_logic(SAMPLE_GAME_ID, return_dataframe=True)
    json_response, dataframes = result
    print(f"Defensive DataFrames returned: {list(dataframes.keys())}")

    # Test Summary
    print("\nTesting Summary box score:")
    result = fetch_boxscore_summary_logic(SAMPLE_GAME_ID, return_dataframe=True)
    json_response, dataframes = result
    print(f"Summary DataFrames returned: {list(dataframes.keys())}")

    # Check CSV files
    csv_files = [f for f in os.listdir(BOXSCORE_CSV_DIR) if f.startswith(SAMPLE_GAME_ID)]
    print(f"\nTotal CSV files created: {len(csv_files)}")
    print(f"CSV files: {csv_files[:5]}...")  # Show first 5 files

    print("\n=== Other box score types test completed ===")

def test_fetch_boxscore_misc():
    """Test fetching miscellaneous box score data with DataFrame output."""
    print("\n=== Testing fetch_boxscore_misc_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_boxscore_misc_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"

    json_response, dataframes = result

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nDataFrame '{key}' shape: {df.shape}")
            print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns
            print(f"Sample data (first row):")
            first_row_dict = {col: df.iloc[0][col] for col in df.columns[:5]}  # First 5 columns
            print(first_row_dict)

    print("\n=== Miscellaneous box score test completed ===")
    return json_response, dataframes

def test_fetch_boxscore_playertrack():
    """Test fetching player tracking box score data with DataFrame output."""
    print("\n=== Testing fetch_boxscore_playertrack_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_boxscore_playertrack_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"

    json_response, dataframes = result

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nDataFrame '{key}' shape: {df.shape}")
            print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns
            print(f"Sample data (first row):")
            first_row_dict = {col: df.iloc[0][col] for col in df.columns[:5]}  # First 5 columns
            print(first_row_dict)

    print("\n=== Player tracking box score test completed ===")
    return json_response, dataframes

def test_fetch_boxscore_scoring():
    """Test fetching scoring box score data with DataFrame output."""
    print("\n=== Testing fetch_boxscore_scoring_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_boxscore_scoring_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"

    json_response, dataframes = result

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nDataFrame '{key}' shape: {df.shape}")
            print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns
            print(f"Sample data (first row):")
            first_row_dict = {col: df.iloc[0][col] for col in df.columns[:5]}  # First 5 columns
            print(first_row_dict)

    print("\n=== Scoring box score test completed ===")
    return json_response, dataframes

def test_fetch_boxscore_hustle():
    """Test fetching hustle box score data with DataFrame output."""
    print("\n=== Testing fetch_boxscore_hustle_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_boxscore_hustle_logic(SAMPLE_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"

    json_response, dataframes = result

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nDataFrame '{key}' shape: {df.shape}")
            print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns
            print(f"Sample data (first row):")
            first_row_dict = {col: df.iloc[0][col] for col in df.columns[:5]}  # First 5 columns
            print(first_row_dict)

    print("\n=== Hustle box score test completed ===")
    return json_response, dataframes

def run_all_tests():
    """Run all tests in sequence."""
    print(f"=== Running game_boxscores smoke tests at {datetime.now().isoformat()} ===\n")

    try:
        # Run the tests
        traditional_json, traditional_dfs = test_fetch_boxscore_traditional()
        advanced_json, advanced_dfs = test_fetch_boxscore_advanced()
        test_other_boxscore_types()
        test_fetch_boxscore_misc()
        test_fetch_boxscore_playertrack()
        test_fetch_boxscore_scoring()
        hustle_json, hustle_dfs = test_fetch_boxscore_hustle()

        print("\n=== All tests completed successfully ===")
        return True
    except Exception as e:
        print(f"\n!!! Test failed with error: {str(e)} !!!")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import sys
    # Add the parent directory to the Python path
    sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    success = run_all_tests()
    sys.exit(0 if success else 1)


===== backend\smoke_tests\test_game_boxscore_matchups.py =====
"""
Smoke tests for the game_boxscore_matchups module.
Tests fetching player matchup data using BoxScoreMatchupsV3 endpoint.
"""
import os
import sys
import json
import pandas as pd
from typing import Dict, Any
from datetime import datetime

# Add the parent directory to sys.path to allow importing from backend
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(backend_dir)
sys.path.insert(0, project_root)

from backend.api_tools.game_boxscore_matchups import (
    fetch_game_boxscore_matchups_logic,
    BOXSCORE_MATCHUPS_CSV_DIR
)

# Test constants
TEST_GAME_ID = "0022300001"  # Use a known game ID for testing
INVALID_GAME_ID = "invalid_id"
NONEXISTENT_GAME_ID = "9999999999"  # Valid format but doesn't exist

def test_fetch_game_boxscore_matchups_basic():
    """Test basic functionality of fetch_game_boxscore_matchups_logic."""
    print("\n=== Testing fetch_game_boxscore_matchups_logic ===")

    # Call the function
    json_response = fetch_game_boxscore_matchups_logic(TEST_GAME_ID)

    # Parse the JSON response
    data = json.loads(json_response)

    # Check if the response has the expected structure
    assert isinstance(data, dict), "Response should be a dictionary"

    # Check if there's an error in the response
    if "error" in data:
        print(f"API returned an error: {data['error']}")
        print("This might be expected if the NBA API is unavailable or rate-limited.")
        print("Continuing with other tests...")
    else:
        # Check if the game_id field exists and matches the input
        assert "game_id" in data, "Response should have a 'game_id' field"
        assert data["game_id"] == TEST_GAME_ID, f"game_id should be {TEST_GAME_ID}"

        # Check if the matchups field exists and is a list
        assert "matchups" in data, "Response should have a 'matchups' field"
        assert isinstance(data["matchups"], list), "'matchups' field should be a list"

        # Print some information about the data
        print(f"Number of matchups: {len(data['matchups'])}")

        if data["matchups"]:
            # Print details of the first matchup
            first_matchup = data["matchups"][0]
            print("\nFirst matchup details:")
            print(f"Offensive Player: {first_matchup.get('nameIOff', 'N/A')}")
            print(f"Defensive Player: {first_matchup.get('nameIDef', 'N/A')}")
            print(f"Matchup Minutes: {first_matchup.get('matchupMinutes', 'N/A')}")
            print(f"Points Scored: {first_matchup.get('playerPoints', 'N/A')}")
            print(f"FG: {first_matchup.get('matchupFieldGoalsMade', 'N/A')}/{first_matchup.get('matchupFieldGoalsAttempted', 'N/A')} ({first_matchup.get('matchupFieldGoalsPercentage', 'N/A')})")

    print("\n=== JSON test completed ===")

    return json_response

def test_fetch_game_boxscore_matchups_with_dataframe():
    """Test fetch_game_boxscore_matchups_logic with DataFrame output."""
    print("\n=== Testing fetch_game_boxscore_matchups_logic with DataFrame output ===")

    # Test with return_dataframe=True
    result = fetch_game_boxscore_matchups_logic(TEST_GAME_ID, return_dataframe=True)

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"
    assert len(result) == 2, "Result tuple should have 2 elements"

    json_response, dataframes = result

    # Check if the first element is a JSON string
    assert isinstance(json_response, str), "First element should be a JSON string"

    # Check if the second element is a dictionary of DataFrames
    assert isinstance(dataframes, dict), "Second element should be a dictionary of DataFrames"

    # Print DataFrame info
    print(f"\nDataFrames returned: {list(dataframes.keys())}")
    for key, df in dataframes.items():
        print(f"\nDataFrame '{key}' shape: {df.shape}")
        print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns

    # Check if the CSV files were created
    csv_files = [f for f in os.listdir(BOXSCORE_MATCHUPS_CSV_DIR) if f.startswith(TEST_GAME_ID)]
    print(f"\nCSV files created: {csv_files}")

    # Display a sample of one DataFrame if not empty
    for key, df in dataframes.items():
        if not df.empty:
            print(f"\nSample of DataFrame '{key}' (first 3 rows):")
            print(df.head(3)[['gameId', 'nameIOff', 'nameIDef', 'matchupMinutes', 'playerPoints']])
            break

    print("\n=== DataFrame test completed ===")
    return json_response, dataframes

def test_fetch_game_boxscore_matchups_invalid_id():
    """Test fetch_game_boxscore_matchups_logic with an invalid game ID."""
    print("\n=== Testing fetch_game_boxscore_matchups_logic with invalid game ID ===")

    # Call the function with an invalid game ID
    result = fetch_game_boxscore_matchups_logic(INVALID_GAME_ID)

    # Parse the JSON response
    response = json.loads(result)

    # Check for error
    assert "error" in response, "Expected error for invalid game ID"
    print(f"Error message: {response['error']}")

    print("\n=== Invalid game ID test completed ===")

def test_fetch_game_boxscore_matchups_empty_id():
    """Test fetch_game_boxscore_matchups_logic with an empty game ID."""
    print("\n=== Testing fetch_game_boxscore_matchups_logic with empty game ID ===")

    # Call the function with an empty game ID
    result = fetch_game_boxscore_matchups_logic("")

    # Parse the JSON response
    response = json.loads(result)

    # Check for error
    assert "error" in response, "Expected error for empty game ID"
    print(f"Error message: {response['error']}")

    print("\n=== Empty game ID test completed ===")

def run_all_tests():
    """Run all tests in sequence."""
    print(f"=== Running game_boxscore_matchups smoke tests at {datetime.now().isoformat()} ===\n")

    try:
        # Run the tests
        json_response = test_fetch_game_boxscore_matchups_basic()
        json_response, dataframes = test_fetch_game_boxscore_matchups_with_dataframe()
        test_fetch_game_boxscore_matchups_invalid_id()
        test_fetch_game_boxscore_matchups_empty_id()

        print("\n=== All tests completed successfully ===")
        return True
    except Exception as e:
        print(f"\n!!! Test failed with error: {str(e)} !!!")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)


===== backend\smoke_tests\test_hustle_stats_boxscore.py =====
"""
Smoke tests for the hustle_stats_boxscore module using real API calls.

These tests verify that the hustle stats boxscore API functions work correctly
by making actual calls to the NBA API.
"""

import os
import sys
import unittest
import pandas as pd
import time
import json

# Add the parent directory to the path so we can import the api_tools module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from api_tools.hustle_stats_boxscore import (
    fetch_hustle_stats_logic,
    get_hustle_stats_boxscore,
    _get_csv_path_for_hustle_stats,
    HUSTLE_STATS_CSV_DIR
)


class TestHustleStatsBoxScoreReal(unittest.TestCase):
    """Test cases for the hustle_stats_boxscore module using real API calls."""

    def setUp(self):
        """Set up the test environment."""
        # Create the cache directory if it doesn't exist
        os.makedirs(HUSTLE_STATS_CSV_DIR, exist_ok=True)

    def tearDown(self):
        """Clean up after the tests."""
        # We're keeping the CSV files for inspection, so no cleanup needed
        pass

    def test_get_csv_path_for_hustle_stats(self):
        """Test that the CSV path is generated correctly."""
        # Test with default parameters
        path = _get_csv_path_for_hustle_stats("0022400001")
        self.assertIn("hustle_stats_game0022400001", path)
        self.assertIn("HustleStats", path)
        
        # Test with custom parameters
        path = _get_csv_path_for_hustle_stats(
            game_id="0022400002",
            data_set_name="PlayerStats"
        )
        self.assertIn("hustle_stats_game0022400002", path)
        self.assertIn("PlayerStats", path)

    def test_fetch_hustle_stats_logic_json(self):
        """Test fetching hustle stats in JSON format."""
        # Call the function with real API
        json_response = fetch_hustle_stats_logic(
            game_id="0022400001"
        )
        
        # Parse the JSON response
        data = json.loads(json_response)
        
        # Verify the result
        self.assertIsInstance(data, dict)
        
        # Check if there's an error in the response
        if "error" in data:
            print(f"API returned an error: {data['error']}")
            print("This might be expected if the NBA API is unavailable or rate-limited.")
        else:
            # Check parameters
            self.assertIn("parameters", data)
            self.assertEqual(data["parameters"]["game_id"], "0022400001")
            
            # Check data sets
            self.assertIn("data_sets", data)
            data_sets = data["data_sets"]
            self.assertIsInstance(data_sets, dict)
            self.assertGreater(len(data_sets), 0)
            
            # Print information about the data
            print(f"Parameters: {data.get('parameters', {})}")
            print(f"Data Sets: {list(data_sets.keys())}")
            
            # Check the main data sets
            if "PlayerStats" in data_sets:
                player_stats = data_sets["PlayerStats"]
                self.assertIsInstance(player_stats, list)
                self.assertGreater(len(player_stats), 15)  # Should have 19-22 players
                print(f"Player Stats count: {len(player_stats)}")
                print(f"Sample player: {player_stats[0] if player_stats else 'No data'}")
            
            if "TeamStats" in data_sets:
                team_stats = data_sets["TeamStats"]
                self.assertIsInstance(team_stats, list)
                self.assertEqual(len(team_stats), 2)  # Should have 2 teams
                print(f"Team Stats count: {len(team_stats)}")
                print(f"Sample team: {team_stats[0] if team_stats else 'No data'}")

    def test_fetch_hustle_stats_logic_dataframe(self):
        """Test fetching hustle stats in DataFrame format."""
        # Call the function with real API
        json_response, dataframes = fetch_hustle_stats_logic(
            game_id="0022400002",
            return_dataframe=True
        )
        
        # Parse the JSON response
        data = json.loads(json_response)
        
        # Verify the result
        self.assertIsInstance(data, dict)
        self.assertIsInstance(dataframes, dict)
        
        # Check if there's an error in the response
        if "error" in data:
            print(f"API returned an error: {data['error']}")
            print("This might be expected if the NBA API is unavailable or rate-limited.")
        else:
            # Check parameters
            self.assertIn("parameters", data)
            self.assertEqual(data["parameters"]["game_id"], "0022400002")
            
            # Check dataframes
            self.assertGreater(len(dataframes), 0)
            
            for data_set_name, df in dataframes.items():
                self.assertIsInstance(df, pd.DataFrame)
                print(f"DataFrame '{data_set_name}' shape: {df.shape}")
                print(f"Columns: {df.columns.tolist()}")
                
                # Verify expected columns for hustle stats
                if data_set_name == "PlayerStats":
                    expected_columns = ["GAME_ID", "PLAYER_ID", "PLAYER_NAME", "CONTESTED_SHOTS", "DEFLECTIONS", "LOOSE_BALLS_RECOVERED"]
                    for col in expected_columns:
                        if col in df.columns:
                            print(f"Found expected column: {col}")
                elif data_set_name == "TeamStats":
                    expected_columns = ["GAME_ID", "TEAM_ID", "TEAM_NAME", "CONTESTED_SHOTS", "DEFLECTIONS"]
                    for col in expected_columns:
                        if col in df.columns:
                            print(f"Found expected column: {col}")
            
            # Verify CSV files were created
            for data_set_name in dataframes.keys():
                csv_path = _get_csv_path_for_hustle_stats("0022400002", data_set_name)
                if os.path.exists(csv_path):
                    print(f"CSV file created: {csv_path}")
                    print(f"File size: {os.path.getsize(csv_path)} bytes")
                else:
                    print(f"CSV file was not created at: {csv_path}")

    def test_fetch_hustle_stats_logic_csv_cache(self):
        """Test that CSV caching works correctly."""
        # Parameters for testing
        params = {
            "game_id": "0022400003"
        }
        
        # Check if CSV already exists
        csv_paths = []
        for data_set_name in ["GameStatus", "PlayerStats", "TeamStats"]:
            csv_path = _get_csv_path_for_hustle_stats(params["game_id"], data_set_name)
            csv_paths.append(csv_path)
            if os.path.exists(csv_path):
                print(f"CSV file already exists: {csv_path}")
                print(f"File size: {os.path.getsize(csv_path)} bytes")
                print(f"Last modified: {time.ctime(os.path.getmtime(csv_path))}")
        
        # First call to create the cache if it doesn't exist
        print("Making first API call to create/update CSV...")
        _, dataframes1 = fetch_hustle_stats_logic(**params, return_dataframe=True)
        
        # Verify the CSV files were created
        for csv_path in csv_paths:
            if os.path.exists(csv_path):
                print(f"CSV file exists: {csv_path}")
        
        # Get the modification times of the CSV files
        mtimes1 = {}
        for csv_path in csv_paths:
            if os.path.exists(csv_path):
                mtimes1[csv_path] = os.path.getmtime(csv_path)
                print(f"CSV file: {os.path.basename(csv_path)}")
                print(f"File size: {os.path.getsize(csv_path)} bytes")
                print(f"Last modified: {time.ctime(mtimes1[csv_path])}")
        
        # Wait a moment to ensure the modification time would be different if the file is rewritten
        time.sleep(0.1)
        
        # Second call should use the cache
        print("Making second API call (should use CSV cache)...")
        _, dataframes2 = fetch_hustle_stats_logic(**params, return_dataframe=True)
        
        # Verify the CSV files weren't modified
        for csv_path in csv_paths:
            if os.path.exists(csv_path) and csv_path in mtimes1:
                mtime2 = os.path.getmtime(csv_path)
                self.assertEqual(mtimes1[csv_path], mtime2)
                print(f"CSV file was not modified (same timestamp): {time.ctime(mtime2)}")
        
        # Verify the data is the same
        if dataframes1 and dataframes2:
            for key in dataframes1:
                if key in dataframes2:
                    pd.testing.assert_frame_equal(dataframes1[key], dataframes2[key], check_dtype=False)
                    print(f"Verified data for {key} matches between calls")
        
        print(f"Data loaded from CSV matches original data")

    def test_get_hustle_stats_boxscore(self):
        """Test the get_hustle_stats_boxscore function."""
        # Call the function with real API
        json_response, dataframes = get_hustle_stats_boxscore(
            game_id="0022400001",
            return_dataframe=True
        )
        
        # Parse the JSON response
        data = json.loads(json_response)
        
        # Verify the result
        self.assertIsInstance(data, dict)
        self.assertIsInstance(dataframes, dict)
        
        # Check if there's an error in the response
        if "error" in data:
            print(f"API returned an error: {data['error']}")
            print("This might be expected if the NBA API is unavailable or rate-limited.")
        else:
            # Check parameters
            self.assertIn("parameters", data)
            self.assertEqual(data["parameters"]["game_id"], "0022400001")
            
            # Check dataframes
            self.assertGreater(len(dataframes), 0)
            
            for data_set_name, df in dataframes.items():
                self.assertIsInstance(df, pd.DataFrame)
                print(f"DataFrame '{data_set_name}' shape: {df.shape}")

    def test_100_percent_parameter_coverage(self):
        """Test 100% parameter coverage for HustleStatsBoxScore endpoint."""
        print("\n=== Testing 100% Parameter Coverage ===")
        
        # Test parameter combinations that work
        test_cases = [
            # NBA games from current season
            {"game_id": "0022400001"},
            {"game_id": "0022400002"},
            {"game_id": "0022400003"},
            {"game_id": "0022400004"},
            {"game_id": "0022400005"},
            
            # NBA games from previous season
            {"game_id": "0022300001"},
            {"game_id": "0022300002"},
            
            # WNBA games (might not have hustle stats but should not error)
            {"game_id": "1022400001"},
            {"game_id": "1022400002"},
        ]
        
        successful_tests = 0
        total_tests = len(test_cases)
        
        for i, test_case in enumerate(test_cases):
            print(f"\nTest {i+1}/{total_tests}: {test_case}")
            
            try:
                # Call the API with these parameters
                json_response, dataframes = get_hustle_stats_boxscore(
                    **test_case,
                    return_dataframe=True
                )
                
                # Parse the JSON response
                data = json.loads(json_response)
                
                # Verify the result
                self.assertIsInstance(data, dict)
                self.assertIsInstance(dataframes, dict)
                
                # Check if there's an error in the response
                if "error" in data:
                    print(f"  ✗ API returned an error: {data['error']}")
                else:
                    # Check parameters match
                    if "parameters" in data:
                        params = data["parameters"]
                        self.assertEqual(params["game_id"], test_case["game_id"])
                        
                        print(f"  ✓ Parameters match")
                    
                    # Check data sets
                    if "data_sets" in data and data["data_sets"]:
                        data_sets = data["data_sets"]
                        print(f"  ✓ Data sets: {list(data_sets.keys())}")
                        
                        # Check dataframes
                        if dataframes:
                            for data_set_name, df in dataframes.items():
                                print(f"  ✓ DataFrame '{data_set_name}': {len(df)} records")
                                
                                # Verify CSV was created
                                csv_path = _get_csv_path_for_hustle_stats(
                                    test_case["game_id"],
                                    data_set_name
                                )
                                if os.path.exists(csv_path):
                                    print(f"  ✓ CSV created: {os.path.basename(csv_path)}")
                                else:
                                    print(f"  ○ CSV not found: {os.path.basename(csv_path)}")
                        else:
                            print(f"  ○ No dataframes returned")
                    else:
                        print(f"  ○ No data sets returned (might be empty for this game)")
                    
                    successful_tests += 1
                    
            except Exception as e:
                print(f"  ✗ Exception: {e}")
                # Don't fail the test for individual game IDs that might not work
                # This is expected for some games that might not have hustle stats
        
        print(f"\n=== Parameter Coverage Summary ===")
        print(f"Total test cases: {total_tests}")
        print(f"Successful tests: {successful_tests}")
        print(f"Coverage: {(successful_tests/total_tests)*100:.1f}%")
        
        # We expect at least 70% of test cases to work (WNBA games might not have hustle stats)
        self.assertGreater(successful_tests, total_tests * 0.7, 
                          f"Expected at least 70% of parameter combinations to work, got {(successful_tests/total_tests)*100:.1f}%")


if __name__ == '__main__':
    unittest.main()


===== backend\smoke_tests\test_scoreboard_tools.py =====
"""
Smoke test for the scoreboard_tools module.
Tests the functionality of fetching scoreboard data with DataFrame output.
"""
import os
import sys
import json
import pandas as pd
from datetime import datetime, date, timedelta

# Add the project root directory to the Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(backend_dir)
sys.path.insert(0, project_root)

from backend.api_tools.scoreboard_tools import fetch_scoreboard_data_logic
from nba_api.stats.library.parameters import LeagueID

def test_fetch_scoreboard_data_basic():
    """Test fetching scoreboard data with default parameters."""
    print("\n=== Testing fetch_scoreboard_data_logic (basic) ===")

    # Use today's date
    test_date = date.today().strftime('%Y-%m-%d')
    print(f"Testing with date: {test_date}")

    # Test with default parameters (JSON output)
    json_response = fetch_scoreboard_data_logic(
        game_date=test_date,
        league_id=LeagueID.nba
    )

    # Parse the JSON response
    data = json.loads(json_response)

    # Check if the response has the expected structure
    assert isinstance(data, dict), "Response should be a dictionary"

    # Check if there's an error in the response
    if "error" in data:
        print(f"API returned an error: {data['error']}")
        print("This might be expected if the NBA API is unavailable or rate-limited.")
        print("Continuing with other tests...")
    else:
        # Check if the key fields exist
        assert "gameDate" in data, "Response should have a 'gameDate' field"
        assert "games" in data, "Response should have a 'games' field"

        # Print some information about the data
        print(f"Game Date: {data['gameDate']}")
        games = data["games"]
        print(f"Number of games: {len(games)}")

        # Print details of the first few games
        if games:
            print("\nFirst 2 games:")
            for i, game in enumerate(games[:2]):
                print(f"\nGame {i+1}:")
                print(f"  Game ID: {game.get('gameId', 'N/A')}")
                print(f"  Status: {game.get('gameStatusText', 'N/A')}")

                home_team = game.get("homeTeam", {})
                away_team = game.get("awayTeam", {})

                print(f"  Home Team: {home_team.get('teamTricode', 'N/A')} - Score: {home_team.get('score', 'N/A')}")
                print(f"  Away Team: {away_team.get('teamTricode', 'N/A')} - Score: {away_team.get('score', 'N/A')}")

    print("\n=== Basic scoreboard data test completed ===")
    return data

def test_fetch_scoreboard_data_dataframe():
    """Test fetching scoreboard data with DataFrame output."""
    print("\n=== Testing fetch_scoreboard_data_logic with DataFrame output ===")

    # Use May 18, 2025 which had games
    test_date = "2025-05-18"  # May 18, 2025 with games
    print(f"Testing with date: {test_date}")

    # Test with return_dataframe=True
    result = fetch_scoreboard_data_logic(
        game_date=test_date,
        league_id=LeagueID.nba,
        return_dataframe=True
    )

    # Check if the result is a tuple
    assert isinstance(result, tuple), "Result should be a tuple when return_dataframe=True"
    assert len(result) == 2, "Result tuple should have 2 elements"

    json_response, dataframes = result

    # Check if the first element is a JSON string
    assert isinstance(json_response, str), "First element should be a JSON string"

    # Check if the second element is a dictionary of DataFrames
    assert isinstance(dataframes, dict), "Second element should be a dictionary of DataFrames"

    # Parse the JSON response
    data = json.loads(json_response)

    # Check if there's an error in the response
    if "error" in data:
        print(f"API returned an error: {data['error']}")
        print("This might be expected if the NBA API is unavailable or rate-limited.")
        print("Continuing with other tests...")
    else:
        # Print DataFrame info
        print(f"\nDataFrames returned: {list(dataframes.keys())}")
        for key, df in dataframes.items():
            if not df.empty:
                print(f"\nDataFrame '{key}' shape: {df.shape}")
                print(f"DataFrame '{key}' columns: {df.columns.tolist()[:5]}...")  # Show first 5 columns

        # Check if the CSV files were created
        if "dataframe_info" in data:
            for df_key, df_info in data["dataframe_info"].get("dataframes", {}).items():
                csv_path = df_info.get("csv_path")
                if csv_path:
                    full_path = os.path.join(backend_dir, csv_path)
                    if os.path.exists(full_path):
                        print(f"\nCSV file exists: {csv_path}")
                        csv_size = os.path.getsize(full_path)
                        print(f"CSV file size: {csv_size} bytes")
                    else:
                        print(f"\nCSV file does not exist: {csv_path}")

        # Display a sample of each DataFrame if not empty
        for key, df in dataframes.items():
            if not df.empty:
                print(f"\nSample of DataFrame '{key}' (first 2 rows):")
                print(df.head(2))

    print("\n=== DataFrame scoreboard data test completed ===")
    return result

def test_fetch_scoreboard_data_invalid_date():
    """Test fetching scoreboard data with an invalid date format."""
    print("\n=== Testing fetch_scoreboard_data_logic with invalid date ===")

    # Test with an invalid date format
    json_response = fetch_scoreboard_data_logic(
        game_date="invalid-date",
        league_id=LeagueID.nba
    )

    # Parse the JSON response
    data = json.loads(json_response)

    # Check if there's an error in the response
    assert "error" in data, "Response should contain an error for invalid date format"
    print(f"Error message: {data['error']}")

    print("\n=== Invalid date test completed ===")
    return data

def run_all_tests():
    """Run all tests in sequence."""
    print(f"=== Running scoreboard_tools smoke tests at {datetime.now().isoformat()} ===\n")

    try:
        # Run the tests
        basic_data = test_fetch_scoreboard_data_basic()
        df_result = test_fetch_scoreboard_data_dataframe()
        invalid_date_result = test_fetch_scoreboard_data_invalid_date()

        print("\n=== All tests completed successfully ===")
        return True
    except Exception as e:
        print(f"\n!!! Test failed with error: {str(e)} !!!")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)


===== backend\tool_kits\advanced_analytics_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Advanced Analytics logic functions
from ..api_tools.advanced_metrics import fetch_player_advanced_analysis_logic
from ..api_tools.player_shot_charts import fetch_player_shotchart_logic # Individual player shot chart
from ..api_tools.advanced_shot_charts import process_shot_data_for_visualization as generate_advanced_shot_chart_visual
from ..api_tools.player_comparison import compare_player_shots as compare_player_shots_visual # Visual shot chart comparison


from ..config import settings
from nba_api.stats.library.parameters import SeasonTypeAllStar

class AdvancedAnalyticsToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_player_advanced_analysis,
            self.get_player_shot_chart_data,
            self.generate_player_advanced_shot_chart_visualization,
            self.compare_players_shot_charts_visualization,
        ]
        super().__init__(name="advanced_analytics_toolkit", tools=tools, **kwargs)

    def get_player_advanced_analysis(
        self,
        player_name: str,
        season: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches advanced metrics, RAPTOR-style ratings (if available), skill grades,
        and similar players for a specified player.

        Args:
            player_name (str): The full name or ID of the player to analyze.
            season (Optional[str], optional): The NBA season in YYYY-YY format (e.g., "2023-24").
                                              If None, uses the current season defined in settings.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with advanced analysis or an error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int,
                        "advanced_metrics": {{
                            // RAPTOR metrics if available (e.g., RAPTOR_OFFENSE, RAPTOR_DEFENSE, WAR, ELO_RATING)
                            // OR Fallback metrics (e.g., ORTG, DRTG, NETRTG, USG_PCT, NBA_PLUS, ELO_RATING)
                        }},
                        "skill_grades": {{
                            "perimeter_shooting": str, "interior_scoring": str, "playmaking": str, ... (grades A+ to F)
                        }},
                        "similar_players": [
                            {{"player_id": int, "player_name": str, "similarity_score": float}}, ...
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys depend on RAPTOR availability:
                    - If RAPTOR: 'raptor_metrics', 'skill_grades', 'similar_players', 'basic_stats', 'player_basic_stats'.
                    - Else (Fallback): 'advanced_metrics', 'skill_grades', 'similar_players'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"AdvancedAnalyticsToolkit: get_player_advanced_analysis called for {player_name}, season {season or settings.CURRENT_NBA_SEASON}")
        return fetch_player_advanced_analysis_logic(
            player_name=player_name,
            season=season,
            return_dataframe=return_dataframe
        )

    def get_player_shot_chart_data(
        self,
        player_name: str,
        season: Optional[str] = None,
        season_type: str = SeasonTypeAllStar.regular,
        context_measure: str = "FGA", # Field Goal Attempts
        last_n_games: int = 0,
        return_dataframe: bool = False,
        # Include all other optional params from fetch_player_shotchart_logic
        league_id: str = "00",
        month: int = 0,
        opponent_team_id: int = 0,
        period: int = 0,
        vs_division_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        game_segment_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        date_from_nullable: Optional[str] = None,
        game_id_nullable: Optional[str] = None,
        player_position_nullable: Optional[str] = None,
        rookie_year_nullable: Optional[str] = None,
        context_filter_nullable: Optional[str] = None,
        clutch_time_nullable: Optional[str] = None,
        ahead_behind_nullable: Optional[str] = None,
        point_diff_nullable: Optional[str] = None,
        position_nullable: Optional[str] = None,
        range_type_nullable: Optional[str] = None,
        start_period_nullable: Optional[str] = None,
        start_range_nullable: Optional[str] = None,
        end_period_nullable: Optional[str] = None,
        end_range_nullable: Optional[str] = None
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches detailed shot chart data for a specified player, including shot locations and zone analysis.
        This tool returns the raw data; for a generated image, use `generate_player_advanced_shot_chart_visualization`.

        Args:
            player_name (str): The full name or ID of the player.
            season (Optional[str], optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible: "Regular Season", "Playoffs", "Pre Season", "All Star".
            context_measure (str, optional): Statistical measure for API context. Defaults to "FGA".
                                             Possible: "FGA", "FGM", "FG_PCT", etc.
            last_n_games (int, optional): Filter by last N games. Defaults to 0 (all games).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.
            league_id (str, optional): League ID. Defaults to "00".
            month (int, optional): Month filter. Defaults to 0.
            opponent_team_id (int, optional): Opponent team ID. Defaults to 0.
            period (int, optional): Period filter. Defaults to 0.
            // ... (other optional filters from the logic function signature)

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with shot chart data, zone analysis, and overall stats.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "team_name": str, "team_id": int,
                        "season": str, "season_type": str,
                        "shots": [{{ "x": float, "y": float, "made": bool, "value": int, "shot_type": str, "shot_zone": str, ... }}],
                        "zones": [{{ "zone": str, "attempts": int, "made": int, "percentage": float, "leaguePercentage": float, ... }}],
                        "overall_stats": {{ "total_shots": int, "made_shots": int, "field_goal_percentage": float }},
                        "visualization_path": Optional[str], // Path if generated by underlying logic, usually None for this data-focused tool
                        "visualization_error": Optional[str]
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'shots' (processed), 'zones', 'raw_shots' (from API), 'league_averages'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"AdvancedAnalyticsToolkit: get_player_shot_chart_data called for {player_name}, season {season or settings.CURRENT_NBA_SEASON}")
        return fetch_player_shotchart_logic(
            player_name=player_name, season=season, season_type=season_type,
            context_measure=context_measure, last_n_games=last_n_games, return_dataframe=return_dataframe,
            league_id=league_id, month=month, opponent_team_id=opponent_team_id, period=period,
            vs_division_nullable=vs_division_nullable, vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable, outcome_nullable=outcome_nullable,
            location_nullable=location_nullable, game_segment_nullable=game_segment_nullable,
            date_to_nullable=date_to_nullable, date_from_nullable=date_from_nullable,
            game_id_nullable=game_id_nullable, player_position_nullable=player_position_nullable,
            rookie_year_nullable=rookie_year_nullable, context_filter_nullable=context_filter_nullable,
            clutch_time_nullable=clutch_time_nullable, ahead_behind_nullable=ahead_behind_nullable,
            point_diff_nullable=point_diff_nullable, position_nullable=position_nullable,
            range_type_nullable=range_type_nullable, start_period_nullable=start_period_nullable,
            start_range_nullable=start_range_nullable, end_period_nullable=end_period_nullable,
            end_range_nullable=end_range_nullable
        )

    def generate_player_advanced_shot_chart_visualization(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        chart_type: str = "scatter", # "scatter", "heatmap", "hexbin", "animated", "frequency", "distance"
        output_format: str = "base64", # "base64" or "file"
        use_cache: bool = True,
        return_dataframe: bool = False # For underlying shot data
    ) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
        """
        Generates an advanced shot chart visualization for a player (scatter, heatmap, hexbin, animated, frequency, or distance chart).
        The visualization is returned as base64 encoded data or a file path.

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible: "Regular Season", "Playoffs", "Pre Season", "All Star".
            chart_type (str, optional): Type of chart to create. Defaults to "scatter".
                                        Possible: "scatter", "heatmap", "hexbin", "animated", "frequency", "distance".
            output_format (str, optional): Output format for the visualization. Defaults to "base64".
                                           Possible: "base64", "file".
            use_cache (bool, optional): Whether to use cached visualizations if available. Defaults to True.
            return_dataframe (bool, optional): If True, also returns the underlying shot DataFrames ('shots', 'league_averages')
                                               used to generate the visualization. Defaults to False.

        Returns:
            Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: Dictionary containing visualization data or an error message.
                    Example for base64 scatter:
                    {{
                        "image_data": "data:image/png;base64,...", // or "animation_data" for GIF
                        "chart_type": "scatter"
                    }}
                    Example for file output:
                    {{
                        "file_path": "/path/to/shotchart_player_season.png",
                        "chart_type": "scatter"
                    }}
                If return_dataframe=True: Tuple (visualization_dict, dataframes_dict).
                    dataframes_dict keys: 'shots', 'league_averages'.
                    CSV cache paths for these DataFrames included in visualization_dict under 'dataframe_info'.
        """
        logger.info(f"AdvancedAnalyticsToolkit: generate_player_advanced_shot_chart_visualization called for {player_name}, chart_type: {chart_type}")
        return generate_advanced_shot_chart_visual(
            player_name=player_name,
            season=season,
            season_type=season_type,
            chart_type=chart_type,
            output_format=output_format,
            use_cache=use_cache,
            return_dataframe=return_dataframe
        )

    def compare_players_shot_charts_visualization(
        self,
        player_names: List[str], # List of 2 to 4 player names or IDs
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        output_format: str = "base64", # "base64" or "file"
        chart_type: str = "scatter", # "scatter", "heatmap", "zones"
        context_measure: str = "FGA",
        return_dataframe: bool = False
    ) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
        """
        Compares shot charts visually for multiple players (2 to 4 players).
        Generates scatter plots, heatmaps, or zone efficiency bar charts for comparison.

        Args:
            player_names (List[str]): List of 2 to 4 player names or IDs to compare.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
            output_format (str, optional): Output format ("base64" or "file"). Defaults to "base64".
            chart_type (str, optional): Type of comparison chart ("scatter", "heatmap", "zones"). Defaults to "scatter".
            context_measure (str, optional): Context measure for shot chart data fetching. Defaults to "FGA".
            return_dataframe (bool, optional): If True, also returns underlying shot DataFrames for each player
                                               and a combined zone breakdown DataFrame. Defaults to False.

        Returns:
            Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: Dictionary containing visualization data or an error message.
                    Example for base64 scatter comparison:
                    {{
                        "image_data": "data:image/png;base64,...",
                        "chart_type": "comparison_scatter" // or comparison_heatmap, comparison_zones
                    }}
                If return_dataframe=True: Tuple (visualization_dict, dataframes_dict).
                    dataframes_dict keys: `shots_{player_id}` and `league_avg_{player_id}` for each player,
                                          and `zone_breakdown` for combined zone stats.
                    CSV cache paths for these DataFrames included in visualization_dict under 'dataframe_info'.
        """
        logger.info(f"AdvancedAnalyticsToolkit: compare_players_shot_charts_visualization called for players: {player_names}, chart_type: {chart_type}")
        return compare_player_shots_visual(
            player_names=player_names,
            season=season,
            season_type=season_type,
            output_format=output_format,
            chart_type=chart_type,
            context_measure=context_measure,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\comparison_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Comparison specific stats logic functions
from ..api_tools.player_compare import fetch_player_compare_logic
from ..api_tools.teamvsplayer import fetch_teamvsplayer_logic
from ..api_tools.matchup_tools import fetch_league_season_matchups_logic, fetch_matchups_rollup_logic # Player vs Player Season and Defensive Rollup
from ..api_tools.player_comparison import compare_player_shots as compare_player_shots_visual # Visual shot chart comparison

from ..config import settings
from nba_api.stats.library.parameters import (
    SeasonTypePlayoffs, PerModeDetailed, MeasureTypeDetailedDefense,
    SeasonTypeAllStar # For matchup_tools
)


class ComparisonToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.compare_players_stats,
            self.get_team_vs_player_comparison,
            self.get_player_vs_player_season_matchups,
            self.get_player_defensive_matchup_rollup,
            self.compare_player_shot_charts_visual,
        ]
        super().__init__(name="comparison_toolkit", tools=tools, **kwargs)

    def compare_players_stats(
        self,
        player_id_list: List[str], # Typically 2 to 5 players
        vs_player_id_list: Optional[List[str]] = None, # Optional for direct comparison without a "vs" context
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_playoffs: str = SeasonTypePlayoffs.regular,
        per_mode_detailed: str = PerModeDetailed.totals,
        measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
        league_id_nullable: str = "", # Can be "" for NBA, or "00", "10"
        last_n_games: int = 0,
        pace_adjust: str = "N", # "Y" or "N"
        plus_minus: str = "N",  # "Y" or "N"
        rank: str = "N",        # "Y" or "N"
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Compares statistical performance between a list of players, optionally against another list of players.
        Fetches data using the PlayerCompare endpoint.

        Args:
            player_id_list (List[str]): A list of player IDs (as strings) for the primary group of players to compare.
                                        Typically 2 to 5 players.
            vs_player_id_list (Optional[List[str]], optional): A list of player IDs (as strings) for the comparison group.
                                                               If None, players in `player_id_list` are compared against each other or their averages.
                                                               Defaults to None.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type_playoffs (str, optional): Type of season. Defaults to "Regular Season".
                                                  Possible values from SeasonTypePlayoffs enum (e.g., "Regular Season", "Playoffs").
            per_mode_detailed (str, optional): Statistical mode. Defaults to "Totals".
                                               Possible values from PerModeDetailed enum (e.g., "PerGame", "Per100Possessions").
            measure_type_detailed_defense (str, optional): Type of stats. Defaults to "Base".
                                                           Possible values from MeasureTypeDetailedDefense enum (e.g., "Base", "Advanced").
            league_id_nullable (str, optional): League ID. Defaults to "" (usually implies NBA "00").
                                                Possible values: "00" (NBA), "10" (WNBA), "" (default/NBA).
            last_n_games (int, optional): Filter by last N games. Defaults to 0 (all games).
            pace_adjust (str, optional): Pace adjust stats ("Y" or "N"). Defaults to "N".
            plus_minus (str, optional): Include plus-minus ("Y" or "N"). Defaults to "N".
            rank (str, optional): Include rank ("Y" or "N"). Defaults to "N".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player comparison data or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...api parameters used...}},
                        "data_sets": {{
                            "OverallComparison": [{{ "GROUP_SET": "Overall", "DESCRIPTION": "Player A vs Player B", "MIN": float, "PTS": float, ... (many other stats) ... }}],
                            "IndividualComparison": [{{ "PLAYER_ID": int, "PLAYER_NAME": str, "MIN": float, "PTS": float, ... (many other stats) ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "OverallComparison", "IndividualComparison".
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"ComparisonToolkit: compare_players_stats called for players {player_id_list} vs {vs_player_id_list or 'overall'}")
        # The logic function expects tuples for lru_cache
        vs_player_tuple = tuple(vs_player_id_list) if vs_player_id_list is not None else tuple()
        player_tuple = tuple(player_id_list)

        return fetch_player_compare_logic(
            vs_player_id_list=vs_player_tuple,
            player_id_list=player_tuple,
            season=season,
            season_type_playoffs=season_type_playoffs,
            per_mode_detailed=per_mode_detailed,
            measure_type_detailed_defense=measure_type_detailed_defense,
            league_id_nullable=league_id_nullable,
            last_n_games=last_n_games,
            pace_adjust=pace_adjust,
            plus_minus=plus_minus,
            rank=rank,
            return_dataframe=return_dataframe
        )

    def get_team_vs_player_comparison(
        self,
        team_identifier: str,
        vs_player_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular, # API uses season_type_playoffs
        per_mode: str = PerModeDetailed.totals,
        measure_type: str = MeasureTypeDetailedDefense.base,
        player_identifier: Optional[str] = None, # Player on the specified team to analyze their on/off court impact vs opponent
        last_n_games: int = 0,
        month: int = 0,
        opponent_team_id: int = 0,
        pace_adjust: str = "N",
        period: int = 0,
        plus_minus: str = "N",
        rank: str = "N",
        vs_division_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        league_id_nullable: Optional[str] = None,
        game_segment_nullable: Optional[str] = None,
        date_from_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches detailed statistics for a team when a specific opposing player is on/off the court.
        Optionally, can further analyze the impact of one of the team's own players in this context using `player_identifier`.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            vs_player_identifier (str): Name or ID of the opposing player to analyze against.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible values from SeasonTypePlayoffs enum.
            per_mode (str, optional): Statistical mode. Defaults to "Totals".
                                      Possible values from PerModeDetailed enum.
            measure_type (str, optional): Type of stats. Defaults to "Base".
                                          Possible values from MeasureTypeDetailedDefense enum.
            player_identifier (Optional[str], optional): Name or ID of a player on the primary `team_identifier` team.
                                                         If provided, stats will show the team's performance vs. `vs_player_identifier`
                                                         when this specific `player_identifier` is on/off the court.
            last_n_games (int, optional): Filter by last N games. Defaults to 0.
            month (int, optional): Filter by month (1-12). Defaults to 0.
            opponent_team_id (int, optional): Filter by opponent team ID (different from vs_player_id). Defaults to 0.
            pace_adjust (str, optional): Pace adjust stats ("Y" or "N"). Defaults to "N".
            period (int, optional): Filter by period. Defaults to 0.
            plus_minus (str, optional): Include plus-minus ("Y" or "N"). Defaults to "N".
            rank (str, optional): Include rank ("Y" or "N"). Defaults to "N".
            vs_division_nullable (Optional[str], optional): Filter by opponent's division.
            vs_conference_nullable (Optional[str], optional): Filter by opponent's conference.
            season_segment_nullable (Optional[str], optional): Filter by season segment.
            outcome_nullable (Optional[str], optional): Filter by game outcome ('W' or 'L').
            location_nullable (Optional[str], optional): Filter by game location ('Home' or 'Road').
            league_id_nullable (Optional[str], optional): League ID.
            game_segment_nullable (Optional[str], optional): Filter by game segment.
            date_from_nullable (Optional[str], optional): Start date YYYY-MM-DD.
            date_to_nullable (Optional[str], optional): End date YYYY-MM-DD.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with team vs player comparison data or an error message.
                    Expected success structure:
                    {{
                        "team_name": str, "team_id": int, "vs_player_name": str, "vs_player_id": int,
                        "parameters": {{...api parameters used...}},
                        "overall": [{{ ... team's overall stats in games involving vs_player ... }}],
                        "on_off_court": [{{ ... team's stats when vs_player is ON/OFF court ... }}],
                        "shot_area_overall": [{{ ... team's shooting by area in games involving vs_player ... }}],
                        "shot_area_on_court": [{{ ... team's shooting by area when vs_player is ON court ... }}],
                        "shot_area_off_court": [{{ ... team's shooting by area when vs_player is OFF court ... }}],
                        "shot_distance_overall": [{{ ... team's shooting by distance ... }}],
                        "shot_distance_on_court": [{{ ... team's shooting by distance when vs_player is ON court ... }}],
                        "shot_distance_off_court": [{{ ... team's shooting by distance when vs_player is OFF court ... }}],
                        "vs_player_overall": [{{ ... direct stats of vs_player against the team ... }}]
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "overall", "on_off_court", "shot_area_overall", etc.
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"ComparisonToolkit: get_team_vs_player_comparison called for team {team_identifier} vs player {vs_player_identifier}")
        return fetch_teamvsplayer_logic(
            team_identifier=team_identifier,
            vs_player_identifier=vs_player_identifier,
            season=season,
            season_type=season_type,
            per_mode=per_mode,
            measure_type=measure_type,
            player_identifier=player_identifier,
            last_n_games=last_n_games,
            month=month,
            opponent_team_id=opponent_team_id,
            pace_adjust=pace_adjust,
            period=period,
            plus_minus=plus_minus,
            rank=rank,
            vs_division_nullable=vs_division_nullable,
            vs_conference_nullable=vs_conference_nullable,
            season_segment_nullable=season_segment_nullable,
            outcome_nullable=outcome_nullable,
            location_nullable=location_nullable,
            league_id_nullable=league_id_nullable,
            game_segment_nullable=game_segment_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            return_dataframe=return_dataframe
        )

    def get_player_vs_player_season_matchups(
        self,
        def_player_identifier: str,
        off_player_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular, # API uses season_type_playoffs
        bypass_cache: bool = False,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches season-long head-to-head matchup statistics between two specific players.
        Details how the offensive player performed when guarded by the defensive player.

        Args:
            def_player_identifier (str): Name or ID of the defensive player.
            off_player_identifier (str): Name or ID of the offensive player.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible values from SeasonTypeAllStar enum (e.g., "Regular Season", "Playoffs", "Preseason").
            bypass_cache (bool, optional): If True, ignores cached raw data for the API call. Defaults to False.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'matchups': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player vs. player matchup data or an error message.
                    Expected success structure:
                    {{
                        "def_player_id": int, "def_player_name": str,
                        "off_player_id": int, "off_player_name": str,
                        "parameters": {{"season": str, "season_type": str}},
                        "matchups": [
                            {{
                                "MATCHUP_MIN": float, "PARTIAL_POSS": float, "PLAYER_PTS": int, // Offensive player's points
                                "OPP_PLAYER_PTS": int, // Defensive player's points (contextual, usually 0 in this direct matchup view)
                                "FGM": int, "FGA": int, "FG_PCT": float, // Offensive player's shooting when guarded by defensive player
                                "FG3M": int, "FG3A": int, "FG3_PCT": float,
                                // ... other matchup stats like AST, TOV, BLK, STL ...
                            }}
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, {{'matchups': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"ComparisonToolkit: get_player_vs_player_season_matchups called for Def: {def_player_identifier} vs Off: {off_player_identifier}")
        return fetch_league_season_matchups_logic(
            def_player_identifier=def_player_identifier,
            off_player_identifier=off_player_identifier,
            season=season,
            season_type=season_type,
            bypass_cache=bypass_cache,
            return_dataframe=return_dataframe
        )

    def get_player_defensive_matchup_rollup(
        self,
        def_player_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular, # API uses season_type_playoffs
        bypass_cache: bool = False,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches matchup rollup statistics for a defensive player. This shows how various offensive players
        performed when guarded by the specified defensive player over a season.

        Args:
            def_player_identifier (str): Name or ID of the defensive player.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible values from SeasonTypeAllStar enum (e.g., "Regular Season", "Playoffs", "Preseason").
            bypass_cache (bool, optional): If True, ignores cached raw data for the API call. Defaults to False.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'rollup': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with defensive matchup rollup data or an error message.
                    Expected success structure:
                    {{
                        "def_player_id": int, "def_player_name": str,
                        "parameters": {{"season": str, "season_type": str}},
                        "rollup": [
                            {{
                                "OFF_PLAYER_ID": int, "OFF_PLAYER_NAME": str, "MATCHUP_MIN": float,
                                "PARTIAL_POSS": float, "FGM": int, "FGA": int, "FG_PCT": float, ...
                                // Stats reflect how the OFF_PLAYER performed against the DEF_PLAYER
                            }}
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, {{'rollup': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"ComparisonToolkit: get_player_defensive_matchup_rollup called for Def Player: {def_player_identifier}")
        return fetch_matchups_rollup_logic(
            def_player_identifier=def_player_identifier,
            season=season,
            season_type=season_type,
            bypass_cache=bypass_cache,
            return_dataframe=return_dataframe
        )

    def compare_player_shot_charts_visual(
        self,
        player_names: List[str], # List of 2 to 4 player names or IDs
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        output_format: str = "base64", # "base64" or "file"
        chart_type: str = "scatter", # "scatter", "heatmap", "zones"
        context_measure: str = "FGA",
        return_dataframe: bool = False
    ) -> Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
        """
        Compares shot charts visually for multiple players (2 to 4 players).
        Generates scatter plots, heatmaps, or zone efficiency bar charts for comparison.

        Args:
            player_names (List[str]): List of 2 to 4 player names or IDs to compare.
            season (str, optional): NBA season in YYYY-YY format. Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible: "Regular Season", "Playoffs", "Pre Season", "All Star".
            output_format (str, optional): Output format for the visualization.
                                           Defaults to "base64". Possible: "base64", "file".
            chart_type (str, optional): Type of comparison chart. Defaults to "scatter".
                                        Possible: "scatter", "heatmap", "zones".
            context_measure (str, optional): Context measure for shot chart data fetching (e.g., "FGA", "FGM").
                                             Defaults to "FGA".
            return_dataframe (bool, optional): If True, also returns underlying shot DataFrames for each player
                                               and a combined zone breakdown DataFrame. Defaults to False.

        Returns:
            Union[Dict[str, Any], Tuple[Dict[str, Any], Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: Dictionary containing visualization data (e.g., "image_data" or "file_path") and metadata.
                    Example for base64 scatter comparison:
                    {{
                        "image_data": "data:image/png;base64,...",
                        "chart_type": "comparison_scatter" // or comparison_heatmap, comparison_zones
                    }}
                If return_dataframe=True: Tuple (visualization_dict, dataframes_dict).
                    dataframes_dict keys: `shots_{{player_id}}` and `league_avg_{{player_id}}` for each player,
                                          and `zone_breakdown` for combined zone stats.
                    CSV cache paths for these DataFrames included in visualization_dict under 'dataframe_info'.
        """
        logger.info(f"ComparisonToolkit: compare_player_shot_charts_visual called for players: {player_names}, chart: {chart_type}")
        return compare_player_shots_visual(
            player_names=player_names,
            season=season,
            season_type=season_type,
            output_format=output_format,
            chart_type=chart_type,
            context_measure=context_measure,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\draft_combine_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Draft Combine and History logic functions
from ..api_tools.draft_combine_drill_results import fetch_draft_combine_drill_results_logic as fetch_combine_drill_results_actual_logic
from ..api_tools.draft_combine_drills import fetch_draft_combine_drills_logic # Old, potentially redundant name?
from ..api_tools.draft_combine_nonshooting import fetch_draft_combine_nonshooting_logic
from ..api_tools.draft_combine_player_anthro import fetch_draft_combine_player_anthro_logic
from ..api_tools.draft_combine_spot_shooting import fetch_draft_combine_spot_shooting_logic
from ..api_tools.draft_combine_stats import fetch_draft_combine_stats_logic # Comprehensive combine stats
from ..api_tools.league_draft import fetch_draft_history_logic # General draft history

from nba_api.stats.library.parameters import LeagueID

class DraftCombineToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_draft_combine_drill_results,
            self.get_draft_combine_non_stationary_shooting_stats,
            self.get_draft_combine_player_anthropometrics,
            self.get_draft_combine_spot_shooting_stats,
            self.get_comprehensive_draft_combine_stats,
            self.get_nba_draft_history,
        ]
        super().__init__(name="draft_combine_toolkit", tools=tools, **kwargs)

    def get_draft_combine_drill_results(
        self,
        league_id: str = "00", # Typically NBA "00"
        season_year: str = "2024", # YYYY format
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA Draft Combine athletic drill results data for a specific season year.
        Includes metrics like vertical leap, agility times, sprint times, and bench press.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible values from LeagueID enum (e.g., "00" for NBA, "10" for WNBA).
            season_year (str, optional): Season year in YYYY format (e.g., "2023" for the 2023 draft combine).
                                         Defaults to "2024".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'DraftCombineDrillResults': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with draft combine drill results or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{"league_id": str, "season_year": str}},
                        "data_sets": {{
                            "DraftCombineDrillResults": [
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str, "POSITION": str,
                                    "STANDING_VERTICAL_LEAP": Optional[float], "MAX_VERTICAL_LEAP": Optional[float],
                                    "LANE_AGILITY_TIME": Optional[float], "MODIFIED_LANE_AGILITY_TIME": Optional[float],
                                    "THREE_QUARTER_SPRINT": Optional[float], "BENCH_PRESS": Optional[int]
                                    // ... other potential columns like DRAFT_PICK ...
                                }}, ...
                            ]
                            // Might have other keys like "DraftCombineDrillResults_1" if API returns multiple tables.
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'DraftCombineDrillResults': pd.DataFrame, ...}}).
                    The DataFrame contains detailed drill results for each player at the combine.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_draft_combine_drill_results called for season_year: {season_year}, league_id: {league_id}")
        # Using the more explicitly named logic function
        return fetch_combine_drill_results_actual_logic(
            league_id=league_id,
            season_year=season_year,
            return_dataframe=return_dataframe
        )

    def get_draft_combine_non_stationary_shooting_stats(
        self,
        season_year: str, # YYYY format
        league_id: str = "00",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches draft combine non-stationary shooting data for a specific season year.
        Includes off-dribble and on-the-move shooting metrics from various distances.

        Args:
            season_year (str): Season year in YYYY format (e.g., "2023"). This is required.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible: "00" (NBA), "10" (WNBA), "20" (G-League).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'Results': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with non-stationary shooting data or error.
                    Expected success structure:
                    {{
                        "parameters": {{"season_year": str, "league_id": str}},
                        "data_sets": {{
                            "Results": [ // Often 'Results' or specific like 'DraftCombineNonStationaryShooting'
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str,
                                    "OFF_DRIB_FIFTEEN_BREAK_LEFT_MADE": Optional[int],
                                    "OFF_DRIB_FIFTEEN_BREAK_LEFT_ATTEMPT": Optional[int],
                                    "OFF_DRIB_FIFTEEN_BREAK_LEFT_PCT": Optional[float],
                                    // ... many other columns for different non-stationary shooting drills ...
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'Results': pd.DataFrame}} or similar key).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_draft_combine_non_stationary_shooting_stats for year {season_year}")
        return fetch_draft_combine_nonshooting_logic(
            season_year=season_year,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_draft_combine_player_anthropometrics(
        self,
        season_year: str, # YYYY format
        league_id: str = "00",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches draft combine player anthropometric (physical measurement) data for a specific season year.
        Includes height (with/without shoes), weight, wingspan, standing reach, body fat %, hand length/width.

        Args:
            season_year (str): Season year in YYYY format (e.g., "2023"). This is required.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible: "00" (NBA), "10" (WNBA), "20" (G-League).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'Results': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player anthropometric data or error.
                    Expected success structure:
                    {{
                        "parameters": {{"season_year": str, "league_id": str}},
                        "data_sets": {{
                            "Results": [ // Often 'Results' or specific like 'DraftCombinePlayerAnthro'
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str, "POSITION": str,
                                    "HEIGHT_WO_SHOES": Optional[float], "HEIGHT_W_SHOES": Optional[float],
                                    "WEIGHT": Optional[str], "WINGSPAN": Optional[float],
                                    "STANDING_REACH": Optional[float], "BODY_FAT_PCT": Optional[float],
                                    "HAND_LENGTH": Optional[float], "HAND_WIDTH": Optional[float]
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'Results': pd.DataFrame}} or similar key).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_draft_combine_player_anthropometrics for year {season_year}")
        return fetch_draft_combine_player_anthro_logic(
            season_year=season_year,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_draft_combine_spot_shooting_stats(
        self,
        season_year: str, # YYYY format
        league_id: str = "00",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches draft combine spot-up shooting data for a specific season year.
        Includes shooting metrics from 15-foot, college range, and NBA range from various spots.

        Args:
            season_year (str): Season year in YYYY format (e.g., "2023"). This is required.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible: "00" (NBA), "10" (WNBA), "20" (G-League).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'Results': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with spot shooting data or error.
                    Expected success structure:
                    {{
                        "parameters": {{"season_year": str, "league_id": str}},
                        "data_sets": {{
                            "Results": [ // Often 'Results' or specific like 'DraftCombineSpotShooting'
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str,
                                    "FIFTEEN_CORNER_LEFT_MADE": Optional[int],
                                    "FIFTEEN_CORNER_LEFT_ATTEMPT": Optional[int],
                                    "FIFTEEN_CORNER_LEFT_PCT": Optional[float],
                                    // ... many other columns for different spot shooting drills ...
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'Results': pd.DataFrame}} or similar key).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_draft_combine_spot_shooting_stats for year {season_year}")
        return fetch_draft_combine_spot_shooting_logic(
            season_year=season_year,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_comprehensive_draft_combine_stats(
        self,
        season_year: str, # YYYY-YY format or "All Time"
        league_id: str = "00",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches comprehensive draft combine statistics for a specific season or all time.
        Includes anthropometrics, physical testing, and shooting stats.

        Args:
            season_year (str): Season year in YYYY-YY format (e.g., "2022-23") or "All Time". This is required.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible: "00" (NBA), "10" (WNBA), "20" (G-League).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'DraftCombineStats': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with comprehensive combine stats or error.
                    Expected success structure:
                    {{
                        "parameters": {{"season_year": str, "league_id": str}},
                        "data_sets": {{
                            "DraftCombineStats": [ // Or a similar key if API names it differently
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str, "POSITION": str,
                                    "HEIGHT_WO_SHOES": Optional[float], "WEIGHT": Optional[str],
                                    "WINGSPAN": Optional[float], "STANDING_VERTICAL_LEAP": Optional[float],
                                    // ... many other columns including shooting stats ...
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'DraftCombineStats': pd.DataFrame}} or similar key).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_comprehensive_draft_combine_stats for season {season_year}")
        return fetch_draft_combine_stats_logic(
            season_year=season_year,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_nba_draft_history(
        self,
        season_year_nullable: Optional[str] = None, # YYYY format
        league_id_nullable: str = LeagueID.nba,
        team_id_nullable: Optional[int] = None,
        round_num_nullable: Optional[int] = None,
        overall_pick_nullable: Optional[int] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA draft history, filterable by year, league, team, round, or overall pick.

        Args:
            season_year_nullable (Optional[str], optional): Four-digit draft year (e.g., '2023').
                                                            If None, returns all years. Defaults to None.
            league_id_nullable (str, optional): League ID. Defaults to "00" (NBA).
                                                Possible from LeagueID enum.
            team_id_nullable (Optional[int], optional): NBA team ID to filter by team. Defaults to None.
            round_num_nullable (Optional[int], optional): Draft round number to filter by round. Defaults to None.
            overall_pick_nullable (Optional[int], optional): Overall pick number to filter by pick. Defaults to None.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'draft_history': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with draft history or an error message.
                    Expected success structure:
                    {{
                        "season_year_requested": str, "league_id": str,
                        "team_id_filter": Optional[int], "round_num_filter": Optional[int],
                        "overall_pick_filter": Optional[int],
                        "draft_picks": [
                            {{
                                "PERSON_ID": int, "PLAYER_NAME": str, "SEASON": str,
                                "ROUND_NUMBER": int, "ROUND_PICK": int, "OVERALL_PICK": int,
                                "TEAM_ID": int, "TEAM_CITY": str, "TEAM_NAME": str,
                                "TEAM_ABBREVIATION": str, "ORGANIZATION": str, "ORGANIZATION_TYPE": str
                            }}, ...
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, {{'draft_history': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"DraftCombineToolkit: get_nba_draft_history for year {season_year_nullable or 'All'}")
        return fetch_draft_history_logic(
            season_year_nullable=season_year_nullable,
            league_id_nullable=league_id_nullable,
            team_id_nullable=team_id_nullable,
            round_num_nullable=round_num_nullable,
            overall_pick_nullable=overall_pick_nullable,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\financial_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Financials specific stats logic functions
from ..api_tools.contracts_data import (
    fetch_contracts_data_logic,
    get_player_contract as get_player_contract_logic, # Alias to avoid conflict
    get_team_payroll as get_team_payroll_logic,       # Alias
    get_highest_paid_players as get_highest_paid_players_logic, # Alias
    search_player_contracts as search_player_contracts_logic # Alias
)
from ..api_tools.free_agents_data import (
    fetch_free_agents_data_logic,
    get_free_agent_info as get_free_agent_info_logic, # Alias
    get_team_free_agents as get_team_free_agents_logic, # Alias
    get_top_free_agents as get_top_free_agents_logic,   # Alias
    search_free_agents as search_free_agents_logic   # Alias
)

class FinancialsToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_contracts_data,
            self.get_player_contract_details,
            self.get_team_payroll_summary,
            self.get_highest_paid_players_list,
            self.search_player_contracts_by_name,
            self.get_free_agents_data,
            self.get_free_agent_player_info,
            self.get_team_former_free_agents,
            self.get_top_available_free_agents,
            self.search_free_agents_by_name,
        ]
        super().__init__(name="financials_toolkit", tools=tools, **kwargs)

    # --- Contract Data Tools ---

    def get_contracts_data(
        self,
        player_id: Optional[int] = None,
        team_id: Optional[int] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA player contract data, with optional filtering by player ID or team ID.
        Data is sourced from a pre-cleaned CSV file.

        Args:
            player_id (Optional[int], optional): NBA API player ID to filter contracts for a specific player.
                                                 Defaults to None (no player filter).
            team_id (Optional[int], optional): NBA API team ID to filter contracts for a specific team.
                                               Defaults to None (no team filter).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'contracts': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with contract data or an error message.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "contracts": [
                                {{
                                    "Player": str, "Team": str, "Age": int, "YOS": int, // Years of Service
                                    "Y1": Optional[float], ..., "Y8": Optional[float], // Salary per year
                                    "Signed Using": Optional[str], "Guaranteed": Optional[float],
                                    "Agent": Optional[str], "nba_player_id": Optional[float],
                                    "nba_team_id": Optional[float]
                                }}, ...
                            ]
                        }},
                        "parameters": {{"player_id": Optional[int], "team_id": Optional[int]}}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'contracts': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_contracts_data called with player_id: {player_id}, team_id: {team_id}")
        return fetch_contracts_data_logic(
            player_id=player_id,
            team_id=team_id,
            return_dataframe=return_dataframe
        )

    def get_player_contract_details(
        self,
        player_id: int,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets contract information for a specific player by their NBA API player ID.

        Args:
            player_id (int): NBA API player ID. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'contracts': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the player's contract data or an error.
                If return_dataframe=True: Tuple (json_string, {{'contracts': pd.DataFrame}}).
                    The DataFrame contains contract details for the specified player.
                    See `get_contracts_data` for the expected JSON structure of the contract list.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_player_contract_details called for player_id: {player_id}")
        return get_player_contract_logic( # Alias for fetch_contracts_data_logic with player_id
            player_id=player_id,
            return_dataframe=return_dataframe
        )

    def get_team_payroll_summary(
        self,
        team_id: int,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets payroll summary (list of player contracts) for a specific team by their NBA API team ID.

        Args:
            team_id (int): NBA API team ID. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'contracts': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the team's payroll data or an error.
                If return_dataframe=True: Tuple (json_string, {{'contracts': pd.DataFrame}}).
                    The DataFrame contains contract details for all players on the specified team.
                    See `get_contracts_data` for the expected JSON structure of the contract list.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_team_payroll_summary called for team_id: {team_id}")
        return get_team_payroll_logic( # Alias for fetch_contracts_data_logic with team_id
            team_id=team_id,
            return_dataframe=return_dataframe
        )

    def get_highest_paid_players_list(
        self,
        limit: int = 50,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets a list of the highest-paid players based on guaranteed money.

        Args:
            limit (int, optional): Maximum number of players to return. Defaults to 50.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'highest_paid_players': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the list of highest-paid players or an error.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "highest_paid_players": [ {{ ... contract details for each player ... }} ]
                        }},
                        "parameters": {{"limit": int}}
                    }}
                    See `get_contracts_data` for the expected structure of individual contract details.
                If return_dataframe=True: Tuple (json_string, {{'highest_paid_players': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_highest_paid_players_list called with limit: {limit}")
        return get_highest_paid_players_logic(
            limit=limit,
            return_dataframe=return_dataframe
        )

    def search_player_contracts_by_name(
        self,
        player_name: str,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Searches for player contracts by player name (allows partial matches, case-insensitive).
        Results are sorted by guaranteed money in descending order.

        Args:
            player_name (str): Player name to search for. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'player_contracts': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with matching player contracts or an error.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "player_contracts": [ {{ ... contract details for each matching player ... }} ]
                        }},
                        "parameters": {{"player_name": str}}
                    }}
                    See `get_contracts_data` for the expected structure of individual contract details.
                If return_dataframe=True: Tuple (json_string, {{'player_contracts': pd.DataFrame}}).
        """
        logger.info(f"FinancialsToolkit: search_player_contracts_by_name called for player: '{player_name}'")
        return search_player_contracts_logic(
            player_name=player_name,
            return_dataframe=return_dataframe
        )

    # --- Free Agent Data Tools ---

    def get_free_agents_data(
        self,
        player_id: Optional[int] = None,
        team_id: Optional[int] = None, # Represents the player's old team ID
        position: Optional[str] = None,
        free_agent_type: Optional[str] = None, # e.g., "ufa" (Unrestricted), "rfa" (Restricted)
        min_ppg: Optional[float] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA free agent data with optional filtering.
        Data is sourced from a pre-cleaned CSV file.

        Args:
            player_id (Optional[int], optional): NBA API player ID to filter for a specific free agent. Defaults to None.
            team_id (Optional[int], optional): NBA API ID of the player's previous team to filter by. Defaults to None.
            position (Optional[str], optional): Player position to filter by (e.g., "G", "F", "C", "F-C"). Case-insensitive. Defaults to None.
            free_agent_type (Optional[str], optional): Type of free agent to filter by (e.g., "ufa", "rfa"). Defaults to None.
            min_ppg (Optional[float], optional): Minimum Points Per Game (PPG) to filter by. Defaults to None.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'free_agents': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with free agent data or an error message.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "free_agents": [
                                {{
                                    "playerDisplayName": str, "position": str, "type": str, // ufa/rfa
                                    "old_team": str, "PPG": Optional[float], "RPG": Optional[float], "APG": Optional[float],
                                    "nba_player_id": Optional[float], "nba_old_team_id": Optional[float]
                                }}, ...
                            ]
                        }},
                        "parameters": {{... applied filters ...}}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'free_agents': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_free_agents_data called with player_id: {player_id}, team_id: {team_id}, position: {position}, type: {free_agent_type}, min_ppg: {min_ppg}")
        return fetch_free_agents_data_logic(
            player_id=player_id,
            team_id=team_id,
            position=position,
            free_agent_type=free_agent_type,
            min_ppg=min_ppg,
            return_dataframe=return_dataframe
        )

    def get_free_agent_player_info(
        self,
        player_id: int,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets free agent information for a specific player by their NBA API player ID.

        Args:
            player_id (int): NBA API player ID. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'free_agents': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the player's free agent data or an error.
                If return_dataframe=True: Tuple (json_string, {{'free_agents': pd.DataFrame}}).
                    The DataFrame contains free agent details for the specified player.
                    See `get_free_agents_data` for the expected JSON structure of the free agent list.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_free_agent_player_info called for player_id: {player_id}")
        return get_free_agent_info_logic( # Alias for fetch_free_agents_data_logic with player_id
            player_id=player_id,
            return_dataframe=return_dataframe
        )

    def get_team_former_free_agents(
        self,
        team_id: int,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets all free agents who previously played for a specific team, identified by NBA API team ID.

        Args:
            team_id (int): NBA API team ID. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'free_agents': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the list of former free agents for the team or an error.
                If return_dataframe=True: Tuple (json_string, {{'free_agents': pd.DataFrame}}).
                    The DataFrame contains details of free agents previously on the specified team.
                    See `get_free_agents_data` for the expected JSON structure of the free agent list.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_team_former_free_agents called for team_id: {team_id}")
        return get_team_free_agents_logic( # Alias for fetch_free_agents_data_logic with team_id
            team_id=team_id,
            return_dataframe=return_dataframe
        )

    def get_top_available_free_agents(
        self,
        position: Optional[str] = None,
        free_agent_type: Optional[str] = None, # e.g., "ufa", "rfa"
        limit: int = 50,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Gets a list of top free agents, ranked by Points Per Game (PPG), with optional filters for position and free agent type.

        Args:
            position (Optional[str], optional): Player position to filter by (e.g., "G", "F", "C"). Case-insensitive. Defaults to None.
            free_agent_type (Optional[str], optional): Type of free agent ("ufa" or "rfa"). Defaults to None.
            limit (int, optional): Maximum number of free agents to return. Defaults to 50.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'top_free_agents': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with the list of top free agents or an error.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "top_free_agents": [ {{ ... free agent details for each player, sorted by PPG ... }} ]
                        }},
                        "parameters": {{... applied filters ...}}
                    }}
                    See `get_free_agents_data` for the expected structure of individual free agent details.
                If return_dataframe=True: Tuple (json_string, {{'top_free_agents': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"FinancialsToolkit: get_top_available_free_agents called with position: {position}, type: {free_agent_type}, limit: {limit}")
        return get_top_free_agents_logic(
            position=position,
            free_agent_type=free_agent_type,
            limit=limit,
            return_dataframe=return_dataframe
        )

    def search_free_agents_by_name(
        self,
        player_name: str,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Searches for free agents by player name (allows partial matches, case-insensitive).
        Results are sorted by Points Per Game (PPG) in descending order.

        Args:
            player_name (str): Player name to search for. This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'free_agent_search': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with matching free agents or an error.
                    Expected success structure:
                    {{
                        "data_sets": {{
                            "free_agent_search": [ {{ ... free agent details for each matching player ... }} ]
                        }},
                        "parameters": {{"player_name": str}}
                    }}
                    See `get_free_agents_data` for the expected structure of individual free agent details.
                If return_dataframe=True: Tuple (json_string, {{'free_agent_search': pd.DataFrame}}).
        """
        logger.info(f"FinancialsToolkit: search_free_agents_by_name called for player: '{player_name}'")
        return search_free_agents_logic(
            player_name=player_name,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\game_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Game specific stats logic functions
from ..api_tools.game_boxscores import (
    fetch_boxscore_traditional_logic,
    fetch_boxscore_advanced_logic,
    fetch_boxscore_four_factors_logic,
    fetch_boxscore_usage_logic,
    fetch_boxscore_defensive_logic,
    fetch_boxscore_summary_logic,
    fetch_boxscore_misc_logic,
    fetch_boxscore_playertrack_logic,
    fetch_boxscore_scoring_logic,
    fetch_boxscore_hustle_logic
)
from ..api_tools.game_boxscore_matchups import fetch_game_boxscore_matchups_logic
from ..api_tools.game_playbyplay import fetch_playbyplay_logic
from ..api_tools.game_rotation import fetch_game_rotation_logic
from ..api_tools.game_visuals_analytics import fetch_shotchart_logic as fetch_game_shotchart_logic, fetch_win_probability_logic # Alias to avoid name clash

from ..config import settings # Not directly used here but good for consistency
from nba_api.stats.library.parameters import (
    EndPeriod, EndRange, RangeType, StartPeriod, StartRange, RunType
)


class GameToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_game_boxscore_traditional,
            self.get_game_boxscore_advanced,
            self.get_game_boxscore_four_factors,
            self.get_game_boxscore_usage,
            self.get_game_boxscore_defensive,
            self.get_game_boxscore_summary,
            self.get_game_boxscore_misc,
            self.get_game_boxscore_player_tracking,
            self.get_game_boxscore_scoring,
            self.get_game_boxscore_hustle,
            self.get_game_boxscore_matchups,
            self.get_game_play_by_play,
            self.get_game_rotation_data,
            self.get_game_shotchart_data, # For overall game shot chart
            self.get_game_win_probability,
        ]
        super().__init__(name="game_toolkit", tools=tools, **kwargs)

    def get_game_boxscore_traditional(
        self,
        game_id: str,
        start_period: int = StartPeriod.default,
        end_period: int = EndPeriod.default,
        start_range: int = StartRange.default,
        end_range: int = EndRange.default,
        range_type: int = RangeType.default,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Traditional Box Score data (V3) for a given game_id.
        Includes team stats, player stats, and starter/bench breakdowns.

        Args:
            game_id (str): The 10-digit ID of the game.
            start_period (int, optional): Starting period number (1-4 for quarters, 0 for full game). Defaults to 0.
            end_period (int, optional): Ending period number (1-4 for quarters, 0 for full game). Defaults to 0.
            start_range (int, optional): Starting range in seconds from start of game (e.g., 0). Defaults to StartRange.default.
            end_range (int, optional): Ending range in seconds from start of game (e.g., 28800 for full game). Defaults to EndRange.default.
            range_type (int, optional): Type of range. Defaults to RangeType.default.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string containing the traditional boxscore data or an error.
                    Expected success structure:
                    {{
                        "game_id": str,
                        "teams": [{{ ... team stats ... }}],
                        "players": [{{ ... player stats ... }}],
                        "starters_bench": [{{ ... team starter/bench breakdown ... }}],
                        "parameters": {{ ... applied filters ... }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "teams", "players", "starters_bench".
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_traditional called for game_id {game_id}")
        return fetch_boxscore_traditional_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            start_range=start_range, end_range=end_range, range_type=range_type,
            return_dataframe=return_dataframe
        )

    def get_game_boxscore_advanced(
        self,
        game_id: str,
        start_period: int = StartPeriod.default,
        end_period: int = EndPeriod.default,
        start_range: int = StartRange.default,
        end_range: int = EndRange.default,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Advanced Box Score data (V3) for a given game_id.
        Includes advanced metrics like Offensive/Defensive Rating, PIE, USG%, etc. for players and teams.

        Args:
            game_id (str): The 10-digit ID of the game.
            start_period (int, optional): Starting period. Defaults to 0 (full game).
            end_period (int, optional): Ending period. Defaults to 0 (full game).
            start_range (int, optional): Start range in seconds. Defaults to StartRange.default.
            end_range (int, optional): End range in seconds. Defaults to EndRange.default.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with advanced boxscore data or error.
                    Expected success structure:
                    {{
                        "game_id": str,
                        "player_stats": [{{ ... advanced player stats ... }}],
                        "team_stats": [{{ ... advanced team stats ... }}],
                        "parameters": {{ ... applied filters ... }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "player_stats", "team_stats".
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_advanced called for game_id {game_id}")
        return fetch_boxscore_advanced_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            start_range=start_range, end_range=end_range, return_dataframe=return_dataframe
        )

    def get_game_boxscore_four_factors(
        self,
        game_id: str,
        start_period: int = StartPeriod.default,
        end_period: int = EndPeriod.default,
        return_dataframe: bool = False # start_range and end_range are not in logic signature, assuming not used
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Four Factors Box Score data (V3) for a given game_id.
        Includes eFG%, FTA Rate, TOV%, OREB% for players and teams.

        Args:
            game_id (str): The 10-digit ID of the game.
            start_period (int, optional): Starting period. Defaults to 0 (full game).
            end_period (int, optional): Ending period. Defaults to 0 (full game).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with four factors boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_stats': df, 'team_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_four_factors called for game_id {game_id}")
        return fetch_boxscore_four_factors_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            return_dataframe=return_dataframe
        )

    def get_game_boxscore_usage(
        self,
        game_id: str,
        return_dataframe: bool = False # Other period/range filters not in logic signature
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Usage Box Score data (V3) for a given game_id.
        Includes USG%, %FGM, %AST, %REB, etc. for players and teams.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with usage boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_usage_stats': df, 'team_usage_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_usage called for game_id {game_id}")
        return fetch_boxscore_usage_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_boxscore_defensive(
        self,
        game_id: str,
        return_dataframe: bool = False # Other period/range filters not in logic signature
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Defensive Box Score data (V2) for a given game_id.
        Includes stats like DREB, STL, BLK, opponent FG% when player is on court.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with defensive boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_defensive_stats': df, 'team_defensive_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_defensive called for game_id {game_id}")
        return fetch_boxscore_defensive_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_boxscore_summary(
        self,
        game_id: str,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches a comprehensive summary box score (V2) for a given game_id.
        Includes game info, line score, officials, inactive players, etc.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with game summary data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict with multiple keys like 'game_info', 'line_score', etc.).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_summary called for game_id {game_id}")
        return fetch_boxscore_summary_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_boxscore_misc(
        self,
        game_id: str,
        start_period: int = StartPeriod.default,
        end_period: int = EndPeriod.default,
        start_range: int = StartRange.default,
        end_range: int = EndRange.default,
        range_type: int = RangeType.default,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Miscellaneous Box Score data (V3) for a given game_id.
        Includes points off turnovers, second chance points, fast break points, points in paint, etc.

        Args:
            game_id (str): The 10-digit ID of the game.
            // ... (other args from fetch_boxscore_misc_logic)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with miscellaneous boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_stats': df, 'team_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_misc called for game_id {game_id}")
        return fetch_boxscore_misc_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            start_range=start_range, end_range=end_range, range_type=range_type,
            return_dataframe=return_dataframe
        )

    def get_game_boxscore_player_tracking(
        self,
        game_id: str,
        return_dataframe: bool = False # Other period/range filters not in logic signature
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Player Tracking Box Score data (V3) for a given game_id.
        Includes metrics like speed, distance, touches, passes, etc.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player tracking boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_stats': df, 'team_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_player_tracking called for game_id {game_id}")
        return fetch_boxscore_playertrack_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_boxscore_scoring(
        self,
        game_id: str,
        start_period: int = StartPeriod.default,
        end_period: int = EndPeriod.default,
        start_range: int = StartRange.default,
        end_range: int = EndRange.default,
        range_type: int = RangeType.default,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Scoring Box Score data (V3) for a given game_id.
        Includes detailed scoring breakdowns (e.g., %PTS from 2PT, %PTS from FT, %PTS Off TOV).

        Args:
            game_id (str): The 10-digit ID of the game.
            // ... (other args from fetch_boxscore_scoring_logic)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with scoring boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_stats': df, 'team_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_scoring called for game_id {game_id}")
        return fetch_boxscore_scoring_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            start_range=start_range, end_range=end_range, range_type=range_type,
            return_dataframe=return_dataframe
        )

    def get_game_boxscore_hustle(
        self,
        game_id: str,
        return_dataframe: bool = False # Other period/range filters not in logic signature
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches Hustle Box Score data (V2) for a given game_id.
        Includes metrics like contested shots, deflections, charges drawn, screen assists.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with hustle boxscore data or error.
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'player_stats': df, 'team_stats': df}}).
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_hustle called for game_id {game_id}")
        return fetch_boxscore_hustle_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_boxscore_matchups(
        self,
        game_id: str,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player matchup data for a given game using BoxScoreMatchupsV3 endpoint.
        Provides detailed player-vs-player matchup statistics.

        Args:
            game_id (str): The 10-digit ID of the game.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'matchups': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with matchup data or an error.
                    Expected success structure:
                    {{
                        "game_id": str,
                        "matchups": [{{ "GAME_ID": str, "DEF_PLAYER_ID": int, "DEF_PLAYER_NAME": str, "OFF_PLAYER_ID": int, "OFF_PLAYER_NAME": str, "MATCHUP_MIN": float, "PARTIAL_POSS": float, "PLAYER_PTS": int, ... }}],
                        "parameters": {{ "note": "Using BoxScoreMatchupsV3" }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'matchups': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_boxscore_matchups called for game_id {game_id}")
        return fetch_game_boxscore_matchups_logic(
            game_id=game_id, return_dataframe=return_dataframe
        )

    def get_game_play_by_play(
        self,
        game_id: str,
        start_period: int = 0,
        end_period: int = 0,
        event_types: Optional[List[str]] = None,
        player_name: Optional[str] = None,
        person_id: Optional[int] = None,
        team_id: Optional[int] = None,
        team_tricode: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches play-by-play data for a game. Attempts live data first if no period filters
        are applied, otherwise falls back to or directly uses historical data (PlayByPlayV3).
        Provides granular filtering options.

        Args:
            game_id (str): The 10-digit ID of the game.
            start_period (int, optional): Starting period filter (1-4, 0 for all). Defaults to 0.
            end_period (int, optional): Ending period filter (1-4, 0 for all). Defaults to 0.
            event_types (Optional[List[str]], optional): List of event types to filter by (e.g., ['SHOT', 'REBOUND']).
                                                        Common types: SHOT, REBOUND, TURNOVER, FOUL, FREE_THROW, SUBSTITUTION, TIMEOUT, JUMP_BALL, BLOCK, STEAL, VIOLATION.
            player_name (Optional[str], optional): Filter plays by player name (case-insensitive partial match).
            person_id (Optional[int], optional): Filter plays by player ID.
            team_id (Optional[int], optional): Filter plays by team ID.
            team_tricode (Optional[str], optional): Filter plays by team tricode (e.g., 'LAL', 'BOS').
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with PBP data or error.
                    Expected success structure:
                    {{
                        "game_id": str, "has_video": bool, "source": str ("live" or "historical_v3"),
                        "filtered_periods": Optional[Dict], "filters_applied": Optional[Dict],
                        "periods": [
                            {{
                                "period": int,
                                "plays": [
                                    {{
                                        "event_num": int, "clock": str, "score": Optional[str],
                                        "team_tricode": Optional[str], "person_id": Optional[int],
                                        "player_name": Optional[str], "description": str,
                                        "action_type": str, "sub_type": Optional[str],
                                        "event_type": str (e.g., "SHOT_MADE"), "video_available": Optional[bool]
                                    }}, ...
                                ]
                            }}, ...
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'play_by_play', and 'available_video' (if source is historical_v3).
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_play_by_play called for game_id {game_id}")
        return fetch_playbyplay_logic(
            game_id=game_id, start_period=start_period, end_period=end_period,
            event_types=event_types, player_name=player_name, person_id=person_id,
            team_id=team_id, team_tricode=team_tricode, return_dataframe=return_dataframe
        )

    def get_game_rotation_data(
        self,
        game_id: str,
        league_id: str = "00", # LeagueID is a parameter for GameRotation
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches game rotation data, including player in/out times and performance during stints.

        Args:
            game_id (str): The 10-digit ID of the game.
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10" (WNBA).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with game rotation data or error.
                    Expected success structure:
                    {{
                        "parameters": {{"game_id": str, "league_id": str}},
                        "data_sets": {{
                            "GameRotation": [ // Player rotation stints
                                {{
                                    "GAME_ID": str, "TEAM_ID": int, "PERSON_ID": int,
                                    "PLAYER_FIRST": str, "PLAYER_LAST": str,
                                    "IN_TIME_REAL": float, "OUT_TIME_REAL": float,
                                    "PLAYER_PTS": int, "PT_DIFF": int, "USG_PCT": float, ...
                                }}
                            ],
                            "AvailableRotation": [ // Team-level rotation availability summary (often similar to GameRotation but aggregated)
                                {{ ... similar structure to GameRotation but for team totals ... }}
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "GameRotation", "AvailableRotation".
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_rotation_data called for game_id {game_id}")
        return fetch_game_rotation_logic(
            game_id=game_id, league_id=league_id, return_dataframe=return_dataframe
        )

    def get_game_shotchart_data(
        self,
        game_id: str,
        team_id_nullable: Optional[int] = None, # Team ID to filter shots for that team
        player_id_nullable: Optional[int] = None, # Player ID to filter shots for that player
        # ... other filters from fetch_shotchart_logic in game_visuals_analytics.py
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches shot chart data for all players in a specific NBA game.
        Can be filtered by team or player.

        Args:
            game_id (str): The 10-digit ID of the game.
            team_id_nullable (Optional[int], optional): Filter shots by a specific team ID. Defaults to None (all teams).
            player_id_nullable (Optional[int], optional): Filter shots by a specific player ID. Defaults to None (all players).
            // ... (other optional filters like period, shot_type, zone_basic, etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with game-wide shot chart data or error.
                    Expected success structure (if not filtered by player/team, it's a list of shots):
                    {{
                        "game_id": str, "teams": [{{team_id, team_name, shots: [...]}}], "league_averages": [...]
                        // If filtered, might be more specific. The logic in game_visuals_analytics.py handles this.
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict contains 'shots' and 'league_averages'. If filtered, 'shots' reflects filtered data.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_shotchart_data called for game_id {game_id}")
        # Note: The underlying logic function expects player_name, not player_id for primary lookup,
        # but the game_visuals_analytics.py version is designed for game-wide or filtered.
        # Here we assume game_id is primary, and player/team IDs are for filtering that game's data.
        return fetch_game_shotchart_logic(
            game_id=game_id,
            team_id=team_id_nullable, # Pass through directly
            player_id=player_id_nullable, # Pass through directly
            # Pass other relevant filters from signature if added to logic function
            return_dataframe=return_dataframe
        )

    def get_game_win_probability(
        self,
        game_id: str,
        run_type: str = RunType.default,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches win probability data play-by-play for a specific NBA game.

        Args:
            game_id (str): The 10-digit ID of the game.
            run_type (str, optional): Run type for win probability calculation.
                                      Defaults to "each play".
                                      Possible: "each play", "each second", "each poss".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with game info and win probability PBP data or error.
                    Expected success structure:
                    {{
                        "game_id": str,
                        "game_info": {{ "GAME_ID": str, "GAME_DATE_EST": str, "HOME_TEAM_ID": int, ... }},
                        "win_probability": [
                            {{
                                "EVENT_NUM": int, "PCTIMESTRING": str, "HOME_PCT": float, "VISITOR_PCT": float,
                                "HOME_PTS": int, "VISITOR_PTS": int, "HOME_POSS_IND": int, ...
                            }}
                        ],
                        "run_type": str
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'game_info': df, 'win_probability': df}}).
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"GameToolkit: get_game_win_probability called for game_id {game_id}")
        return fetch_win_probability_logic(
            game_id=game_id, run_type=run_type, return_dataframe=return_dataframe
        )

===== backend\tool_kits\league_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# League-wide stats logic functions
from ..api_tools.all_time_leaders_grids import fetch_all_time_leaders_logic
from ..api_tools.assist_leaders import fetch_assist_leaders_logic
from ..api_tools.homepage_leaders import fetch_homepage_leaders_logic
from ..api_tools.homepage_v2 import fetch_homepage_v2_logic
from ..api_tools.ist_standings import fetch_ist_standings_logic
from ..api_tools.leaders_tiles import fetch_leaders_tiles_logic
from ..api_tools.league_dash_player_bio import fetch_league_player_bio_stats_logic
from ..api_tools.league_dash_player_clutch import fetch_league_player_clutch_stats_logic
from ..api_tools.league_dash_player_pt_shot import fetch_league_dash_player_pt_shot_logic
from ..api_tools.league_dash_player_shot_locations import fetch_league_dash_player_shot_locations_logic
from ..api_tools.league_dash_player_stats import fetch_league_player_stats_logic
from ..api_tools.league_dash_pt_defend import fetch_league_dash_pt_defend_logic
from ..api_tools.league_dash_pt_stats import fetch_league_dash_pt_stats_logic
from ..api_tools.league_dash_pt_team_defend import fetch_league_dash_pt_team_defend_logic
from ..api_tools.league_dash_team_clutch import fetch_league_team_clutch_stats_logic
from ..api_tools.league_dash_team_pt_shot import fetch_league_dash_team_pt_shot_logic
from ..api_tools.league_dash_team_shot_locations import fetch_league_team_shot_locations_logic
from ..api_tools.league_dash_team_stats import fetch_league_team_stats_logic
from ..api_tools.league_draft import fetch_draft_history_logic
from ..api_tools.league_game_log import fetch_league_game_log_logic
from ..api_tools.league_hustle_stats_team import fetch_league_hustle_stats_team_logic
from ..api_tools.league_leaders_data import fetch_league_leaders_logic
from ..api_tools.league_lineups import fetch_league_dash_lineups_logic as fetch_league_lineups_data_logic
from ..api_tools.league_lineup_viz import fetch_league_lineup_viz_logic
from ..api_tools.league_standings import fetch_league_standings_logic
from ..api_tools.player_index import fetch_player_index_logic
from ..api_tools.player_listings import fetch_common_all_players_logic
from ..api_tools.playoff_picture import fetch_playoff_picture_logic
from ..api_tools.playoff_series import fetch_common_playoff_series_logic
from ..api_tools.schedule_league_v2_int import fetch_schedule_league_v2_int_logic
from ..api_tools.shot_chart_league_wide import fetch_shot_chart_league_wide_logic
from ..api_tools.trending_team_tools import fetch_top_teams_logic
from ..api_tools.trending_tools import fetch_top_performers_logic
from ..api_tools.player_estimated_metrics import fetch_player_estimated_metrics_logic as fetch_league_player_estimated_metrics_logic # Alias


from ..config import settings
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense,
    PerModeSimple, PerModeTime, PlayerOrTeam, PlayerScope, StatCategory, GameScopeDetailed,
    Direction, Sorter, PlayerOrTeamAbbreviation, StatCategoryAbbreviation, PerMode48, Scope,
    DefenseCategory, PtMeasureType, DistanceRange, MeasureTypeSimple, SeasonTypeAllStar, SeasonTypeAllStar
)


class LeagueToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_all_time_leaders_grids,
            self.get_assist_leaders,
            self.get_homepage_leaders,
            self.get_homepage_v2_leaders,
            self.get_in_season_tournament_standings,
            self.get_leaders_tiles,
            self.get_league_player_bio_stats,
            self.get_league_player_clutch_stats,
            self.get_league_player_shooting_stats,
            self.get_league_player_shot_locations,
            self.get_league_player_stats,
            self.get_league_player_tracking_defense_stats,
            self.get_league_player_tracking_stats,
            self.get_league_team_tracking_defense_stats,
            self.get_league_team_clutch_stats,
            self.get_league_team_player_tracking_shot_stats,
            self.get_league_team_shot_locations,
            self.get_league_team_stats,
            self.get_draft_history,
            self.get_league_game_log,
            self.get_league_hustle_stats_team,
            self.get_league_leaders,
            self.get_league_lineups_data,
            self.get_league_lineup_viz_data,
            self.get_league_standings,
            self.get_player_index,
            self.get_common_all_players,
            self.get_playoff_picture,
            self.get_common_playoff_series,
            self.get_league_schedule,
            self.get_shot_chart_league_wide,
            self.get_top_performing_teams,
            self.get_top_performing_players,
            self.get_league_player_estimated_metrics, # Added this one
        ]
        super().__init__(name="league_toolkit", tools=tools, **kwargs)

    def get_all_time_leaders_grids(
        self,
        league_id: str = "00",
        per_mode: str = "Totals",
        season_type: str = "Regular Season",
        topx: int = 10,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches all-time statistical leaders data grids from the NBA API.
        Provides leaders in categories like Points, Rebounds, Assists, Steals, Blocks, etc.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible values: "00" (NBA), "10" (WNBA), "20" (G-League).
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
                                      Possible values: "Totals", "PerGame".
            season_type (str, optional): Season type context for leadership. Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Pre Season".
            topx (int, optional): Number of top players to return for each category. Defaults to 10.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with all-time leaders data or error.
                    Expected success structure:
                    {{
                        "parameters": {{"league_id": str, "per_mode": str, "season_type": str, "topx": int}},
                        "data_sets": {{
                            "PTSLeaders": [{{ "PLAYER_ID": int, "PLAYER_NAME": str, "PTS": float, "TEAM_ABBREVIATION": str, ... }}],
                            "ASTLeaders": [{{ ... similar structure ... }}],
                            // ... other statistical leader categories such as BLKLeaders, REBLeaders, STLLeaders, etc.
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys match the leader categories (e.g., 'PTSLeaders', 'ASTLeaders').
                    A combined DataFrame with a 'Category' column is saved to CSV if DataFrames are returned.
                    CSV cache path for the combined data included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_all_time_leaders_grids called for League: {league_id}, PerMode: {per_mode}, TopX: {topx}")
        return fetch_all_time_leaders_logic(
            league_id=league_id,
            per_mode=per_mode,
            season_type=season_type,
            topx=topx,
            return_dataframe=return_dataframe
        )

    def get_assist_leaders(
        self,
        league_id: str = "00",
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = "Totals",
        player_or_team: str = "Team",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches assist leaders statistics for players or teams for a given season.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10", "20".
            season (str, optional): Season in YYYY-YY format. Defaults to current NBA season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible from SeasonTypeAllStar enum: "Regular Season", "Playoffs", "Pre Season", "All Star".
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
                                      Possible: "Totals", "PerGame". (API uses per_mode_simple)
            player_or_team (str, optional): Filter by Player or Team. Defaults to "Team".
                                            Possible: "Team", "Player".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'AssistLeaders': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with assist leaders data or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "AssistLeaders": [
                                {{
                                    "RANK": int, "PLAYER_ID" (or "TEAM_ID"): int, "PLAYER" (or "TEAM"): str,
                                    "AST": float, "GP": int, "MIN": float, ...
                                }}
                            ]
                            // Potentially other datasets like "AssistLeaders_1" if API returns multiple tables.
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict {{'AssistLeaders': pd.DataFrame, ...}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_assist_leaders called for League: {league_id}, Season: {season}")
        return fetch_assist_leaders_logic(
            league_id=league_id,
            season=season,
            season_type=season_type, # API logic maps this to season_type_playoffs
            per_mode=per_mode, # API logic maps this to per_mode_simple
            player_or_team=player_or_team,
            return_dataframe=return_dataframe
        )

    def get_homepage_leaders(
        self,
        league_id: str = "00",
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_playoffs: str = SeasonTypeAllStar.regular,
        player_or_team: str = PlayerOrTeam.team,
        player_scope: str = PlayerScope.all_players,
        stat_category: str = StatCategory.points,
        game_scope_detailed: str = GameScopeDetailed.season,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches homepage leaders data, including team/player leaders for a specific stat category, league averages, and league highs.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10" (WNBA).
            season (str, optional): Season in YYYY-YY format. Defaults to current NBA season.
            season_type_playoffs (str, optional): Season type. Defaults to "Regular Season".
                                                 Possible from SeasonTypePlayoffs enum: "Regular Season", "Playoffs".
            player_or_team (str, optional): Filter by Player or Team. Defaults to "Team".
                                            Possible from PlayerOrTeam enum.
            player_scope (str, optional): Scope of players. Defaults to "All Players".
                                          Possible from PlayerScope enum: "All Players", "Rookies".
            stat_category (str, optional): Statistical category. Defaults to "Points".
                                           Possible from StatCategory enum: "Points", "Rebounds", "Assists".
            game_scope_detailed (str, optional): Game scope. Defaults to "Season".
                                               Possible from GameScopeDetailed enum: "Season", "Last 10".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with homepage leaders data or error.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "HomePageLeaders": [{{ "TEAM_ID": int, "TEAM_NAME": str, "PTS": float, ... }}], // Or PLAYER_ID, PLAYER_NAME if player_or_team is "Player"
                            "LeagueAverage": [{{ "STAT": str, "VALUE": float, ... }}],
                            "LeagueHigh": [{{ "STAT": str, "VALUE": float, "PLAYER_NAME": str, ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "HomePageLeaders", "LeagueAverage", "LeagueHigh".
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_homepage_leaders called for League: {league_id}, Season: {season}, Stat: {stat_category}")
        return fetch_homepage_leaders_logic(
            league_id=league_id, season=season, season_type_playoffs=season_type_playoffs,
            player_or_team=player_or_team, player_scope=player_scope, stat_category=stat_category,
            game_scope_detailed=game_scope_detailed, return_dataframe=return_dataframe
        )

    def get_homepage_v2_leaders(
        self,
        league_id: str = "00",
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_playoffs: str = SeasonTypeAllStar.regular,
        player_or_team: str = PlayerOrTeam.team,
        player_scope: str = PlayerScope.all_players,
        stat_type: str = "Traditional",
        game_scope_detailed: str = GameScopeDetailed.season,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches enhanced homepage leaders data (V2) across multiple statistical categories (Points, Rebounds, Assists, etc.).

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10" (WNBA).
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type_playoffs (str, optional): Season type. Defaults to "Regular Season".
                                                 Possible from SeasonTypePlayoffs enum.
            player_or_team (str, optional): Filter Player or Team. Defaults to "Team".
                                            Possible from PlayerOrTeam enum.
            player_scope (str, optional): Player scope. Defaults to "All Players".
                                          Possible from PlayerScope enum.
            stat_type (str, optional): Statistical type. Defaults to "Traditional".
                                       Possible: "Traditional", "Advanced".
            game_scope_detailed (str, optional): Game scope. Defaults to "Season".
                                               Possible from GameScopeDetailed enum.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with V2 homepage leaders or error.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "Points": [{{ "TEAM_ID": int, "TEAM_NAME": str, "PTS": float, ... }}], // Or PLAYER_ID if player_or_team is "Player"
                            "Rebounds": [{{ ... }}],
                            "Assists": [{{ ... }}],
                            // ... other categories: Steals, FieldGoalPct, FreeThrowPct, ThreePointPct, Blocks
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "Points", "Rebounds", "Assists", etc.
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_homepage_v2_leaders called for League: {league_id}, Season: {season}, StatType: {stat_type}")
        return fetch_homepage_v2_logic(
            league_id=league_id, season=season, season_type_playoffs=season_type_playoffs,
            player_or_team=player_or_team, player_scope=player_scope, stat_type=stat_type,
            game_scope_detailed=game_scope_detailed, return_dataframe=return_dataframe
        )

    def get_in_season_tournament_standings(
        self,
        league_id: str = "00",
        season: str = settings.CURRENT_NBA_SEASON, # API logic expects YYYY-YY for season
        section: str = "group",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches In-Season Tournament (IST) standings data.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Only "00" is typically valid for IST.
            season (str, optional): Season in YYYY-YY format for which to get IST standings. Defaults to current NBA season.
            section (str, optional): Section type of the tournament. Defaults to "group".
                                     Possible: "group", "knockout".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'ISTStandings': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with IST standings or error.
                    Expected success structure:
                    {{
                        "parameters": {{"league_id": str, "season": str, "section": str}},
                        "data_sets": {{
                            "ISTStandings": [
                                {{
                                    "leagueId": str, "seasonYear": int, "teamId": int, "teamCity": str,
                                    "teamName": str, "teamAbbreviation": str, "istGroup": str,
                                    "wins": int, "losses": int, "pct": float, "pts": int, "oppPts": int, ...
                                    // Also includes many game-specific columns like game1Id, game1Opponent, etc.
                                }}
                            ]
                            // May include other datasets depending on API version
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'ISTStandings': pd.DataFrame, ...}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_in_season_tournament_standings called for League: {league_id}, Season: {season}")
        return fetch_ist_standings_logic(
            league_id=league_id,
            season=season, # The logic function takes YYYY-YY season
            section=section,
            return_dataframe=return_dataframe
        )

    def get_leaders_tiles(
        self,
        game_scope_detailed: str = GameScopeDetailed.season,
        league_id: str = "00",
        player_or_team: str = PlayerOrTeam.team,
        player_scope: str = PlayerScope.all_players,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_playoffs: str = SeasonTypeAllStar.regular,
        stat: str = "PTS", # API param is stat, not stat_category
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches statistical leaders tiles data, including current season, all-time high/low, and last season leaders for a specific stat.

        Args:
            game_scope_detailed (str, optional): Game scope. Defaults to "Season".
                                               Possible from GameScopeDetailed enum.
            league_id (str, optional): League ID. Defaults to "00" (NBA). Only "00" supported.
            player_or_team (str, optional): Filter Player or Team. Defaults to "Team".
                                            Possible from PlayerOrTeam enum.
            player_scope (str, optional): Player scope. Defaults to "All Players".
                                          Possible from PlayerScope enum.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type_playoffs (str, optional): Season type. Defaults to "Regular Season".
                                                 Possible from SeasonTypePlayoffs enum.
            stat (str, optional): Statistical category abbreviation. Defaults to "PTS".
                                  Possible: "PTS", "REB", "AST", "STL", "BLK", "FG_PCT", "FG3_PCT", "FT_PCT".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with leaders tiles data or error.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "CurrentSeasonLeaders": [{{ "RANK": int, "TEAM_ID": int (or "PLAYER_ID"), "TEAM_NAME": str (or "PLAYER_NAME"), "PTS": float (or current stat), ... }}],
                            "AllTimeHigh": [{{ ... similar structure for all-time high record holder ... }}],
                            "LastSeasonLeaders": [{{ ... similar structure for last season's leaders ... }}],
                            "AllTimeLow": [{{ ... similar structure for all-time low record holder (less common) ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    Keys: "CurrentSeasonLeaders", "AllTimeHigh", "LastSeasonLeaders", "AllTimeLow".
                    CSV cache paths for each DataFrame included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_leaders_tiles called for Stat: {stat}, Season: {season}")
        return fetch_leaders_tiles_logic(
            game_scope_detailed=game_scope_detailed, league_id=league_id, player_or_team=player_or_team,
            player_scope=player_scope, season=season, season_type_playoffs=season_type_playoffs,
            stat=stat, return_dataframe=return_dataframe
        )

    # ... (Implementations for the rest of the league-wide tools will follow the same pattern) ...
    # You'll need to carefully craft the docstrings for each.

    def get_league_player_bio_stats(
        self,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        per_mode: str = "PerGame",
        league_id: str = "00",
        team_id: Optional[str] = None,
        player_position: Optional[str] = None,
        player_experience: Optional[str] = None,
        starter_bench: Optional[str] = None,
        college: Optional[str] = None,
        country: Optional[str] = None,
        draft_year: Optional[str] = None,
        height: Optional[str] = None,
        weight: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player biographical statistics for all players in a league, with various filters.
        Includes demographics, college, country, draft info, and basic/advanced stats.

        Args:
            season (str, optional): Season in YYYY-YY format. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Playoffs", "Pre Season", "All Star".
            per_mode (str, optional): Per mode for stats. Defaults to "PerGame".
                                      Possible values: "Totals", "PerGame".
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible: "00", "10" (WNBA), "20" (G-League).
            team_id (Optional[str], optional): Team ID to filter players by.
            player_position (Optional[str], optional): Player position filter (e.g., "Forward", "Guard", "Center", "Center-Forward").
            player_experience (Optional[str], optional): Player experience filter (e.g., "Rookie", "Sophomore", "Veteran").
            starter_bench (Optional[str], optional): Starter/bench filter (e.g., "Starters", "Bench").
            college (Optional[str], optional): College filter.
            country (Optional[str], optional): Country filter.
            draft_year (Optional[str], optional): Draft year filter (YYYY).
            height (Optional[str], optional): Height filter (e.g., "6-5", "GT 6-10").
            weight (Optional[str], optional): Weight filter (e.g., "LT 200", "GT 250").
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'LeagueDashPlayerBioStats': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player bio stats or error.
                    Expected success structure:
                    {{
                        "parameters": {{...applied filters...}},
                        "data_sets": {{
                            "LeagueDashPlayerBioStats": [
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str, "TEAM_ABBREVIATION": str, "AGE": int,
                                    "PLAYER_HEIGHT_INCHES": int, "PLAYER_WEIGHT": str, "COLLEGE": str,
                                    "COUNTRY": str, "DRAFT_YEAR": Optional[str], "GP": int, "PTS": float, ... (other stats)
                                }}
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'LeagueDashPlayerBioStats': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_league_player_bio_stats called for Season: {season}, Type: {season_type}")
        return fetch_league_player_bio_stats_logic(
            season=season, season_type=season_type, per_mode=per_mode, league_id=league_id,
            team_id=team_id, player_position=player_position, player_experience=player_experience,
            starter_bench=starter_bench, college=college, country=country, draft_year=draft_year,
            height=height, weight=weight, return_dataframe=return_dataframe
        )

    # ... (Continue for all other methods in the tools list)
    # This is very extensive. I'll provide stubs for the remaining ones.
    # You'll need to fill in the call to the logic function and write the full docstring.

    def get_league_player_clutch_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = "Regular Season", per_mode: str = "PerGame", measure_type: str = "Base", clutch_time: str = "Last 5 Minutes", ahead_behind: str = "Ahead or Behind", point_diff: int = 5, league_id: str = "00", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide player clutch statistics with various filters. See original `league_dash_player_clutch.py` for all parameters."""
        logger.info(f"LeagueToolkit: get_league_player_clutch_stats for Season: {season}, Type: {season_type}, Clutch: {clutch_time}")
        return fetch_league_player_clutch_stats_logic(season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, clutch_time=clutch_time, ahead_behind=ahead_behind, point_diff=point_diff, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_player_shooting_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerModeSimple.per_game, league_id: str = LeagueID.nba, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide player shooting statistics (LeagueDashPlayerPtShot). See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_player_shooting_stats for season {season}, type {season_type}")
        return fetch_league_dash_player_pt_shot_logic(season=season, season_type=season_type, per_mode=per_mode, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_player_shot_locations(self, distance_range: str = "By Zone", season: str = settings.CURRENT_NBA_SEASON, season_type_all_star: str = SeasonTypeAllStar.regular, league_id_nullable: str = "", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide player shot location data. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_player_shot_locations for season {season}, distance_range {distance_range}")
        return fetch_league_dash_player_shot_locations_logic(distance_range=distance_range, season=season, season_type_all_star=season_type_all_star, league_id_nullable=league_id_nullable, return_dataframe=return_dataframe, **kwargs)

    def get_league_player_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = "Regular Season", per_mode: str = "PerGame", measure_type: str = "Base", league_id: str = "00", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches comprehensive league-wide player statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_player_stats for season {season}, measure_type {measure_type}")
        return fetch_league_player_stats_logic(season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_player_tracking_defense_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerModeSimple.per_game, defense_category: str = DefenseCategory.overall, league_id: str = LeagueID.nba, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide defensive player tracking statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_player_tracking_defense_stats for season {season}, category {defense_category}")
        return fetch_league_dash_pt_defend_logic(season=season, season_type=season_type, per_mode=per_mode, defense_category=defense_category, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_player_tracking_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerModeSimple.per_game, player_or_team: str = PlayerOrTeam.team, pt_measure_type: str = PtMeasureType.speed_distance, league_id_nullable: Optional[str] = None, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide player or team tracking statistics (Speed, Rebounding, Possessions, etc.). See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_player_tracking_stats for season {season}, measure {pt_measure_type}")
        return fetch_league_dash_pt_stats_logic(season=season, season_type=season_type, per_mode=per_mode, player_or_team=player_or_team, pt_measure_type=pt_measure_type, league_id_nullable=league_id_nullable, return_dataframe=return_dataframe, **kwargs)

    def get_league_team_tracking_defense_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerModeSimple.per_game, defense_category: str = DefenseCategory.overall, league_id: str = LeagueID.nba, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide defensive team tracking statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_team_tracking_defense_stats for season {season}, category {defense_category}")
        return fetch_league_dash_pt_team_defend_logic(season=season, season_type=season_type, per_mode=per_mode, defense_category=defense_category, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_team_clutch_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = "Regular Season", per_mode: str = "PerGame", measure_type: str = "Base", clutch_time: str = "Last 5 Minutes", ahead_behind: str = "Ahead or Behind", point_diff: int = 5, league_id: str = "00", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide team clutch statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_team_clutch_stats for season {season}, clutch_time {clutch_time}")
        return fetch_league_team_clutch_stats_logic(season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, clutch_time=clutch_time, ahead_behind=ahead_behind, point_diff=point_diff, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_league_team_player_tracking_shot_stats(self, league_id: str = "00", per_mode_simple: str = PerModeSimple.totals, season: str = settings.CURRENT_NBA_SEASON, season_type_all_star: str = SeasonTypeAllStar.regular, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide team player tracking shot data. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_team_player_tracking_shot_stats for league {league_id}, season {season}")
        return fetch_league_dash_team_pt_shot_logic(league_id=league_id, per_mode_simple=per_mode_simple, season=season, season_type_all_star=season_type_all_star, return_dataframe=return_dataframe, **kwargs)

    def get_league_team_shot_locations(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerModeDetailed.per_game, measure_type: str = MeasureTypeSimple.base, distance_range: str = DistanceRange.by_zone, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide team shot location statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_team_shot_locations for season {season}, distance_range {distance_range}")
        return fetch_league_team_shot_locations_logic(season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, distance_range=distance_range, return_dataframe=return_dataframe, **kwargs)

    def get_league_team_stats(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = "Regular Season", per_mode: str = "PerGame", measure_type: str = "Base", league_id: str = "00", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches comprehensive league-wide team statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_team_stats for season {season}, measure_type {measure_type}")
        return fetch_league_team_stats_logic(season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type, league_id=league_id, return_dataframe=return_dataframe, **kwargs)

    def get_draft_history(self, season_year_nullable: Optional[str] = None, league_id_nullable: str = LeagueID.nba, team_id_nullable: Optional[int] = None, round_num_nullable: Optional[int] = None, overall_pick_nullable: Optional[int] = None, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from league_draft.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_draft_history for year {season_year_nullable or 'All'}")
        return fetch_draft_history_logic(season_year_nullable, league_id_nullable, team_id_nullable, round_num_nullable, overall_pick_nullable, return_dataframe)

    def get_league_game_log(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, player_or_team: str = PlayerOrTeamAbbreviation.team, league_id: str = LeagueID.nba, direction: str = Direction.asc, sorter: str = Sorter.date, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches game logs for all teams or players in the league. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_game_log for season {season}, entity: {player_or_team}")
        return fetch_league_game_log_logic(season=season, season_type=season_type, player_or_team=player_or_team, league_id=league_id, direction=direction, sorter=sorter, return_dataframe=return_dataframe, **kwargs)

    def get_league_hustle_stats_team(self, per_mode_time: str = PerModeTime.totals, season: str = settings.CURRENT_NBA_SEASON, season_type_all_star: str = SeasonTypeAllStar.regular, league_id_nullable: str = "", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league-wide team hustle statistics. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_hustle_stats_team for season {season}, per_mode {per_mode_time}")
        return fetch_league_hustle_stats_team_logic(per_mode_time=per_mode_time, season=season, season_type_all_star=season_type_all_star, league_id_nullable=league_id_nullable, return_dataframe=return_dataframe, **kwargs)

    def get_league_leaders(self, season: str, stat_category: str = StatCategoryAbbreviation.pts, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerMode48.per_game, league_id: str = LeagueID.nba, scope: str = Scope.s, top_n: int = 10, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league leaders for a specific statistical category. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_leaders for season {season}, category {stat_category}")
        return fetch_league_leaders_logic(season=season, stat_category=stat_category, season_type=season_type, per_mode=per_mode, league_id=league_id, scope=scope, top_n=top_n, return_dataframe=return_dataframe)

    def get_league_lineups_data(self, season: str, group_quantity: int = 5, measure_type: str = MeasureTypeDetailedDefense.base, per_mode: str = PerModeDetailed.totals, season_type: str = SeasonTypeAllStar.regular, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: # API uses season_type_all_star
        """Fetches league-wide lineup statistics with extensive filtering. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_lineups_data for season {season}, group_quantity {group_quantity}")
        return fetch_league_lineups_data_logic(season=season, group_quantity=group_quantity, measure_type=measure_type, per_mode=per_mode, season_type=season_type, return_dataframe=return_dataframe, **kwargs) # Ensure logic func also uses season_type_all_star

    def get_league_lineup_viz_data(self, minutes_min: int = 5, group_quantity: int = 5, season: str = settings.CURRENT_NBA_SEASON, season_type_all_star: str = SeasonTypeAllStar.regular, league_id_nullable: str = "", return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches league lineup visualization data. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_league_lineup_viz_data for season {season}, group_quantity {group_quantity}")
        return fetch_league_lineup_viz_logic(minutes_min=minutes_min, group_quantity=group_quantity, season=season, season_type_all_star=season_type_all_star, league_id_nullable=league_id_nullable, return_dataframe=return_dataframe, **kwargs)

    def get_league_standings(self, season: Optional[str] = None, season_type: str = SeasonTypeAllStar.regular, league_id: str = LeagueID.nba, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from league_standings.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_league_standings for season {season or settings.CURRENT_NBA_SEASON}, type {season_type}")
        return fetch_league_standings_logic(season, season_type, league_id, return_dataframe)

    def get_player_index(self, league_id: str = "00", season: str = settings.CURRENT_NBA_SEASON, active: Optional[str] = None, allstar: Optional[str] = None, historical: Optional[str] = None, return_dataframe: bool = False, **kwargs) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """Fetches a comprehensive directory of all players with filters. See original file for all optional args."""
        logger.info(f"LeagueToolkit: get_player_index for league {league_id}, season {season}")
        return fetch_player_index_logic(league_id=league_id, season=season, active=active, allstar=allstar, historical=historical, return_dataframe=return_dataframe, **kwargs)

    def get_common_all_players(self, season: str, league_id: str = LeagueID.nba, is_only_current_season: int = 1, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from player_listings.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_common_all_players for season {season}, league {league_id}")
        return fetch_common_all_players_logic(season, league_id, is_only_current_season, return_dataframe)

    def get_playoff_picture(self, league_id: str = "00", season_id: str = "22024", return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: # CURRENT_NBA_SEASON_ID logic is "22024"
        """[Docstring from playoff_picture.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_playoff_picture for league {league_id}, season_id {season_id}")
        return fetch_playoff_picture_logic(league_id, season_id, return_dataframe)

    def get_common_playoff_series(self, season: str, league_id: str = LeagueID.nba, series_id: Optional[str] = None, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from playoff_series.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_common_playoff_series for season {season}, league {league_id}")
        return fetch_common_playoff_series_logic(season, league_id, series_id, return_dataframe)

    def get_league_schedule(self, league_id: str = "00", season: str = settings.CURRENT_NBA_SEASON, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from schedule_league_v2_int.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_league_schedule for league {league_id}, season {season}")
        return fetch_schedule_league_v2_int_logic(league_id, season, return_dataframe)

    def get_shot_chart_league_wide(self, league_id: str = "00", season: str = settings.CURRENT_NBA_SEASON, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from shot_chart_league_wide.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_shot_chart_league_wide for league {league_id}, season {season}")
        return fetch_shot_chart_league_wide_logic(league_id, season, return_dataframe)

    def get_top_performing_teams(self, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, league_id: str = LeagueID.nba, top_n: int = 5, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from trending_team_tools.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_top_performing_teams for season {season}, top {top_n}")
        return fetch_top_teams_logic(season, season_type, league_id, top_n, return_dataframe=return_dataframe)

    def get_top_performing_players(self, category: str = StatCategoryAbbreviation.pts, season: str = settings.CURRENT_NBA_SEASON, season_type: str = SeasonTypeAllStar.regular, per_mode: str = PerMode48.per_game, scope: str = Scope.s, league_id: str = LeagueID.nba, top_n: int = 5, return_dataframe: bool = False) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """[Docstring from trending_tools.py, adapted for toolkit]"""
        logger.info(f"LeagueToolkit: get_top_performing_players for category {category}, season {season}, top {top_n}")
        return fetch_top_performers_logic(category, season, season_type, per_mode, scope, league_id, top_n, return_dataframe=return_dataframe)

    def get_league_player_estimated_metrics(
        self,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, pd.DataFrame]]: # Note: Original returns pd.DataFrame directly, adapting for toolkit
        """
        Fetches player estimated metrics (E_OFF_RATING, E_DEF_RATING, E_NET_RATING, etc.)
        for all players in a given season and season type.

        Args:
            season (str, optional): NBA season in YYYY-YY format (e.g., '2023-24'). Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible from SeasonTypeAllStar enum.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Possible from LeagueID enum.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, DataFrame).
                                               The DataFrame contains all player estimated metrics. Defaults to False.

        Returns:
            Union[str, Tuple[str, pd.DataFrame]]:
                If return_dataframe=False: JSON string with estimated metrics for all players or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "player_estimated_metrics": [
                            {{
                                "PLAYER_ID": int, "PLAYER_NAME": str, "TEAM_ABBREVIATION": str,
                                "E_OFF_RATING": float, "E_DEF_RATING": float, "E_NET_RATING": float,
                                "E_PACE": float, ... (other estimated metrics and ranks) ...
                            }}
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, pd.DataFrame). The DataFrame is the primary data.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LeagueToolkit: get_league_player_estimated_metrics called for season {season}")
        json_response, df_or_dict = fetch_league_player_estimated_metrics_logic(
            season=season,
            season_type=season_type,
            league_id=league_id,
            return_dataframe=return_dataframe # Pass through the flag
        )
        if return_dataframe:
            # The logic function already returns Tuple[str, pd.DataFrame], so we directly return it
            return json_response, {"player_estimated_metrics": df_or_dict} # Ensure dict format for DataFrame
        return json_response

===== backend\tool_kits\live_updates_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Live data logic functions
from ..api_tools.live_game_tools import fetch_league_scoreboard_logic
from ..api_tools.odds_tools import fetch_odds_data_logic
from ..api_tools.scoreboard_tools import fetch_scoreboard_data_logic as fetch_static_or_live_scoreboard_logic # More comprehensive scoreboard

from nba_api.stats.library.parameters import LeagueID # For default league ID

class LiveUpdatesToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_live_league_scoreboard,
            self.get_todays_game_odds,
            self.get_scoreboard_for_date,
        ]
        super().__init__(name="live_updates_toolkit", tools=tools, **kwargs)

    def get_live_league_scoreboard(
        self,
        bypass_cache: bool = False,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches and formats live scoreboard data for current NBA games using nba_api.live.
        This provides real-time game status, scores, and basic team info.
        Cache for this endpoint is typically short-lived (e.g., 10 seconds).

        Args:
            bypass_cache (bool, optional): If True, ignores cached data and fetches fresh data.
                                           Defaults to False.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'games': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string of current game information or an error message.
                    Expected success structure:
                    {{
                        "date": str (YYYY-MM-DD),
                        "games": [
                            {{
                                "game_id": Optional[str],
                                "start_time_utc": str,
                                "status": {{
                                    "clock": Optional[str], "period": int,
                                    "state_code": int (1=Scheduled, 2=In Progress, 3=Final),
                                    "state_text": str (e.g., "Halftime", "Q1 0:00", "Final")
                                }},
                                "home_team": {{ "id": Optional[int], "code": str, "name": Optional[str], "score": int, "record": Optional[str], "wins": Optional[int], "losses": Optional[int] }},
                                "away_team": {{ "id": Optional[int], "code": str, "name": Optional[str], "score": int, "record": Optional[str], "wins": Optional[int], "losses": Optional[int] }}
                            }}, ...
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, {{'games': pd.DataFrame}}).
                    The DataFrame flattens the game information for easier analysis.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LiveUpdatesToolkit: get_live_league_scoreboard called, bypass_cache: {bypass_cache}")
        return fetch_league_scoreboard_logic(
            bypass_cache=bypass_cache,
            return_dataframe=return_dataframe
        )

    def get_todays_game_odds(
        self,
        bypass_cache: bool = False,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches live betting odds for today's NBA games.
        Data includes various markets (moneyline, spread, total) from different bookmakers.
        Cache for this endpoint is typically around 1 hour.

        Args:
            bypass_cache (bool, optional): If True, ignores cached data and fetches fresh data.
                                           Defaults to False.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'odds': DataFrame}}).
                                               The DataFrame flattens the game-market-book-outcome structure.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string containing a list of today's games with their odds.
                    Expected success structure:
                    {{
                        "games": [
                            {{
                                "gameId": str, "awayTeamId": int, "homeTeamId": int, "gameTime": str,
                                "gameStatus": int, "gameStatusText": str,
                                "markets": [
                                    {{
                                        "marketId": str, "name": str, // e.g., "Moneyline"
                                        "books": [
                                            {{
                                                "bookId": str, "name": str, // e.g., "DraftKings"
                                                "outcomes": [
                                                    {{ "type": str, "odds": str, "value": Optional[str] }}
                                                ]
                                            }}
                                        ]
                                    }}
                                ]
                            }}
                        ]
                    }}
                    Returns {{"games": []}} if no odds data or an error occurs.
                If return_dataframe=True: Tuple (json_string, {{'odds': pd.DataFrame}}).
                    The DataFrame flattens the nested structure into one record per game-market-book-outcome.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"LiveUpdatesToolkit: get_todays_game_odds called, bypass_cache: {bypass_cache}")
        return fetch_odds_data_logic(
            bypass_cache=bypass_cache,
            return_dataframe=return_dataframe
        )

    def get_scoreboard_for_date(
        self,
        game_date: Optional[str] = None,
        league_id: str = LeagueID.nba,
        day_offset: int = 0,
        bypass_cache: bool = False,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA scoreboard data for a specific date.
        Uses live API for today's date, static API (scoreboardv2) for past/future dates or if live is stale.

        Args:
            game_date (Optional[str], optional): Date for the scoreboard in YYYY-MM-DD format.
                                                 Defaults to the current local date if None.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
                                       Primarily applies to the static ScoreboardV2 for non-current dates.
            day_offset (int, optional): Day offset from `game_date`. Defaults to 0.
                                        Primarily applies to static ScoreboardV2.
            bypass_cache (bool, optional): If True, ignores cached data. Defaults to False.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               DataFrames: {{'games': df_games, 'teams': df_teams_combined}}.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with formatted scoreboard data or an error.
                    Expected success structure:
                    {{
                        "gameDate": str (YYYY-MM-DD),
                        "games": [
                            {{
                                "gameId": str, "gameStatus": int, "gameStatusText": str, "period": int,
                                "gameClock": Optional[str],
                                "homeTeam": {{"teamId": int, "teamTricode": str, "score": int, "wins": Optional[int], "losses": Optional[int]}},
                                "awayTeam": {{"teamId": int, "teamTricode": str, "score": int, "wins": Optional[int], "losses": Optional[int]}},
                                "gameEt": str // Game start time (UTC for live, EST for static)
                            }}, ...
                        ]
                    }}
                    Returns {{"games": []}} if no games are found.
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict contains 'games' (one row per game) and 'teams' (one row per team per game).
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        effective_date = game_date if game_date else datetime.now().strftime('%Y-%m-%d')
        logger.info(f"LiveUpdatesToolkit: get_scoreboard_for_date called for date: {effective_date}")
        return fetch_static_or_live_scoreboard_logic(
            game_date=game_date,
            league_id=league_id,
            day_offset=day_offset,
            bypass_cache=bypass_cache,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\player_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Player specific stats logic functions
from ..api_tools.analyze import analyze_player_stats_logic
from ..api_tools.player_aggregate_stats import fetch_player_stats_logic
from ..api_tools.player_career_data import fetch_player_career_stats_logic, fetch_player_awards_logic
from ..api_tools.player_clutch import fetch_player_clutch_stats_logic
from ..api_tools.player_common_info import fetch_player_info_logic, get_player_headshot_url
from ..api_tools.player_dashboard_game import fetch_player_dashboard_game_splits_logic
from ..api_tools.player_dashboard_general import fetch_player_dashboard_general_splits_logic
from ..api_tools.player_dashboard_lastn import fetch_player_dashboard_lastn_games_logic
from ..api_tools.player_dashboard_shooting import fetch_player_dashboard_shooting_splits_logic
# Note: player_estimated_metrics.py contains league-wide data. Player-specific estimated metrics are part of advanced_metrics.py.
from ..api_tools.player_fantasy_profile import fetch_player_fantasy_profile_logic
from ..api_tools.player_fantasy_profile_bar_graph import fetch_player_fantasy_profile_bar_graph_logic
from ..api_tools.player_gamelogs import fetch_player_gamelog_logic
from ..api_tools.player_game_streak_finder import fetch_player_game_streak_finder_logic
from ..api_tools.player_passing import fetch_player_passing_stats_logic
from ..api_tools.player_rebounding import fetch_player_rebounding_stats_logic
from ..api_tools.player_shooting_tracking import fetch_player_shots_tracking_logic
from ..api_tools.player_career_by_college import fetch_player_career_by_college_logic


from ..config import settings
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense,
    PerModeSimple, PerMode36, MeasureTypeDetailed, MeasureTypeBase, PerModeTime
)


class PlayerToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.analyze_player_dashboard_stats,
            self.get_player_aggregate_stats,
            self.get_player_career_stats,
            self.get_player_awards,
            self.get_player_clutch_stats,
            self.get_player_common_info,
            self.get_player_headshot_image_url,
            self.get_player_dashboard_by_game_splits,
            self.get_player_dashboard_by_general_splits,
            self.get_player_dashboard_by_last_n_games,
            self.get_player_dashboard_by_shooting_splits,
            self.get_player_fantasy_profile,
            self.get_player_fantasy_profile_bar_graph,
            self.get_player_gamelog,
            self.get_player_game_streaks,
            self.get_player_passing_stats,
            self.get_player_rebounding_stats,
            self.get_player_shooting_tracking_stats,
            self.get_player_career_stats_by_college,
        ]
        super().__init__(name="player_toolkit", tools=tools, **kwargs)

    def analyze_player_dashboard_stats(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeDetailed.per_game,
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches a player's overall dashboard statistics for a specified season and type.
        Primarily returns 'OverallPlayerDashboard' data from the PlayerDashboardByYearOverYear endpoint.

        Args:
            player_name (str): Full name or ID of the player (e.g., "LeBron James", "2544").
            season (str, optional): NBA season in YYYY-YY format (e.g., "2023-24").
                                    Defaults to the current NBA season defined in `settings`.
            season_type (str, optional): Type of season.
                                         Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Playoffs", "Pre Season".
            per_mode (str, optional): Statistical mode.
                                      Defaults to "PerGame".
                                      Possible values from PerModeDetailed enum (e.g., "PerGame", "Totals", "Per36", "Per100Possessions").
            league_id (str, optional): League ID.
                                       Defaults to "00" (NBA).
                                       Possible values from LeagueID enum (e.g., "00" for NBA, "10" for WNBA, "20" for G-League).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'overall_dashboard': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string of overall dashboard stats or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "season": str, "season_type": str,
                        "per_mode": str, "league_id": str,
                        "overall_dashboard_stats": {{
                            "GROUP_SET": str, "PLAYER_ID": int, "PLAYER_NAME": str, "GP": int,
                            "W": int, "L": int, "W_PCT": float, "MIN": float, "FGM": float, "FGA": float,
                            "FG_PCT": float, "FG3M": float, "FG3A": float, "FG3_PCT": float,
                            "FTM": float, "FTA": float, "FT_PCT": float, "OREB": float, "DREB": float,
                            "REB": float, "AST": float, "TOV": float, "STL": float, "BLK": float,
                            "BLKA": float, "PF": float, "PFD": float, "PTS": float, "PLUS_MINUS": float,
                            "NBA_FANTASY_PTS": Optional[float], "DD2": Optional[int], "TD3": Optional[int],
                            ... (other rank fields might be present) ...
                        }}
                    }}
                    Returns {{"overall_dashboard_stats": {{}}}} if no data.
                If return_dataframe=True: Tuple (json_string, {{'overall_dashboard': pd.DataFrame}}).
                    DataFrame contains detailed dashboard stats for the player.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: analyze_player_dashboard_stats called for {player_name}, season {season}")
        return analyze_player_stats_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            per_mode=per_mode,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_player_aggregate_stats(
        self,
        player_name: str,
        season: Optional[str] = None, # Season for game log part
        season_type: str = SeasonTypeAllStar.regular, # Season type for game log part
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Aggregates various player statistics including common info, career stats,
        game logs for a specified season, and awards history.

        Args:
            player_name (str): The name or ID of the player.
            season (Optional[str], optional): The season for which to fetch game logs (YYYY-YY format).
                                              Defaults to the current NBA season if None.
            season_type (str, optional): The type of season for game logs.
                                         Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Playoffs", "Pre Season", "All Star".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, Dict of DataFrames).
                                               DataFrames include 'player_info', 'headline_stats', 'available_seasons',
                                               'season_totals_regular_season', 'career_totals_regular_season',
                                               'season_totals_post_season', 'career_totals_post_season',
                                               'gamelog', 'awards'.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string of aggregated player stats or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int,
                        "season_requested_for_gamelog": str,
                        "season_type_requested_for_gamelog": str,
                        "info": {{ ... (player biographical data, team, position, etc.) ... }},
                        "headline_stats": {{ ... (PTS, REB, AST for current season) ... }},
                        "available_seasons": [{{ "SEASON_ID": str, ... }}],
                        "career_stats": {{
                            "season_totals_regular_season": [{{ ... regular season stats per season ... }}],
                            "career_totals_regular_season": {{ ... career aggregate regular season stats ... }},
                            "season_totals_post_season": [{{ ... post-season stats per season ... }}],
                            "career_totals_post_season": {{ ... career aggregate post-season stats ... }}
                        }},
                        "season_gamelog": [{{ ... game log data for specified season and type ... }}],
                        "awards": [{{ ... award details like DESCRIPTION, SEASON, TYPE ... }}]
                    }}
                If return_dataframe=True: Tuple (json_string, Dict of DataFrames).
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_aggregate_stats called for {player_name}, gamelog season {season or settings.CURRENT_NBA_SEASON}")
        return fetch_player_stats_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            return_dataframe=return_dataframe
        )

    def get_player_career_stats(
        self,
        player_name: str,
        per_mode: str = PerModeDetailed.per_game,
        league_id_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player career statistics including regular season and postseason totals, broken down by season and career.

        Args:
            player_name (str): The name or ID of the player.
            per_mode (str, optional): Statistical mode for career/season stats.
                                      Defaults to "PerGame".
                                      Possible values from PerModeDetailed or PerMode36 enums (e.g., "PerGame", "Totals", "Per36").
            league_id_nullable (Optional[str], optional): League ID to filter data (e.g., "00" for NBA).
                                                         Defaults to None (all applicable leagues for the player).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: A JSON string containing player career statistics or an error message.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "per_mode_requested": str,
                        "data_retrieved_mode": str, "league_id": Optional[str],
                        "season_totals_regular_season": [{{ "SEASON_ID": str, "TEAM_ABBREVIATION": str, "PTS": float, ... }}],
                        "career_totals_regular_season": {{ "PLAYER_ID": int, "GP": int, "PTS": float, ... }},
                        "season_totals_post_season": [{{ ... similar to regular season totals ... }}],
                        "career_totals_post_season": {{ ... similar to career regular season totals ... }}
                    }}
                If return_dataframe=True: A tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'season_totals_regular_season', 'career_totals_regular_season',
                                          'season_totals_post_season', 'career_totals_post_season'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_career_stats called for {player_name}, per_mode {per_mode}")
        return fetch_player_career_stats_logic(
            player_name=player_name,
            per_mode=per_mode,
            league_id_nullable=league_id_nullable,
            return_dataframe=return_dataframe
        )

    def get_player_awards(
        self,
        player_name: str,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches a list of awards received by the specified player throughout their career.

        Args:
            player_name (str): The name or ID of the player.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'awards': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: A JSON string containing a list of player awards or an error message.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int,
                        "awards": [
                            {{
                                "DESCRIPTION": str, "ALL_NBA_TEAM_NUMBER": Optional[int],
                                "SEASON": str, "MONTH": Optional[int], "WEEK": Optional[int],
                                "CONFERENCE": Optional[str], "TYPE": str, "SUBTYPE1": Optional[str],
                                "PLAYER_ID": int, "PERSON_ID": int, "TEAM": str, "AWARD_ID": str
                            }},
                            ...
                        ]
                    }}
                If return_dataframe=True: A tuple (json_string, {{'awards': pd.DataFrame}}).
                    The DataFrame contains detailed information for each award.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_awards called for {player_name}")
        return fetch_player_awards_logic(
            player_name=player_name,
            return_dataframe=return_dataframe
        )

    def get_player_clutch_stats(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        measure_type: str = MeasureTypeDetailed.base,
        per_mode: str = PerModeDetailed.totals,
        league_id: Optional[str] = "00",
        plus_minus: str = "N",
        pace_adjust: str = "N",
        rank: str = "N",
        last_n_games: int = 0,
        month: int = 0,
        opponent_team_id: int = 0,
        period: int = 0,
        shot_clock_range_nullable: Optional[str] = None,
        game_segment_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        vs_division_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        date_from_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player clutch performance statistics using the PlayerDashboardByClutch endpoint.
        Various clutch scenarios are returned as different data sets within the response.

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible values from SeasonTypeAllStar enum: "Regular Season", "Playoffs", "Pre Season".
            measure_type (str, optional): Type of stats. Defaults to "Base".
                                          Possible values from MeasureTypeDetailed enum (e.g., "Base", "Advanced", "Scoring").
            per_mode (str, optional): Statistical mode. Defaults to "Totals".
                                      Possible values from PerModeDetailed enum.
            league_id (Optional[str], optional): League ID. Defaults to "00" (NBA).
            plus_minus (str, optional): Flag for plus-minus stats ("Y" or "N"). Defaults to "N".
            pace_adjust (str, optional): Flag for pace adjustment ("Y" or "N"). Defaults to "N".
            rank (str, optional): Flag for ranking ("Y" or "N"). Defaults to "N".
            last_n_games (int, optional): Filter by last N games. Defaults to 0 (all games).
            month (int, optional): Filter by month (1-12). Defaults to 0 (all months).
            opponent_team_id (int, optional): Filter by opponent team ID. Defaults to 0 (all opponents).
            period (int, optional): Filter by period (e.g., 1, 2, 3, 4 for quarters, 0 for all). Defaults to 0.
            shot_clock_range_nullable (Optional[str], optional): Filter by shot clock range (e.g., '24-22', '4-0 Very Late').
            game_segment_nullable (Optional[str], optional): Filter by game segment (e.g., 'First Half', 'Overtime').
            location_nullable (Optional[str], optional): Filter by location ('Home' or 'Road').
            outcome_nullable (Optional[str], optional): Filter by game outcome ('W' or 'L').
            vs_conference_nullable (Optional[str], optional): Filter by opponent conference (e.g., 'East', 'West').
            vs_division_nullable (Optional[str], optional): Filter by opponent division (e.g., 'Atlantic', 'Pacific').
            season_segment_nullable (Optional[str], optional): Filter by season segment (e.g., 'Post All-Star').
            date_from_nullable (Optional[str], optional): Start date filter (YYYY-MM-DD).
            date_to_nullable (Optional[str], optional): End date filter (YYYY-MM-DD).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with clutch stats dashboards or an error message.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int,
                        "parameters_used": {{ ... api parameters ...}},
                        "data_sets": {{
                            "OverallPlayerDashboard": [{{ ... stats ... }}],
                            "Last5Min5PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last3Min5PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last1Min5PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last30Sec3PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last10Sec3PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last5MinPlusMinus5PtPlayerDashboard": [{{ ... stats ... }}],
                            "Last1MinPlusMinus5PtPlayerDashboard": [{{ ... stats ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys match the dataset names listed above.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_clutch_stats called for {player_name}")
        return fetch_player_clutch_stats_logic(
            player_name, season, season_type, measure_type, per_mode, league_id,
            plus_minus, pace_adjust, rank, last_n_games, month, opponent_team_id, period,
            shot_clock_range_nullable, game_segment_nullable, location_nullable,
            outcome_nullable, vs_conference_nullable, vs_division_nullable,
            season_segment_nullable, date_from_nullable, date_to_nullable, return_dataframe
        )

    def get_player_common_info(
        self,
        player_name: str,
        league_id_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches common player information, headline stats, and available seasons.

        Args:
            player_name (str): Name or ID of the player.
            league_id_nullable (Optional[str], optional): League ID to filter results (e.g., "00" for NBA).
                                                         Defaults to None.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: A JSON string containing player info, headline stats, and available seasons.
                    Expected success structure:
                    {{
                        "player_info": {{ "PERSON_ID": int, "DISPLAY_FIRST_LAST": str, "TEAM_NAME": str, ... }},
                        "headline_stats": {{ "PLAYER_ID": int, "PTS": float, "REB": float, "AST": float, ... }},
                        "available_seasons": [{{ "SEASON_ID": str, "PLAYER_ID": int }}]
                    }}
                If return_dataframe=True: A tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'player_info', 'headline_stats', 'available_seasons'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_common_info called for {player_name}")
        return fetch_player_info_logic(
            player_name=player_name,
            league_id_nullable=league_id_nullable,
            return_dataframe=return_dataframe
        )

    def get_player_headshot_image_url(
        self,
        player_id: int
    ) -> str:
        """
        Constructs the URL for a player's headshot image.

        Args:
            player_id (int): The unique ID of the player.

        Returns:
            str: The URL string for the player's headshot.
                 Example: "https://ak-static.cms.nba.com/wp-content/uploads/headshots/nba/latest/260x190/{player_id}.png"
                 Returns an error string if player_id is invalid.
        """
        logger.info(f"PlayerToolkit: get_player_headshot_image_url called for player_id {player_id}")
        try:
            return get_player_headshot_url(player_id=player_id)
        except ValueError as e:
            return str(e) # Return error message as string

    def get_player_dashboard_by_game_splits(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        measure_type: str = "Base",
        per_mode: str = "Totals",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player dashboard statistics by game splits (Overall, ByHalf, ByPeriod, ByScoreMargin, ByActualMargin).

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Playoffs", "Pre Season", "All Star".
            measure_type (str, optional): Statistical category. Defaults to "Base".
                                          Possible values from MeasureTypeDetailed enum.
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
                                      Possible values from PerModeDetailed enum.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with dashboard data or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "OverallPlayerDashboard": [{{ ... stats ... }}],
                            "ByHalfPlayerDashboard": [{{ ... stats ... }}],
                            "ByPeriodPlayerDashboard": [{{ ... stats ... }}],
                            "ByScoreMarginPlayerDashboard": [{{ ... stats ... }}],
                            "ByActualMarginPlayerDashboard": [{{ ... stats ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys match the dataset names listed above.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_dashboard_by_game_splits called for {player_name}")
        return fetch_player_dashboard_game_splits_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            return_dataframe=return_dataframe
        )

    def get_player_dashboard_by_general_splits(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        measure_type: str = "Base",
        per_mode: str = "Totals",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player dashboard statistics by general splits (Location, Wins/Losses, Month, Pre/Post All-Star, Days Rest, Starting Position).

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            measure_type (str, optional): Statistical category. Defaults to "Base".
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with dashboard data or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "OverallPlayerDashboard": [{{ ... stats ... }}],
                            "LocationPlayerDashboard": [{{ ... stats ... }}],
                            // ... other general split dashboards
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    DataFrames dict keys match the dataset names.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_dashboard_by_general_splits called for {player_name}")
        return fetch_player_dashboard_general_splits_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            return_dataframe=return_dataframe
        )

    def get_player_dashboard_by_last_n_games(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        measure_type: str = "Base",
        per_mode: str = "Totals",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player dashboard statistics by last N games splits (Overall, Last 5, Last 10, Last 15, Last 20, GameNumber).

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            measure_type (str, optional): Statistical category. Defaults to "Base".
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with dashboard data or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "OverallPlayerDashboard": [{{ ... stats ... }}],
                            "Last5PlayerDashboard": [{{ ... stats ... }}],
                            // ... other LastNGames split dashboards
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    DataFrames dict keys match the dataset names.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_dashboard_by_last_n_games called for {player_name}")
        return fetch_player_dashboard_lastn_games_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            return_dataframe=return_dataframe
        )

    def get_player_dashboard_by_shooting_splits(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        measure_type: str = "Base",
        per_mode: str = "Totals",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player dashboard statistics by shooting splits (Shot Type, Shot Area, Shot Distance, Assisted/Unassisted).

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            measure_type (str, optional): Statistical category. Defaults to "Base".
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with dashboard data or error.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "OverallPlayerDashboard": [{{ ... stats ... }}],
                            "Shot5FTPlayerDashboard": [{{ ... stats ... }}], // Stats for shots 0-5ft, 5-9ft, etc.
                            "ShotAreaPlayerDashboard": [{{ ... stats by court area ... }}],
                            "ShotTypePlayerDashboard": [{{ ... stats by shot type (jump, layup) ... }}],
                            // ... other shooting split dashboards
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    DataFrames dict keys match the dataset names.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_dashboard_by_shooting_splits called for {player_name}")
        return fetch_player_dashboard_shooting_splits_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            return_dataframe=return_dataframe
        )

    def get_player_fantasy_profile(
        self,
        player_id: str, # PlayerFantasyProfile endpoint uses player_id directly
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_playoffs: str = SeasonTypeAllStar.regular,
        measure_type_base: str = MeasureTypeBase.base, # API uses MeasureTypeBase
        per_mode36: str = PerMode36.totals, # API uses PerMode36
        league_id_nullable: str = "",
        pace_adjust_no: str = "N",
        plus_minus_no: str = "N",
        rank_no: str = "N",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player fantasy profile data, including overall, location-based, last N games, days rest, and vs opponent stats.

        Args:
            player_id (str): The NBA Player ID.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type_playoffs (str, optional): Season type. Defaults to "Regular Season".
                                                 Possible values from SeasonTypePlayoffs enum.
            measure_type_base (str, optional): Measure type. Defaults to "Base".
                                               Possible values from MeasureTypeBase enum (typically just "Base").
            per_mode36 (str, optional): Per mode. Defaults to "Totals".
                                        Possible values from PerMode36 enum (e.g., "Totals", "PerGame", "Per36Minutes").
            league_id_nullable (str, optional): League ID. Defaults to "" (NBA).
            pace_adjust_no (str, optional): Pace adjust flag ("Y" or "N"). Defaults to "N".
            plus_minus_no (str, optional): Plus-minus flag ("Y" or "N"). Defaults to "N".
            rank_no (str, optional): Rank flag ("Y" or "N"). Defaults to "N".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player fantasy profile data or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "Overall": [{{ ... stats ... }}],
                            "Location": [{{ ... stats by Home/Road ... }}],
                            "LastNGames": [{{ ... stats for last N games segments ... }}],
                            "DaysRest": [{{ ... stats by days rest ... }}],
                            "VsOpponent": [{{ ... stats vs specific opponents ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "Overall", "Location", "LastNGames", "DaysRest", "VsOpponent".
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_fantasy_profile called for player ID {player_id}")
        return fetch_player_fantasy_profile_logic(
            player_id=player_id, season=season, season_type_playoffs=season_type_playoffs,
            measure_type_base=measure_type_base, per_mode36=per_mode36,
            league_id_nullable=league_id_nullable, pace_adjust_no=pace_adjust_no,
            plus_minus_no=plus_minus_no, rank_no=rank_no, return_dataframe=return_dataframe
        )

    def get_player_fantasy_profile_bar_graph(
        self,
        player_id: str, # PlayerFantasyProfileBarGraph endpoint uses player_id directly
        season: str = settings.CURRENT_NBA_SEASON,
        league_id_nullable: str = "",
        season_type_all_star_nullable: str = "", # API Param is season_type_all_star_nullable
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player fantasy profile data optimized for bar graph visualization, including season and recent game stats.

        Args:
            player_id (str): The NBA Player ID.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            league_id_nullable (str, optional): League ID. Defaults to "" (NBA).
            season_type_all_star_nullable (str, optional): Season type. Defaults to "".
                                                          Possible values from SeasonTypeAllStar enum or "".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player fantasy profile bar graph data or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "SeasonStats": [{{ "PLAYER_ID": int, "PLAYER_NAME": str, "TEAM_ABBREVIATION": str, "FAN_DUEL_PTS": float, ... }}],
                            "RecentStats": [{{ ... similar structure for recent games ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "SeasonStats", "RecentStats".
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_fantasy_profile_bar_graph called for player ID {player_id}")
        return fetch_player_fantasy_profile_bar_graph_logic(
            player_id=player_id, season=season, league_id_nullable=league_id_nullable,
            season_type_all_star_nullable=season_type_all_star_nullable, return_dataframe=return_dataframe
        )

    def get_player_gamelog(
        self,
        player_name: str,
        season: str,
        season_type: str = SeasonTypeAllStar.regular,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player game logs for a specified player, season, and season type.

        Args:
            player_name (str): Name or ID of the player.
            season (str): NBA season YYYY-YY (e.g., "2023-24"). This is mandatory.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible values from SeasonTypeAllStar enum: "Regular Season", "Playoffs", "Pre Season", "All Star".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'gamelog': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: A JSON string containing a list of game logs or an error message.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "season": str, "season_type": str,
                        "gamelog": [
                            {{
                                "GAME_ID": str, "GAME_DATE": str, "MATCHUP": str, "WL": str,
                                "MIN": int, "FGM": int, ... (other standard box score stats) ..., "PLUS_MINUS": float
                            }},
                            ...
                        ]
                    }}
                If return_dataframe=True: A tuple (json_string, {{'gamelog': pd.DataFrame}}).
                    The DataFrame contains detailed stats for each game.
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_gamelog called for {player_name}, season {season}")
        return fetch_player_gamelog_logic(
            player_name=player_name,
            season=season,
            season_type=season_type,
            return_dataframe=return_dataframe
        )

    def get_player_game_streaks(
        self,
        player_id_nullable: str = "",
        season_nullable: str = "",
        season_type_nullable: str = "", # From PlayerGameStreakFinder params
        league_id_nullable: str = "",
        active_streaks_only_nullable: str = "",
        location_nullable: str = "",
        outcome_nullable: str = "",
        gt_pts_nullable: str = "", # Example of a stat-specific filter
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player game streak data. Can be used for a specific player or league-wide.

        Args:
            player_id_nullable (str, optional): Player ID to filter for specific player streaks. Defaults to "" (league-wide).
            season_nullable (str, optional): Season YYYY-YY. Defaults to "" (all seasons).
            season_type_nullable (str, optional): Season type. Defaults to "".
                                                 Possible: "Regular Season", "Playoffs", "".
            league_id_nullable (str, optional): League ID. Defaults to "" (NBA "00").
                                                Possible: "00", "10" (WNBA), "".
            active_streaks_only_nullable (str, optional): Filter for active streaks ("Y" or "N"). Defaults to "".
            location_nullable (str, optional): Filter by game location ("Home", "Road"). Defaults to "".
            outcome_nullable (str, optional): Filter by game outcome ("W", "L"). Defaults to "".
            gt_pts_nullable (str, optional): Filter for streaks where points scored were greater than this value. Defaults to "".
                                             Other stat filters like gt_reb_nullable, gt_ast_nullable, etc., also exist.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'PlayerGameStreakFinder': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player game streak data or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "PlayerGameStreakFinder": [
                                {{
                                    "PLAYER_NAME_LAST_FIRST": str, "PLAYER_ID": int, "GAMESTREAK": int,
                                    "STARTDATE": str, "ENDDATE": str, "ACTIVESTREAK": str,
                                    "NUMSEASONS": int, "LASTSEASON": str, "FIRSTSEASON": str
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'PlayerGameStreakFinder': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_game_streaks called for player ID '{player_id_nullable or 'all players'}'")
        return fetch_player_game_streak_finder_logic(
            player_id_nullable=player_id_nullable, season_nullable=season_nullable,
            season_type_nullable=season_type_nullable, league_id_nullable=league_id_nullable,
            active_streaks_only_nullable=active_streaks_only_nullable,
            location_nullable=location_nullable, outcome_nullable=outcome_nullable,
            gt_pts_nullable=gt_pts_nullable, return_dataframe=return_dataframe
        )

    def get_player_passing_stats(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        last_n_games: int = 0,
        league_id: str = "00",
        month: int = 0,
        opponent_team_id: int = 0,
        vs_division_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        date_from_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player passing statistics (passes made and received) for a given season.
        Requires resolving player's team_id for the specified season.

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
                                      Possible values from PerModeSimple enum.
            last_n_games (int, optional): Filter by last N games. Defaults to 0.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            month (int, optional): Filter by month (1-12). Defaults to 0.
            opponent_team_id (int, optional): Filter by opponent team ID. Defaults to 0.
            vs_division_nullable (Optional[str], optional): Filter by opponent division.
            vs_conference_nullable (Optional[str], optional): Filter by opponent conference.
            season_segment_nullable (Optional[str], optional): Filter by season segment.
            outcome_nullable (Optional[str], optional): Filter by game outcome.
            location_nullable (Optional[str], optional): Filter by game location.
            date_to_nullable (Optional[str], optional): End date YYYY-MM-DD.
            date_from_nullable (Optional[str], optional): Start date YYYY-MM-DD.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, DataFrames dict).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player passing stats or an error message.
                    Expected success structure:
                    {{
                        "player_name": str, "player_id": int, "parameters": {{...}},
                        "passes_made": [{{ "PASS_TEAMMATE_PLAYER_ID": int, "PLAYER_NAME_LAST_FIRST": str, "PASSES": int, ... }}],
                        "passes_received": [{{ "PASS_TEAMMATE_PLAYER_ID": int, "PLAYER_NAME_LAST_FIRST": str, "PASSES": int, ... }}]
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'passes_made', 'passes_received'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_passing_stats called for {player_name}")
        return fetch_player_passing_stats_logic(
            player_name, season, season_type, per_mode, last_n_games, league_id, month,
            opponent_team_id, vs_division_nullable, vs_conference_nullable, season_segment_nullable,
            outcome_nullable, location_nullable, date_to_nullable, date_from_nullable, return_dataframe
        )

    def get_player_rebounding_stats(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        last_n_games: int = 0,
        league_id: str = "00",
        month: int = 0,
        opponent_team_id: int = 0,
        period: int = 0,
        vs_division_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        game_segment_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        date_from_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player rebounding statistics, broken down by categories like shot type, contest level,
        shot distance, and rebound distance. Requires resolving player's team_id.

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
            // ... (other args from original docstring for fetch_player_rebounding_stats_logic) ...
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
                JSON structure includes 'overall', 'by_shot_type', 'by_contest', 'by_shot_distance', 'by_rebound_distance'.
                DataFrames dict keys match these categories.
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_rebounding_stats called for {player_name}")
        return fetch_player_rebounding_stats_logic(
            player_name, season, season_type, per_mode, last_n_games, league_id, month,
            opponent_team_id, period, vs_division_nullable, vs_conference_nullable,
            season_segment_nullable, outcome_nullable, location_nullable, game_segment_nullable,
            date_to_nullable, date_from_nullable, return_dataframe
        )

    def get_player_shooting_tracking_stats(
        self,
        player_name: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.totals,
        opponent_team_id: int = 0,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        last_n_games: int = 0,
        league_id: str = "00",
        month: int = 0,
        period: int = 0,
        vs_division_nullable: Optional[str] = None,
        vs_conference_nullable: Optional[str] = None,
        season_segment_nullable: Optional[str] = None,
        outcome_nullable: Optional[str] = None,
        location_nullable: Optional[str] = None,
        game_segment_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches player shooting tracking statistics (general, by shot clock, dribble, touch time, defender distance).
        Requires resolving player's team_id.

        Args:
            player_name (str): Name or ID of the player.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "Totals".
            // ... (other args from original docstring for fetch_player_shots_tracking_logic) ...
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
                JSON structure includes 'general_shooting', 'by_shot_clock', etc.
                DataFrames dict keys match these categories.
                CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_shooting_tracking_stats called for {player_name}")
        return fetch_player_shots_tracking_logic(
            player_name, season, season_type, per_mode, opponent_team_id, date_from, date_to,
            last_n_games, league_id, month, period, vs_division_nullable, vs_conference_nullable,
            season_segment_nullable, outcome_nullable, location_nullable, game_segment_nullable, return_dataframe
        )

    def get_player_career_stats_by_college(
        self,
        college: str,
        league_id: str = "00",
        per_mode_simple: str = PerModeSimple.totals,
        season_type_all_star: str = SeasonTypeAllStar.regular,
        season_nullable: str = "",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches career statistics for all NBA/WNBA players from a specific college.

        Args:
            college (str): The name of the college (e.g., "Duke", "Kentucky"). This is required.
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10" (WNBA).
            per_mode_simple (str, optional): Statistical mode. Defaults to "Totals". Possible: "Totals", "PerGame".
            season_type_all_star (str, optional): Season type for stats. Defaults to "Regular Season". Possible: "Regular Season", "Playoffs", "All Star".
            season_nullable (str, optional): Specific season YYYY-YY to filter for, or "" for all seasons. Defaults to "".
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'PlayerCareerByCollege': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with player career stats by college or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "PlayerCareerByCollege": [
                                {{
                                    "PLAYER_ID": int, "PLAYER_NAME": str, "COLLEGE": str,
                                    "GP": int, "MIN": float, "PTS": float, ... (other career stats) ...
                                }}, ...
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'PlayerCareerByCollege': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"PlayerToolkit: get_player_career_stats_by_college called for college {college}")
        return fetch_player_career_by_college_logic(
            college=college,
            league_id=league_id,
            per_mode_simple=per_mode_simple,
            season_type_all_star=season_type_all_star,
            season_nullable=season_nullable,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\search_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

from ..api_tools.search import (
    search_players_logic,
    search_teams_logic,
    search_games_logic,
)
from ..api_tools.game_finder import fetch_league_games_logic
from ..core.constants import MAX_SEARCH_RESULTS, MIN_PLAYER_SEARCH_LENGTH
from ..config import settings # For default season
from nba_api.stats.library.parameters import SeasonTypeAllStar, LeagueID

class SearchToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.search_players,
            self.search_teams,
            self.search_games,
            self.find_league_games,
        ]
        super().__init__(name="search_toolkit", tools=tools, **kwargs)

    def search_players(
        self,
        query: str,
        limit: int = MAX_SEARCH_RESULTS,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Searches for NBA players by name fragment.

        Args:
            query (str): The search query string for the player's name.
                         Must be at least {MIN_PLAYER_SEARCH_LENGTH} characters long.
            limit (int, optional): Maximum number of results to return.
                                   Defaults to {MAX_SEARCH_RESULTS}.
            return_dataframe (bool, optional): If True, returns a tuple containing the JSON response string
                                               and a dictionary of DataFrames: `{'players': df}`.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False:
                    A JSON string containing a list of matching players or an error message.
                    Expected structure:
                    {{
                        "players": [
                            {{"id": int, "full_name": str, "is_active": bool}},
                            ...
                        ]
                    }}
                If return_dataframe=True:
                    A tuple (json_string, {'players': pd.DataFrame}).
                    The DataFrame has columns: 'id', 'full_name', 'is_active'.
                    CSV cache path is included in json_string under 'dataframe_info'.
        """
        logger.info(f"SearchToolkit: search_players called with query: '{query}'")
        return search_players_logic(query=query, limit=limit, return_dataframe=return_dataframe)

    def search_teams(
        self,
        query: str,
        limit: int = MAX_SEARCH_RESULTS,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Searches for NBA teams by name, city, nickname, or abbreviation.

        Args:
            query (str): The search query string.
            limit (int, optional): Maximum number of results to return.
                                   Defaults to {MAX_SEARCH_RESULTS}.
            return_dataframe (bool, optional): If True, returns a tuple containing the JSON response string
                                               and a dictionary of DataFrames: `{'teams': df}`.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False:
                    A JSON string containing a list of matching teams or an error message.
                    Expected structure:
                    {{
                        "teams": [
                            {{
                                "id": int, "full_name": str, "abbreviation": str,
                                "nickname": str, "city": str, "state": str, "year_founded": int
                            }},
                            ...
                        ]
                    }}
                If return_dataframe=True:
                    A tuple (json_string, {'teams': pd.DataFrame}).
                    The DataFrame includes columns like 'id', 'full_name', 'abbreviation', etc.
                    CSV cache path is included in json_string under 'dataframe_info'.
        """
        logger.info(f"SearchToolkit: search_teams called with query: '{query}'")
        return search_teams_logic(query=query, limit=limit, return_dataframe=return_dataframe)

    def search_games(
        self,
        query: str,
        season: str,
        season_type: str = SeasonTypeAllStar.regular,
        limit: int = MAX_SEARCH_RESULTS,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Searches for NBA games based on a query (e.g., "TeamA vs TeamB", "Lakers").
        Requires a season to be specified.

        Args:
            query (str): The search query string. Can be a team name or a matchup like "TeamA vs TeamB".
            season (str): The NBA season in YYYY-YY format (e.g., "2023-24"). This is mandatory.
            season_type (str, optional): The type of season.
                                         Defaults to "Regular Season".
                                         Possible values include "Regular Season", "Playoffs", "Pre Season", "All Star".
            limit (int, optional): Maximum number of results to return.
                                   Defaults to {MAX_SEARCH_RESULTS}.
            return_dataframe (bool, optional): If True, returns a tuple containing the JSON response string
                                               and a dictionary of DataFrames: `{'games': df}`.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False:
                    A JSON string containing a list of matching games or an error message.
                    Games are typically sorted by date in descending order.
                    Expected structure:
                    {{
                        "games": [
                            {{ "GAME_ID": str, "GAME_DATE": str, "MATCHUP": str, "WL": str, ... (other stats) ... }},
                            ...
                        ]
                    }}
                If return_dataframe=True:
                    A tuple (json_string, {'games': pd.DataFrame}).
                    The DataFrame includes columns like 'GAME_ID', 'GAME_DATE', 'MATCHUP', 'WL', 'PTS', etc.
                    CSV cache path is included in json_string under 'dataframe_info'.
        """
        logger.info(f"SearchToolkit: search_games called with query: '{query}', season: {season}")
        return search_games_logic(query=query, season=season, season_type=season_type, limit=limit, return_dataframe=return_dataframe)

    def find_league_games(
        self,
        player_or_team_abbreviation: str = 'T',
        player_id_nullable: Optional[int] = None,
        team_id_nullable: Optional[int] = None,
        season_nullable: Optional[str] = None,
        season_type_nullable: Optional[str] = None,
        league_id_nullable: Optional[str] = LeagueID.nba,
        date_from_nullable: Optional[str] = None,
        date_to_nullable: Optional[str] = None,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches NBA league games using LeagueGameFinder with various filters.
        This tool can find games for a specific player, team, season, or date range.
        Date filtering is applied post-API call due to API instability with date ranges.
        Results for broad queries (no player, team, or season filter) are limited.

        Args:
            player_or_team_abbreviation (str, optional): 'P' for player or 'T' for team. Defaults to 'T'.
            player_id_nullable (Optional[int], optional): Player ID. Required if player_or_team_abbreviation is 'P'.
            team_id_nullable (Optional[int], optional): Team ID.
            season_nullable (Optional[str], optional): Season in 'YYYY-YY' format (e.g., "2023-24").
            season_type_nullable (Optional[str], optional): Season type (e.g., 'Regular Season', 'Playoffs').
            league_id_nullable (Optional[str], optional): League ID. Defaults to NBA "00".
            date_from_nullable (Optional[str], optional): Start date for filtering games, format 'YYYY-MM-DD'.
            date_to_nullable (Optional[str], optional): End date for filtering games, format 'YYYY-MM-DD'.
            return_dataframe (bool, optional): If True, returns a tuple containing the JSON response string
                                               and a dictionary of DataFrames: `{'games': df}`.
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False:
                    A JSON string containing a list of games or an error message.
                    The 'GAME_DATE' in the response is formatted as 'YYYY-MM-DD'.
                    Expected structure:
                    {{
                        "games": [
                            {{
                                "SEASON_ID": str, "TEAM_ID": int, "TEAM_ABBREVIATION": str,
                                "TEAM_NAME": str, "GAME_ID": str, "GAME_DATE": str, "MATCHUP": str,
                                "WL": str, "MIN": int, "PTS": int, ... (other game stats) ...,
                                "GAME_DATE_FORMATTED": str (YYYY-MM-DD)
                            }},
                            ...
                        ]
                    }}
                If return_dataframe=True:
                    A tuple (json_string, {'games': pd.DataFrame}).
                    The DataFrame includes various game statistics and details.
                    CSV cache path is included in json_string under 'dataframe_info'.
        """
        logger.info(f"SearchToolkit: find_league_games called.")
        return fetch_league_games_logic(
            player_or_team_abbreviation=player_or_team_abbreviation,
            player_id_nullable=player_id_nullable,
            team_id_nullable=team_id_nullable,
            season_nullable=season_nullable,
            season_type_nullable=season_type_nullable,
            league_id_nullable=league_id_nullable,
            date_from_nullable=date_from_nullable,
            date_to_nullable=date_to_nullable,
            return_dataframe=return_dataframe
        )

===== backend\tool_kits\team_toolkit.py =====
from typing import Optional, Dict, Any, Union, Tuple, List
import pandas as pd
from agno.tools import Toolkit
from agno.utils.log import logger

# Team specific stats logic functions
from ..api_tools.team_dash_lineups import fetch_team_lineups_logic
from ..api_tools.team_dash_pt_shots import fetch_team_dash_pt_shots_logic
from ..api_tools.league_dash_pt_team_defend import fetch_league_dash_pt_team_defend_logic # Note: Logic function name implies league but can be filtered by team
from ..api_tools.team_dashboard_shooting import fetch_team_dashboard_shooting_splits_logic
from ..api_tools.team_details import fetch_team_details_logic
from ..api_tools.team_estimated_metrics import fetch_team_estimated_metrics_logic # Note: This is league-wide but returns data per team
from ..api_tools.team_game_logs import fetch_team_game_logs_logic
from ..api_tools.team_general_stats import fetch_team_stats_logic
from ..api_tools.team_historical_leaders import fetch_team_historical_leaders_logic
from ..api_tools.team_history import fetch_common_team_years_logic # Note: This is league-wide team history, not for a single team
from ..api_tools.team_info_roster import fetch_team_info_and_roster_logic
from ..api_tools.team_passing_analytics import fetch_team_passing_stats_logic
from ..api_tools.team_player_dashboard import fetch_team_player_dashboard_logic
from ..api_tools.team_player_on_off_details import fetch_team_player_on_off_details_logic
from ..api_tools.teamplayeronoffsummary import fetch_teamplayeronoffsummary_logic
from ..api_tools.team_rebounding_tracking import fetch_team_rebounding_stats_logic
from ..api_tools.team_shooting_tracking import fetch_team_shooting_stats_logic
from ..api_tools.teamvsplayer import fetch_teamvsplayer_logic
from ..api_tools.franchise_history import fetch_franchise_history_logic # League-wide but team-centric context
from ..api_tools.franchise_leaders import fetch_franchise_leaders_logic
from ..api_tools.franchise_players import fetch_franchise_players_logic


from ..config import settings
from nba_api.stats.library.parameters import (
    SeasonTypeAllStar, PerModeDetailed, LeagueID, MeasureTypeDetailedDefense,
    PerModeSimple, PerModeTime, DefenseCategory
)


class TeamToolkit(Toolkit):
    def __init__(self, **kwargs):
        tools = [
            self.get_team_lineup_stats,
            self.get_team_player_tracking_shot_stats,
            self.get_team_player_tracking_defense_stats, # Renamed from league_dash_pt_team_defend
            self.get_team_dashboard_shooting_splits,
            self.get_team_details,
            self.get_league_team_estimated_metrics, # Note: league-wide but returns per-team data
            self.get_team_game_logs,
            self.get_team_general_stats,
            self.get_team_historical_leaders,
            self.get_common_team_years, # Note: league-wide team history
            self.get_team_info_and_roster,
            self.get_team_passing_stats,
            self.get_team_player_dashboard_stats,
            self.get_team_player_on_off_details,
            self.get_team_player_on_off_summary,
            self.get_team_rebounding_tracking_stats,
            self.get_team_shooting_tracking_stats,
            self.get_team_vs_player_stats,
            self.get_league_franchise_history, # League-wide but can be viewed in team context
            self.get_franchise_leaders, # Specific to a team
            self.get_franchise_players, # Specific to a team
        ]
        super().__init__(name="team_toolkit", tools=tools, **kwargs)

    def get_team_lineup_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        measure_type: str = MeasureTypeDetailedDefense.base,
        per_mode: str = PerModeDetailed.totals,
        group_quantity: int = 5,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team lineup statistics, providing insights into performance of different player combinations.
        Data includes overall team stats when specific lineups are on the court and the lineups themselves.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): NBA season in YYYY-YY format (e.g., "2023-24"). Defaults to current season.
            season_type (str, optional): Type of season. Defaults to "Regular Season".
                                         Possible values from SeasonTypeAllStar enum (e.g., "Regular Season", "Playoffs").
            measure_type (str, optional): Statistical category. Defaults to "Base".
                                          Possible values from MeasureTypeDetailedDefense enum (e.g., "Base", "Advanced", "Scoring").
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
                                      Possible values from PerModeDetailed enum (e.g., "PerGame", "Per100Possessions").
            group_quantity (int, optional): Number of players in the lineup (2-5). Defaults to 5.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'Lineups': df, 'Overall': df}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with lineups data or error.
                    Expected success structure:
                    {{
                        "team_name": str, "team_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "Lineups": [{{ "GROUP_ID": str, "GROUP_NAME": str, "TEAM_ABBREVIATION": str, "GP": int, "MIN": float, "PTS": float, ... }}],
                            "Overall": [{{ "GROUP_SET": str, "GP": int, "MIN": float, "PTS": float, ... }}] // Team's overall stats for context
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: 'Lineups', 'Overall'.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_lineup_stats called for {team_identifier}, season {season}")
        return fetch_team_lineups_logic(
            team_identifier=team_identifier,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            group_quantity=group_quantity,
            return_dataframe=return_dataframe
        )

    def get_team_player_tracking_shot_stats(
        self,
        team_identifier: str, # TeamDashPtShots requires a team_id
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_all_star: str = SeasonTypeAllStar.regular,
        per_mode_simple: str = PerModeSimple.totals,
        league_id: str = LeagueID.nba, # Parameter from endpoint
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team-level player tracking shot statistics (e.g., overall shooting, 2pt/3pt frequencies and percentages).
        This endpoint (TeamDashPtShots) is specific to a team.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team. This is required.
            season (str, optional): Season in YYYY-YY format. Defaults to current NBA season.
            season_type_all_star (str, optional): Season type. Defaults to "Regular Season".
                                                 Possible from SeasonTypeAllStar enum.
            per_mode_simple (str, optional): Per mode for stats. Defaults to "Totals".
                                             Possible from PerModeSimple enum (e.g., "Totals", "PerGame").
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'TeamPtShot': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with team player tracking shot stats or error.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "data_sets": {{
                            "TeamPtShot": [ // List of shot types (Overall, Catch and Shoot, Pullups, etc.) for the team
                                {{
                                    "TEAM_ID": int, "TEAM_NAME": str, "SHOT_TYPE": str,
                                    "FGA_FREQUENCY": float, "FGM": int, "FGA": int, "FG_PCT": float,
                                    "EFG_PCT": float, "FG2A_FREQUENCY": float, ...
                                }}
                            ]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, {{'TeamPtShot': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_player_tracking_shot_stats called for {team_identifier}, season {season}")
        # The logic function `fetch_team_dash_pt_shots_logic` expects team_id as string.
        # We first resolve team_identifier to team_id.
        try:
            team_id_val, _ = find_team_id_or_error(team_identifier)
        except Exception as e:
            logger.error(f"Error finding team ID for '{team_identifier}': {e}", exc_info=True)
            error_response = format_response(error=str(e))
            if return_dataframe:
                return error_response, {}
            return error_response

        return fetch_team_dash_pt_shots_logic(
            team_id=str(team_id_val), # Ensure it's passed as string if required by underlying
            season=season,
            season_type_all_star=season_type_all_star,
            per_mode_simple=per_mode_simple,
            league_id=league_id, # Passed as league_id in logic
            return_dataframe=return_dataframe
        )

    def get_team_player_tracking_defense_stats(
        self,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        defense_category: str = DefenseCategory.overall,
        league_id: str = LeagueID.nba,
        team_identifier_nullable: Optional[str] = None, # To filter for a specific team
        # ... other filters from fetch_league_dash_pt_team_defend_logic
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches defensive team tracking statistics. Can be league-wide or filtered for a specific team.
        Shows how teams perform when defending shots in different categories (Overall, 3 Pointers, etc.).

        Args:
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
            defense_category (str, optional): Defense category. Defaults to "Overall".
                                              Possible from DefenseCategory enum (e.g., "Overall", "3 Pointers", "Less Than 6Ft").
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            team_identifier_nullable (Optional[str], optional): Name, abbreviation, or ID of a specific team to filter for.
                                                                If None, returns league-wide data.
            // ... (other optional filters like conference_nullable, date_from_nullable, etc. as in the logic function)
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, {{'pt_team_defend': DataFrame}}).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with defensive team tracking stats or error.
                    Expected success structure:
                    {{
                        "parameters": {{...}},
                        "pt_team_defend": [
                            {{
                                "TEAM_ID": int, "TEAM_NAME": str, "GP": int, "FREQ": float,
                                "D_FGM": float, "D_FGA": float, "D_FG_PCT": float,
                                "NORMAL_FG_PCT": float, "PCT_PLUSMINUS": float, ...
                            }}
                        ]
                    }}
                If return_dataframe=True: Tuple (json_string, {{'pt_team_defend': pd.DataFrame}}).
                    CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_player_tracking_defense_stats called for season {season}, team: {team_identifier_nullable or 'League-wide'}")
        team_id_val_nullable = None
        if team_identifier_nullable:
            try:
                team_id_val_nullable, _ = find_team_id_or_error(team_identifier_nullable)
            except Exception as e:
                logger.warning(f"Could not resolve team_identifier '{team_identifier_nullable}' for defense stats: {e}")
                # Proceed with None or handle error as appropriate

        return fetch_league_dash_pt_team_defend_logic(
            season=season,
            season_type=season_type,
            per_mode=per_mode,
            defense_category=defense_category,
            league_id=league_id,
            team_id_nullable=str(team_id_val_nullable) if team_id_val_nullable else None, # API expects string or None
            # Pass through other relevant optional filters here
            return_dataframe=return_dataframe
        )

    def get_team_dashboard_shooting_splits(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        measure_type: str = "Base",
        per_mode: str = "Totals",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team dashboard statistics by shooting splits (Shot Type, Shot Area, Shot Distance, Assisted/Unassisted).

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current NBA season.
            season_type (str, optional): Season type. Defaults to "Regular Season".
                                         Possible values: "Regular Season", "Playoffs", "Pre Season", "All Star".
            measure_type (str, optional): Statistical category. Defaults to "Base".
                                          Possible values from MeasureTypeDetailedDefense enum (Note: API uses this for team shooting splits too).
            per_mode (str, optional): Per mode for stats. Defaults to "Totals".
                                      Possible values from PerModeDetailed enum.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with dashboard data or error.
                    Expected success structure:
                    {{
                        "team_name": str, "team_id": int, "parameters": {{...}},
                        "data_sets": {{
                            "OverallTeamDashboard": [{{ ... team overall stats ... }}],
                            "Shot5FTTeamDashboard": [{{ ... stats by 5ft ranges ... }}],
                            "ShotAreaTeamDashboard": [{{ ... stats by court area ... }}],
                            "ShotTypeTeamDashboard": [{{ ... stats by shot type (jump, layup) ... }}],
                            "AssitedShotTeamDashboard": [{{ ... stats for assisted shots ... }}],
                            "AssistedBy": [{{ ... stats on who assisted shots for this team ... }}] // Note: This dataset is for players assisting team shots.
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys match the dataset names.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_dashboard_shooting_splits called for {team_identifier}")
        return fetch_team_dashboard_shooting_splits_logic(
            team_identifier=team_identifier,
            season=season,
            season_type=season_type,
            measure_type=measure_type,
            per_mode=per_mode,
            return_dataframe=return_dataframe
        )

    def get_team_details(
        self,
        team_id: str, # TeamDetails endpoint requires team_id as string
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches comprehensive team details including info, history, social media, championships, retired players, and HOF players.

        Args:
            team_id (str): The NBA Team ID (e.g., "1610612739" for Cleveland Cavaliers). This is required.
            return_dataframe (bool, optional): If True, returns a tuple (JSON response, dictionary of DataFrames).
                                               Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
                If return_dataframe=False: JSON string with team details or an error message.
                    Expected success structure:
                    {{
                        "parameters": {{"team_id": str}},
                        "data_sets": {{
                            "TeamInfo": [{{ ... basic team details ... }}],
                            "TeamHistory": [{{ ... historical team names/years ... }}],
                            "SocialMediaAccounts": [{{ ... social media links ... }}],
                            "Championships": [{{ ... championship years ... }}],
                            "ConferenceChampionships": [{{ ... conference championship years ... }}],
                            "DivisionChampionships": [{{ ... division championship years ... }}],
                            "RetiredPlayers": [{{ ... retired player details ... }}],
                            "HallOfFamePlayers": [{{ ... HOF player details ... }}]
                        }}
                    }}
                If return_dataframe=True: Tuple (json_string, dataframes_dict).
                    dataframes_dict keys: "TeamInfo", "TeamHistory", etc.
                    CSV cache paths included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_details called for team ID {team_id}")
        return fetch_team_details_logic(
            team_id=team_id,
            return_dataframe=return_dataframe
        )

    # ... (Continue for other team-related tools)
    def get_league_team_estimated_metrics(
        self,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team estimated metrics (E_OFF_RATING, E_DEF_RATING, etc.) for ALL teams in a given season.

        Args:
            season (str, optional): NBA season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season". Possible: "Regular Season", "Playoffs", "Pre Season".
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10", "".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, pd.DataFrame]]: JSON string or (JSON, DataFrame).
            JSON includes 'team_estimated_metrics' list for all teams.
            DataFrame key: 'team_estimated_metrics'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_league_team_estimated_metrics called for season {season}")
        return fetch_team_estimated_metrics_logic(
            season=season,
            season_type=season_type,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_team_game_logs(
        self,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = "Regular Season",
        per_mode: str = "PerGame",
        measure_type: str = "Base",
        team_id: str = "",
        date_from: str = "",
        date_to: str = "",
        # ... (other params from original logic function)
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team game logs for a specific team or league-wide.

        Args:
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Per mode. Defaults to "PerGame".
            measure_type (str, optional): Measure type. Defaults to "Base".
            team_id (str, optional): Team ID to filter. Defaults to "" (league-wide).
            date_from (str, optional): Start date MM/DD/YYYY. Defaults to "".
            date_to (str, optional): End date MM/DD/YYYY. Defaults to "".
            // ... (include other relevant optional args here like league_id, location, outcome etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrame).
            JSON includes 'data_sets' with key like 'TeamGameLogs'.
            DataFrame dict key: 'TeamGameLogs'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_game_logs called for season {season}, team ID: {team_id or 'League-wide'}")
        return fetch_team_game_logs_logic(
            season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type,
            team_id=team_id, date_from=date_from, date_to=date_to,
            # Pass other relevant parameters from the signature
            return_dataframe=return_dataframe
        )

    def get_team_general_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeDetailed.per_game,
        measure_type: str = MeasureTypeDetailedDefense.base,
        # ... (other params from original logic function)
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches comprehensive team statistics: current season dashboard stats and historical year-by-year performance.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season for dashboard stats YYYY-YY. Defaults to current.
            season_type (str, optional): Season type for dashboard. Defaults to "Regular Season".
            per_mode (str, optional): Per mode for dashboard. Defaults to "PerGame".
            measure_type (str, optional): Measure type for dashboard. Defaults to "Base".
            // ... (include other relevant optional args like league_id, opponent_team_id etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'current_season_dashboard_stats' and 'historical_year_by_year_stats'.
            DataFrames dict keys: 'dashboard', 'historical'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_general_stats called for {team_identifier}, season {season}")
        return fetch_team_stats_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, measure_type=measure_type,
            # Pass other relevant parameters from the signature
            return_dataframe=return_dataframe
        )

    def get_team_historical_leaders(
        self,
        team_identifier: str,
        season_id: str, # e.g., "22022" for 2022-23 season
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches historical career leaders for a specific team as of a given season_id.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season_id (str): The 5-digit season ID (e.g., "22022" for the 2022-23 season).
                             This specifies the point in time for historical leaders.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrame).
            JSON includes 'career_leaders_by_team' list.
            DataFrame dict key: 'career_leaders_by_team'.
        """
        logger.info(f"TeamToolkit: get_team_historical_leaders for team {team_identifier}, season_id {season_id}")
        return fetch_team_historical_leaders_logic(
            team_identifier=team_identifier,
            season_id=season_id,
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_common_team_years(
        self,
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches a list of all team years for a given league, indicating the range of seasons each team existed.
        This is a league-wide utility but useful in team contexts for historical data.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10", "20".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrame).
            JSON includes 'team_years' list.
            DataFrame dict key: 'team_years'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_common_team_years called for league {league_id}")
        return fetch_common_team_years_logic(
            league_id=league_id,
            return_dataframe=return_dataframe
        )

    def get_team_info_and_roster(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        league_id: str = LeagueID.nba,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches comprehensive team information: basic details, ranks, roster, and coaches.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'info', 'ranks', 'roster', 'coaches'.
            DataFrames dict keys: 'team_info', 'team_ranks', 'roster', 'coaches'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_info_and_roster called for {team_identifier}, season {season}")
        return fetch_team_info_and_roster_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            league_id=league_id, return_dataframe=return_dataframe
        )

    def get_team_passing_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team passing statistics (passes made and received).

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
            // ... (include other relevant optional args like league_id, opponent_team_id etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'passes_made' and 'passes_received' lists.
            DataFrames dict keys: 'passes_made', 'passes_received'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_passing_stats called for {team_identifier}, season {season}")
        return fetch_team_passing_stats_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, # Pass other relevant params from signature
            return_dataframe=return_dataframe
        )

    def get_team_player_dashboard_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeDetailed.totals,
        measure_type: str = MeasureTypeDetailedDefense.base,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team player dashboard statistics (PlayersSeasonTotals and TeamOverall).

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Per mode. Defaults to "Totals".
            measure_type (str, optional): Measure type. Defaults to "Base".
            // ... (include other relevant optional args like league_id, opponent_team_id etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'players_season_totals' and 'team_overall'.
            DataFrames dict keys: 'players_season_totals', 'team_overall'.
        """
        logger.info(f"TeamToolkit: get_team_player_dashboard_stats for {team_identifier}, season {season}")
        return fetch_team_player_dashboard_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, measure_type=measure_type,
            # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_team_player_on_off_details(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeDetailed.totals,
        measure_type: str = MeasureTypeDetailedDefense.base,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches detailed on/off court statistics for players of a specific team.
        Retrieves 'OverallTeamPlayerOnOffDetails', 'PlayersOffCourtTeamPlayerOnOffDetails', and 'PlayersOnCourtTeamPlayerOnOffDetails'.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "Totals".
            measure_type (str, optional): Measure type. Defaults to "Base". (API uses MeasureTypeDetailedDefense).
            // ... (include other relevant optional args like league_id, opponent_team_id, date_from, date_to, etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON structure includes keys for each dataset (overall, off_court, on_court).
            DataFrames dict keys: 'overall', 'off_court', 'on_court'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_player_on_off_details called for {team_identifier}, season {season}")
        return fetch_team_player_on_off_details_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, measure_type=measure_type,
            # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_team_player_on_off_summary(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type_all_star: str = SeasonTypeAllStar.regular,
        per_mode_detailed: str = PerModeDetailed.totals,
        measure_type_detailed_defense: str = MeasureTypeDetailedDefense.base,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team player on/off summary statistics.
        Includes 'OverallTeamPlayerOnOffSummary', 'PlayersOffCourtTeamPlayerOnOffSummary', 'PlayersOnCourtTeamPlayerOnOffSummary'.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type_all_star (str, optional): Season type. Defaults to "Regular Season".
            per_mode_detailed (str, optional): Per mode. Defaults to "Totals".
            measure_type_detailed_defense (str, optional): Measure type. Defaults to "Base".
            // ... (include other relevant optional args from the logic function)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            DataFrames dict keys: 'overall', 'off_court', 'on_court'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_player_on_off_summary for {team_identifier}, season {season}")
        return fetch_teamplayeronoffsummary_logic(
            team_identifier=team_identifier, season=season, season_type_all_star=season_type_all_star,
            per_mode_detailed=per_mode_detailed, measure_type_detailed_defense=measure_type_detailed_defense,
            # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_team_rebounding_tracking_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team-level rebounding tracking statistics (overall, by shot type, contest, shot distance, rebound distance).

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
            // ... (include other relevant optional args from the logic function)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'overall', 'by_shot_type', 'by_contest', etc.
            DataFrames dict keys match these categories.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_rebounding_tracking_stats for {team_identifier}, season {season}")
        return fetch_team_rebounding_stats_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_team_shooting_tracking_stats(
        self,
        team_identifier: str,
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeSimple.per_game,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches team-level shooting tracking statistics (general, by shot clock, dribble, defender distance, touch time).

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "PerGame".
            // ... (include other relevant optional args from the logic function)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'overall_shooting', 'general_shooting_splits', 'by_shot_clock', etc.
            DataFrames dict keys match these categories.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_shooting_tracking_stats for {team_identifier}, season {season}")
        return fetch_team_shooting_stats_logic(
            team_identifier=team_identifier, season=season, season_type=season_type,
            per_mode=per_mode, # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_team_vs_player_stats(
        self,
        team_identifier: str,
        vs_player_identifier: str, # Name or ID of opposing player
        season: str = settings.CURRENT_NBA_SEASON,
        season_type: str = SeasonTypeAllStar.regular,
        per_mode: str = PerModeDetailed.totals,
        measure_type: str = MeasureTypeDetailedDefense.base,
        # ... other params from original logic function
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches detailed statistics for a team when a specific opposing player is on/off the court.
        Includes overall team performance, player's performance against the team, and shot breakdowns.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team.
            vs_player_identifier (str): Name or ID of the opposing player to analyze against.
            season (str, optional): Season YYYY-YY. Defaults to current.
            season_type (str, optional): Season type. Defaults to "Regular Season".
            per_mode (str, optional): Statistical mode. Defaults to "Totals".
            measure_type (str, optional): Measure type. Defaults to "Base".
            // ... (include other relevant optional args like league_id, date_from, location etc.)
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'overall', 'on_off_court', 'shot_area_overall', 'vs_player_overall', etc.
            DataFrames dict keys match these categories.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_team_vs_player_stats for team {team_identifier} vs player {vs_player_identifier}, season {season}")
        return fetch_teamvsplayer_logic(
            team_identifier=team_identifier, vs_player_identifier=vs_player_identifier,
            season=season, season_type=season_type, per_mode=per_mode, measure_type=measure_type,
            # Pass other relevant parameters
            return_dataframe=return_dataframe
        )

    def get_league_franchise_history(
        self,
        league_id: str = "00",
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches franchise history data for ALL teams in a league, including current and defunct teams.

        Args:
            league_id (str, optional): League ID. Defaults to "00" (NBA). Possible: "00", "10" (WNBA).
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrames).
            JSON includes 'data_sets' with keys 'FranchiseHistory' and 'DefunctTeams'.
            DataFrames dict keys: 'FranchiseHistory', 'DefunctTeams'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_league_franchise_history for league {league_id}")
        return fetch_franchise_history_logic(
            league_id=league_id, return_dataframe=return_dataframe
        )

    def get_franchise_leaders(
        self,
        team_identifier: str, # Team ID is required for this endpoint.
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches all-time statistical leaders for a specific franchise.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team. The underlying API requires team_id.
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrame).
            JSON includes 'data_sets' with key 'FranchiseLeaders'.
            DataFrame dict key: 'FranchiseLeaders'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_franchise_leaders for team {team_identifier}")
        try:
            team_id_val, _ = find_team_id_or_error(team_identifier)
        except Exception as e:
            logger.error(f"Error finding team ID for '{team_identifier}': {e}", exc_info=True)
            error_response = format_response(error=str(e))
            if return_dataframe:
                return error_response, {}
            return error_response

        return fetch_franchise_leaders_logic(
            team_id=str(team_id_val), return_dataframe=return_dataframe
        )

    def get_franchise_players(
        self,
        team_identifier: str, # Team ID is required for this endpoint.
        league_id: str = "00",
        per_mode_detailed: str = PerModeDetailed.totals,
        season_type_all_star: str = SeasonTypeAllStar.regular,
        return_dataframe: bool = False
    ) -> Union[str, Tuple[str, Dict[str, pd.DataFrame]]]:
        """
        Fetches all players who have ever played for a specific franchise, with their stats for that franchise.

        Args:
            team_identifier (str): Name, abbreviation, or ID of the team. The underlying API requires team_id.
            league_id (str, optional): League ID. Defaults to "00" (NBA).
            per_mode_detailed (str, optional): Statistical mode. Defaults to "Totals".
            season_type_all_star (str, optional): Season type. Defaults to "Regular Season".
            return_dataframe (bool, optional): If True, returns DataFrames. Defaults to False.

        Returns:
            Union[str, Tuple[str, Dict[str, pd.DataFrame]]]: JSON string or (JSON, DataFrame).
            JSON includes 'data_sets' with key 'FranchisePlayers'.
            DataFrame dict key: 'FranchisePlayers'.
            CSV cache path included in json_string under 'dataframe_info'.
        """
        logger.info(f"TeamToolkit: get_franchise_players for team {team_identifier}")
        try:
            team_id_val, _ = find_team_id_or_error(team_identifier)
        except Exception as e:
            logger.error(f"Error finding team ID for '{team_identifier}': {e}", exc_info=True)
            error_response = format_response(error=str(e))
            if return_dataframe:
                return error_response, {}
            return error_response

        return fetch_franchise_players_logic(
            team_id=str(team_id_val), league_id=league_id, per_mode_detailed=per_mode_detailed,
            season_type_all_star=season_type_all_star, return_dataframe=return_dataframe
        )

===== backend\tool_kits\__init__.py =====
# This file makes the 'tool_kits' directory a Python package.



=== FRONTEND FILES ===

