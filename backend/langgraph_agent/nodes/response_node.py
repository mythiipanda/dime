"""
Response node implementation.
Following Single Responsibility Principle (SRP).
"""

import logging
from typing import Dict, Any
from langchain_core.messages import AIMessage

from langgraph_agent.state import AgentState
from langgraph_agent.nodes.base_node import BaseAgentNode

logger = logging.getLogger(__name__)


class ResponseNode(BaseAgentNode):
    """Node that formulates the final response."""
    
    def _execute_internal(self, state: AgentState) -> Dict[str, Any]:
        """Execute the response node logic."""
        self._write_status("finalizing", "Preparing final response...")
        
        current_messages = state.get('messages', [])
        final_answer_content = "Response Node: Default final answer."
        streaming_log = []
        
        last_ai_message = self._get_last_ai_message(current_messages)
        
        if self._is_valid_final_response(last_ai_message):
            final_answer_content = last_ai_message.content
            self._write_status("complete", "Final answer extracted from LLM response")
        elif state.get('error_message'):
            final_answer_content = f"Error encountered: {state['error_message']}"
            streaming_log.append(final_answer_content)
            self._write_status("error", "Error state detected")
        else:
            final_answer_content = self._handle_fallback_response(last_ai_message, streaming_log)
        
        final_log_message = f"Final Answer: {final_answer_content}"
        streaming_log.append(final_log_message)
        
        self._write_status("ready", "Final response ready for delivery")
        
        return {
            "final_answer": final_answer_content,
            "streaming_output": streaming_log,
            "current_step": self._increment_step(state)
        }
    
    def _get_last_ai_message(self, messages) -> AIMessage:
        """Get the last AI message from the message list."""
        if messages:
            for message in reversed(messages):
                if isinstance(message, AIMessage):
                    return message
        return None
    
    def _is_valid_final_response(self, message: AIMessage) -> bool:
        """Check if the message is a valid final response."""
        return (message and
                not getattr(message, 'tool_calls', None) and
                message.content and
                len(message.content.strip()) > 0)
    
    def _handle_fallback_response(self, last_ai_message: AIMessage, streaming_log: list) -> str:
        """Handle fallback response when no valid final response is found."""
        warning_msg = (
            f"Response Node: Could not determine a final answer from the last LLM message. "
            f"Last AI message: {last_ai_message}"
        )
        
        if last_ai_message and getattr(last_ai_message, 'tool_calls', None):
            warning_msg += (
                f" (It contained tool calls: {getattr(last_ai_message, 'tool_calls', [])}) - "
                f"this might indicate a routing issue if response_node was reached directly."
            )
        
        logger.warning(warning_msg)
        streaming_log.append(warning_msg)
        
        self._write_status("fallback", "Using fallback response generation")
        
        return (last_ai_message.content 
                if last_ai_message and last_ai_message.content 
                else "No conclusive text response generated by LLM.")